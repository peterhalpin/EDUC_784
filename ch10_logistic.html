<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>EDUC 784 - 10&nbsp; Logistic regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ch9_polynomial.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch10_logistic.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Logistic regression</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">EDUC 784</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch1_review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2_simple_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Simple regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3_two_predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Two predictors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4_categorical_predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Categorical predictors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5_interactions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Interactions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch6_model_building.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Model building</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch7_assumption_checking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Assumption checking</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch8_loglinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Log-linear regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch9_polynomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Polynomial regression, etc</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch10_logistic.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Logistic regression</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-chd-example-10" id="toc-sec-chd-example-10" class="nav-link active" data-scroll-target="#sec-chd-example-10"><span class="header-section-number">10.1</span> The CHD example</a></li>
  <li><a href="#sec-logit-10" id="toc-sec-logit-10" class="nav-link" data-scroll-target="#sec-logit-10"><span class="header-section-number">10.2</span> Logit &amp; logistic functions</a>
  <ul class="collapse">
  <li><a href="#from-binary-to-probability" id="toc-from-binary-to-probability" class="nav-link" data-scroll-target="#from-binary-to-probability"><span class="header-section-number">10.2.1</span> From binary to probability</a></li>
  <li><a href="#from-probability-to-logistic" id="toc-from-probability-to-logistic" class="nav-link" data-scroll-target="#from-probability-to-logistic"><span class="header-section-number">10.2.2</span> From probability to logistic</a></li>
  <li><a href="#logistic-to-log-odds-logit" id="toc-logistic-to-log-odds-logit" class="nav-link" data-scroll-target="#logistic-to-log-odds-logit"><span class="header-section-number">10.2.3</span> Logistic to log odds (logit)</a></li>
  <li><a href="#pop-quiz" id="toc-pop-quiz" class="nav-link" data-scroll-target="#pop-quiz"><span class="header-section-number">10.2.4</span> Pop quiz</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps"><span class="header-section-number">10.2.5</span> Next steps</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">10.2.6</span> Summary</a></li>
  </ul></li>
  <li><a href="#sec-simple-10" id="toc-sec-simple-10" class="nav-link" data-scroll-target="#sec-simple-10"><span class="header-section-number">10.3</span> Simple logistic regression</a>
  <ul class="collapse">
  <li><a href="#odds-ratio" id="toc-odds-ratio" class="nav-link" data-scroll-target="#odds-ratio"><span class="header-section-number">10.3.1</span> Odds ratio</a></li>
  <li><a href="#other-interpretations-predicted-probabilities" id="toc-other-interpretations-predicted-probabilities" class="nav-link" data-scroll-target="#other-interpretations-predicted-probabilities"><span class="header-section-number">10.3.2</span> Other interpretations: Predicted probabilities</a></li>
  <li><a href="#other-interpretations-equal-odds" id="toc-other-interpretations-equal-odds" class="nav-link" data-scroll-target="#other-interpretations-equal-odds"><span class="header-section-number">10.3.3</span> Other interpretations: Equal odds</a></li>
  <li><a href="#other-interpretations-rate-of-change" id="toc-other-interpretations-rate-of-change" class="nav-link" data-scroll-target="#other-interpretations-rate-of-change"><span class="header-section-number">10.3.4</span> Other interpretations: Rate of change</a></li>
  <li><a href="#relation-to-linear-probability-model" id="toc-relation-to-linear-probability-model" class="nav-link" data-scroll-target="#relation-to-linear-probability-model"><span class="header-section-number">10.3.5</span> Relation to linear probability model</a></li>
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1"><span class="header-section-number">10.3.6</span> Summary</a></li>
  </ul></li>
  <li><a href="#sec-estimation-10" id="toc-sec-estimation-10" class="nav-link" data-scroll-target="#sec-estimation-10"><span class="header-section-number">10.4</span> Estimation</a>
  <ul class="collapse">
  <li><a href="#ml-vs-ols" id="toc-ml-vs-ols" class="nav-link" data-scroll-target="#ml-vs-ols"><span class="header-section-number">10.4.1</span> ML vs OLS</a></li>
  <li><a href="#the-likelihood" id="toc-the-likelihood" class="nav-link" data-scroll-target="#the-likelihood"><span class="header-section-number">10.4.2</span> The likelihood</a></li>
  <li><a href="#maximizing-the-likelihood" id="toc-maximizing-the-likelihood" class="nav-link" data-scroll-target="#maximizing-the-likelihood"><span class="header-section-number">10.4.3</span> Maximizing the likelihood</a></li>
  <li><a href="#summary-2" id="toc-summary-2" class="nav-link" data-scroll-target="#summary-2"><span class="header-section-number">10.4.4</span> Summary</a></li>
  </ul></li>
  <li><a href="#sec-inference-10" id="toc-sec-inference-10" class="nav-link" data-scroll-target="#sec-inference-10"><span class="header-section-number">10.5</span> Inference</a>
  <ul class="collapse">
  <li><a href="#wald-test" id="toc-wald-test" class="nav-link" data-scroll-target="#wald-test"><span class="header-section-number">10.5.1</span> Wald test</a></li>
  <li><a href="#odds-ratio-or" id="toc-odds-ratio-or" class="nav-link" data-scroll-target="#odds-ratio-or"><span class="header-section-number">10.5.2</span> Odds ratio (OR)</a></li>
  <li><a href="#likelihood-ratio-test" id="toc-likelihood-ratio-test" class="nav-link" data-scroll-target="#likelihood-ratio-test"><span class="header-section-number">10.5.3</span> Likelihood ratio test</a></li>
  </ul></li>
  <li><a href="#sec-pseudo-rsquared-10" id="toc-sec-pseudo-rsquared-10" class="nav-link" data-scroll-target="#sec-pseudo-rsquared-10"><span class="header-section-number">10.6</span> (Pseudo) R-squared</a>
  <ul class="collapse">
  <li><a href="#mcfaddens-pseudo-r-squared" id="toc-mcfaddens-pseudo-r-squared" class="nav-link" data-scroll-target="#mcfaddens-pseudo-r-squared"><span class="header-section-number">10.6.1</span> McFadden’s Pseudo R-squared</a></li>
  </ul></li>
  <li><a href="#sec-assumption-checking-10" id="toc-sec-assumption-checking-10" class="nav-link" data-scroll-target="#sec-assumption-checking-10"><span class="header-section-number">10.7</span> Assumption checking</a>
  <ul class="collapse">
  <li><a href="#more-details-on-the-hl-test" id="toc-more-details-on-the-hl-test" class="nav-link" data-scroll-target="#more-details-on-the-hl-test"><span class="header-section-number">10.7.1</span> More details on the HL test*</a></li>
  </ul></li>
  <li><a href="#workbook" id="toc-workbook" class="nav-link" data-scroll-target="#workbook"><span class="header-section-number">10.8</span> Workbook</a></li>
  <li><a href="#sec-exercise-10" id="toc-sec-exercise-10" class="nav-link" data-scroll-target="#sec-exercise-10"><span class="header-section-number">10.9</span> Exercises: Multiple logistic regression</a>
  <ul class="collapse">
  <li><a href="#chd-and-smoking" id="toc-chd-and-smoking" class="nav-link" data-scroll-target="#chd-and-smoking"><span class="header-section-number">10.9.1</span> CHD and smoking</a></li>
  <li><a href="#predicted-probabilities" id="toc-predicted-probabilities" class="nav-link" data-scroll-target="#predicted-probabilities"><span class="header-section-number">10.9.2</span> Predicted probabilities</a></li>
  <li><a href="#interactions" id="toc-interactions" class="nav-link" data-scroll-target="#interactions"><span class="header-section-number">10.9.3</span> Interactions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-chap-10" class="quarto-section-identifier"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Logistic regression</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!--Review ALL of the workbook stuff before first lesson so you remember the numerical values for the examples. And you need to review ALL of the exercise stuff for the second lesson, so you can get through the example in order. -->
<p>The topic we address in this chapter is logistic regression. Like the previous two chapters, logistic regression is an extension of multiple linear regression to situations in which the “standard” model does not directly apply. This time, the extension is to binary <em>outcome</em> variables.</p>
<p>There are many situations in which the outcome of interest can be thought of as a binary or “yes / no” or “true / false” outcome:</p>
<ul>
<li>Death (the original logistic outcome; <a href="https://doi.org/10.2307%2F2280041">Berkson, 1944</a>)</li>
<li>Onset of disease or condition</li>
<li>Employment status</li>
<li>College enrollment</li>
<li>Passing a course or completing a credential</li>
<li>Provide a correct response to a test question</li>
<li>…</li>
</ul>
<p>Just like with linear regression, we often we want to relate these types of binary variables to predictors, such as medical history, family background, personal characteristics, etc. That is what logistic regression does.</p>
<p>The main theme of this chapter is also an extension of the previous two chapters. We have seen a general strategy for how to deal with data that do not “fit” the assumptions of linear regression:</p>
<ol type="1">
<li>Transform one or more variables so that the data <em>do</em> fit the assumptions linear regression.</li>
<li>Run the analysis as usual.</li>
<li>Then work out the interpretation of results in terms of the original variable(s).</li>
</ol>
<p>We will see this basic approach again in this chapter. In fact, its kind of a general-purpose hack for quantitative research – when you are faced with a problem you don’t know how to deal with, turn it into something you do know how to deal with. Certainly this is not the most creative approach, but it has the advantage of letting us “port over” many of the tools we have developed in one context (regression with a continuous outcome) into a new context (regression with a binary outcome).</p>
<p>In terms of statistical modeling, the move to binary outcome variables is a pretty big deal. Everything we have done up until now has focused on OLS regression, using the same basic principles we discussed in Chapters <a href="ch2_simple_regression.html"><span>Chapter&nbsp;2</span></a> and <a href="ch3_two_predictors.html"><span>Chapter&nbsp;3</span></a>. However, logistic regression takes us into the wider framework of <em>generalized linear models</em> (GLMs), which are estimated using maximum likelihood (ML) rather than OLS. Thus we will need to start at the “ground floor” to build up our knowledge of logistic regression, which then provides a stepping stone to GLMs, which can additionally handle other types of outcome variables (e.g., count data, ordered categorical data).</p>
<p>Since we are starting with a new modeling approach, let’s kick things off with a new example.</p>
<section id="sec-chd-example-10" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="sec-chd-example-10"><span class="header-section-number">10.1</span> The CHD example</h2>
<p>As a working example, we will use data contained in the file <code>CHD.RData</code> to explore the relationship between age in years (“age”) and evidence (absence or presence) of coronary heart disease (“chd”). The data set contains 100 cases. Respondents’ ages range from 20 to 69, while clinical evidence of CHD is coded 0 when it is absent and 1 when it is present. A sample of 20 cases is shown below. (Source: Applied Logistic Regression by David W. Hosmer and Stanley Lemeshow, 1989, John Wiley and Sons.)</p>
<p>For the sake of brevity, I will say that a person either “has CHD” or not. This is less accurate (but much simpler) than saying that the variable denotes clinical evidence about the presence or absence of CHD.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"CHD.RData"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(chd.data)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">list</span>(chd.data[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="dv">10</span>),<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>],   </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                  chd.data[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="dv">10</span>),<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]), </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">row.names =</span> F, </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"The CHD example"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">

<table class="kable_wrapper">
<caption>
The CHD example
</caption>
<tbody>
<tr>
<td>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">chd</th>
<th style="text-align: right;">age</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0</td>
<td style="text-align: right;">34</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">63</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">64</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">34</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0</td>
<td style="text-align: right;">42</td>
</tr>
<tr class="even">
<td style="text-align: right;">0</td>
<td style="text-align: right;">36</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">56</td>
</tr>
<tr class="even">
<td style="text-align: right;">0</td>
<td style="text-align: right;">38</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">57</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">44</td>
</tr>
</tbody>
</table>
</td>
<td>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">chd</th>
<th style="text-align: right;">age</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">54</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">65</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">52</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">62</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">50</td>
</tr>
<tr class="even">
<td style="text-align: right;">0</td>
<td style="text-align: right;">37</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0</td>
<td style="text-align: right;">41</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">59</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">62</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">61</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>

</table>
</div>
</div>
<p>If we regress CHD on age using linear regression, this is referred to as the “linear probability model.” The diagnostic plots and summary output are below:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = chd ~ age, data = chd.data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.85793 -0.33992 -0.07274  0.31656  0.99269 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.537960   0.168809  -3.187  0.00193 ** 
age          0.021811   0.003679   5.929 4.57e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.429 on 98 degrees of freedom
Multiple R-squared:  0.264, Adjusted R-squared:  0.2565 
F-statistic: 35.15 on 1 and 98 DF,  p-value: 4.575e-08</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-lpm" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch10_logistic_files/figure-html/fig-lpm-1.png" class="img-fluid figure-img" width="1152"></p>
<figcaption class="figure-caption">Figure&nbsp;10.1: Linear probability model with CHD example</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Before moving on, please take a moment to write down you conclusions (and rationale) about whether the assumptions of linear regression are met for these data.</strong></p>
<p>I’ll note that researchers who have a strong preference for OLS methods (AKA economists) often approach binary outcomes using the linear probability model. As we can see, this approach violates all of the assumptions of linear regression, can lead to predicted probabilities outside of the range [0,1], produces incorrect standard errors for model parameters (need to use HC standard errors), and is, in a word, wrong. Yet, despite all this, it works pretty well in some situations and has the benefit of being easier to interpret than logistic regression. We will consider the situations in which the linear probability model is “close enough” at the end of the next section.</p>
</section>
<section id="sec-logit-10" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="sec-logit-10"><span class="header-section-number">10.2</span> Logit &amp; logistic functions</h2>
<p>The general game plan for dealing with a binary outcome is to transform it into a different variable that is easier to work with, run the analysis, and then “reverse-transform” the model coefficients so that they are interpretable in terms of the original binary variable. This strategy should sound familiar from <a href="ch8_loglinear.html"><span>Chapter&nbsp;8</span></a> – it’s the same overall approach we used for log-linear regression. Also in common with <a href="ch8_loglinear.html"><span>Chapter&nbsp;8</span></a>, we are going to use logs and exponents as the main workhorse for this approach (that is where the “log” in logistic comes from).</p>
<p>However, the overall strategy for transforming the <span class="math inline">\(Y\)</span> variable in logistic regression is a bit more complicated than the log-linear model. So, it is helpful to start wit an overall “roadmap”.</p>
<ul>
<li><p>Step 1 (from binary to probability). First, we are going to work with probabilities rather than the original binary variable. In terms of our example, we are going to shift focus from whether or not a person has CHD to the <em>probability</em> of a person having CHD.</p></li>
<li><p>Step 2 (from probability to logistic). The logistic function is widely-used model for probabilities. In terms of our example, we are going to use the logistic function to relate the probability of a person having CHD to their age.</p></li>
<li><p>Step 3 (from logistic to logit). The logistic function has a nice interpretation, but it is not a linear function of age. So, we are going to transform it into something that is linear in age, which will let us “port over” a lot of what we have learned about linear models. Actually, the reason we choose the logistic function as a model of probability is because this transform is relatively straightforward and can be “undone” afterwards when interpreting the model coefficients, just like with log-linear regression. The transformation two steps:</p>
<ul>
<li><p>Step3A (probability to odds). First we transform the probability of having CHD into the <em>odds</em> of having CHD. If <span class="math inline">\(p\)</span> denotes probability then odds are just <span class="math inline">\(p / (1-p)\)</span>. We will spend a while talking about how to interpret odds.</p></li>
<li><p>Step 3B (odds to logit). Then we take the log of the odds, which is called the <em>logit</em>. The logit turns out to be a linear function of age, so we can model the relationship between age and the logit of CHD in a way that is very similar to regular linear regression.</p></li>
</ul></li>
</ul>
<p>So, that’s the overall approach to dealing with a binary variable in logistic regression. Clear as mud, right? Don’t worry, we will walk through each step in the following subsections. If you find yourself getting lost in the details, it can be helpful to refer back to this overall strategy. In short, the overall game plan is:</p>
<p><span class="math display">\[ \text{binary outcome} \rightarrow \text{probability} \rightarrow \text{logistic} \rightarrow \text{logit (log odds) }\]</span></p>
<p>Once we have all these concepts in play, we can start doing logistic regression.</p>
<section id="from-binary-to-probability" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="from-binary-to-probability"><span class="header-section-number">10.2.1</span> From binary to probability</h3>
<p>The following table presents the example data in terms of the proportion of cases with CHD, broken down by age groups. The first column shows the age groups, the second shows the number of cases without CHD, the third shows the number of cases with CHD, and the last column shows the proportion of cases with CHD.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/props.png"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-props" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="files/images/props.png" class="img-fluid figure-img" width="610"></p>
<figcaption class="figure-caption">Figure&nbsp;10.2: From a binary variable to proportions</figcaption>
</figure>
</div>
</div>
</div>
<p>Recall that a proportion is computed as the number of cases of interest over the total number of cases. In terms of the table above:</p>
<p><span id="eq-prob"><span class="math display">\[ p(CHD = 1) = \frac{ N_1}{N_0 + N_1 }  \tag{10.1}\]</span></span></p>
<p>were <span class="math inline">\(N_1\)</span> denotes the number of cases with CDH, and <span class="math inline">\(N_0\)</span> is the number of cases without.</p>
<p>The number <span class="math inline">\(p(CHD = 1)\)</span> can be interpreted in many ways, which leads to a lot of terminology here.</p>
<ul>
<li>The <strong>proportion</strong> of cases in our sample with CHD.</li>
<li>If we multiply by 100, it is the <strong>percentage</strong> of cases with CHD (i.e., cases per 100) in our sample.</li>
<li>If we multiply by a number other than 100 (say 1000), it is the <strong>rate</strong> of CHD (e.g., cases per 1000) in our sample.</li>
<li>Since a proportion is just the mean of binary variable, it is the <strong>mean</strong> or expected value of CHD in our sample.</li>
<li>And finally, since proportions are one interpretation of probability, it is the <strong>probability</strong> of CHD in our sample.</li>
</ul>
<p>You might hear all of these terms (i.e., proportion, percentage, rate, mean, probability) used in connection with logistic regression. But, they are all just different ways of interpreting the rightmost column of <a href="#fig-props">Figure&nbsp;<span>10.2</span></a>. I will try to make a point of using all of these terms so you get used to interpreting them in this context :)</p>
<p>Another concept that will be useful for interpreting our data is <em>odds</em>. Odds are closely related to, but not the same as, probability. The figure below adds the odds of having CHD to Figure <a href="#fig-props">Figure&nbsp;<span>10.2</span></a>.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/odds.png"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-odds" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="files/images/odds.png" class="img-fluid figure-img" width="858"></p>
<figcaption class="figure-caption">Figure&nbsp;10.3: Proportions and odds</figcaption>
</figure>
</div>
</div>
</div>
<p>As shown in the table, the odds are also a function of the two sample sizes, <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_0\)</span>:</p>
<p><span id="eq-odds"><span class="math display">\[\text{odds}(CHD = 1) = \frac{N_1}{N_0}. \tag{10.2}\]</span></span></p>
<p>Let’s take a moment to compare the interpretation of probability versus odds.</p>
<ul>
<li><p>The first row of the table tells us that the probability of having CHD in your 20’s is “1 in 10”. Loosely, this means that for every 10 people in their 20s, one of them will have CHD.</p></li>
<li><p>By contrast, the odds of having CHD in your twenties is “1 to 9”. Roughly, this means that for every person in their twenties with CHD, there are nine without CHD.</p></li>
</ul>
<p>Clearly, probabilities and odds are just two different ways of packaging the same information. The following equations shows the relation between odds and probability (these are derived from Equations <a href="#eq-prob">Equation&nbsp;<span>10.1</span></a> and <a href="#eq-odds">Equation&nbsp;<span>10.2</span></a> using algebra)</p>
<p><span id="eq-p2o"><span class="math display">\[
\begin{align}
p(CHD = 1) &amp; = \frac{\text{odds}(CHD = 1)}{1 + \text{odds}(CHD = 1)} \\ \\
\text{odds}(CHD = 1) &amp; = \frac{p(CHD = 1)}{1 - p(CHD = 1)}
\end{align}
\tag{10.3}\]</span></span></p>
<p>We will see these relations again shortly. But, before moving on, let’s get some more practice interpreting odds and probabilities using the data in <a href="#fig-odds">Figure&nbsp;<span>10.3</span></a>. <strong>Please write down your answers to the following questions and be prepared to share them in class. For each question provide a verbal interpretation of the numerical answer (e.g, odds of 2 to 1 means that for every two people with a trait, there is one without.) </strong></p>
<ol type="1">
<li>What is the probability of a person in their 40s having CHD?</li>
<li>What are the odds of a person in their 40s having CHD?</li>
<li>What is the probability of someone in their 50s <strong>not</strong> having CHD?</li>
<li>What are the odds of someone in their 50s <strong>not</strong> having CHD?</li>
<li>What is probability of having CHD in your 40s, compared to your 30s? (e.g., is 3 times higher? 4 times higher?)</li>
<li>What are the odds of having CHD in your 40s, compared to your 30s?</li>
</ol>
<p>The answers hidden below (use the Code button to reveal), but you won’t learn anything if you don’t try the question yourself first!</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. .39, so about 40% of people</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 11/17, so for 11 people with CHD there are 17 without</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 1 - .72 = .28, so about 28% of people </span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. (18/7)^-1 = 7/18, so 7 out ever 18 people</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. .39 / .19 ~= 2, so the probability of having CHD in your 40s is about 2 times higher than the probability of having CHD in your 30s. This is called a relative risk, or a risk ratio. </span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">#6. (11/17)/(5/22) ~= 2.8, so the odds of having CHD in your 40s is about 2.8 times higher than the odds of having CHD in your 30s. This is called an odds ratio. </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="from-probability-to-logistic" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="from-probability-to-logistic"><span class="header-section-number">10.2.2</span> From probability to logistic</h3>
<p>On thing you may have noted about the CHD data is that the proportion of cases with CHD increases with age. This relationship is shown visually in <a href="#fig-chd2">Figure&nbsp;<span>10.4</span></a>.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample proportions</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>prop <span class="ot">&lt;-</span> <span class="fu">tapply</span>(chd, catage, mean)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Age categories</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>years <span class="ot">&lt;-</span> <span class="fu">unique</span>(catage)<span class="sc">*</span><span class="dv">10</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(years, </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>     prop, </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"p(CHD =1)"</span>, </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Age categories"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-chd2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch10_logistic_files/figure-html/fig-chd2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;10.4: Proportion of cases with CHD as a function of age</figcaption>
</figure>
</div>
</div>
</div>
<p>Looking at the plot, we might suspect that the relationship between the probability of CHD and age is non-linear. In particular, we know that probabilities cannot take on values outside of the range <span class="math inline">\((0, 1)\)</span>, so the relationship is going to have to “flatten out” in the tails. For example, even if you are a baby, your probability of having CHD cannot be less than 0. And, even if you are centenarian, the probability can’t be great than 1.</p>
<p>Based on this reasoning, we know that the relationship between age and the rate of CHD should take on a sort of “S-shaped” curve or “sigmoid”. This S-shape is hinted at in <a href="#fig-chd2">Figure&nbsp;<span>10.4</span></a> but is not very clear. Some clearer examples are shown in <a href="#fig-sigmoids">Figure&nbsp;<span>10.5</span></a>.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Logistic function</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>logistic <span class="ot">&lt;-</span> <span class="cf">function</span>(x, a, b){<span class="fu">exp</span>(a<span class="sc">*</span>x <span class="sc">+</span> b) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(a<span class="sc">*</span>x <span class="sc">+</span> b))}</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">by =</span> .<span class="dv">1</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">logistic</span>(x, <span class="dv">1</span>, <span class="dv">0</span>), </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="dv">2</span>, </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"logistic"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="fu">logistic</span>(x, .<span class="dv">75</span>, <span class="sc">-</span><span class="fl">1.5</span>), </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="dv">3</span>, </span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">"logistic"</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="fu">logistic</span>(x, <span class="fl">1.5</span>,<span class="sc">-</span> <span class="dv">1</span>), </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="dv">4</span>, </span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">"logistic"</span>)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="fu">logistic</span>(x, <span class="dv">3</span>, <span class="dv">2</span>), </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>       <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="dv">5</span>, </span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">"logistic"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-sigmoids" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch10_logistic_files/figure-html/fig-sigmoids-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;10.5: Examples of sigmoids</figcaption>
</figure>
</div>
</div>
</div>
<p>The mathematical equation used to create these S-shaped curves is called the logistic function, the namesake of logistic regression. All you need to take-away from <a href="#fig-sigmoids">Figure&nbsp;<span>10.5</span></a> is that there is mathematical function that produces the kind of relations we are expecting between age (continuous) the the probability of having CHD (bounded to the interval <span class="math inline">\((0, 1)\)</span>).</p>
<p>Returning to our example, we can see in <a href="#fig-chd3">Figure&nbsp;<span>10.6</span></a> that the logistic function provides a reasonable approximation for the relationship between the rate of CHD and age.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(years, prop, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">ylab =</span> <span class="st">"p(CHD =1)"</span>, <span class="at">xlab =</span> <span class="st">"Age categories"</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">20</span><span class="sc">:</span><span class="dv">60</span>, <span class="fu">logistic</span>(<span class="dv">20</span><span class="sc">:</span><span class="dv">60</span>, .<span class="dv">12</span>, <span class="sc">-</span><span class="fl">5.2</span>), <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">ylab =</span> <span class="st">"logistic"</span>, <span class="at">xlab =</span> <span class="st">"Age in years"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-chd3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch10_logistic_files/figure-html/fig-chd3-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;10.6: Proportion of cases with CHD, data versus logistic</figcaption>
</figure>
</div>
</div>
</div>
<p>One important thing to notice about <a href="#fig-chd3">Figure&nbsp;<span>10.6</span></a> is that the plot on the left required re-coding age into a categorical variable and computing the proportion of cases with CHD in each age category (see <a href="#fig-props">Figure&nbsp;<span>10.2</span></a>). However, the logistic plot on the right did not require categorizing age. So, one advantage of using the logistic function is that we can model the probability of CHD as a function of age “directly”, without having to categorize our predictor variables.</p>
<p>The take home message of this section is that the logistic function is a nice way to model how a proportion depends on a continuous variable like age. Next, we’ll talk about the math of the logistic function in a bit more detail.</p>
</section>
<section id="logistic-to-log-odds-logit" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="logistic-to-log-odds-logit"><span class="header-section-number">10.2.3</span> Logistic to log odds (logit)</h3>
<p>The formula for the logistic function (i.e., the function that produced the curves in <a href="#fig-sigmoids">Figure&nbsp;<span>10.5</span></a> is</p>
<p><span id="eq-logistic"><span class="math display">\[p = \frac{\exp(x)}{1 + \exp(x)}. \tag{10.4}\]</span></span></p>
<p>This function maps the variable <span class="math inline">\(x\)</span> onto the interval <span class="math inline">\((0, 1)\)</span>. In <a href="#fig-chd3">Figure&nbsp;<span>10.6</span></a> we saw that the logistic function can provide a nice model for probabilities. We also saw that the logistic function is non-linear function of <span class="math inline">\(x\)</span> (i.e., it is sigmoidal or S-shaped).</p>
<p>However, a nice thing about the logistic function is that we can transform it into a linear function of <span class="math inline">\(x\)</span>. Since we already know how to deal with linear functions (that is what this whole course has been about!), transforming the logistic into a linear function of <span class="math inline">\(x\)</span> will let us port over a lot of what we know about linear regression to situations in which the outcome variable is binary. (In fact, the real motivation for choosing the logistic function in the first place, rather than some other S-shaped curve.)</p>
<p>So, let’s see how to get from our S-shaped logistic function of <span class="math inline">\(x\)</span> to a linear function of <span class="math inline">\(x\)</span>. Algebra with <a href="#eq-logistic">Equation&nbsp;<span>10.4</span></a> shows that we can re-express the logistic function in terms of the odds:</p>
<p><span id="eq-exp-odds"><span class="math display">\[\frac{p}{1- p} = \exp(x). \tag{10.5}\]</span></span></p>
<p>Note that Equations <a href="#eq-logistic">Equation&nbsp;<span>10.4</span></a> and <a href="#eq-exp-odds">Equation&nbsp;<span>10.5</span></a> directly parallel the two expressions in Equation <a href="#eq-p2o">Equation&nbsp;<span>10.3</span></a>. The only difference is that, in the logistic model, the odds are represented as an exponential function of the variable <span class="math inline">\(x\)</span>, which is what Equation <a href="#eq-exp-odds">Equation&nbsp;<span>10.5</span></a> is telling us.</p>
<p>In order to turn <a href="#eq-exp-odds">Equation&nbsp;<span>10.5</span></a> into a linear function of <span class="math inline">\(x\)</span>, all we need to do is get rid of the exponent. Do you remember how?? That’s right, just take the log (see <a href="ch8_loglinear.html#sec-math-review-8"><span>Section&nbsp;8.1</span></a>):</p>
<p><span id="eq-logit"><span class="math display">\[ \log\left(\frac{p}{1- p}\right) = x. \tag{10.6}\]</span></span></p>
<p>This equation is telling us that the log of the odds is linear in <span class="math inline">\(x\)</span>. The log-odds is also called the <em>logit</em>, which is short for “logistic unit.”</p>
<p>The relationship among the logistic, odds, and logit are summarized in <a href="#fig-logit">Figure&nbsp;<span>10.7</span></a>.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/logit.png"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-logit" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="files/images/logit.png" class="img-fluid figure-img" width="900"></p>
<figcaption class="figure-caption">Figure&nbsp;10.7: Logistic, odds, and logit</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li><p>The left-hand panel shows the logistic function. This is our “intuitive-but-nonlinear” model for probabilities. In terms of our example, this panel is saying that the probability of having CHD is a logistic or S-shaped function of age.</p></li>
<li><p>The middle panel shows that the odds are an exponential function of <span class="math inline">\(x\)</span>. In terms of our example, this means that the odds of having CHD are an exponential function of age. This is the main assumption of the logistic model, and we will revisit this assumption again when we get to <a href="#sec-assumption-checking-10"><span>Section&nbsp;10.7</span></a>.</p></li>
<li><p>Finally, the right-hand panel shows the “not-really-intuitive-but-definitely-linear” model for the logit. In terms of our example, the logit of having CHD is a linear function of age.</p></li>
</ul>
<p>The next section discusses how to interpret the logit by reverse-transforming it back to the odds and probabilities. The situation is a lot like log-linear regression (<a href="ch9_polynomial.html"><span>Chapter&nbsp;9</span></a>).</p>
</section>
<section id="pop-quiz" class="level3" data-number="10.2.4">
<h3 data-number="10.2.4" class="anchored" data-anchor-id="pop-quiz"><span class="header-section-number">10.2.4</span> Pop quiz</h3>
<p>Before moving, lets nail down the relation between probability, odds, and logits. <a href="#fig-logit-table">Figure&nbsp;<span>10.8</span></a>) presents the relationship in tabular form.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-logit-table" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="files/images/logit-table.png" class="img-fluid figure-img" width="455"></p>
<figcaption class="figure-caption">Figure&nbsp;10.8: Logistic, odds, and logit</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>I will asks some questions along the following lines in class.</strong></p>
<ul>
<li>If probability of an event is equal to .1, what are the odds the event? What is the logit of the event?</li>
<li>If odds of an event are 4 to 1, what is the probability of the event? What is the logit of the event?</li>
<li>If logit &lt; 0 then probability &lt; ? and odds &lt; ?</li>
<li>What is more likely: a event with probability of .9 or an event with odds of .9?</li>
<li>If a probability of <span class="math inline">\(p\)</span> corresponds to a <span class="math inline">\(\text{logit}\)</span> of <span class="math inline">\(x\)</span>, what is the logit corresponding to <span class="math inline">\(1-p\)</span>? (Hint, try some numerical examples from the table).</li>
</ul>
</section>
<section id="next-steps" class="level3" data-number="10.2.5">
<h3 data-number="10.2.5" class="anchored" data-anchor-id="next-steps"><span class="header-section-number">10.2.5</span> Next steps</h3>
<p>The logit is our workhorse for logistic regression. In <a href="#sec-simple-10"><span>Section&nbsp;10.3</span></a>, we will replace the variable <span class="math inline">\(x\)</span> with a simple regression model <span class="math inline">\(a + bX\)</span> to get simple logistic regression. In <span class="quarto-unresolved-ref">?sec-multiple-10</span> we will extend simple logistic regression to multiple logistic regression, just like we did for multiple linear regression.</p>
<p>Although the logit is the workhorse, we generally don’t want to work with the logit when it comes time to interpret the results. The situation here is a lot like log-linear regression (<a href="ch9_polynomial.html"><span>Chapter&nbsp;9</span></a>). In log-linear regression, we treated <span class="math inline">\(\log(Y)\)</span> as a linear function of our predictor variable(s). However, we didn’t want to interpret the model in terms of <span class="math inline">\(\log(Y)\)</span>, because, well, who thinks in log units? Instead we wanted an interpretation in terms of the original outcome, <span class="math inline">\(Y\)</span>.</p>
<p>The same situation applies here. You may have already noted that the relationship between the logit (i.e., <span class="math inline">\(\log(\text{odds})\)</span>) and <span class="math inline">\(\text{odds}\)</span> in logistic regression is the same as the relationship between <span class="math inline">\(\log(Y)\)</span> and <span class="math inline">\(Y\)</span> in log-linear regression. The parallel between the two model is as follows:</p>
<ul>
<li><p>In the log-linear model we interpreted a <span class="math inline">\(b\)</span> unit increase in <span class="math inline">\(\log(Y)\)</span> in terms of an <span class="math inline">\((\exp(b) - 1) \times 100\%\)</span> change in <span class="math inline">\(Y\)</span> (see Section <span class="citation" data-cites="ref-interpretation-8">(<a href="#ref-ref-interpretation-8" role="doc-biblioref"><strong>ref-interpretation-8?</strong></a>)</span>)).</p></li>
<li><p>In logistic regression we will interpret a <span class="math inline">\(b\)</span> unit increase in <span class="math inline">\(\text{logit}(Y)\)</span> in terms of an <span class="math inline">\((\exp(b) - 1) \times 100\%\)</span> times change in <span class="math inline">\(\text{odds}(Y)\)</span>.</p></li>
</ul>
<p>So, while we use the logit function for modeling, we often use the odds for interpretation. One subtle difference to be aware of is that, in logistic regression, we usually report results in terms of relative magnitude (called the odds ratio) rather than relative change, although relative change is often used for verbal reporting. We will see examples in the next section.</p>
<p>Some authors have argued that people don’t really know how to interpret odds properly. These authors suggest that we interpret the logistic model in terms of probabilities, rather than odds. We will discuss how to do this as well.</p>
</section>
<section id="summary" class="level3" data-number="10.2.6">
<h3 data-number="10.2.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">10.2.6</span> Summary</h3>
<p>At this point we have covered the overall logic of how we can model a binary outcome variable like CHD in terms of the logistic function. The overall situation is very similar to, but a bit more complicated than, log-linear regression. The main take aways are</p>
<ul>
<li><p>We use the logit (log-odds) for statistical analysis, because it results in a linear function, and we already know how to deal with linear functions.</p></li>
<li><p>We use the odds for interpretation, because the logistic model leads to proportional change in the odds, in the same way that the log-linear model leads to proportional change in <span class="math inline">\(Y\)</span>.</p></li>
<li><p>We can also use probabilities for interpretation, but, since the logistic model implies that probabilities are non-linear (sigmoidal), things can get a bit complicated with this approach.</p></li>
</ul>
</section>
</section>
<section id="sec-simple-10" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="sec-simple-10"><span class="header-section-number">10.3</span> Simple logistic regression</h2>
<p>In this section we move onto logistic regression proper. For the CHD example, the model we are interested in is</p>
<p><span class="math display">\[\text{logit}(CHD) = a + b (\text{age}). \]</span></p>
<p>We are going to skip a few steps and go right into the interpretation of the R output. Once we know how to interpret the output, we will loop back to discuss details of estimation and inference in the following sections.</p>
<p>The summary R output for the example is below. The focus for now is just the interpretation of the values under the “Estimate” heading.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age, <span class="at">family =</span> binomial, <span class="at">data =</span> chd.data)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = chd ~ age, family = binomial, data = chd.data)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.30945    1.13365  -4.683 2.82e-06 ***
age          0.11092    0.02406   4.610 4.02e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 107.35  on 98  degrees of freedom
AIC: 111.35

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>Plugging the estimates into our logit model, we have the following equation</p>
<p><span class="math display">\[ \text{logit}(CHD) = -5.31  + .11 (\text{age}). \]</span></p>
<p>The “literal” interpretation of this equation is:</p>
<ul>
<li>When <span class="math inline">\(\text{age} = 0\)</span>, <span class="math inline">\(\text{logit}(CHD) = -5.31\)</span>.</li>
<li>Each unit of increase in age (i.e., each additional year) is associated with a .11 unit increase in <span class="math inline">\(\text{logit}(CHD)\)</span>.</li>
</ul>
<p>While this interpretation is perfectly correct, most applied audiences are not going to know how to interpret <span class="math inline">\(\text{logit}(CHD)\)</span>. So, instead, we often work with the odds and probabilities, as outlined in the next few sections.</p>
<section id="odds-ratio" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="odds-ratio"><span class="header-section-number">10.3.1</span> Odds ratio</h3>
<p>The logistic regression model implies</p>
<p><span id="eq-OR"><span class="math display">\[\frac{\text{odds} (X+1)}{\text{odds}(X)} = \exp(b) \tag{10.7}\]</span></span></p>
<p>where <span class="math inline">\(\text{odds}(X)\)</span> are the odds of the outcome associated with a given value of the predictor <span class="math inline">\(X\)</span>. Equation <a href="#eq-OR">Equation&nbsp;<span>10.7</span></a> is called the <em>odds ratio</em> (abbreviated OR) associated with a one-unit increase in <span class="math inline">\(X\)</span>.</p>
<p>If you refer back to section <a href="ch8_loglinear.html#sec-derivation-8"><span>Section&nbsp;8.5.5</span></a>, you can see we are using the exact same approach from log-linear regression, but in <a href="#eq-OR">Equation&nbsp;<span>10.7</span></a> we interpret the regression coefficient in term of the odds that <span class="math inline">\(Y = 1\)</span>, rather than the <span class="math inline">\(Y\)</span> variable itself.</p>
<p>For the CHD example, the OR is:</p>
<p><span class="math display">\[\exp(b) = \exp(.11) = 1.1163 \]</span></p>
<p>This means that each additional year of age is associated with an OR of 1.11. For example, the odds for someone aged 21 having CHD is 1.11 time larger (relative magnitude) that someone aged 20. The really useful thing about the OR is that it is constant over values of the predictor. So, regardless of whether we are comparing a 21-year-old to a 20-year-old, or 41-year-old to a 40-year-old, the OR is the same.</p>
<p>Just like the log-linear model, we can also report the results of our analysis in terms of relative change rather than relative magnitude. In particular, the percent increase in the odds of CHD associated with each additional year of age is:</p>
<p><span class="math display">\[(\exp(.11) - 1) \times 100 = 11.63\% \]</span></p>
<p>This means that the predicted odds of CHD increase 11.63% for each additional year of age.</p>
<p>Whether you use relative magnitude (i.e., the odds ratio) or relative change (i.e., percent change in odds) to report the results of logistic regression is up to you. In many fields, it is conventional to reports the odds ratios in tables, but to use percent change when writing about results in a sentence.</p>
<p>Before moving, <strong>please practice your interpretation of the OR in simple logistic regression using the following examples</strong></p>
<ul>
<li>If <span class="math inline">\(b=0\)</span> what is the OR equal to? What is the percent change in the odds for a one unit increase in <span class="math inline">\(X\)</span>?</li>
<li>If <span class="math inline">\(b=.25\)</span> what is the OR equal to? What is the percent change in the odds for a one unit increase in <span class="math inline">\(X\)</span>?</li>
<li>If <span class="math inline">\(b=-.025\)</span> what is the OR equal to? What is the percent change in the odds for a 10 unit increase in <span class="math inline">\(X\)</span>?</li>
<li>If the odds increase 100% for a one unit increase in <span class="math inline">\(X\)</span>, what <span class="math inline">\(b\)</span> equal to?</li>
</ul>
<p>Answers hidden below (use Code button), but please try out the questions yourself first!</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. OR = exp(0) = 1 and percent change equals (exp(0) - 1) X 100 = 0%. So, "no relationship" means OR = 1. </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. OR = exp(.25) = 1.2840 and percent change equals (exp(.25) - 1) X 100 = 28.40% increase</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. OR = exp(-.025) = 0.9753 and percent change for one unit equals (exp(-.025) - 1) X 100 = (-.02469 X 100 = -2.469%. For 10 units of change, multiply by 10, which gives 24.69% decrease (negative sign is decrease). </span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. (exp(b) - 1) X 100 = 100 --&gt; exp(b) = 2 --&gt; b = log(2) = .6931</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="other-interpretations-predicted-probabilities" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="other-interpretations-predicted-probabilities"><span class="header-section-number">10.3.2</span> Other interpretations: Predicted probabilities</h3>
<p>Another way to interpret the logistic model is in terms of the predicted probabilities, which are plotted below for the example data.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>visreg<span class="sc">::</span><span class="fu">visreg</span>(mod2, <span class="at">xvar =</span> <span class="st">"age"</span>, <span class="at">scale =</span> <span class="st">"response"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-pred-prob" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch10_logistic_files/figure-html/fig-pred-prob-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;10.9: Predicted probabilities</figcaption>
</figure>
</div>
</div>
</div>
<p>Using this plot, we can read off the probability of CHD for any given age. We might also want to report the probability of CHD for two or more chosen ages, which is an example of the MERV approach to marginal effects (see <a href="ch5_interactions.html#sec-inference-for-interactions-5"><span>Section&nbsp;5.4</span></a>):</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(emmeans)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">emmeans</span>(mod2, <span class="at">specs =</span> <span class="st">"age"</span>, <span class="at">at =</span> <span class="fu">list</span>(<span class="at">age =</span> <span class="fu">c</span>(<span class="dv">20</span>, <span class="dv">40</span>)), <span class="at">type =</span> <span class="st">"response"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> age   prob     SE  df asymp.LCL asymp.UCL
  20 0.0435 0.0279 Inf    0.0121     0.145
  40 0.2947 0.0578 Inf    0.1951     0.419

Confidence level used: 0.95 
Intervals are back-transformed from the logit scale </code></pre>
</div>
</div>
<p>The ratio of two probabilities is often called the <em>risk ratio</em> or the <em>relative risk</em>. So, we could also say that the risk ratio of CHD for someone who in their 40s as compared to someone who is 20 is .2947 / .0435 = 6.77. Otherwise stated, the risk of having CHD in you are 40s is almost 7 times higher than in your 20s (relative magnitude).</p>
</section>
<section id="other-interpretations-equal-odds" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="other-interpretations-equal-odds"><span class="header-section-number">10.3.3</span> Other interpretations: Equal odds</h3>
<p>Another interpretation of logistic regression is to report the value of <span class="math inline">\(X\)</span> at which the <span class="math inline">\(\text{odds}\)</span> of the outcome are equal to 1 (equivalently, the probability of the outcome is equal to .5). This idea is illustrated in <a href="#fig-equal-odds">Figure&nbsp;<span>10.10</span></a>.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">age =</span> <span class="dv">20</span><span class="sc">:</span><span class="dv">70</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod2, <span class="at">newdata =</span> x, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x<span class="sc">$</span>age, prob, <span class="at">xlab =</span> <span class="st">"age"</span>, <span class="at">ylab =</span> <span class="st">"p(CHD)"</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> <span class="dv">15</span>, <span class="at">y0 =</span> .<span class="dv">5</span>, <span class="at">x1 =</span> <span class="dv">48</span>, <span class="at">y1 =</span> .<span class="dv">5</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> <span class="dv">48</span>, <span class="at">y0 =</span> <span class="dv">0</span>, <span class="at">x1 =</span> <span class="dv">48</span>, <span class="at">y1 =</span> .<span class="dv">5</span>, <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-equal-odds" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch10_logistic_files/figure-html/fig-equal-odds-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;10.10: The Equal Odds Interpretation</figcaption>
</figure>
</div>
</div>
</div>
<p>First we find the probability of .5 on the <span class="math inline">\(Y\)</span> axis and then follow the horizontal dashed line to the logistic curve. Then we follow the vertical dashed line down to the value of <span class="math inline">\(X\)</span>. This gives use the age at which the probability of CHD is “50-50”. Based on the plot we can say that, after your 48th birthday, your chances of having CHD are above 50%.</p>
<p>The math behind this interpretation is below. Since</p>
<p><span class="math display">\[\log(.5/.5) = \log(1) = 0 \]</span></p>
<p>we can solve</p>
<p><span class="math display">\[ a + b(\text{age}) = 0 \]</span></p>
<p>to find the age at which someone has equal odds of CHD, leading to</p>
<p><span class="math display">\[\text{age} = - a/b. \]</span></p>
<p>For the example data</p>
<p><span class="math display">\[ \text{age} = - a/b = - (-5.31) / .11 = 48.27, \]</span></p>
<p>which confirms the conclusion we made looking at the plot.</p>
<p>In summary, another way of interpreting regression coefficients in simple logistic regression is to compute <span class="math inline">\(-a / b\)</span>, which gives the value of <span class="math inline">\(X\)</span> at which <span class="math inline">\(p(Y = 1) = .5\)</span>.</p>
</section>
<section id="other-interpretations-rate-of-change" class="level3" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="other-interpretations-rate-of-change"><span class="header-section-number">10.3.4</span> Other interpretations: Rate of change</h3>
<p>Yet another interpretation is in terms of the slope of the straight line (tangent) through the point <span class="math inline">\(p(CHD = 1) = .5\)</span>. The slope of this line describes the rate of change in the probability of CHD for people who are “close to” the age of equal odds (48 years in our example). The tangent line for our example is shown in <a href="#fig-tangent">Figure&nbsp;<span>10.11</span></a>.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">age =</span> <span class="dv">20</span><span class="sc">:</span><span class="dv">70</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod2, <span class="at">newdata =</span> x, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x<span class="sc">$</span>age, prob, <span class="at">xlab =</span> <span class="st">"age"</span>, <span class="at">ylab =</span> <span class="st">"p(CHD)"</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="sc">-</span>.<span class="dv">815</span>, <span class="at">b =</span> .<span class="dv">11</span><span class="sc">/</span><span class="dv">4</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tangent" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch10_logistic_files/figure-html/fig-tangent-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;10.11: The Rate of Change Interpretation</figcaption>
</figure>
</div>
</div>
</div>
<p>It turns out that the slope of the tangent line is equal to exactly <span class="math inline">\(b/4\)</span>. The derivation requires calculus and is omitted (ask in class if you are interested!). For the example data <span class="math inline">\(b / 4 = .11 / 4 = .0275\)</span>. So, for every additional year, the predicted probability of CHD increases by .0275. Keep in mind, this interpretation only applies to people who “around” the age of equal odds (48 years old in this example). Looking at the plot, we can see that this approximation is pretty good for people between the ages of 40 and 60.</p>
</section>
<section id="relation-to-linear-probability-model" class="level3" data-number="10.3.5">
<h3 data-number="10.3.5" class="anchored" data-anchor-id="relation-to-linear-probability-model"><span class="header-section-number">10.3.5</span> Relation to linear probability model</h3>
<p>Notice that in our example, the logistic function is roughly linear for probabilities in the range <span class="math inline">\([.2, .8]\)</span>. As mentioned in the introduction of this chapter, this is the situation in which using linear regression with a binary outcome (i.e., the linear probability model) works “well enough”. Also note that the regression coefficient from the linear probability model in <a href="#sec-chd-example-10"><span>Section&nbsp;10.1</span></a> (<span class="math inline">\(b_{OLS} = .0218\)</span>) is in the ballpark of the coefficient computed above (<span class="math inline">\(b_{logistic} / 4 = .0275\)</span>). These numbers are both describing how the probability of CHD is related to a person’s age.</p>
<p>The logistic model also provides us with a way of “diagnosing” whether the linear probability model is a good approximation. As noted, the logistic function is roughly linear for probabilities in the range <span class="math inline">\([.2, .8]\)</span>. If we ran a linear regression on CHD and all of the fitted / predicted values were within the range <span class="math inline">\([.2, .8]\)</span>, we would be in the situation where we might prefer to use linear regression (despite it being technically wrong). Referring back to residual vs.&nbsp;plotted in <a href="#sec-chd-example-10"><span>Section&nbsp;10.1</span></a>, we can see that the predicted values were outside of this range, so the logistic model is the better way to approach for this example.</p>
</section>
<section id="summary-1" class="level3" data-number="10.3.6">
<h3 data-number="10.3.6" class="anchored" data-anchor-id="summary-1"><span class="header-section-number">10.3.6</span> Summary</h3>
<p>The simple logistic regression model</p>
<p><span class="math display">\[ \text{logit}(CHD) = a + b (\text{age}) \]</span></p>
<p>has the following interpretations.</p>
<ul>
<li>In terms of the logit:
<ul>
<li>The intercept (<span class="math inline">\(a\)</span>) is the predicted value of the log-odds of CHD when age = 0.</li>
<li>The slope (<span class="math inline">\(b\)</span>) is how much the predicted log-odds of CHD changes for a one unit (year) increase in age.</li>
</ul></li>
<li>In terms of the odds:
<ul>
<li>The exponent of the regression parameter, <span class="math inline">\(\exp(b)\)</span>, is the odds ratio associated with a one unit increase in age (relative magnitude).</li>
<li><span class="math inline">\((\exp(b) - 1) \times 100\)</span> is the percentage change in the odds of CHD for a one unit increase in age (relative change).</li>
<li>For the intercept, <span class="math inline">\(\exp(a)\)</span> is the predicted odds of having CHD when <span class="math inline">\(age = 0\)</span>. This isn’t a very useful number in our example, but it can be useful when the predictor(s) are centered.</li>
</ul></li>
<li>In terms of predicted probabilities:
<ul>
<li>The logistic plot provides a visual summary of how the probability of CHD changes as a function of age – if you want to report in terms of probabilities, this plot is usually a good choice.</li>
<li>Predicted probabilities can be reported for specific values of age (MERVs), and risk ratios / relative risk can be computed to compare the predicted probabilities at specific ages. This is the same approach we used for following-up interactions in <a href="ch5_interactions.html#sec-inference-for-interactions-5"><span>Section&nbsp;5.4</span></a> (i.e., you can use <code>emmeans</code> in R).</li>
<li><span class="math inline">\(–a / b\)</span> corresponds the age at which the probability of having CHD is “50-50”.</li>
<li><span class="math inline">\(b / 4\)</span> is the approximate rate of increase (slope) of the probability of having CHD for people close to the “50-50” point.</li>
</ul></li>
</ul>
<p><strong>Please write down the numerical values of each of the above summaries for the CHD example (except the predicted probability plot). You can select any values of age to report the predicted probabilities and risk ratios, and you can “eye ball” the probabilities using <a href="#fig-pred-prob">Figure&nbsp;<span>10.9</span></a>.</strong></p>
<p>Answers are hidden below (use the Code button), but please try them yourself first.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Logit: </span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predicted logit when age = 0: a = -5.31;</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># expected increase in logit when age increases by one year: b = .11</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Odds:</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># OR = exp(b) = exp(.11) = 1.1163</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># % change in Odds = (exp(b) - 1) X 100 = 11.63%</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># In the example, used risk for Age = 40 relative to Age = 20: .2947/.0435 = 6.77</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co"># "50-50" age: -a/b = 5.31 / .11 = 48.27 </span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Rate of change at "50-50" age: b/4 = .11/4 = .0275</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="sec-estimation-10" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="sec-estimation-10"><span class="header-section-number">10.4</span> Estimation</h2>
<p>The previous section discussed the interpretation of simple logistic regression. In this section we discuss how to get the numbers in the “Estimate” column of the R <code>summary()</code> table, and in the next section we discuss how to compute statistical tests and confidence intervals.</p>
<p>The material in these next two sections involves a lot of mathematical and statistical concepts, although I try to keep the formulas to a minimum. Because the material is so technical, there will be a mini-lecture on these sections during class. It may also take some extra time to read through these next couple of sections, and you may want to write down any additional questions you have about the material so that we can address them in class together. Basically, the next two sections are pretty tough, so hang in there :)</p>
<p>The short version:</p>
<ul>
<li>In logistic regression, we use maximum likelihood (ML) rather than ordinary least squares (OLS) to estimate the model parameters.</li>
<li>The resulting estimates are called the maximum likelihood estimates (MLEs).</li>
<li>Unlike OLS estimates in linear regression, MLEs in logistic regression cannot be written down in “closed form.” This means that we need to define MLEs algorithmically, rather algebraically – more on this below.</li>
<li>Inference for the MLEs is based on <em>asymptotics</em>. In statistics, “asymptotic” means “as the sample size goes to infinity…and beyond!” The implication is that using ML also changes how we compute standard errors, statistical tests, confidence intervals, etc.</li>
<li>Our overall goal is to know enough about this technical stuff to be able to interpret and recognize the limitations of logistic regression.</li>
</ul>
<section id="ml-vs-ols" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="ml-vs-ols"><span class="header-section-number">10.4.1</span> ML vs OLS</h3>
<p>Let’s start by contrasting ML with OLS. Recall that in linear regression, OLS leads to “closed form” expressions for the regression coefficients ( <a href="ch2_simple_regression.html#sec-ols-2"><span>Section&nbsp;2.3</span></a>). This means we can write down an explicit algebraic equation for the regression coefficients, e.g.,</p>
<p><span class="math display">\[ b = \text{cov}(Y, X) / s^2_X. \]</span></p>
<p>As we discussed in <a href="#sec-chd-example-10"><span>Section&nbsp;10.1</span></a>, these estimates often aren’t very good when the outcome is binary. So, instead we use ML.</p>
<p>Unlike OLS with linear regression, ML with logistic regression does not lead to closed-form expressions for the regression coefficients. This means we can’t write down the other side of the equation <span class="math inline">\(b = ?\)</span> using an algebraic expression like the one above. Instead, we must define the regression parameters in terms of a computational procedure (i.e., an algorithm) that produces the intended result. That computational procedure is maximization of the likelihood function.</p>
<p>We discussed the difference between algebraic and algorithmic definitions when we compared the mean and median last semester, and we can address it again in class if you have further questions. In the present context, the take home message is:</p>
<ul>
<li>In OLS, we didn’t really need to know much about least squares <em>per se</em>, because we could just work with the formulas for the regression coefficients.<br>
</li>
<li>In ML, we don’t get any formulas for the coefficients, so were are going to have to dig into what the likelihood is and how to use it.</li>
</ul>
</section>
<section id="the-likelihood" class="level3" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="the-likelihood"><span class="header-section-number">10.4.2</span> The likelihood</h3>
<p>The likelihood is a function that tells us the probability of a sample being drawn from a population. For our CHD example data, we have <span class="math inline">\(N = 100\)</span> cases in which there are 43 people with CHD and 57 people without CHD. The likelihood tells us the probability of this sample being drawn from the population. In this section, we build up a mathematical expression for the likelihood function. Note that when the outcome variable in a regression is binary, this is more specifically called the <em>binomial</em> likelihood.</p>
<p>The building block of the binomial likelihood is the probability of a person having CHD, which we denote as</p>
<p><span class="math display">\[ p = p(CHD = 1). \]</span></p>
<p>Keep in mind that we are defining the probability of sample being drawn from a population. So, we are imaging that a person is sampled from a population, and some proportion of that population has CHD. That proportion is what we are representing as the probability <span class="math inline">\(p\)</span>.</p>
<p>For a single person (i.e., a sample of <span class="math inline">\(N = 1\)</span>), the likelihood describes the probability that the person either does or does not have CHD. The likelihood is given by the following formula:</p>
<p><span id="eq-Li"><span class="math display">\[ L = p ^ {CHD} \times (1 - p)^{1 - CHD}  \tag{10.8}\]</span></span></p>
<p>This looks complicated, but its just a way of writing the population probability of having CHD, or not having CHD, in a single equation. All it says is:</p>
<ul>
<li>The probability of sampling someone with CHD is <span class="math inline">\(p\)</span>.
<ul>
<li>i.e., plug in <span class="math inline">\(CHD = 1\)</span> into <a href="#eq-Li">Equation&nbsp;<span>10.8</span></a> and the result is <span class="math inline">\(L = p\)</span>.</li>
</ul></li>
<li>The probability of sampling someone without CHD is <span class="math inline">\(1 - p\)</span>.
<ul>
<li>i.e., plug in <span class="math inline">\(CHD = 0\)</span> into <a href="#eq-Li">Equation&nbsp;<span>10.8</span></a> and the answer is <span class="math inline">\(L = 1 - p\)</span>.</li>
</ul></li>
</ul>
<p>You should do the plugging-in now to see how this works.</p>
<p>For a simple random sample of <span class="math inline">\(i = 1, \dots, N\)</span> people, the likelihood of observing the full sample is just the product of the likelihoods for each person in the sample. To write the likelihood for person <span class="math inline">\(i\)</span>, we will use the notation <span class="math inline">\(L_i\)</span>, and to write the likelihood for the entire sample, we will keep using <span class="math inline">\(L\)</span> without a subscript:</p>
<p><span id="eq-L"><span class="math display">\[ L = \Pi_{i = 1}^N L_i = L_1 \times  L_2 \times \cdots \times L_N  \tag{10.9}\]</span></span></p>
<p>(The symbol <span class="math inline">\(\Pi\)</span> is shorthand for multiplication, just like <span class="math inline">\(\Sigma\)</span> is used for addition. )</p>
<p>If we plug-in to <a href="#eq-L">Equation&nbsp;<span>10.9</span></a> using the values of <span class="math inline">\(CHD\)</span> in our sample, this gives us the likelihood of observing the sample. For example, the values of CHD for the first four cases in the data set (1, 1, 0, 1). So, the first four terms in the likelihood are:</p>
<p><span class="math display">\[ L = p_1 \times  p_2 \times (1-p_3) \times p_4 \times\cdots \]</span> <strong>Please take a moment to verify this result for yourself using <a href="#eq-Li">Equation&nbsp;<span>10.8</span></a> and <a href="#eq-L">Equation&nbsp;<span>10.9</span></a>, and we will walk through the derivation together in class.</strong></p>
</section>
<section id="maximizing-the-likelihood" class="level3" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="maximizing-the-likelihood"><span class="header-section-number">10.4.3</span> Maximizing the likelihood</h3>
<p>So far we have addressed the general form of the likelihood for a binary variable. But we haven’t talked about how to get the probabilities <span class="math inline">\(p_i\)</span>. This is where the logistic regression comes in. As discussed in <a href="#sec-logit-10"><span>Section&nbsp;10.2</span></a>, the purpose of logistic regression is to model the probability of having CHD. Thus, in order to turn <a href="#eq-L">Equation&nbsp;<span>10.9</span></a> into the likelihood function for logistic regression, we just plug-in for <span class="math inline">\(p_i\)</span> using <a href="#eq-logistic">Equation&nbsp;<span>10.4</span></a>.</p>
<p>As you might expect, this equation turns out to be pretty complicated to write down, and we aren’t going to do that here. The point is that we <em>can</em> write down the likelihood for logistic regression, and once we have it written down we can use standard procedures from calculus to find its maximum (e.g., Newton’s method).</p>
<p>This is about as much math as we need to state the overall idea behind maximum likelihood estimation: We select the regression coefficients of the logistic model so that the likelihood in <a href="#eq-L">Equation&nbsp;<span>10.9</span></a> is maximized.</p>
<p>You might be asking, why is maximizing the likelihood a good way to get regression coefficients for logistic regression? There are lots of ways to answer this question, but I think the best answer is that the resulting estimates have good properties – they are unbiased and precise, at least with large samples. We can discuss the rationale behind maximum likelihood more in class, so if you are interested please just let me know.</p>
<p>Doing the computations that maximize the likelihood is quite complicated, and the details are beyond the scope of this course (for more details, check out Chapter 15 of <span class="citation" data-cites="cite:fox">(<a href="#ref-cite:fox" role="doc-biblioref"><strong>cite:fox?</strong></a>)</span>). However, there is one issue that we should be aware of: the likelihood can cause so-called “underflow” issues in computing. This is because we are multiplying together lots of numbers less than 1, which can lead to very, very small values – too small for a computer represent (i.e., underflow). To address this issue, we usually work with the log-likelihood, rather than the likelihood itself:</p>
<p><span id="eq-ell"><span class="math display">\[ \ell = \log(L) = \log \prod_{i = 1}^N L_i = \sum_{i = 1}^N \log(L_i)  \tag{10.10}\]</span></span></p>
<p>The last equality uses the “addition with logs” rule from <a href="ch8_loglinear.html#sec-math-review-8"><span>Section&nbsp;8.1</span></a>. For our purposes here, we just need to know that taking the log of the likelihood changes a very small number (the likelihood) into a large negative number (the log-likelihood), which prevents our computer from breaking when doing the computations.</p>
<p>The log-likelihood, <span class="math inline">\(\ell\)</span>, also shows up a lot in what follows. Its main uses are summarized below</p>
<ul>
<li><span class="math inline">\(-2 \times \ell\)</span> is called the <em>deviance</em> of the model. I am not sure how it got this name, but you will see it a lot in the R output.</li>
<li>We can use the deviance to compute R-squared-like statistics that describe the overall quality of the predictions made using a logistic regression model (see <a href="#sec-pseudo-rsquared-10"><span>Section&nbsp;10.6</span></a>)<br>
</li>
<li>Two logistic regression models can be compared using their deviances. This leads to the logistic regression equivalent of the F-test of R-squared change (<a href="ch6_model_building.html"><span>Chapter&nbsp;6</span></a>), which is called the likelihood ratio test (see <a href="#sec-inference-10"><span>Section&nbsp;10.5</span></a>).</li>
</ul>
</section>
<section id="summary-2" class="level3" data-number="10.4.4">
<h3 data-number="10.4.4" class="anchored" data-anchor-id="summary-2"><span class="header-section-number">10.4.4</span> Summary</h3>
<p>Lets take a look at our <code>summary()</code> output for the CHD data again:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = chd ~ age, family = binomial, data = chd.data)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.30945    1.13365  -4.683 2.82e-06 ***
age          0.11092    0.02406   4.610 4.02e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 107.35  on 98  degrees of freedom
AIC: 111.35

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>This section has addressed the question: how do we get the numbers in the “Estimate” column of the output shown above? The answer is maximum likelihood (ML). In short:</p>
<ul>
<li>The likelihood function tells us the probability of randomly sampling <span class="math inline">\(N\)</span> people from a population.
<ul>
<li>When the outcome variable is binary, it is called the binomial likelihood.</li>
</ul></li>
<li>ML is based on the idea that the values of the model parameters that make the data most likely are the “best”
<ul>
<li>The resulting values are called the maximum likelihood estimates (MLEs).</li>
</ul></li>
<li>In logistic regression, ML is accomplished using a computer algorithm – we can’t write down algebraic expressions for the MLEs, we only have an algorithmic definition, which is to maximize the likelihood
<ul>
<li>FYI: The algorithm <code>R</code> uses for logistic regression is a version of Newton-Raphson called Fisher Scoring.</li>
</ul></li>
</ul>
</section>
</section>
<section id="sec-inference-10" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="sec-inference-10"><span class="header-section-number">10.5</span> Inference</h2>
<p>In this section we talk about statistical tests and confidence intervals for logistic regression. There are three main types of inference for logistic regression:</p>
<ol type="1">
<li><p><em>Wald test of regression coefficients</em>. The Wald test is a z-test that can be used with large samples (<span class="math inline">\(N &gt; 100\)</span>). This is what is reported in R’s <code>summary()</code> output for logistic regression.</p>
<ul>
<li>Although the Wald test is reported by default in <code>R</code>, you should be careful when interpreting it. The Wald test is computed just like a regular t-test. However, rather than using a t-distribution, the Wald test is based on the normal distribution. This approach only applies in large samples (technically, when <span class="math inline">\(N \rightarrow \infty\)</span>). In smaller samples (<span class="math inline">\(N &lt; 100\)</span>), the standard errors can be too large, especially if the coefficient is large in value. This means that the Wald test can be under-powered, leading to too many Type I Errors in small samples. So, when you have fewer than 100 cases, it is good practice to use one of the other two procedures discussed below.</li>
</ul></li>
<li><p><em>Confidence intervals for odds ratios</em>. Odds ratios can be tested using confidence intervals. If the confidence interval includes the value 1, then the odds ratio is not statistically significant at the chosen level of alpha / confidence. Confidence intervals can be computed in two ways for logistic regression, either using the Wald test (just like a usual confidence based on a t-test), or using a procedure called <em>profile likelihood</em>. The latter is preferred because it is more accurate, and it is the default in <code>R</code>. Ask in class if you want to know more :)</p></li>
<li><p><em>Likelihood ratio test of nested models</em>. The likelihood ratio test plays the same role as the F-test of <span class="math inline">\(\Delta R^2\)</span> in OLS regression. It allows you to compare nested models. We will talk about the analogue statistic for <span class="math inline">\(\Delta R^2\)</span> in <a href="#sec-pseudo-rsquared-10"><span>Section&nbsp;10.6</span></a>.</p></li>
</ol>
<p>You might be wondering how the likelihood-based procedures (2 and 3) avoid the issues with the Wald test. Technically, the likelihood-based procedures also assume large samples sizes (<span class="math inline">\(N \rightarrow \infty\)</span>), but they do not require computing the standard errors of the regression coefficients, which is the problem with the Wald test. So, although both procedures have the same assumptions, the likelihood-based procedures perform better than the Wald test in practice, especially with smaller samples.</p>
<p>The rest this section briefly illustrates each of these three methods using our example data.</p>
<section id="wald-test" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="wald-test"><span class="header-section-number">10.5.1</span> Wald test</h3>
<p>The MLEs are asymptotically normally (based on the central limit theorem), so we can divide the MLEs by their standard errors to get an asymptotic z-test. This test is called a Wald test, named after the person (Abraham Wald) who first invented it. The Wald test is what R reports in the <code>summary()</code> table below.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = chd ~ age, family = binomial, data = chd.data)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.30945    1.13365  -4.683 2.82e-06 ***
age          0.11092    0.02406   4.610 4.02e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 107.35  on 98  degrees of freedom
AIC: 111.35

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>At this point in the semester, you should have no problem interpreting whether these tests are statistically significant or not. The main wrinkle that comes up in logistic regression is that we are working with an (asymptotic) z-test rather than usual t-test from OLS regression. As noted, this z-test is also called Wald test.</p>
<p>In summary:</p>
<ul>
<li>The null hypotheses <span class="math inline">\(H_0: b = b_0\)</span> can be tested against the alternative <span class="math inline">\(H_A: b \neq b_0\)</span> using the following z-test, which has a standard normal distribution when the null hypothesis is true:</li>
</ul>
<p><span class="math display">\[ z = (\hat b - b_0) / SE({\hat b}), \]</span></p>
<ul>
<li><p>This tests assumes:</p>
<ul>
<li><span class="math inline">\(\hat b\)</span> is the MLE (this requires that model is correct – more on this in <a href="#sec-assumption-checking-10"><span>Section&nbsp;10.7</span></a>)</li>
<li>The sample size is “approximately” infinite (<span class="math inline">\(N &gt; 100\)</span> is a practical lower limit).</li>
</ul></li>
</ul>
<p>The Wald test also applies to the intercept.</p>
</section>
<section id="odds-ratio-or" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="odds-ratio-or"><span class="header-section-number">10.5.2</span> Odds ratio (OR)</h3>
<p>In <a href="#sec-simple-10"><span>Section&nbsp;10.3</span></a>, the OR was introduced as a more intuitive way of interpreting logistic regression. But, how do we make inferences about this more interpretable quantity? The short answer: use confidence intervals.</p>
<p>The reason we use confidence intervals instead of tests of significance is because the null hypothesis <span class="math inline">\(b = 0\)</span> corresponds to</p>
<p><span class="math display">\[OR = \exp(b) = \exp(0) = 1. \]</span></p>
<p>So, if the regression coefficient is zero (i.e., no relationship), this means the OR is one. In terms of percent change, <span class="math inline">\(OR = 1\)</span> means 0% percent change:</p>
<p><span class="math display">\[(OR - 1) \times 100   = (1 - 1) \times 100 = 0. \]</span> To obtain confidence intervals on the OR, we can use the following two steps:</p>
<ol type="1">
<li>Compute the confidence intervals for the regression coefficient <span class="math inline">\(b\)</span>:</li>
</ol>
<p><span class="math display">\[ [b_{\text{lower}} ,b_{\text{upper}}]. \]</span></p>
<ol start="2" type="1">
<li>Then exponentiate the confidence intervals to get a confidence interval for the OR.</li>
</ol>
<p><span class="math display">\[ [\exp(b_{\text{lower}}) ,\exp(b_{\text{upper}})]. \]</span></p>
<p>The output below shows the result of this procedure for the CHD data.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>ci.b <span class="ot">&lt;-</span> <span class="fu">confint</span>(mod2)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>ci.table <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">coefs =</span> <span class="fu">coef</span>(mod2), ci.b)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(ci.table)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                  coefs        2.5 %    97.5 %
(Intercept) 0.004944629 0.0004412621 0.0389236
age         1.117306795 1.0692223156 1.1758681</code></pre>
</div>
</div>
<p>In APA notation, we could write the results for age as <span class="math inline">\(OR = 1.12, 95\% \text{ CI: } [1.07, 1.19]\)</span>. <strong>Please write down your interpretation of the OR for age and its confidence interval. In your interpretation, please mention the relative change in the odds that is associated with each additional year of age and whether or not this is statistically different from zero.</strong></p>
<p>Note that the confidence intervals produced by <span class="math inline">\(R\)</span> use the likelihood rather than the Wald test. So, the problems we discussed with the Wald test don’t apply to these confidence intervals. You can get Wald confidence intervals form the <code>confint</code> function, but this is not the default. Computing likelihood-based confidence intervals is complicated; you can learn about the overall approach here: <a href="https://documentation.sas.com/doc/en/etscdc/14.2/etsug/etsug_model_sect148.htm">https://documentation.sas.com/doc/en/etscdc/14.2/etsug/etsug_model_sect148.htm</a></p>
</section>
<section id="likelihood-ratio-test" class="level3" data-number="10.5.3">
<h3 data-number="10.5.3" class="anchored" data-anchor-id="likelihood-ratio-test"><span class="header-section-number">10.5.3</span> Likelihood ratio test</h3>
<p>The likelihood ratio (LR) test is analogous to the F-test of R-squared change in OLS regression. Its a general workhorse for model building and can be used in other creative ways. For example, the LR test can be used to replace the Wald test in simple logistic regression. To do this, we compare a model with only the intercept to a model with one predictor. And as noted above, the LR test avoids the problems associated with the Wald test, so it is usually preferred when the sample size is smaller (<span class="math inline">\(N &lt; 100\)</span>).</p>
<p>The LR tests works as follows:</p>
<ul>
<li><p>Assume that Model 1 with <span class="math inline">\(K_1\)</span> predictors is nested within Model 2 with <span class="math inline">\(K_2\)</span> predictors (i.e., the predictors in Model 1 are a subset of the predictors in Model 2; see <a href="ch6_model_building.html"><span>Chapter&nbsp;6</span></a> for a refresher on nested models).</p></li>
<li><p>If it is true that the additional predictors in Model 2 all have regression coefficients equal to zero in the population, then it is also true that the models have equal likelihoods. This leads to the null hypothesis that the ratio of the likelihoods is equal to 1 (this is why its called a likelihood ratio test):</p></li>
</ul>
<p><span class="math display">\[H_0: \frac{L_1}{L_2} = 1\]</span></p>
<ul>
<li>When <span class="math inline">\(H_0\)</span> is true, the following test statistic has a chi-square distribution with degrees of freedom equal to <span class="math inline">\(df = K_2 - K_1\)</span>.</li>
</ul>
<p><span class="math display">\[\chi^2 = -2 \log \frac{\hat L_1}{\hat L_2} = 2 (\hat \ell_2 - \hat \ell_1) \]</span></p>
<p>In this equation, the symbol <span class="math inline">\(\chi\)</span> is the Greek letter for lower-case “x” and is pronounced “chi”. The chi-square test is computed as negative 2 times the log of the likelihood ratio, which ends up being the difference of the deviances of the two models (remember, the deviance is just a weird word for -2 times the log likelihood). Also note that the “hats” denote the likelihoods / log-likelihoods evaluated at the MLEs.</p>
<p>We have not covered the chi-square distribution previously, but it is a lot like the F-distribution. In fact, the F-static is a ratio of two chi-square statistics. Some examples of the chi-square distribution are shown below (<span class="math inline">\(k\)</span> denotes the degrees of freedom).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/3/35/Chi-square_pdf.svg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Chi-square distributions (Source: https://en.wikipedia.org/wiki/Chi-squared_distribution)</figcaption>
</figure>
</div>
<p>To sum up:</p>
<ul>
<li><p>The LR test is the analogue of the F-test of R-squared change from linear regression (see <a href="ch6_model_building.html"><span>Chapter&nbsp;6</span></a>). It is used to compare two nested models.</p></li>
<li><p>If LR test is significant, we conclude that model with more predictors does a better job or explaining our data (in the sense of R-squared). In practical terms, this means that we reject the models with fewer predictors in favor of the model with more predictors.</p></li>
<li><p>If the LR test is not significant, we conclude that the model with more predictors does not add anything beyond the model with fewer predictors. In practical terms, this means that we reject the models with more predictors and stick with the model with fewer predictors.</p></li>
<li><p>In the case where the two models differ by only one predictor, the LR test is conceptually equivalent to testing whether the regression coefficient of that predictor is equal to zero, and hence the LR test can replace the Wald test reported in R’s <code>summary()</code> output. This is again analogous to testing R-squared change in linear regression (see <a href="ch6_model_building.html"><span>Chapter&nbsp;6</span></a>).</p></li>
</ul>
<p>Using our example, let’s conduct an LR test of the model with age as a predictor against a model without this predictor (i.e., the model with only the intercept). To do this, we can use the <code>lrtest</code> function of the <code>lmtest</code> package which produces the following output:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The model with no predictors</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>mod0 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> <span class="dv">1</span>, <span class="at">family =</span> <span class="st">"binomial"</span>, <span class="at">data =</span> chd.data)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The LR test (enter the smaller model first)</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">lrtest</span>(mod0, mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Likelihood ratio test

Model 1: chd ~ 1
Model 2: chd ~ age
  #Df  LogLik Df Chisq Pr(&gt;Chisq)    
1   1 -68.331                        
2   2 -53.677  1 29.31  6.168e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>In APA format, we could write this output as <span class="math inline">\(\chi^2(1) = 29.31, p &lt; .001\)</span>. <strong>Please write down your interpretation of the LR test for the CHD example. Your interpretation should mention which two models were compared, whether the LR test was significant or not, and your conclusion about which model provided a better explanation of the data.</strong></p>
</section>
</section>
<section id="sec-pseudo-rsquared-10" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="sec-pseudo-rsquared-10"><span class="header-section-number">10.6</span> (Pseudo) R-squared</h2>
<p>As we have just discussed, the likelihood ratio test in logistic regression is analogous to the F-test of R-squared change in linear regression. But we have not yet discussed how to compute R-squared for logistic regression. That is what this section is about.</p>
<p>There are actually a number of statistics, called <em>pseudo R-squareds</em>, that have been developed to play the role of R-squared in logistic regression. The pseudo R-squareds are not computed as proportions of variance, which is why the are called <em>pseudo</em>. But they each try to describe the “preditive utility” of the logistic regression model, and each has their own shortcomings and limitations.</p>
<p>There is no clear “victor” in the battle of pseudo R-squareds, so we will just present one widely used exemplar that is straight forward to compute and interpret. You can learn about some other options here: <a href="https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/">https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/</a></p>
<section id="mcfaddens-pseudo-r-squared" class="level3" data-number="10.6.1">
<h3 data-number="10.6.1" class="anchored" data-anchor-id="mcfaddens-pseudo-r-squared"><span class="header-section-number">10.6.1</span> McFadden’s Pseudo R-squared</h3>
<p>McFadden’s Pseudo R-squared focuses on how much the likelihood improves when comparing two nested models. This is close to, but not quite the same as, the interpretation of <span class="math inline">\(\Delta R^2\)</span> in linear regression.</p>
<p>McFadden’s Pseudo R-squared takes on values between 0 and 1 and its interpretation can be explained by considering these two extreme values.</p>
<p>For Model 1 nested within Model 2, McFadden’s R-squared is defined as:</p>
<p><span class="math display">\[ R^2 = 1 - \frac{ \ell_2}{\ell_1} \]</span></p>
<p>Note that if Model 2 does not “add anything” to Model 1, then the two models have the same likelihood (see the discussion of the LR test in <a href="#sec-inference-10"><span>Section&nbsp;10.5</span></a>). In this situation,</p>
<p><span class="math display">\[\frac{ \ell_2}{ \ell_1} = 1 \]</span></p>
<p>which implies <span class="math inline">\(R^2 = 0.\)</span></p>
<p>On the other hand, if Model 2 provides perfect predictions for all cases, (i.e., <span class="math inline">\(L_i = 1\)</span> for each <span class="math inline">\(i\)</span>), then</p>
<p><span class="math display">\[ \hat \ell_2 = \sum_i \log L_{2i} = \sum_i \log 1 = 0. \]</span></p>
<p>This implies</p>
<p><span class="math display">\[ R^2 = 1 - \frac{\hat \ell_2}{\hat \ell_1} = 1 - \frac{0}{\hat \ell_1} = 1. \]</span></p>
<p>In summary, McFadden’s Rsquared</p>
<ul>
<li><p>is equal to zero if Model 2 does not add anything to Model 1,</p></li>
<li><p>is equal to 1 if Model 2 provides perfect prediction of each data point (but model 1 does not).</p></li>
</ul>
<p>Because McFadden’s R-squared is not a proportion of variance, we can’t interpret it the same way as OLS R-squared. Instead, it is usually interpreted in terms of model <em>fit</em>. Here “fit” means how much better a nesting model’s predictions are compared to a nested model, with zero meaning “not at all” and one meaning “as good as it gets”.</p>
<p>The following output shows McFadden’s R-squared for the example data. <strong>Please write down an interpretation of the R-square value and we will discuss your interpretations in class.</strong></p>
<p>McFadden’s R-squared:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using the deviance of the models</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> mod2<span class="sc">$</span>deviance<span class="sc">/</span>mod0<span class="sc">$</span>deviance</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2144684</code></pre>
</div>
</div>
<p>You can use the “show code” button to see an example interpretation below (but try it by yourself first!)</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpretation: Relative a model with no predictors, </span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the inclusion of age improved model fit by about 21.4% </span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># as much as a model with perfect predictions. </span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="co"># More elliptically: Using McFadden's R-squared the inclusion </span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co"># of age improved model fit by about 21.4\%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="sec-assumption-checking-10" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="sec-assumption-checking-10"><span class="header-section-number">10.7</span> Assumption checking</h2>
<p>The final topic we need to cover is assumption checking. As we saw when deriving the binomial likelihood in <a href="#sec-estimation-10"><span>Section&nbsp;10.4</span></a>, there are two main assumptions in play when estimating logistic regression using maximum likelihood.</p>
<ol type="1">
<li><p>The different respondents (person, units) are independent (i.e., a simple random sample).</p></li>
<li><p>We have the right model for each individual’s probability of CHD, <span class="math inline">\(p_i\)</span>, which in this case is the logistic model.</p></li>
</ol>
<p>Assumption 1 is ensured by random sampling. If your data wasn’t collected using random sampling, you’ll need to use a different modeling approach (e.g, multilevel modeling, see EDUC 935).</p>
<p>Assumption 2 can be checked with the Hosmer-Lemeshow (HL) test. The HL test is a chi-square test that compares the empirical probabilities of the outcome variable to the probabilities implied by the logistic regression model. In other words, we want to compare the two curves depicted below:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="files/images/HL_test.png" class="img-fluid figure-img" width="495"></p>
<figcaption class="figure-caption">The idea behind the HL test</figcaption>
</figure>
</div>
</div>
</div>
<p>The null hypothesis of the HL test is that these two curves are equivalent representations of the data. So, if we reject the null hypothesis, we reject claim that the model fits the data.</p>
<p>The HL test for the example data is reported below. The R code is shown by default to provide details about how the test is computed.</p>
<p>The <code>logitgof</code> function uses 3 arguments:</p>
<ul>
<li><code>obs</code> is the binary outcome variable (chd)</li>
<li><code>exp</code> is the fitted values (predicted probabilities) from the logistic regression.</li>
<li><code>g</code> is the number of groups / bins to use. For this example, 5 is a good number.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Note: Install package "generalhoslem" if you haven't do so already</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("generalhoslem")</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>hl_test <span class="ot">&lt;-</span> generalhoslem<span class="sc">::</span><span class="fu">logitgof</span>(<span class="at">obs =</span> chd, <span class="at">exp =</span> <span class="fu">fitted</span>(mod2), <span class="at">g =</span> <span class="dv">5</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>hl_test</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Hosmer and Lemeshow test (binary model)

data:  chd, fitted(mod2)
X-squared = 0.046893, df = 3, p-value = 0.9973</code></pre>
</div>
</div>
<p>Using APA notation, we could write the results of the test as <span class="math inline">\(\chi^2(3) = .05, p = .99\)</span>. <strong>Please write down your interpretation of the HL test and be prepared to share you answer in class. Your interpretation should mention whether the assumption that the logistic model fits the data is problematic or not.</strong></p>
<section id="more-details-on-the-hl-test" class="level3" data-number="10.7.1">
<h3 data-number="10.7.1" class="anchored" data-anchor-id="more-details-on-the-hl-test"><span class="header-section-number">10.7.1</span> More details on the HL test*</h3>
<p>The HL statistic is computed using the observed and expected (model-implied) counts in each of the groups. Computational details are given below. The number of groups must be chosen by the researcher and should be based on considerations about sample size (e.g., the number of observation per group should be at least 20 to estimate a proportion).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Observed and expected counts</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(hl_test<span class="sc">$</span>observed, hl_test<span class="sc">$</span>expected)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>               y0 y1     yhat0     yhat1
[0.0435,0.174] 18  2 17.870104  2.129896
(0.174,0.318]  16  5 16.131098  4.868902
(0.318,0.504]  12  9 12.268366  8.731634
(0.504,0.734]   8 15  7.935627 15.064373
(0.734,0.912]   3 12  2.794804 12.205196</code></pre>
</div>
</div>
<p>The test statistic is the built up using the differences between the observed and expected values:</p>
<p><span class="math display">\[HL = \sum_{g = 1}^5 \left(\frac{(\text{y0}_g - \text{yhat0}_g)^2}{\text{yhat0}_g} + \frac{(\text{y1}_g - \text{yhat1}_g)^2}{\text{yhat1}_g}\right)\]</span></p>
<p>Hosmer and Lemeshow showed that this statistic has a chi-square distribution on <span class="math inline">\(df = N_\text{groups} - 2\)</span> when the null hypothesis is true (i.e., when the model fits the data).</p>
</section>
</section>
<section id="workbook" class="level2" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="workbook"><span class="header-section-number">10.8</span> Workbook</h2>
<p>This section collects the questions asked in this chapter. The lessons for this chapter will focus on discussing these questions and then working on the exercises in <span class="quarto-unresolved-ref">?sec-exercises-10</span>. If you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.</p>
<p><a href="#sec-chd-example-10"><span>Section&nbsp;10.1</span></a></p>
<ul>
<li>Please take a moment to write down you conclusions (and rationale) about whether the assumptions of linear regression are met for the CHD data.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch10_logistic_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="1152"></p>
<figcaption class="figure-caption">Linear probability model with CHD example</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = chd ~ age, data = chd.data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.85793 -0.33992 -0.07274  0.31656  0.99269 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.537960   0.168809  -3.187  0.00193 ** 
age          0.021811   0.003679   5.929 4.57e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.429 on 98 degrees of freedom
Multiple R-squared:  0.264, Adjusted R-squared:  0.2565 
F-statistic: 35.15 on 1 and 98 DF,  p-value: 4.575e-08</code></pre>
</div>
</div>
<p><a href="#sec-logit-10"><span>Section&nbsp;10.2</span></a></p>
<ul>
<li><p>Using the Table below, please write down your answers to the following questions and be prepared to share them in class. For each question provide a verbal interpretation of the numerical answer (e.g, odds of 2 to 1 means that for every two people with a trait, there is one without).</p>
<ol type="1">
<li>What is the probability of a person in their 40s having CHD?</li>
<li>What are the odds of a person in their 40s having CHD?</li>
<li>What is the probability of someone in their 50s <strong>not</strong> having CHD?</li>
<li>What are the odds of someone in their 50s <strong>not</strong> having CHD?</li>
<li>What is probability of having CHD in your 40s, compared to your 30s? (e.g., is 3 times higher? 4 times higher?)</li>
<li>What are the odds of having CHD in your 40s, compared to your 30s?</li>
</ol></li>
</ul>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/odds.png"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="files/images/odds.png" class="img-fluid figure-img" width="858"></p>
<figcaption class="figure-caption">Proportions and odds</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li><p>Using the Table below, please answer the following questions.</p>
<ul>
<li>If probability of an event is equal to .1, what are the odds the event? What is the logit of the event?</li>
<li>If odds of an event are 4 to 1, what is the probability of the event? What is the logit of the event?</li>
<li>If logit &lt; 0 then probability &lt; ? and odds &lt; ?</li>
<li>What is more likely: a event with probability of .9 or an event with odds of .9?</li>
<li>If a probability of <span class="math inline">\(p\)</span> corresponds to a <span class="math inline">\(\text{logit}\)</span> of <span class="math inline">\(x\)</span>, what is the logit corresponding to <span class="math inline">\(1-p\)</span>? (Hint, try some numerical examples from the table).</li>
</ul></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="files/images/logit-table.png" class="img-fluid figure-img" width="455"></p>
<figcaption class="figure-caption">Logistic, odds, and logit</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#sec-simple-10"><span>Section&nbsp;10.3</span></a></p>
<ul>
<li>Please practice your interpretation of the OR in simple logistic regression using the following examples:
<ul>
<li>If <span class="math inline">\(b=0\)</span> what is the OR equal to? What is the percent change in the odds for a one unit increase in <span class="math inline">\(X\)</span>?</li>
<li>If <span class="math inline">\(b=.25\)</span> what is the OR equal to? What is the percent change in the odds for a one unit increase in <span class="math inline">\(X\)</span>?</li>
<li>If <span class="math inline">\(b=-.025\)</span> what is the OR equal to? What is the percent change in the odds for a 10 unit increase in <span class="math inline">\(X\)</span>?</li>
<li>If the odds increase 100% for a one unit increase in <span class="math inline">\(X\)</span>, what <span class="math inline">\(b\)</span> equal to?</li>
</ul></li>
<li>For the CHD example (model given below), please write down the numerical values of each of the summaries listed below. You can select any values of age to report the predicted probabilities and risk ratios, and you can “eye ball” the probabilities using <a href="#fig-pred-prob">Figure&nbsp;<span>10.9</span></a>.
<ul>
<li>In terms of the logit.</li>
<li>In terms of the odds.</li>
<li>In terms of predicted probabilities.
<ul>
<li>Risk ratios / relative risk for some chosen ages.</li>
<li>The age at which the probability of having CHD is “50-50”.</li>
<li>The approximate rate of increase (slope) of the probability of having CHD for people close to the “50-50” point.</li>
</ul></li>
</ul></li>
</ul>
<p><span class="math display">\[ \text{logit}(CHD) = -5.31  + .11 (\text{age}). \]</span></p>
<p><a href="#sec-estimation-10"><span>Section&nbsp;10.4</span></a></p>
<p>To write the likelihood for person <span class="math inline">\(i\)</span>, we use the notation <span class="math inline">\(L_i\)</span></p>
<p><span class="math display">\[ L = p ^ {CHD} \times (1 - p)^{1 - CHD} \]</span> and to write the likelihood for the full sample, we use use <span class="math inline">\(L\)</span> without a subscript:</p>
<p><span class="math display">\[ L = \Pi_{i = 1}^N L_i = L_1 \times  L_2 \times \cdots \times L_N. \]</span></p>
<p>If we plug into the last equation using the values of <span class="math inline">\(CHD\)</span> in our sample, this gives us the likelihood of observing the sample. For example, the values of CHD for the first four cases in the example are (1, 1, 0, 1). So, the first four terms in the likelihood are:</p>
<p><span class="math display">\[ L = p_1 \times  p_2 \times (1-p_3) \times p_4 \times\cdots \]</span></p>
<p>Please take a moment to verify this result for yourself and we will walk through the derivation together in class.</p>
<p><a href="#sec-inference-10"><span>Section&nbsp;10.5</span></a></p>
<ul>
<li>The output below shows the odds ratios and confidence intervals for the example data. In APA notation we could write the results for age as <span class="math inline">\(OR = 1.12, 95\% \text{ CI: } [1.07, 1.19]\)</span>. Please write down your interpretation of the OR for age and its confidence interval. In your interpretation, please mention the relative change in the odds that is associated with each additional year of age and whether or not this is statistically different from zero.</li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>ci.b <span class="ot">&lt;-</span> <span class="fu">confint</span>(mod2)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>ci.table <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">coefs =</span> <span class="fu">coef</span>(mod2), ci.b)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(ci.table)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                  coefs        2.5 %    97.5 %
(Intercept) 0.004944629 0.0004412621 0.0389236
age         1.117306795 1.0692223156 1.1758681</code></pre>
</div>
</div>
<ul>
<li>To conduct an LR test of the model with age as a predictor against a model without this predictor (i.e., the model with only the intercept), we can use the <code>lrtest</code> function of the <code>lmtest</code> package to obtain the following output. In APA format, we could write this output as <span class="math inline">\(\chi^2(1) = 29.31, p &lt; .001\)</span> Please write down your interpretation of the LR test for the example. Your interpretation should mention which two models were compared, whether the LR test was significant or not, and your conclusion about which model provided a better explanation of the data.</li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The model with no predictors</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>mod0 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> <span class="dv">1</span>, <span class="at">family =</span> <span class="st">"binomial"</span>, <span class="at">data =</span> chd.data)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The LR test (enter the smaller model first)</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">lrtest</span>(mod0, mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Likelihood ratio test

Model 1: chd ~ 1
Model 2: chd ~ age
  #Df  LogLik Df Chisq Pr(&gt;Chisq)    
1   1 -68.331                        
2   2 -53.677  1 29.31  6.168e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p><a href="#sec-pseudo-rsquared-10"><span>Section&nbsp;10.6</span></a></p>
<ul>
<li>The following output shows McFadden’s R-squared for the example data (i.e., comparing a model with age as a predictor to a model with just the intercept). Please write down an interpretation of the R-square value and we will discuss you interpretations in class.</li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using the deviance of the models</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> mod2<span class="sc">$</span>deviance<span class="sc">/</span>mod0<span class="sc">$</span>deviance</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2144684</code></pre>
</div>
</div>
<p><a href="#sec-assumption-checking-10"><span>Section&nbsp;10.7</span></a></p>
<p>The HL test for the example data is reported below. Using APA notation, we could write the results of the test as <span class="math inline">\(\chi^2(3) = .05, p = .99\)</span>. Please write down your interpretation of the HL test and be prepared to share you answer in class. Your interpretation should mention whether the assumption that the logistic model fits the data is problematic or not.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Note: Install package "generalhoslem" if you haven't do so already</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("generalhoslem")</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>hl_test <span class="ot">&lt;-</span> generalhoslem<span class="sc">::</span><span class="fu">logitgof</span>(<span class="at">obs =</span> chd, <span class="at">exp =</span> <span class="fu">fitted</span>(mod2), <span class="at">g =</span> <span class="dv">5</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>hl_test</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
    Hosmer and Lemeshow test (binary model)

data:  chd, fitted(mod2)
X-squared = 0.046893, df = 3, p-value = 0.9973</code></pre>
</div>
</div>
</section>
<section id="sec-exercise-10" class="level2" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="sec-exercise-10"><span class="header-section-number">10.9</span> Exercises: Multiple logistic regression</h2>
<p>These exercises are a bit different than usual. Rather than repeating the code that produced the previously discussed example, we extend the example to illustrate multiple logistic regression. The general idea is that, everything you can do in OLS linear regression you can also do in logistic regression, but some of the details are different.</p>
<p>We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.) Before staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”</p>
<section id="chd-and-smoking" class="level3" data-number="10.9.1">
<h3 data-number="10.9.1" class="anchored" data-anchor-id="chd-and-smoking"><span class="header-section-number">10.9.1</span> CHD and smoking</h3>
<p>This section works through multiple logistic regression with interactions using the CHD data again. Coronary Heart Disease (<code>chd</code>; 1 = yes, 0 = no) is still the outcome, and we predict it using a person’s age in years (<code>age</code>) as well as an indicator for whether a person has ever (or currently) smokes (<code>smokes</code>: 1 = yes, 0 = no).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data, take a look</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co">#load("CHD.RData")</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="co">#attach(chd.data)</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">cbind</span>(chd, age, smokes))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>      chd            age            smokes    
 Min.   :0.00   Min.   :20.00   Min.   :0.00  
 1st Qu.:0.00   1st Qu.:34.75   1st Qu.:0.00  
 Median :0.00   Median :44.00   Median :0.00  
 Mean   :0.43   Mean   :44.38   Mean   :0.42  
 3rd Qu.:1.00   3rd Qu.:55.00   3rd Qu.:1.00  
 Max.   :1.00   Max.   :69.00   Max.   :1.00  </code></pre>
</div>
</div>
<p>Let’s start by regressing CHD on smoking and age, and refreshing our interpretations of the parameter estimates and tests. The regression coefficients are interpreted analogously to multiple regression (e.g., as holding constant or controlling for the other predictors).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress CHD on smokes and age</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age <span class="sc">+</span> smokes, <span class="at">family =</span> binomial)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = chd ~ age + smokes, family = binomial)

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -6.72152    1.33396  -5.039 4.68e-07 ***
age          0.11849    0.02639   4.489 7.16e-06 ***
smokes       2.54953    0.59260   4.302 1.69e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.663  on 99  degrees of freedom
Residual deviance:  84.004  on 97  degrees of freedom
AIC: 90.004

Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
<p>For illustrative purposes, let’s double check that smoking predicts CHD over and above age using the LR test. Recall that this test is analogous to the F-test of R-squared change in OLS regression</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LR test coefficient on age, and age + smokes </span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age, <span class="at">family =</span> binomial)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age <span class="sc">+</span> smokes, <span class="at">family =</span> binomial)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">lrtest</span>(mod1, mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Likelihood ratio test

Model 1: chd ~ age
Model 2: chd ~ age + smokes
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   2 -53.677                         
2   3 -42.002  1 23.349  1.351e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Both the Wald test and LR test lead to the same overall interpretation for smoking.</p>
<p>How should we compute R-squared for the two-predictor model? Two different approaches are shown below. Both of these are defensible, but note the difference in interpretation as we change the “baseline model”.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare against model 1</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>(R_21 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mod2<span class="sc">$</span>deviance<span class="sc">/</span>mod1<span class="sc">$</span>deviance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2174937</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare against model with only the intercept</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>mod0 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> <span class="dv">1</span>, <span class="at">family =</span> binomial)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>(R_20 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mod2<span class="sc">$</span>deviance<span class="sc">/</span>mod0<span class="sc">$</span>deviance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3853166</code></pre>
</div>
</div>
<p>Recall that it is common to report the output using odds ratios rather than logits. Make sure to check your interpretation of the ORs.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(mod2), <span class="fu">confint</span>(mod2)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Waiting for profiling to be done...</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                      OR        2.5 %      97.5 %
(Intercept)  0.001204708 6.635861e-05  0.01314643
age          1.125791093 1.073105e+00  1.19145228
smokes      12.801117760 4.292057e+00 45.11382912</code></pre>
</div>
</div>
</section>
<section id="predicted-probabilities" class="level3" data-number="10.9.2">
<h3 data-number="10.9.2" class="anchored" data-anchor-id="predicted-probabilities"><span class="header-section-number">10.9.2</span> Predicted probabilities</h3>
<p>We can plot the regression lines for smokers and non-smokers using the same techniques as for linear regression.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plots of the regression lines in logit scale</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>visreg<span class="sc">::</span><span class="fu">visreg</span>(mod2, </span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">xvar =</span> <span class="st">"age"</span>, </span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">by =</span> <span class="st">"smokes"</span>, </span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">scale =</span> <span class="st">"linear"</span>, </span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">overlay =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch10_logistic_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plots of the regression lines in probability scale</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>visreg<span class="sc">::</span><span class="fu">visreg</span>(mod2, </span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">xvar =</span> <span class="st">"age"</span>, </span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">by =</span> <span class="st">"smokes"</span>, </span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">scale =</span> <span class="st">"response"</span>, </span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">overlay =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch10_logistic_files/figure-html/unnamed-chunk-27-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can also test for differences between smokers and non-smokers at specific ages, just like we did for the OLS regression. Below is an example of using <code>emmeans</code> to compute the risk ratio associated with smoking at +/- 1SD on age.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1. Make a function to compute +/- SD on age</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>sd_function <span class="ot">&lt;-</span> <span class="cf">function</span>(x) { <span class="fu">c</span>(<span class="fu">mean</span>(x) <span class="sc">-</span> <span class="fu">sd</span>(x), <span class="fu">mean</span>(x) <span class="sc">+</span> <span class="fu">sd</span>(x)) }</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2. Compute and test the values on the odds scale</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(emmeans)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>gap_odds <span class="ot">&lt;-</span> <span class="fu">emmeans</span>(mod2, </span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">specs =</span> <span class="st">"smokes"</span>, </span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">by =</span> <span class="st">"age"</span>, </span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">cov.reduce =</span> sd_function, </span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>                    <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert odds to probabilities</span></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>gap_prob <span class="ot">&lt;-</span> <span class="fu">regrid</span>(gap_odds, <span class="st">"log"</span>)</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute risk ratios for smoking, at different ages.</span></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a><span class="fu">contrast</span>(gap_prob, <span class="at">method =</span> <span class="st">"revpairwise"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>age = 32.6586734987249:
 contrast          ratio    SE  df null z.ratio p.value
 smokes1 / smokes0  7.79 3.898 Inf    1   4.100  &lt;.0001

age = 56.1013265012751:
 contrast          ratio    SE  df null z.ratio p.value
 smokes1 / smokes0  1.92 0.388 Inf    1   3.214  0.0013

Tests are performed on the log scale </code></pre>
</div>
</div>
</section>
<section id="interactions" class="level3" data-number="10.9.3">
<h3 data-number="10.9.3" class="anchored" data-anchor-id="interactions"><span class="header-section-number">10.9.3</span> Interactions</h3>
<p>Next, let’s consider the interaction between age (centered) and smoking. This interaction addresses whether the relationship between age and CHD changes as a function of smoking status.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Interacting smoking with age (centered)</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>age_centered <span class="ot">&lt;-</span> age <span class="sc">-</span> <span class="fu">mean</span>(age)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>mod4 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age_centered<span class="sc">*</span>smokes, <span class="at">family =</span> binomial)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(mod4), <span class="fu">confint</span>(mod4)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Waiting for profiling to be done...</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                            OR      2.5 %     97.5 %
(Intercept)          0.1686540 0.05274623  0.3977169
age_centered         1.1984690 1.10011853  1.3473795
smokes              15.6504935 4.96230836 62.2339766
age_centered:smokes  0.9046719 0.79508393  1.0087164</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>visreg<span class="sc">::</span><span class="fu">visreg</span>(mod4, </span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">xvar =</span> <span class="st">"age_centered"</span>, </span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">by =</span> <span class="st">"smokes"</span>, </span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">scale =</span> <span class="st">"response"</span>, </span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">overlay =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch10_logistic_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(chd.data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We will talk about the interpretation of this model in class.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch9_polynomial.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Polynomial regression, etc</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb66" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="an">fold:</span><span class="co"> true</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="an">editor:</span><span class="co"> </span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="co">  markdown: </span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="co">    wrap: 72</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--Review ALL of the workbook stuff before first lesson so you remember the numerical values for the examples. And you need to review ALL of the exercise stuff for the second lesson, so you can get through the example in order. --&gt;</span> </span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a><span class="fu"># Logistic regression {#sec-chap-10}</span></span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>The topic we address in this chapter is logistic regression. Like the previous two chapters, logistic regression is an extension of multiple linear regression to situations in which the "standard" model does not directly apply. This time, the extension is to binary *outcome* variables. </span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>There are many situations in which the outcome of interest can be thought of as a binary or “yes / no” or “true / false” outcome:</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Death (the original logistic outcome; <span class="co">[</span><span class="ot">Berkson, 1944</span><span class="co">](https://doi.org/10.2307%2F2280041)</span>)</span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Onset of disease or condition</span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Employment status</span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>College enrollment</span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Passing a course or completing a credential</span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Provide a correct response to a test question</span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>... </span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a>Just like with linear regression, we often we want to relate these types of binary variables to predictors, such as medical history, family background, personal characteristics, etc. That is what logistic regression does. </span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a>The main theme of this chapter is also an extension of the previous two chapters. We have seen a general strategy for how to deal with data that do not "fit" the assumptions of linear regression: </span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-28"><a href="#cb66-28" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Transform one or more variables so that the data *do* fit the assumptions linear regression. </span>
<span id="cb66-29"><a href="#cb66-29" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Run the analysis as usual.</span>
<span id="cb66-30"><a href="#cb66-30" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Then work out the interpretation of results in terms of the original variable(s). </span>
<span id="cb66-31"><a href="#cb66-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-32"><a href="#cb66-32" aria-hidden="true" tabindex="-1"></a>We will see this basic approach again in this chapter. In fact, its kind of a general-purpose hack for quantitative research -- when you are faced with a problem you don't know how to deal with, turn it into something you do know how to deal with. Certainly this is not the most creative approach, but it has the advantage of letting us "port over" many of the tools we have developed in one context (regression with a continuous outcome) into a new context (regression with a binary outcome). </span>
<span id="cb66-33"><a href="#cb66-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-34"><a href="#cb66-34" aria-hidden="true" tabindex="-1"></a>In terms of statistical modeling, the move to binary outcome variables is a pretty big deal. Everything we have done up until now has focused on OLS regression, using the same basic principles we discussed in Chapters @sec-chap-2 and  @sec-chap-3. However, logistic regression takes us into the wider framework of *generalized linear models* (GLMs), which are estimated using maximum likelihood (ML) rather than OLS. Thus we will need to start at the "ground floor" to build up our knowledge of logistic regression, which then provides a stepping stone to GLMs, which can additionally handle other types of outcome variables (e.g., count data, ordered categorical data). </span>
<span id="cb66-35"><a href="#cb66-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-36"><a href="#cb66-36" aria-hidden="true" tabindex="-1"></a>Since we are starting with a new modeling approach, let's kick things off with a new example. </span>
<span id="cb66-37"><a href="#cb66-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-38"><a href="#cb66-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## The CHD example {#sec-chd-example-10}</span></span>
<span id="cb66-39"><a href="#cb66-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-40"><a href="#cb66-40" aria-hidden="true" tabindex="-1"></a>As a working example, we will use data contained in the file <span class="in">`CHD.RData`</span> to explore the relationship between age in years ("age") and evidence (absence or presence) of coronary heart disease ("chd"). The data set contains 100 cases. Respondents' ages range from 20 to 69, while clinical evidence of CHD is coded 0 when it is absent and 1 when it is present. A sample of 20 cases is shown  below. (Source: Applied Logistic Regression by David W. Hosmer and Stanley Lemeshow, 1989, John Wiley and Sons.) </span>
<span id="cb66-41"><a href="#cb66-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-42"><a href="#cb66-42" aria-hidden="true" tabindex="-1"></a>For the sake of brevity, I will say that a person either "has CHD" or not. This is less accurate (but much simpler) than saying that the variable denotes clinical evidence about the presence or absence of CHD.</span>
<span id="cb66-43"><a href="#cb66-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-44"><a href="#cb66-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-45"><a href="#cb66-45" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.align='center'}</span></span>
<span id="cb66-46"><a href="#cb66-46" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"CHD.RData"</span>)</span>
<span id="cb66-47"><a href="#cb66-47" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(chd.data)</span>
<span id="cb66-48"><a href="#cb66-48" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">list</span>(chd.data[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="dv">10</span>),<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>],   </span>
<span id="cb66-49"><a href="#cb66-49" aria-hidden="true" tabindex="-1"></a>                  chd.data[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="dv">10</span>),<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]), </span>
<span id="cb66-50"><a href="#cb66-50" aria-hidden="true" tabindex="-1"></a>             <span class="at">row.names =</span> F, </span>
<span id="cb66-51"><a href="#cb66-51" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"The CHD example"</span>)</span>
<span id="cb66-52"><a href="#cb66-52" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-53"><a href="#cb66-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-54"><a href="#cb66-54" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb66-55"><a href="#cb66-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-56"><a href="#cb66-56" aria-hidden="true" tabindex="-1"></a>If we regress CHD on age using linear regression, this is referred to as the "linear probability model." The diagnostic plots and summary output are below: </span>
<span id="cb66-57"><a href="#cb66-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-58"><a href="#cb66-58" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-lpm, echo = F, fig.cap = "Linear probability model with CHD example", fig.align = 'center', fig.width = 12}</span></span>
<span id="cb66-59"><a href="#cb66-59" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(chd <span class="sc">~</span> age, <span class="at">data =</span> chd.data)</span>
<span id="cb66-60"><a href="#cb66-60" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb66-61"><a href="#cb66-61" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="dv">1</span>)</span>
<span id="cb66-62"><a href="#cb66-62" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="dv">2</span>)</span>
<span id="cb66-63"><a href="#cb66-63" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb66-64"><a href="#cb66-64" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-65"><a href="#cb66-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-66"><a href="#cb66-66" aria-hidden="true" tabindex="-1"></a>**Before moving on, please take a moment to write down you conclusions (and rationale) about whether the assumptions of linear regression are met for these data.**</span>
<span id="cb66-67"><a href="#cb66-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-68"><a href="#cb66-68" aria-hidden="true" tabindex="-1"></a>I'll note that researchers who have a strong preference for OLS methods (AKA economists) often approach binary outcomes using the linear probability model. As we can see, this approach violates all of the assumptions of linear regression, can lead to predicted probabilities outside of the range <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>, produces incorrect standard errors for model parameters (need to use HC standard errors), and is, in a word, wrong. Yet, despite all this, it works pretty well in some situations and has the benefit of being easier to interpret than logistic regression. We will consider the situations in which the linear probability model is "close enough" at the end of the next section. </span>
<span id="cb66-69"><a href="#cb66-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-70"><a href="#cb66-70" aria-hidden="true" tabindex="-1"></a><span class="fu">## Logit &amp; logistic functions {#sec-logit-10}</span></span>
<span id="cb66-71"><a href="#cb66-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-72"><a href="#cb66-72" aria-hidden="true" tabindex="-1"></a>The general game plan for dealing with a binary outcome is to transform it into a different variable that is easier to work with, run the analysis, and then "reverse-transform" the model coefficients so that they are interpretable in terms of the original binary variable. This strategy should sound familiar from @sec-chap-8 -- it's the same overall approach we used for log-linear regression. Also in common with @sec-chap-8, we are going to use logs and exponents as the main workhorse for this approach (that is where the "log" in logistic comes from).</span>
<span id="cb66-73"><a href="#cb66-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-74"><a href="#cb66-74" aria-hidden="true" tabindex="-1"></a>However, the overall strategy for transforming the $Y$ variable in logistic regression is a bit more complicated than the log-linear model. So, it is helpful to start wit an overall "roadmap".  </span>
<span id="cb66-75"><a href="#cb66-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-76"><a href="#cb66-76" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Step 1 (from binary to probability). First, we are going to work with probabilities rather than the original binary variable. In terms of our example, we are going to shift focus from whether or not a person has CHD to the *probability* of a person having CHD. </span>
<span id="cb66-77"><a href="#cb66-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-78"><a href="#cb66-78" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Step 2 (from probability to logistic). The logistic function is widely-used model for probabilities. In terms of our example, we are going to use the logistic function to relate the probability of a person having CHD to their age. </span>
<span id="cb66-79"><a href="#cb66-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-80"><a href="#cb66-80" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Step 3 (from logistic to logit). The logistic function has a nice interpretation, but it is not a linear function of age. So, we are going to transform it into something that is linear in age, which will let us "port over" a lot of what we have learned about linear models. Actually, the reason we choose the logistic function as a model of probability is because this transform is relatively straightforward and can be "undone" afterwards when interpreting the model coefficients, just like with log-linear regression. The transformation two steps:</span>
<span id="cb66-81"><a href="#cb66-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-82"><a href="#cb66-82" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Step3A (probability to odds). First we transform the probability of having CHD into the *odds* of having CHD. If $p$ denotes probability then odds are just $p / (1-p)$. We will spend a while talking about how to interpret odds. </span>
<span id="cb66-83"><a href="#cb66-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-84"><a href="#cb66-84" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Step 3B (odds to logit). Then we take the log of the odds, which is called the *logit*. The logit turns out to be a linear function of age, so we can model the relationship between age and the logit of CHD in a way that is very similar to regular linear regression. </span>
<span id="cb66-85"><a href="#cb66-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-86"><a href="#cb66-86" aria-hidden="true" tabindex="-1"></a>So, that's the overall approach to dealing with a binary variable in logistic regression. Clear as mud, right? Don't worry, we will walk through each step in the following subsections. If you find yourself getting lost in the details, it can be helpful to refer back to this overall strategy. In short, the overall game plan is: </span>
<span id="cb66-87"><a href="#cb66-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-88"><a href="#cb66-88" aria-hidden="true" tabindex="-1"></a>$$ \text{binary outcome} \rightarrow \text{probability} \rightarrow \text{logistic} \rightarrow \text{logit (log odds) }$$</span>
<span id="cb66-89"><a href="#cb66-89" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb66-90"><a href="#cb66-90" aria-hidden="true" tabindex="-1"></a>Once we have all these concepts in play, we can start doing logistic regression.</span>
<span id="cb66-91"><a href="#cb66-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-92"><a href="#cb66-92" aria-hidden="true" tabindex="-1"></a><span class="fu">### From binary to probability</span></span>
<span id="cb66-93"><a href="#cb66-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-94"><a href="#cb66-94" aria-hidden="true" tabindex="-1"></a>The following table presents the example data in terms of the proportion of cases with CHD, broken down by age groups. The first column shows the age groups, the second shows the number of cases without CHD, the third shows the number of cases with CHD, and the last column shows the proportion of cases with CHD. </span>
<span id="cb66-95"><a href="#cb66-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-96"><a href="#cb66-96" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-props, echo = T, fig.cap = "From a binary variable to proportions", fig.align = 'center'}</span></span>
<span id="cb66-97"><a href="#cb66-97" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/props.png"</span>)</span>
<span id="cb66-98"><a href="#cb66-98" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-99"><a href="#cb66-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-100"><a href="#cb66-100" aria-hidden="true" tabindex="-1"></a>Recall that a proportion is computed as the number of cases of interest over the total number of cases. In terms of the table above: </span>
<span id="cb66-101"><a href="#cb66-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-102"><a href="#cb66-102" aria-hidden="true" tabindex="-1"></a>$$ p(CHD = 1) = \frac{ N_1}{N_0 + N_1 } $$ {#eq-prob}</span>
<span id="cb66-103"><a href="#cb66-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-104"><a href="#cb66-104" aria-hidden="true" tabindex="-1"></a>were $N_1$ denotes the number of cases with CDH, and $N_0$ is the number of cases without. </span>
<span id="cb66-105"><a href="#cb66-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-106"><a href="#cb66-106" aria-hidden="true" tabindex="-1"></a>The number $p(CHD = 1)$ can be interpreted in many ways, which leads to a lot of terminology here. </span>
<span id="cb66-107"><a href="#cb66-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-108"><a href="#cb66-108" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The **proportion** of cases in our sample with CHD.</span>
<span id="cb66-109"><a href="#cb66-109" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If we multiply by 100, it is the **percentage** of cases with CHD (i.e., cases per 100) in our sample.</span>
<span id="cb66-110"><a href="#cb66-110" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If we multiply by a number other than 100 (say 1000), it is the **rate** of CHD (e.g., cases per 1000) in our sample.</span>
<span id="cb66-111"><a href="#cb66-111" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Since a proportion is just the mean of binary variable, it is the **mean** or expected value of CHD in our sample.</span>
<span id="cb66-112"><a href="#cb66-112" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>And finally, since proportions are one interpretation of probability, it is the **probability** of CHD in our sample. </span>
<span id="cb66-113"><a href="#cb66-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-114"><a href="#cb66-114" aria-hidden="true" tabindex="-1"></a>You might hear all of these terms (i.e., proportion, percentage, rate, mean, probability) used in connection with logistic regression. But, they are all just different ways of interpreting the rightmost column of @fig-props. I will try to make a point of using all of these terms so you get used to interpreting them in this context :)  </span>
<span id="cb66-115"><a href="#cb66-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-116"><a href="#cb66-116" aria-hidden="true" tabindex="-1"></a>Another concept that will be useful for interpreting our data is *odds*. Odds are closely related to, but not the same as, probability. The figure below adds the odds of having CHD to Figure @fig-props. </span>
<span id="cb66-117"><a href="#cb66-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-118"><a href="#cb66-118" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-odds, echo = T, fig.cap = "Proportions and odds", fig.align = 'center'}</span></span>
<span id="cb66-119"><a href="#cb66-119" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/odds.png"</span>)</span>
<span id="cb66-120"><a href="#cb66-120" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-121"><a href="#cb66-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-122"><a href="#cb66-122" aria-hidden="true" tabindex="-1"></a>As shown in the table, the odds are also a function of the two sample sizes, $N_1$ and $N_0$:</span>
<span id="cb66-123"><a href="#cb66-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-124"><a href="#cb66-124" aria-hidden="true" tabindex="-1"></a>$$\text{odds}(CHD = 1) = \frac{N_1}{N_0}.$$ {#eq-odds}</span>
<span id="cb66-125"><a href="#cb66-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-126"><a href="#cb66-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-127"><a href="#cb66-127" aria-hidden="true" tabindex="-1"></a>Let's take a moment to compare the interpretation of probability versus odds. </span>
<span id="cb66-128"><a href="#cb66-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-129"><a href="#cb66-129" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The first row of the table tells us that the probability of having CHD in your 20's is "1 in 10". Loosely, this means that for every 10 people in their 20s, one of them will have CHD. </span>
<span id="cb66-130"><a href="#cb66-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-131"><a href="#cb66-131" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>By contrast, the odds of having CHD in your twenties is "1 to 9". Roughly, this means that for every person in their twenties with CHD, there are nine without CHD. </span>
<span id="cb66-132"><a href="#cb66-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-133"><a href="#cb66-133" aria-hidden="true" tabindex="-1"></a>Clearly, probabilities and odds are just two different ways of packaging the same information. The following equations shows the relation between odds and probability (these are derived from Equations @eq-prob and @eq-odds using algebra)</span>
<span id="cb66-134"><a href="#cb66-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-135"><a href="#cb66-135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb66-136"><a href="#cb66-136" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb66-137"><a href="#cb66-137" aria-hidden="true" tabindex="-1"></a>p(CHD = 1) &amp; = \frac{\text{odds}(CHD = 1)}{1 + \text{odds}(CHD = 1)} <span class="sc">\\</span> <span class="sc">\\</span></span>
<span id="cb66-138"><a href="#cb66-138" aria-hidden="true" tabindex="-1"></a>\text{odds}(CHD = 1) &amp; = \frac{p(CHD = 1)}{1 - p(CHD = 1)} </span>
<span id="cb66-139"><a href="#cb66-139" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb66-140"><a href="#cb66-140" aria-hidden="true" tabindex="-1"></a>$$ {#eq-p2o}</span>
<span id="cb66-141"><a href="#cb66-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-142"><a href="#cb66-142" aria-hidden="true" tabindex="-1"></a>We will see these relations again shortly. But, before moving on, let's get some more practice interpreting odds and probabilities using the data in @fig-odds. **Please write down your answers to the following questions and be prepared to share them in class. For each question provide a verbal interpretation of the numerical answer (e.g, odds of 2 to 1 means that for every two people with a trait, there is one without.) **</span>
<span id="cb66-143"><a href="#cb66-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-144"><a href="#cb66-144" aria-hidden="true" tabindex="-1"></a><span class="ss">  1. </span>What is the probability of a person in their 40s having CHD? </span>
<span id="cb66-145"><a href="#cb66-145" aria-hidden="true" tabindex="-1"></a><span class="ss">  2. </span>What are the odds of a person in their 40s having CHD? </span>
<span id="cb66-146"><a href="#cb66-146" aria-hidden="true" tabindex="-1"></a><span class="ss">  3. </span>What is the probability of someone in their 50s **not** having CHD? </span>
<span id="cb66-147"><a href="#cb66-147" aria-hidden="true" tabindex="-1"></a><span class="ss">  4. </span>What are the odds of someone in their 50s **not** having CHD? </span>
<span id="cb66-148"><a href="#cb66-148" aria-hidden="true" tabindex="-1"></a><span class="ss">  5. </span>What is probability of having CHD in your 40s, compared to your 30s? (e.g., is 3 times higher? 4 times higher?)</span>
<span id="cb66-149"><a href="#cb66-149" aria-hidden="true" tabindex="-1"></a><span class="ss">  6. </span>What are the odds of having CHD in your 40s, compared to your 30s? </span>
<span id="cb66-150"><a href="#cb66-150" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb66-151"><a href="#cb66-151" aria-hidden="true" tabindex="-1"></a>The answers hidden below (use the Code button to reveal), but you won't learn anything if you don't try the question yourself first! </span>
<span id="cb66-152"><a href="#cb66-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-155"><a href="#cb66-155" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-156"><a href="#cb66-156" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. .39, so about 40% of people</span></span>
<span id="cb66-157"><a href="#cb66-157" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 11/17, so for 11 people with CHD there are 17 without</span></span>
<span id="cb66-158"><a href="#cb66-158" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 1 - .72 = .28, so about 28% of people </span></span>
<span id="cb66-159"><a href="#cb66-159" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. (18/7)^-1 = 7/18, so 7 out ever 18 people</span></span>
<span id="cb66-160"><a href="#cb66-160" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. .39 / .19 ~= 2, so the probability of having CHD in your 40s is about 2 times higher than the probability of having CHD in your 30s. This is called a relative risk, or a risk ratio. </span></span>
<span id="cb66-161"><a href="#cb66-161" aria-hidden="true" tabindex="-1"></a><span class="co">#6. (11/17)/(5/22) ~= 2.8, so the odds of having CHD in your 40s is about 2.8 times higher than the odds of having CHD in your 30s. This is called an odds ratio. </span></span>
<span id="cb66-162"><a href="#cb66-162" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-163"><a href="#cb66-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-164"><a href="#cb66-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### From probability to logistic </span></span>
<span id="cb66-165"><a href="#cb66-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-166"><a href="#cb66-166" aria-hidden="true" tabindex="-1"></a>On thing you may have noted about the CHD data is that the proportion of cases with CHD increases with age. This relationship is shown visually in @fig-chd2.</span>
<span id="cb66-167"><a href="#cb66-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-168"><a href="#cb66-168" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-chd2, echo = T, fig.cap = "Proportion of cases with CHD as a function of age", fig.align = 'center'}</span></span>
<span id="cb66-169"><a href="#cb66-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-170"><a href="#cb66-170" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample proportions</span></span>
<span id="cb66-171"><a href="#cb66-171" aria-hidden="true" tabindex="-1"></a>prop <span class="ot">&lt;-</span> <span class="fu">tapply</span>(chd, catage, mean)</span>
<span id="cb66-172"><a href="#cb66-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-173"><a href="#cb66-173" aria-hidden="true" tabindex="-1"></a><span class="co"># Age categories</span></span>
<span id="cb66-174"><a href="#cb66-174" aria-hidden="true" tabindex="-1"></a>years <span class="ot">&lt;-</span> <span class="fu">unique</span>(catage)<span class="sc">*</span><span class="dv">10</span></span>
<span id="cb66-175"><a href="#cb66-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-176"><a href="#cb66-176" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb66-177"><a href="#cb66-177" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(years, </span>
<span id="cb66-178"><a href="#cb66-178" aria-hidden="true" tabindex="-1"></a>     prop, </span>
<span id="cb66-179"><a href="#cb66-179" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb66-180"><a href="#cb66-180" aria-hidden="true" tabindex="-1"></a>     <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb66-181"><a href="#cb66-181" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, </span>
<span id="cb66-182"><a href="#cb66-182" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"p(CHD =1)"</span>, </span>
<span id="cb66-183"><a href="#cb66-183" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Age categories"</span>)</span>
<span id="cb66-184"><a href="#cb66-184" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-185"><a href="#cb66-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-186"><a href="#cb66-186" aria-hidden="true" tabindex="-1"></a>Looking at the plot, we might suspect that the relationship between the probability of CHD and age is non-linear. In particular, we know that probabilities cannot take on values outside of the range $(0, 1)$, so the relationship is going to have to "flatten out" in the tails. For example, even if you are a baby, your probability of having CHD cannot be less than 0. And, even if you are centenarian, the probability can't be great than 1. </span>
<span id="cb66-187"><a href="#cb66-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-188"><a href="#cb66-188" aria-hidden="true" tabindex="-1"></a>Based on this reasoning, we know that the relationship between age and the rate of CHD should take on a sort of  "S-shaped" curve or "sigmoid". This S-shape is hinted at in @fig-chd2 but is not very clear. Some clearer examples are shown in @fig-sigmoids. </span>
<span id="cb66-189"><a href="#cb66-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-190"><a href="#cb66-190" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-sigmoids, echo = T, fig.cap = "Examples of sigmoids", fig.align = 'center'}</span></span>
<span id="cb66-191"><a href="#cb66-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-192"><a href="#cb66-192" aria-hidden="true" tabindex="-1"></a><span class="co"># Logistic function</span></span>
<span id="cb66-193"><a href="#cb66-193" aria-hidden="true" tabindex="-1"></a>logistic <span class="ot">&lt;-</span> <span class="cf">function</span>(x, a, b){<span class="fu">exp</span>(a<span class="sc">*</span>x <span class="sc">+</span> b) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(a<span class="sc">*</span>x <span class="sc">+</span> b))}</span>
<span id="cb66-194"><a href="#cb66-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-195"><a href="#cb66-195" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb66-196"><a href="#cb66-196" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">by =</span> .<span class="dv">1</span>)</span>
<span id="cb66-197"><a href="#cb66-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-198"><a href="#cb66-198" aria-hidden="true" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb66-199"><a href="#cb66-199" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">logistic</span>(x, <span class="dv">1</span>, <span class="dv">0</span>), </span>
<span id="cb66-200"><a href="#cb66-200" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb66-201"><a href="#cb66-201" aria-hidden="true" tabindex="-1"></a>     <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb66-202"><a href="#cb66-202" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="dv">2</span>, </span>
<span id="cb66-203"><a href="#cb66-203" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"logistic"</span>)</span>
<span id="cb66-204"><a href="#cb66-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-205"><a href="#cb66-205" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="fu">logistic</span>(x, .<span class="dv">75</span>, <span class="sc">-</span><span class="fl">1.5</span>), </span>
<span id="cb66-206"><a href="#cb66-206" aria-hidden="true" tabindex="-1"></a>       <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb66-207"><a href="#cb66-207" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb66-208"><a href="#cb66-208" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="dv">3</span>, </span>
<span id="cb66-209"><a href="#cb66-209" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">"logistic"</span>)</span>
<span id="cb66-210"><a href="#cb66-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-211"><a href="#cb66-211" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="fu">logistic</span>(x, <span class="fl">1.5</span>,<span class="sc">-</span> <span class="dv">1</span>), </span>
<span id="cb66-212"><a href="#cb66-212" aria-hidden="true" tabindex="-1"></a>       <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb66-213"><a href="#cb66-213" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb66-214"><a href="#cb66-214" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="dv">4</span>, </span>
<span id="cb66-215"><a href="#cb66-215" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">"logistic"</span>)</span>
<span id="cb66-216"><a href="#cb66-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-217"><a href="#cb66-217" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="fu">logistic</span>(x, <span class="dv">3</span>, <span class="dv">2</span>), </span>
<span id="cb66-218"><a href="#cb66-218" aria-hidden="true" tabindex="-1"></a>       <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb66-219"><a href="#cb66-219" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb66-220"><a href="#cb66-220" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="dv">5</span>, </span>
<span id="cb66-221"><a href="#cb66-221" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">"logistic"</span>)</span>
<span id="cb66-222"><a href="#cb66-222" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-223"><a href="#cb66-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-224"><a href="#cb66-224" aria-hidden="true" tabindex="-1"></a>The mathematical equation used to create these S-shaped curves is called the logistic function, the namesake of logistic regression. All you need to take-away from @fig-sigmoids is that there is mathematical function that produces the kind of relations we are expecting between age (continuous) the the probability of having CHD (bounded to the interval $(0, 1)$). </span>
<span id="cb66-225"><a href="#cb66-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-226"><a href="#cb66-226" aria-hidden="true" tabindex="-1"></a>Returning to our example, we can see in @fig-chd3 that the logistic function provides a reasonable approximation for the relationship between the rate of CHD and age. </span>
<span id="cb66-227"><a href="#cb66-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-228"><a href="#cb66-228" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-chd3, echo = T, fig.cap = "Proportion of cases with CHD, data versus logistic", fig.align = 'center'}</span></span>
<span id="cb66-229"><a href="#cb66-229" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb66-230"><a href="#cb66-230" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(years, prop, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">ylab =</span> <span class="st">"p(CHD =1)"</span>, <span class="at">xlab =</span> <span class="st">"Age categories"</span>)</span>
<span id="cb66-231"><a href="#cb66-231" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">20</span><span class="sc">:</span><span class="dv">60</span>, <span class="fu">logistic</span>(<span class="dv">20</span><span class="sc">:</span><span class="dv">60</span>, .<span class="dv">12</span>, <span class="sc">-</span><span class="fl">5.2</span>), <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">ylab =</span> <span class="st">"logistic"</span>, <span class="at">xlab =</span> <span class="st">"Age in years"</span>)</span>
<span id="cb66-232"><a href="#cb66-232" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-233"><a href="#cb66-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-234"><a href="#cb66-234" aria-hidden="true" tabindex="-1"></a>One important thing to notice about @fig-chd3 is that the plot on the left required re-coding age into a categorical variable and computing the proportion of cases with CHD in each age category (see @fig-props). However, the logistic plot on the right did not require categorizing age. So, one advantage of using the logistic function is that we can model the probability of CHD as a function of age "directly", without having to categorize our predictor variables. </span>
<span id="cb66-235"><a href="#cb66-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-236"><a href="#cb66-236" aria-hidden="true" tabindex="-1"></a>The take home message of this section is that the logistic function is a nice way to model how a proportion depends on a continuous variable like age. Next, we'll talk about the math of the logistic function in a bit more detail. </span>
<span id="cb66-237"><a href="#cb66-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-238"><a href="#cb66-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-239"><a href="#cb66-239" aria-hidden="true" tabindex="-1"></a><span class="fu">### Logistic to log odds (logit) </span></span>
<span id="cb66-240"><a href="#cb66-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-241"><a href="#cb66-241" aria-hidden="true" tabindex="-1"></a>The formula for the logistic function (i.e., the function that produced the curves in @fig-sigmoids is</span>
<span id="cb66-242"><a href="#cb66-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-243"><a href="#cb66-243" aria-hidden="true" tabindex="-1"></a>$$p = \frac{\exp(x)}{1 + \exp(x)}.$$ {#eq-logistic}</span>
<span id="cb66-244"><a href="#cb66-244" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb66-245"><a href="#cb66-245" aria-hidden="true" tabindex="-1"></a>This function maps the variable $x$ onto the interval $(0, 1)$. In @fig-chd3 we saw that the logistic function can provide a nice model for probabilities. We also saw that the logistic function is  non-linear function of $x$ (i.e., it is sigmoidal or S-shaped). </span>
<span id="cb66-246"><a href="#cb66-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-247"><a href="#cb66-247" aria-hidden="true" tabindex="-1"></a>However, a nice thing about the logistic function is that we can transform it into a linear function of $x$. Since we already know how to deal with linear functions (that is what this whole course has been about!), transforming the logistic into a linear function of $x$ will let us port over a lot of what we know about linear regression to situations in which the outcome variable is binary. (In fact, the real motivation for choosing the logistic function in the first place, rather than some other S-shaped curve.)</span>
<span id="cb66-248"><a href="#cb66-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-249"><a href="#cb66-249" aria-hidden="true" tabindex="-1"></a>So, let's see how to get from our S-shaped logistic function of $x$ to a linear function of $x$. Algebra with @eq-logistic shows that we can re-express the logistic function in terms of the odds: </span>
<span id="cb66-250"><a href="#cb66-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-251"><a href="#cb66-251" aria-hidden="true" tabindex="-1"></a>$$\frac{p}{1- p} = \exp(x).$$ {#eq-exp-odds}</span>
<span id="cb66-252"><a href="#cb66-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-253"><a href="#cb66-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-254"><a href="#cb66-254" aria-hidden="true" tabindex="-1"></a>Note that Equations @eq-logistic and @eq-exp-odds directly parallel the two expressions in Equation @eq-p2o. The only difference is that, in the logistic model, the odds are represented as an exponential function of the variable $x$, which is what Equation @eq-exp-odds is telling us. </span>
<span id="cb66-255"><a href="#cb66-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-256"><a href="#cb66-256" aria-hidden="true" tabindex="-1"></a>In order to turn @eq-exp-odds into a linear function of $x$, all we need to do is get rid of the exponent. Do you remember how?? That's right, just take the log (see @sec-math-review-8): </span>
<span id="cb66-257"><a href="#cb66-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-258"><a href="#cb66-258" aria-hidden="true" tabindex="-1"></a>$$ \log\left(\frac{p}{1- p}\right) = x.$$ {#eq-logit}</span>
<span id="cb66-259"><a href="#cb66-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-260"><a href="#cb66-260" aria-hidden="true" tabindex="-1"></a>This equation is telling us that the log of the odds is linear in $x$. The log-odds is also called the *logit*, which is short for "logistic unit." </span>
<span id="cb66-261"><a href="#cb66-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-262"><a href="#cb66-262" aria-hidden="true" tabindex="-1"></a>The relationship among the logistic, odds, and logit are summarized  in @fig-logit. </span>
<span id="cb66-263"><a href="#cb66-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-264"><a href="#cb66-264" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-logit, echo = T, fig.cap = "Logistic, odds, and logit", fig.align = 'center'}</span></span>
<span id="cb66-265"><a href="#cb66-265" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/logit.png"</span>)</span>
<span id="cb66-266"><a href="#cb66-266" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-267"><a href="#cb66-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-268"><a href="#cb66-268" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The left-hand panel shows the logistic function. This is our "intuitive-but-nonlinear" model for probabilities. In terms of our example, this panel is saying that the probability of having CHD is a logistic or S-shaped function of age. </span>
<span id="cb66-269"><a href="#cb66-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-270"><a href="#cb66-270" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The middle panel shows that the odds are an exponential function of $x$. In terms of our example, this means that the odds of having CHD are an exponential function of age. This is the main assumption of the logistic model, and we will revisit this assumption again when we get to @sec-assumption-checking-10.  </span>
<span id="cb66-271"><a href="#cb66-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-272"><a href="#cb66-272" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Finally, the right-hand panel shows the "not-really-intuitive-but-definitely-linear" model for the logit. In terms of our example, the logit of having CHD is a linear function of age. </span>
<span id="cb66-273"><a href="#cb66-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-274"><a href="#cb66-274" aria-hidden="true" tabindex="-1"></a>The next section discusses how to interpret the logit by reverse-transforming it back to the odds and probabilities. The situation is a lot like log-linear regression (@sec-chap-9). </span>
<span id="cb66-275"><a href="#cb66-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-276"><a href="#cb66-276" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pop quiz </span></span>
<span id="cb66-277"><a href="#cb66-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-278"><a href="#cb66-278" aria-hidden="true" tabindex="-1"></a>Before moving, lets nail down the relation between probability, odds, and logits. @fig-logit-table) presents the relationship in tabular form. </span>
<span id="cb66-279"><a href="#cb66-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-280"><a href="#cb66-280" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-logit-table, echo = F, fig.cap = "Logistic, odds, and logit", fig.align = 'center'}</span></span>
<span id="cb66-281"><a href="#cb66-281" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/logit-table.png"</span>)</span>
<span id="cb66-282"><a href="#cb66-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-283"><a href="#cb66-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-284"><a href="#cb66-284" aria-hidden="true" tabindex="-1"></a>**I will asks some questions along the following lines in class.**</span>
<span id="cb66-285"><a href="#cb66-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-286"><a href="#cb66-286" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If probability of an event is equal to .1, what are the odds the event? What is the logit of the event? </span>
<span id="cb66-287"><a href="#cb66-287" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If odds of an event are 4 to 1, what is the probability of the event? What is the logit of the event? </span>
<span id="cb66-288"><a href="#cb66-288" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If logit &lt; 0 then probability &lt;  ? and odds &lt; ? </span>
<span id="cb66-289"><a href="#cb66-289" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>What is more likely: a event with probability of .9 or an event with odds of .9? </span>
<span id="cb66-290"><a href="#cb66-290" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If a probability of $p$ corresponds to a  $\text{logit}$ of $x$, what is the logit corresponding to $1-p$? (Hint, try some numerical examples from the table). </span>
<span id="cb66-291"><a href="#cb66-291" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb66-292"><a href="#cb66-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-293"><a href="#cb66-293" aria-hidden="true" tabindex="-1"></a><span class="fu">### Next steps </span></span>
<span id="cb66-294"><a href="#cb66-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-295"><a href="#cb66-295" aria-hidden="true" tabindex="-1"></a>The logit is our workhorse for logistic regression. In  @sec-simple-10, we will replace the variable $x$ with a simple regression model $a + bX$ to get simple logistic regression. In @sec-multiple-10 we will extend simple logistic regression to multiple logistic regression, just like we did for multiple linear regression. </span>
<span id="cb66-296"><a href="#cb66-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-297"><a href="#cb66-297" aria-hidden="true" tabindex="-1"></a>Although the logit is the workhorse, we generally don't want to work with the logit when it comes time to interpret the results. The situation here is a lot like log-linear regression (@sec-chap-9). In log-linear regression, we treated $\log(Y)$ as a linear function of our predictor variable(s). However, we didn't want to interpret the model in terms of $\log(Y)$, because, well, who thinks in log units? Instead we wanted an interpretation in terms of the original outcome, $Y$.</span>
<span id="cb66-298"><a href="#cb66-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-299"><a href="#cb66-299" aria-hidden="true" tabindex="-1"></a>The same situation applies here. You may have already noted that the relationship between the logit (i.e., $\log(\text{odds})$) and $\text{odds}$ in logistic regression is the same as the relationship between $\log(Y)$ and $Y$ in log-linear regression. The parallel between the two model is as follows: </span>
<span id="cb66-300"><a href="#cb66-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-301"><a href="#cb66-301" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In the log-linear model we interpreted a $b$ unit increase in $\log(Y)$ in terms of an $(\exp(b) - 1) \times 100\%$ change in $Y$ (see Section @ref-interpretation-8)). </span>
<span id="cb66-302"><a href="#cb66-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-303"><a href="#cb66-303" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In logistic regression we will interpret a $b$ unit increase in $\text{logit}(Y)$ in terms of an $(\exp(b) - 1) \times 100\%$ times change in $\text{odds}(Y)$. </span>
<span id="cb66-304"><a href="#cb66-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-305"><a href="#cb66-305" aria-hidden="true" tabindex="-1"></a>So, while we use the logit function for modeling, we often use the odds for interpretation. One subtle difference to be aware of is that, in logistic regression, we usually report results in terms of relative magnitude (called the odds ratio) rather than relative change, although relative change is often used for verbal reporting. We will see examples in the next section.   </span>
<span id="cb66-306"><a href="#cb66-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-307"><a href="#cb66-307" aria-hidden="true" tabindex="-1"></a>Some authors have argued that people don't really know how to interpret odds properly. These authors suggest that we interpret the logistic model in terms of probabilities, rather than odds. We will discuss how to do this as well.</span>
<span id="cb66-308"><a href="#cb66-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-309"><a href="#cb66-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-310"><a href="#cb66-310" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summary</span></span>
<span id="cb66-311"><a href="#cb66-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-312"><a href="#cb66-312" aria-hidden="true" tabindex="-1"></a>At this point we have covered the overall logic of how we can model a binary outcome variable like CHD in terms of the logistic function. The overall situation is very similar to, but a bit more complicated than, log-linear regression. The main take aways are</span>
<span id="cb66-313"><a href="#cb66-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-314"><a href="#cb66-314" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We use the logit (log-odds) for statistical analysis, because it results in a linear function, and we already know how to deal with linear functions. </span>
<span id="cb66-315"><a href="#cb66-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-316"><a href="#cb66-316" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We use the odds for interpretation, because the logistic model leads to proportional change in the odds, in the same way that the log-linear model leads to proportional change in $Y$. </span>
<span id="cb66-317"><a href="#cb66-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-318"><a href="#cb66-318" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We can also use probabilities for interpretation, but, since the logistic model implies that probabilities are non-linear (sigmoidal), things can get a bit complicated with this approach. </span>
<span id="cb66-319"><a href="#cb66-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-320"><a href="#cb66-320" aria-hidden="true" tabindex="-1"></a><span class="fu">## Simple logistic regression {#sec-simple-10}</span></span>
<span id="cb66-321"><a href="#cb66-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-322"><a href="#cb66-322" aria-hidden="true" tabindex="-1"></a>In this section we move onto logistic regression proper. For the CHD example, the model we are interested in is </span>
<span id="cb66-323"><a href="#cb66-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-324"><a href="#cb66-324" aria-hidden="true" tabindex="-1"></a>$$\text{logit}(CHD) = a + b (\text{age}). $$</span>
<span id="cb66-325"><a href="#cb66-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-326"><a href="#cb66-326" aria-hidden="true" tabindex="-1"></a>We are going to skip a few steps and go right into the interpretation of the R output. Once we know how to interpret the output, we will loop back to discuss details of estimation and inference in the following sections. </span>
<span id="cb66-327"><a href="#cb66-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-328"><a href="#cb66-328" aria-hidden="true" tabindex="-1"></a>The summary R output for the example is below. The focus for now is just the interpretation of the values under the "Estimate" heading. </span>
<span id="cb66-329"><a href="#cb66-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-332"><a href="#cb66-332" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-333"><a href="#cb66-333" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age, <span class="at">family =</span> binomial, <span class="at">data =</span> chd.data)</span>
<span id="cb66-334"><a href="#cb66-334" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span>
<span id="cb66-335"><a href="#cb66-335" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-336"><a href="#cb66-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-337"><a href="#cb66-337" aria-hidden="true" tabindex="-1"></a>Plugging the estimates into our logit model, we have the following equation</span>
<span id="cb66-338"><a href="#cb66-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-339"><a href="#cb66-339" aria-hidden="true" tabindex="-1"></a>$$ \text{logit}(CHD) = -5.31  + .11 (\text{age}). $$</span>
<span id="cb66-340"><a href="#cb66-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-341"><a href="#cb66-341" aria-hidden="true" tabindex="-1"></a>The "literal" interpretation of this equation is: </span>
<span id="cb66-342"><a href="#cb66-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-343"><a href="#cb66-343" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>When $\text{age} = 0$, $\text{logit}(CHD) = -5.31$.</span>
<span id="cb66-344"><a href="#cb66-344" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Each unit of increase in age (i.e., each additional year) is associated with a .11 unit increase in $\text{logit}(CHD)$. </span>
<span id="cb66-345"><a href="#cb66-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-346"><a href="#cb66-346" aria-hidden="true" tabindex="-1"></a>While this interpretation is perfectly correct, most applied audiences are not going to know how to interpret   $\text{logit}(CHD)$. So, instead, we often work with the odds and probabilities, as outlined in the next few sections. </span>
<span id="cb66-347"><a href="#cb66-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-348"><a href="#cb66-348" aria-hidden="true" tabindex="-1"></a><span class="fu">### Odds ratio</span></span>
<span id="cb66-349"><a href="#cb66-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-350"><a href="#cb66-350" aria-hidden="true" tabindex="-1"></a>The logistic regression model implies</span>
<span id="cb66-351"><a href="#cb66-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-352"><a href="#cb66-352" aria-hidden="true" tabindex="-1"></a>$$\frac{\text{odds} (X+1)}{\text{odds}(X)} = \exp(b)$$ {#eq-OR}</span>
<span id="cb66-353"><a href="#cb66-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-354"><a href="#cb66-354" aria-hidden="true" tabindex="-1"></a>where $\text{odds}(X)$ are the odds of the outcome associated with a given value of the predictor $X$. Equation @eq-OR is called the *odds ratio* (abbreviated OR) associated with a one-unit increase in $X$. </span>
<span id="cb66-355"><a href="#cb66-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-356"><a href="#cb66-356" aria-hidden="true" tabindex="-1"></a>If you refer back to section @sec-derivation-8, you can see we are using the exact same approach from log-linear regression, but in @eq-OR we interpret the regression coefficient in term of the odds that $Y = 1$, rather than the $Y$ variable itself.  </span>
<span id="cb66-357"><a href="#cb66-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-358"><a href="#cb66-358" aria-hidden="true" tabindex="-1"></a>For the CHD example, the OR is:</span>
<span id="cb66-359"><a href="#cb66-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-360"><a href="#cb66-360" aria-hidden="true" tabindex="-1"></a>$$\exp(b) = \exp(.11) = 1.1163 $$</span>
<span id="cb66-361"><a href="#cb66-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-362"><a href="#cb66-362" aria-hidden="true" tabindex="-1"></a>This means that each additional year of age is associated with an OR of 1.11. For example, the odds for someone aged 21 having CHD is 1.11 time larger (relative magnitude) that someone aged 20. The really useful thing about the OR is that it is constant over values of the predictor. So, regardless of whether we are comparing a 21-year-old to a 20-year-old, or 41-year-old to a 40-year-old, the OR is the same. </span>
<span id="cb66-363"><a href="#cb66-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-364"><a href="#cb66-364" aria-hidden="true" tabindex="-1"></a>Just like the log-linear model, we can also report the results of our analysis in terms of relative change rather than relative magnitude. In particular, the percent increase in the odds of CHD associated with each additional year of age is: </span>
<span id="cb66-365"><a href="#cb66-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-366"><a href="#cb66-366" aria-hidden="true" tabindex="-1"></a>$$(\exp(.11) - 1) \times 100 = 11.63\% $$</span>
<span id="cb66-367"><a href="#cb66-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-368"><a href="#cb66-368" aria-hidden="true" tabindex="-1"></a>This means that the predicted odds of CHD increase 11.63% for each additional year of age. </span>
<span id="cb66-369"><a href="#cb66-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-370"><a href="#cb66-370" aria-hidden="true" tabindex="-1"></a>Whether you use relative magnitude (i.e., the odds ratio) or relative change (i.e., percent change in odds) to report the results of logistic regression is up to you. In many fields, it is conventional to reports the odds ratios in tables, but to use percent change when writing about results in a sentence.</span>
<span id="cb66-371"><a href="#cb66-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-372"><a href="#cb66-372" aria-hidden="true" tabindex="-1"></a>Before moving, **please practice your interpretation of the OR in simple logistic regression using the following examples**</span>
<span id="cb66-373"><a href="#cb66-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-374"><a href="#cb66-374" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If $b=0$ what is the OR equal to? What is the percent change in the odds for a one unit increase in $X$? </span>
<span id="cb66-375"><a href="#cb66-375" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If $b=.25$ what is the OR equal to? What is the percent change in the odds for a one unit increase in $X$? </span>
<span id="cb66-376"><a href="#cb66-376" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If $b=-.025$ what is the OR equal to? What is the percent change in the odds for a 10 unit increase in $X$? </span>
<span id="cb66-377"><a href="#cb66-377" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If the odds increase 100% for a one unit increase in $X$, what $b$ equal to? </span>
<span id="cb66-378"><a href="#cb66-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-379"><a href="#cb66-379" aria-hidden="true" tabindex="-1"></a>Answers hidden below (use Code button), but please try out the questions yourself first! </span>
<span id="cb66-380"><a href="#cb66-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-383"><a href="#cb66-383" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-384"><a href="#cb66-384" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. OR = exp(0) = 1 and percent change equals (exp(0) - 1) X 100 = 0%. So, "no relationship" means OR = 1. </span></span>
<span id="cb66-385"><a href="#cb66-385" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. OR = exp(.25) = 1.2840 and percent change equals (exp(.25) - 1) X 100 = 28.40% increase</span></span>
<span id="cb66-386"><a href="#cb66-386" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. OR = exp(-.025) = 0.9753 and percent change for one unit equals (exp(-.025) - 1) X 100 = (-.02469 X 100 = -2.469%. For 10 units of change, multiply by 10, which gives 24.69% decrease (negative sign is decrease). </span></span>
<span id="cb66-387"><a href="#cb66-387" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. (exp(b) - 1) X 100 = 100 --&gt; exp(b) = 2 --&gt; b = log(2) = .6931</span></span>
<span id="cb66-388"><a href="#cb66-388" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-389"><a href="#cb66-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-390"><a href="#cb66-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-391"><a href="#cb66-391" aria-hidden="true" tabindex="-1"></a><span class="fu">### Other interpretations: Predicted probabilities</span></span>
<span id="cb66-392"><a href="#cb66-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-393"><a href="#cb66-393" aria-hidden="true" tabindex="-1"></a>Another way to interpret the logistic model is in terms of the predicted probabilities, which are plotted below for the example data. </span>
<span id="cb66-394"><a href="#cb66-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-395"><a href="#cb66-395" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-pred-prob, echo = T, fig.cap = "Predicted probabilities", fig.align = 'center'}</span></span>
<span id="cb66-396"><a href="#cb66-396" aria-hidden="true" tabindex="-1"></a>visreg<span class="sc">::</span><span class="fu">visreg</span>(mod2, <span class="at">xvar =</span> <span class="st">"age"</span>, <span class="at">scale =</span> <span class="st">"response"</span>)</span>
<span id="cb66-397"><a href="#cb66-397" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-398"><a href="#cb66-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-399"><a href="#cb66-399" aria-hidden="true" tabindex="-1"></a>Using this plot, we can read off the probability of CHD for any given age. We might also want to report the probability of CHD for two or more chosen ages, which is an example of the MERV approach to marginal effects (see @sec-inference-for-interactions-5):  </span>
<span id="cb66-400"><a href="#cb66-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-403"><a href="#cb66-403" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-404"><a href="#cb66-404" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(emmeans)</span>
<span id="cb66-405"><a href="#cb66-405" aria-hidden="true" tabindex="-1"></a><span class="fu">emmeans</span>(mod2, <span class="at">specs =</span> <span class="st">"age"</span>, <span class="at">at =</span> <span class="fu">list</span>(<span class="at">age =</span> <span class="fu">c</span>(<span class="dv">20</span>, <span class="dv">40</span>)), <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb66-406"><a href="#cb66-406" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-407"><a href="#cb66-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-408"><a href="#cb66-408" aria-hidden="true" tabindex="-1"></a>The ratio of two probabilities is often called the *risk ratio* or the *relative risk*. So, we could also say that the risk ratio of CHD for someone who in their 40s as compared to someone who is 20 is .2947 / .0435 = 6.77. Otherwise stated, the risk of having CHD in you are 40s is almost 7 times higher than in your 20s (relative magnitude).</span>
<span id="cb66-409"><a href="#cb66-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-410"><a href="#cb66-410" aria-hidden="true" tabindex="-1"></a><span class="fu">### Other interpretations: Equal odds</span></span>
<span id="cb66-411"><a href="#cb66-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-412"><a href="#cb66-412" aria-hidden="true" tabindex="-1"></a>Another interpretation of logistic regression is to report the value of $X$ at which the $\text{odds}$ of the outcome are equal to 1 (equivalently, the probability of the outcome is equal to .5). This idea is illustrated in @fig-equal-odds. </span>
<span id="cb66-413"><a href="#cb66-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-414"><a href="#cb66-414" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-equal-odds, echo = T, fig.cap = "The Equal Odds Interpretation", fig.align = 'center'}</span></span>
<span id="cb66-415"><a href="#cb66-415" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">age =</span> <span class="dv">20</span><span class="sc">:</span><span class="dv">70</span>)</span>
<span id="cb66-416"><a href="#cb66-416" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod2, <span class="at">newdata =</span> x, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb66-417"><a href="#cb66-417" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x<span class="sc">$</span>age, prob, <span class="at">xlab =</span> <span class="st">"age"</span>, <span class="at">ylab =</span> <span class="st">"p(CHD)"</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb66-418"><a href="#cb66-418" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> <span class="dv">15</span>, <span class="at">y0 =</span> .<span class="dv">5</span>, <span class="at">x1 =</span> <span class="dv">48</span>, <span class="at">y1 =</span> .<span class="dv">5</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb66-419"><a href="#cb66-419" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> <span class="dv">48</span>, <span class="at">y0 =</span> <span class="dv">0</span>, <span class="at">x1 =</span> <span class="dv">48</span>, <span class="at">y1 =</span> .<span class="dv">5</span>, <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb66-420"><a href="#cb66-420" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-421"><a href="#cb66-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-422"><a href="#cb66-422" aria-hidden="true" tabindex="-1"></a>First we find the probability of .5 on the $Y$ axis and then follow the horizontal dashed line to the logistic curve. Then we follow the vertical dashed line down to the value of $X$. This gives use the age at which the probability of CHD is "50-50". Based on the plot we can say that, after your 48th birthday, your chances of having CHD are above 50%. </span>
<span id="cb66-423"><a href="#cb66-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-424"><a href="#cb66-424" aria-hidden="true" tabindex="-1"></a>The math behind this interpretation is below. Since</span>
<span id="cb66-425"><a href="#cb66-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-426"><a href="#cb66-426" aria-hidden="true" tabindex="-1"></a>$$\log(.5/.5) = \log(1) = 0 $$</span>
<span id="cb66-427"><a href="#cb66-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-428"><a href="#cb66-428" aria-hidden="true" tabindex="-1"></a>we can solve </span>
<span id="cb66-429"><a href="#cb66-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-430"><a href="#cb66-430" aria-hidden="true" tabindex="-1"></a>$$ a + b(\text{age}) = 0 $$</span>
<span id="cb66-431"><a href="#cb66-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-432"><a href="#cb66-432" aria-hidden="true" tabindex="-1"></a>to find the age at which someone has equal odds of CHD, leading to</span>
<span id="cb66-433"><a href="#cb66-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-434"><a href="#cb66-434" aria-hidden="true" tabindex="-1"></a>$$\text{age} = - a/b. $$</span>
<span id="cb66-435"><a href="#cb66-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-436"><a href="#cb66-436" aria-hidden="true" tabindex="-1"></a>For the example data</span>
<span id="cb66-437"><a href="#cb66-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-438"><a href="#cb66-438" aria-hidden="true" tabindex="-1"></a>$$ \text{age} = - a/b = - (-5.31) / .11 = 48.27, $$</span>
<span id="cb66-439"><a href="#cb66-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-440"><a href="#cb66-440" aria-hidden="true" tabindex="-1"></a>which confirms the conclusion we made looking at the plot. </span>
<span id="cb66-441"><a href="#cb66-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-442"><a href="#cb66-442" aria-hidden="true" tabindex="-1"></a>In summary, another way of interpreting regression coefficients in simple logistic regression is to compute $-a / b$, which gives the value of $X$ at which $p(Y = 1) = .5$. </span>
<span id="cb66-443"><a href="#cb66-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-444"><a href="#cb66-444" aria-hidden="true" tabindex="-1"></a><span class="fu">### Other interpretations: Rate of change</span></span>
<span id="cb66-445"><a href="#cb66-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-446"><a href="#cb66-446" aria-hidden="true" tabindex="-1"></a>Yet another interpretation is in terms of the slope of the straight line (tangent) through the point $p(CHD = 1) = .5$. The slope of this line describes the rate of change in the probability of CHD for people who are "close to" the age of equal odds (48 years in our example). The tangent line for our example is shown in @fig-tangent. </span>
<span id="cb66-447"><a href="#cb66-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-448"><a href="#cb66-448" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-tangent, echo = T, fig.cap = "The Rate of Change Interpretation", fig.align = 'center'}</span></span>
<span id="cb66-449"><a href="#cb66-449" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">age =</span> <span class="dv">20</span><span class="sc">:</span><span class="dv">70</span>)</span>
<span id="cb66-450"><a href="#cb66-450" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod2, <span class="at">newdata =</span> x, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb66-451"><a href="#cb66-451" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x<span class="sc">$</span>age, prob, <span class="at">xlab =</span> <span class="st">"age"</span>, <span class="at">ylab =</span> <span class="st">"p(CHD)"</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb66-452"><a href="#cb66-452" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="sc">-</span>.<span class="dv">815</span>, <span class="at">b =</span> .<span class="dv">11</span><span class="sc">/</span><span class="dv">4</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb66-453"><a href="#cb66-453" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-454"><a href="#cb66-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-455"><a href="#cb66-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-456"><a href="#cb66-456" aria-hidden="true" tabindex="-1"></a>It turns out that the slope of the tangent line is equal to exactly $b/4$. The derivation requires calculus and is omitted (ask in class if you are interested!). For the example data $b / 4 = .11 / 4 = .0275$. So, for every additional year, the predicted probability of CHD increases by .0275. Keep in mind, this interpretation only applies to people who "around" the age of equal odds  (48 years old in this example). Looking at the plot, we can see that this approximation is pretty good for people between the ages of 40 and 60. </span>
<span id="cb66-457"><a href="#cb66-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-458"><a href="#cb66-458" aria-hidden="true" tabindex="-1"></a><span class="fu">### Relation to linear probability model </span></span>
<span id="cb66-459"><a href="#cb66-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-460"><a href="#cb66-460" aria-hidden="true" tabindex="-1"></a>Notice that in our example, the logistic function is roughly linear for probabilities in the range $<span class="co">[</span><span class="ot">.2, .8</span><span class="co">]</span>$. As mentioned in the introduction of this chapter, this is the situation in which using linear regression with a binary outcome (i.e., the linear probability model) works "well enough". Also note that the regression coefficient from the linear probability model in @sec-chd-example-10 ($b_{OLS} = .0218$) is in the ballpark of the coefficient computed above ($b_{logistic} / 4 = .0275$). These numbers are both describing how the probability of CHD is related to a person's age. </span>
<span id="cb66-461"><a href="#cb66-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-462"><a href="#cb66-462" aria-hidden="true" tabindex="-1"></a>The logistic model also provides us with a way of "diagnosing" whether the linear probability model is a good approximation. As noted, the logistic function is roughly linear for probabilities in the range $<span class="co">[</span><span class="ot">.2, .8</span><span class="co">]</span>$. If we ran a linear regression on CHD and all of the fitted / predicted values were within the range $<span class="co">[</span><span class="ot">.2, .8</span><span class="co">]</span>$, we would be in the situation where we might prefer to use linear regression (despite it being technically wrong). Referring back to residual vs. plotted in @sec-chd-example-10, we can see that the predicted values were outside of this range, so the logistic model is the better way to approach for this example. </span>
<span id="cb66-463"><a href="#cb66-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-464"><a href="#cb66-464" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summary </span></span>
<span id="cb66-465"><a href="#cb66-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-466"><a href="#cb66-466" aria-hidden="true" tabindex="-1"></a>The simple logistic regression model </span>
<span id="cb66-467"><a href="#cb66-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-468"><a href="#cb66-468" aria-hidden="true" tabindex="-1"></a>$$ \text{logit}(CHD) = a + b (\text{age}) $$</span>
<span id="cb66-469"><a href="#cb66-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-470"><a href="#cb66-470" aria-hidden="true" tabindex="-1"></a>has the following interpretations. </span>
<span id="cb66-471"><a href="#cb66-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-472"><a href="#cb66-472" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In terms of the logit: </span>
<span id="cb66-473"><a href="#cb66-473" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>The intercept ($a$) is the predicted value of the log-odds of CHD when age = 0. </span>
<span id="cb66-474"><a href="#cb66-474" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>The slope ($b$) is how much the predicted log-odds of CHD changes for a one unit (year) increase in age.</span>
<span id="cb66-475"><a href="#cb66-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-476"><a href="#cb66-476" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In terms of the odds: </span>
<span id="cb66-477"><a href="#cb66-477" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>The exponent of the regression parameter, $\exp(b)$, is the odds ratio associated with a one unit increase in age (relative magnitude).</span>
<span id="cb66-478"><a href="#cb66-478" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>$(\exp(b) - 1) \times 100$ is the percentage change in the odds of CHD for a one unit increase in age (relative change). </span>
<span id="cb66-479"><a href="#cb66-479" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>For the intercept, $\exp(a)$ is the predicted odds of having CHD when $age = 0$. This isn't a very useful number in our example, but it can be useful when the predictor(s) are centered.  </span>
<span id="cb66-480"><a href="#cb66-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-481"><a href="#cb66-481" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In terms of predicted probabilities:</span>
<span id="cb66-482"><a href="#cb66-482" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>The logistic plot provides a visual summary of how the probability of CHD changes as a function of age -- if you want to report in terms of probabilities, this plot is usually a good choice.</span>
<span id="cb66-483"><a href="#cb66-483" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Predicted probabilities can be reported for specific values of age (MERVs), and risk ratios / relative risk can be computed to compare the predicted probabilities at specific ages. This is the same approach we used for following-up interactions in @sec-inference-for-interactions-5 (i.e., you can use <span class="in">`emmeans`</span> in R).</span>
<span id="cb66-484"><a href="#cb66-484" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>$–a / b$ corresponds the age at which the probability of having CHD is "50-50".</span>
<span id="cb66-485"><a href="#cb66-485" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>$b / 4$ is the approximate rate of increase (slope) of the probability of having CHD for people close to the "50-50" point.  </span>
<span id="cb66-486"><a href="#cb66-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-487"><a href="#cb66-487" aria-hidden="true" tabindex="-1"></a>**Please write down the numerical values of each of the above summaries for the CHD example (except the predicted probability plot). You can select any values of age to report the predicted probabilities and risk ratios, and you can "eye ball" the probabilities using @fig-pred-prob.** </span>
<span id="cb66-488"><a href="#cb66-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-489"><a href="#cb66-489" aria-hidden="true" tabindex="-1"></a>Answers are hidden below (use the Code button), but please try them yourself first. </span>
<span id="cb66-490"><a href="#cb66-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-493"><a href="#cb66-493" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-494"><a href="#cb66-494" aria-hidden="true" tabindex="-1"></a><span class="co"># Logit: </span></span>
<span id="cb66-495"><a href="#cb66-495" aria-hidden="true" tabindex="-1"></a><span class="co"># predicted logit when age = 0: a = -5.31;</span></span>
<span id="cb66-496"><a href="#cb66-496" aria-hidden="true" tabindex="-1"></a><span class="co"># expected increase in logit when age increases by one year: b = .11</span></span>
<span id="cb66-497"><a href="#cb66-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-498"><a href="#cb66-498" aria-hidden="true" tabindex="-1"></a><span class="co"># Odds:</span></span>
<span id="cb66-499"><a href="#cb66-499" aria-hidden="true" tabindex="-1"></a><span class="co"># OR = exp(b) = exp(.11) = 1.1163</span></span>
<span id="cb66-500"><a href="#cb66-500" aria-hidden="true" tabindex="-1"></a><span class="co"># % change in Odds = (exp(b) - 1) X 100 = 11.63%</span></span>
<span id="cb66-501"><a href="#cb66-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-502"><a href="#cb66-502" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability</span></span>
<span id="cb66-503"><a href="#cb66-503" aria-hidden="true" tabindex="-1"></a><span class="co"># In the example, used risk for Age = 40 relative to Age = 20: .2947/.0435 = 6.77</span></span>
<span id="cb66-504"><a href="#cb66-504" aria-hidden="true" tabindex="-1"></a><span class="co"># "50-50" age: -a/b = 5.31 / .11 = 48.27 </span></span>
<span id="cb66-505"><a href="#cb66-505" aria-hidden="true" tabindex="-1"></a><span class="co"># Rate of change at "50-50" age: b/4 = .11/4 = .0275</span></span>
<span id="cb66-506"><a href="#cb66-506" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-507"><a href="#cb66-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-508"><a href="#cb66-508" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimation {#sec-estimation-10} </span></span>
<span id="cb66-509"><a href="#cb66-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-510"><a href="#cb66-510" aria-hidden="true" tabindex="-1"></a>The previous section discussed the interpretation of simple logistic regression. In this section we discuss how to get the numbers in the “Estimate” column of the R <span class="in">`summary()`</span> table, and in the next section we discuss how to compute statistical tests and confidence intervals. </span>
<span id="cb66-511"><a href="#cb66-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-512"><a href="#cb66-512" aria-hidden="true" tabindex="-1"></a>The material in these next two sections involves a lot of mathematical and statistical concepts, although I try to keep the formulas to a minimum. Because the material is so technical, there will be a mini-lecture on these sections during class. It may also take some extra time to read through these next couple of sections, and you may want to write down any additional questions you have about the material so that we can address them in class together. Basically, the next two sections are pretty tough, so hang in there :) </span>
<span id="cb66-513"><a href="#cb66-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-514"><a href="#cb66-514" aria-hidden="true" tabindex="-1"></a>The short version: </span>
<span id="cb66-515"><a href="#cb66-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-516"><a href="#cb66-516" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In logistic regression, we use maximum likelihood (ML) rather than ordinary least squares (OLS) to estimate the model parameters. </span>
<span id="cb66-517"><a href="#cb66-517" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The resulting estimates are called the maximum likelihood estimates (MLEs). </span>
<span id="cb66-518"><a href="#cb66-518" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Unlike OLS estimates in linear regression, MLEs in logistic regression cannot be written down in "closed form." This means that we need to define MLEs algorithmically, rather algebraically -- more on this below. </span>
<span id="cb66-519"><a href="#cb66-519" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Inference for the MLEs is based on *asymptotics*. In statistics, "asymptotic" means "as the sample size goes to infinity...and beyond!" The implication is that using ML also changes how we compute standard errors, statistical tests, confidence intervals, etc. </span>
<span id="cb66-520"><a href="#cb66-520" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Our overall goal is to know enough about this technical stuff to be able to interpret and recognize the limitations of logistic regression. </span>
<span id="cb66-521"><a href="#cb66-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-522"><a href="#cb66-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-523"><a href="#cb66-523" aria-hidden="true" tabindex="-1"></a><span class="fu">### ML vs OLS</span></span>
<span id="cb66-524"><a href="#cb66-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-525"><a href="#cb66-525" aria-hidden="true" tabindex="-1"></a>Let's start by contrasting ML with OLS. Recall that in linear regression, OLS leads to "closed form" expressions for the regression coefficients ( @sec-ols-2). This means we can write down an explicit algebraic equation for the regression coefficients, e.g., </span>
<span id="cb66-526"><a href="#cb66-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-527"><a href="#cb66-527" aria-hidden="true" tabindex="-1"></a>$$ b = \text{cov}(Y, X) / s^2_X. $$ </span>
<span id="cb66-528"><a href="#cb66-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-529"><a href="#cb66-529" aria-hidden="true" tabindex="-1"></a>As we discussed in @sec-chd-example-10, these estimates often aren't very good when the outcome is binary. So, instead we use ML. </span>
<span id="cb66-530"><a href="#cb66-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-531"><a href="#cb66-531" aria-hidden="true" tabindex="-1"></a>Unlike OLS with linear regression, ML with logistic regression does not lead to closed-form expressions for the regression coefficients. This means we can't  write down the other side of the equation $b = ?$ using an algebraic expression like the one above. Instead, we must define the regression parameters in terms of a computational procedure (i.e., an algorithm) that produces the intended result. That computational procedure is maximization of the likelihood function. </span>
<span id="cb66-532"><a href="#cb66-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-533"><a href="#cb66-533" aria-hidden="true" tabindex="-1"></a>We discussed the difference between algebraic and algorithmic definitions when we compared the mean and median last semester, and we can address it again in class if you have further questions. In the present context, the take home message is:</span>
<span id="cb66-534"><a href="#cb66-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-535"><a href="#cb66-535" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In OLS, we didn’t really need to know much about least squares *per se*, because we could just work with the formulas for the regression coefficients.  </span>
<span id="cb66-536"><a href="#cb66-536" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In ML, we don’t get any  formulas for the coefficients, so were are going to have to dig into what the likelihood is and how to use it. </span>
<span id="cb66-537"><a href="#cb66-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-538"><a href="#cb66-538" aria-hidden="true" tabindex="-1"></a><span class="fu">### The likelihood </span></span>
<span id="cb66-539"><a href="#cb66-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-540"><a href="#cb66-540" aria-hidden="true" tabindex="-1"></a>The likelihood is a function that tells us the probability of a sample being drawn from a population. For our CHD example data, we have $N = 100$ cases in which there are 43 people with CHD and 57 people without CHD. The likelihood tells us the probability of this sample being drawn from the population. In this section, we build up a mathematical expression for the likelihood function. Note that when the outcome variable in a regression is binary, this is more specifically called the *binomial* likelihood. </span>
<span id="cb66-541"><a href="#cb66-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-542"><a href="#cb66-542" aria-hidden="true" tabindex="-1"></a>The building block of the binomial likelihood is the probability of a person having CHD, which we denote as</span>
<span id="cb66-543"><a href="#cb66-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-544"><a href="#cb66-544" aria-hidden="true" tabindex="-1"></a>$$ p = p(CHD = 1). $$</span>
<span id="cb66-545"><a href="#cb66-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-546"><a href="#cb66-546" aria-hidden="true" tabindex="-1"></a>Keep in mind that we are defining the probability of sample being drawn from a population. So, we are imaging that a person is sampled from a population, and some proportion of that population has CHD. That proportion is what we are representing as the probability $p$. </span>
<span id="cb66-547"><a href="#cb66-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-548"><a href="#cb66-548" aria-hidden="true" tabindex="-1"></a>For a single person (i.e., a sample of $N = 1$), the likelihood describes the probability that the person either does or does not have CHD. The likelihood is given by the following formula:</span>
<span id="cb66-549"><a href="#cb66-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-550"><a href="#cb66-550" aria-hidden="true" tabindex="-1"></a>$$ L = p ^ {CHD} \times (1 - p)^{1 - CHD} $${#eq-Li}</span>
<span id="cb66-551"><a href="#cb66-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-552"><a href="#cb66-552" aria-hidden="true" tabindex="-1"></a>This looks complicated, but its just a way of writing the population probability of having CHD, or not having CHD, in a single equation. All it says is: </span>
<span id="cb66-553"><a href="#cb66-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-554"><a href="#cb66-554" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The probability of sampling someone with CHD is $p$.</span>
<span id="cb66-555"><a href="#cb66-555" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>i.e., plug in $CHD = 1$ into @eq-Li and the result is $L = p$.</span>
<span id="cb66-556"><a href="#cb66-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-557"><a href="#cb66-557" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The probability of sampling someone without CHD is $1 - p$.</span>
<span id="cb66-558"><a href="#cb66-558" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>i.e., plug in $CHD = 0$ into @eq-Li and the answer is $L = 1 - p$.</span>
<span id="cb66-559"><a href="#cb66-559" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-560"><a href="#cb66-560" aria-hidden="true" tabindex="-1"></a>You should do the plugging-in now to see how this works. </span>
<span id="cb66-561"><a href="#cb66-561" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-562"><a href="#cb66-562" aria-hidden="true" tabindex="-1"></a>For a simple random sample of $i = 1, \dots, N$ people, the likelihood of observing the full sample is just the product of the likelihoods for each person in the sample. To write the likelihood for  person $i$, we will use the notation $L_i$, and to write the likelihood for the entire sample, we will keep using $L$ without a subscript:</span>
<span id="cb66-563"><a href="#cb66-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-564"><a href="#cb66-564" aria-hidden="true" tabindex="-1"></a>$$ L = \Pi_{i = 1}^N L_i = L_1 \times  L_2 \times \cdots \times L_N $${#eq-L}</span>
<span id="cb66-565"><a href="#cb66-565" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb66-566"><a href="#cb66-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-567"><a href="#cb66-567" aria-hidden="true" tabindex="-1"></a>(The symbol $\Pi$ is shorthand for multiplication, just like $\Sigma$ is used for addition. )</span>
<span id="cb66-568"><a href="#cb66-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-569"><a href="#cb66-569" aria-hidden="true" tabindex="-1"></a>If we plug-in to @eq-L using the values of $CHD$ in our sample, this gives us the likelihood of observing the sample. For example, the values of CHD for the first four cases in the data set (1, 1, 0, 1). So, the first four terms in the likelihood are: </span>
<span id="cb66-570"><a href="#cb66-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-571"><a href="#cb66-571" aria-hidden="true" tabindex="-1"></a>$$ L = p_1 \times  p_2 \times (1-p_3) \times p_4 \times\cdots $$</span>
<span id="cb66-572"><a href="#cb66-572" aria-hidden="true" tabindex="-1"></a>**Please take a moment to verify this result for yourself using @eq-Li and @eq-L, and we will walk through the derivation together in class.** </span>
<span id="cb66-573"><a href="#cb66-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-574"><a href="#cb66-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-575"><a href="#cb66-575" aria-hidden="true" tabindex="-1"></a><span class="fu">### Maximizing the likelihood </span></span>
<span id="cb66-576"><a href="#cb66-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-577"><a href="#cb66-577" aria-hidden="true" tabindex="-1"></a>So far we have addressed the general form of the likelihood for a binary variable. But we haven't talked about how to get the probabilities $p_i$. This is where the logistic regression comes in. As discussed in @sec-logit-10, the purpose of logistic regression is to model the probability of having CHD. Thus, in order to turn @eq-L into the likelihood function for logistic regression, we just plug-in for $p_i$ using @eq-logistic. </span>
<span id="cb66-578"><a href="#cb66-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-579"><a href="#cb66-579" aria-hidden="true" tabindex="-1"></a>As you might expect, this equation turns out to be pretty complicated to write down, and we aren't going to do that here. The point is that we *can* write down the likelihood for logistic regression, and once we have it written down we can use standard procedures from calculus to find its maximum (e.g., Newton's method).  </span>
<span id="cb66-580"><a href="#cb66-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-581"><a href="#cb66-581" aria-hidden="true" tabindex="-1"></a>This is about as much math as we need to state the overall idea behind maximum likelihood estimation: We select the regression coefficients of the logistic model so that the likelihood in @eq-L is maximized. </span>
<span id="cb66-582"><a href="#cb66-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-583"><a href="#cb66-583" aria-hidden="true" tabindex="-1"></a>You might be asking, why is maximizing the likelihood a good way to get regression coefficients for logistic regression? There are lots of ways to answer this question, but I think the best answer is that the resulting estimates have good properties -- they are unbiased and precise, at least with large samples. We can discuss the rationale behind maximum likelihood more in class, so if you are interested please just let me know. </span>
<span id="cb66-584"><a href="#cb66-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-585"><a href="#cb66-585" aria-hidden="true" tabindex="-1"></a>Doing the computations that maximize the likelihood is quite complicated, and the details are beyond the scope of this course (for more details, check out Chapter 15 of @cite:fox). However, there is one issue that we should be aware of: the likelihood can cause so-called "underflow" issues in computing. This is because we are multiplying together lots of numbers less than 1, which can lead to very, very small values -- too small for a computer represent (i.e., underflow). To address this issue, we usually work with the log-likelihood, rather than the likelihood itself: </span>
<span id="cb66-586"><a href="#cb66-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-587"><a href="#cb66-587" aria-hidden="true" tabindex="-1"></a>$$ \ell = \log(L) = \log \prod_{i = 1}^N L_i = \sum_{i = 1}^N \log(L_i) $${#eq-ell}</span>
<span id="cb66-588"><a href="#cb66-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-589"><a href="#cb66-589" aria-hidden="true" tabindex="-1"></a>The last equality uses the "addition with logs" rule from @sec-math-review-8. For our purposes here, we just need to know that taking the log of the likelihood changes a very small number (the likelihood) into a large negative number (the log-likelihood), which prevents our computer from breaking when doing the computations. </span>
<span id="cb66-590"><a href="#cb66-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-591"><a href="#cb66-591" aria-hidden="true" tabindex="-1"></a>The log-likelihood, $\ell$, also shows up a lot in what follows. Its main uses are summarized below</span>
<span id="cb66-592"><a href="#cb66-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-593"><a href="#cb66-593" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$-2 \times \ell$ is called the *deviance* of the model. I am not sure how it got this name, but you will see it a lot in the R output. </span>
<span id="cb66-594"><a href="#cb66-594" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>We can use the deviance to compute R-squared-like statistics that describe the overall quality of the predictions made using a logistic regression model (see @sec-pseudo-rsquared-10)  </span>
<span id="cb66-595"><a href="#cb66-595" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Two logistic regression models can be compared using their deviances. This leads to the logistic regression equivalent of the F-test of R-squared change (@sec-chap-6), which is called the likelihood ratio test (see @sec-inference-10). </span>
<span id="cb66-596"><a href="#cb66-596" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb66-597"><a href="#cb66-597" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summary </span></span>
<span id="cb66-598"><a href="#cb66-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-599"><a href="#cb66-599" aria-hidden="true" tabindex="-1"></a>Lets take a look at our <span class="in">`summary()`</span> output for the CHD data again: </span>
<span id="cb66-600"><a href="#cb66-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-603"><a href="#cb66-603" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-604"><a href="#cb66-604" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span>
<span id="cb66-605"><a href="#cb66-605" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-606"><a href="#cb66-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-607"><a href="#cb66-607" aria-hidden="true" tabindex="-1"></a>This section has addressed the question: how do we get the numbers in the "Estimate" column of the output shown above? The answer is maximum likelihood (ML). In short: </span>
<span id="cb66-608"><a href="#cb66-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-609"><a href="#cb66-609" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The likelihood function tells us the probability of randomly sampling $N$ people from a population.</span>
<span id="cb66-610"><a href="#cb66-610" aria-hidden="true" tabindex="-1"></a><span class="ss">   * </span>When the outcome variable is binary, it is called the binomial likelihood. </span>
<span id="cb66-611"><a href="#cb66-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-612"><a href="#cb66-612" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>ML is based on the idea that the values of the model parameters that make the data most likely are the “best”</span>
<span id="cb66-613"><a href="#cb66-613" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>The resulting values are called the maximum likelihood estimates (MLEs). </span>
<span id="cb66-614"><a href="#cb66-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-615"><a href="#cb66-615" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In logistic regression, ML is accomplished using a computer algorithm -- we can't write down algebraic expressions for the MLEs, we only have an algorithmic definition, which is to maximize the likelihood</span>
<span id="cb66-616"><a href="#cb66-616" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>FYI: The algorithm <span class="in">`R`</span> uses for logistic regression is a version of Newton-Raphson called Fisher Scoring. </span>
<span id="cb66-617"><a href="#cb66-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-618"><a href="#cb66-618" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference {#sec-inference-10}</span></span>
<span id="cb66-619"><a href="#cb66-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-620"><a href="#cb66-620" aria-hidden="true" tabindex="-1"></a>In this section we talk about statistical tests and confidence intervals for logistic regression. There are three main types of inference for logistic regression: </span>
<span id="cb66-621"><a href="#cb66-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-622"><a href="#cb66-622" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>*Wald test of regression coefficients*. The Wald test is a z-test that can be used with large samples ($N &gt; 100$). This is what is reported in R's <span class="in">`summary()`</span> output for logistic regression. </span>
<span id="cb66-623"><a href="#cb66-623" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Although the Wald test is reported by default in <span class="in">`R`</span>, you should be careful when interpreting it. The Wald test is computed just like a regular t-test. However, rather than using a t-distribution, the Wald test is based on the normal distribution. This approach only applies in large samples (technically, when $N \rightarrow \infty$). In smaller samples ($N &lt; 100$), the standard errors can be too large, especially if the coefficient is large in value. This means that the Wald test can be under-powered, leading to too many Type I Errors in small samples. So, when you have fewer than 100 cases, it is good practice to use one of the other two procedures discussed below.  </span>
<span id="cb66-624"><a href="#cb66-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-625"><a href="#cb66-625" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>*Confidence intervals for odds ratios*. Odds ratios can be tested using confidence intervals. If the confidence interval includes the value 1, then the odds ratio is not statistically significant at the chosen level of alpha / confidence. Confidence intervals can be computed in two ways for logistic regression, either using the Wald test (just like a usual confidence based on a t-test), or using a procedure called *profile likelihood*. The latter is preferred because it is more accurate, and it is the default in <span class="in">`R`</span>. Ask in class if you want to know more :) </span>
<span id="cb66-626"><a href="#cb66-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-627"><a href="#cb66-627" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>*Likelihood ratio test of nested models*. The likelihood ratio test plays the same role as the F-test of $\Delta R^2$ in OLS regression. It allows you to compare nested models. We will talk about the analogue statistic for $\Delta R^2$ in @sec-pseudo-rsquared-10.</span>
<span id="cb66-628"><a href="#cb66-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-629"><a href="#cb66-629" aria-hidden="true" tabindex="-1"></a>You might be wondering how the likelihood-based procedures (2 and 3) avoid the issues with the Wald test. Technically, the likelihood-based procedures also assume large samples sizes ($N \rightarrow \infty$), but they do not require computing the standard errors of the regression coefficients, which is the problem with the Wald test. So, although both procedures have the same assumptions, the likelihood-based procedures perform better than the Wald test in practice, especially with smaller samples.  </span>
<span id="cb66-630"><a href="#cb66-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-631"><a href="#cb66-631" aria-hidden="true" tabindex="-1"></a>The rest this section briefly illustrates each of these three methods using our example data. </span>
<span id="cb66-632"><a href="#cb66-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-633"><a href="#cb66-633" aria-hidden="true" tabindex="-1"></a><span class="fu">### Wald test</span></span>
<span id="cb66-634"><a href="#cb66-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-635"><a href="#cb66-635" aria-hidden="true" tabindex="-1"></a>The MLEs are asymptotically normally (based on the central limit theorem), so we can divide the MLEs by their standard errors to get an asymptotic z-test. This test is called a Wald test, named after the person (Abraham Wald) who first invented it. The Wald test is what R reports in the <span class="in">`summary()`</span> table below.   </span>
<span id="cb66-636"><a href="#cb66-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-639"><a href="#cb66-639" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-640"><a href="#cb66-640" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span>
<span id="cb66-641"><a href="#cb66-641" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-642"><a href="#cb66-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-643"><a href="#cb66-643" aria-hidden="true" tabindex="-1"></a>At this point in the semester, you should have no problem interpreting whether these tests are statistically significant or not. The main wrinkle that comes up in logistic regression is that we are working with an (asymptotic) z-test rather than usual t-test from OLS regression. As noted, this z-test is also called Wald test. </span>
<span id="cb66-644"><a href="#cb66-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-645"><a href="#cb66-645" aria-hidden="true" tabindex="-1"></a>In summary: </span>
<span id="cb66-646"><a href="#cb66-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-647"><a href="#cb66-647" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The null hypotheses $H_0: b = b_0$ can be tested against the alternative $H_A: b \neq b_0$ using the following z-test, which has a standard normal distribution when the null hypothesis is true:</span>
<span id="cb66-648"><a href="#cb66-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-649"><a href="#cb66-649" aria-hidden="true" tabindex="-1"></a>$$ z = (\hat b - b_0) / SE({\hat b}), $$</span>
<span id="cb66-650"><a href="#cb66-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-651"><a href="#cb66-651" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>This tests assumes: </span>
<span id="cb66-652"><a href="#cb66-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-653"><a href="#cb66-653" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$\hat b$ is the MLE (this requires that model is correct -- more on this in @sec-assumption-checking-10)</span>
<span id="cb66-654"><a href="#cb66-654" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The sample size is "approximately" infinite ($N &gt; 100$ is a practical lower limit). </span>
<span id="cb66-655"><a href="#cb66-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-656"><a href="#cb66-656" aria-hidden="true" tabindex="-1"></a>The Wald test also applies to the intercept. </span>
<span id="cb66-657"><a href="#cb66-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-658"><a href="#cb66-658" aria-hidden="true" tabindex="-1"></a><span class="fu">### Odds ratio (OR)</span></span>
<span id="cb66-659"><a href="#cb66-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-660"><a href="#cb66-660" aria-hidden="true" tabindex="-1"></a>In @sec-simple-10, the OR was introduced as a more intuitive way of interpreting logistic regression. But, how do we make inferences about this more interpretable quantity? The short answer: use confidence intervals. </span>
<span id="cb66-661"><a href="#cb66-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-662"><a href="#cb66-662" aria-hidden="true" tabindex="-1"></a>The reason we use confidence intervals instead of tests of significance is because the null hypothesis $b = 0$ corresponds to </span>
<span id="cb66-663"><a href="#cb66-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-664"><a href="#cb66-664" aria-hidden="true" tabindex="-1"></a>$$OR = \exp(b) = \exp(0) = 1. $$ </span>
<span id="cb66-665"><a href="#cb66-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-666"><a href="#cb66-666" aria-hidden="true" tabindex="-1"></a>So, if the regression coefficient is zero (i.e., no relationship), this means the OR is one. In terms of percent change, $OR = 1$ means 0% percent change: </span>
<span id="cb66-667"><a href="#cb66-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-668"><a href="#cb66-668" aria-hidden="true" tabindex="-1"></a>$$(OR - 1) \times 100   = (1 - 1) \times 100 = 0. $$</span>
<span id="cb66-669"><a href="#cb66-669" aria-hidden="true" tabindex="-1"></a>To obtain confidence intervals on the OR, we can use the following two steps:</span>
<span id="cb66-670"><a href="#cb66-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-671"><a href="#cb66-671" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Compute the confidence intervals for the regression coefficient $b$: </span>
<span id="cb66-672"><a href="#cb66-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-673"><a href="#cb66-673" aria-hidden="true" tabindex="-1"></a>$$ <span class="co">[</span><span class="ot">b_{\text{lower}} ,b_{\text{upper}}</span><span class="co">]</span>. $$</span>
<span id="cb66-674"><a href="#cb66-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-675"><a href="#cb66-675" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Then exponentiate the confidence intervals to get a confidence interval for the OR.</span>
<span id="cb66-676"><a href="#cb66-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-677"><a href="#cb66-677" aria-hidden="true" tabindex="-1"></a>$$ <span class="co">[</span><span class="ot">\exp(b_{\text{lower}}) ,\exp(b_{\text{upper}})</span><span class="co">]</span>. $$</span>
<span id="cb66-678"><a href="#cb66-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-679"><a href="#cb66-679" aria-hidden="true" tabindex="-1"></a>The output below shows the result of this procedure for the CHD data.</span>
<span id="cb66-680"><a href="#cb66-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-681"><a href="#cb66-681" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, message=F}</span></span>
<span id="cb66-682"><a href="#cb66-682" aria-hidden="true" tabindex="-1"></a>ci.b <span class="ot">&lt;-</span> <span class="fu">confint</span>(mod2)</span>
<span id="cb66-683"><a href="#cb66-683" aria-hidden="true" tabindex="-1"></a>ci.table <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">coefs =</span> <span class="fu">coef</span>(mod2), ci.b)</span>
<span id="cb66-684"><a href="#cb66-684" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(ci.table)</span>
<span id="cb66-685"><a href="#cb66-685" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-686"><a href="#cb66-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-687"><a href="#cb66-687" aria-hidden="true" tabindex="-1"></a>In APA notation, we could write the results for age as $OR = 1.12, 95\% \text{ CI: } <span class="co">[</span><span class="ot">1.07, 1.19</span><span class="co">]</span>$. **Please write down your interpretation of the OR for age and its confidence interval. In your interpretation, please mention the relative change in the odds that is associated with each additional year of age and whether or not this is statistically different from zero.**</span>
<span id="cb66-688"><a href="#cb66-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-689"><a href="#cb66-689" aria-hidden="true" tabindex="-1"></a>Note that the confidence intervals produced by $R$ use the likelihood rather than the Wald test. So, the problems we discussed with the Wald test don't apply to these confidence intervals. You can get Wald confidence intervals form the <span class="in">`confint`</span> function, but this is not the default. Computing likelihood-based confidence intervals is complicated; you can learn about the overall approach here: <span class="co">[</span><span class="ot">https://documentation.sas.com/doc/en/etscdc/14.2/etsug/etsug_model_sect148.htm</span><span class="co">](https://documentation.sas.com/doc/en/etscdc/14.2/etsug/etsug_model_sect148.htm)</span></span>
<span id="cb66-690"><a href="#cb66-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-691"><a href="#cb66-691" aria-hidden="true" tabindex="-1"></a><span class="fu">### Likelihood ratio test</span></span>
<span id="cb66-692"><a href="#cb66-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-693"><a href="#cb66-693" aria-hidden="true" tabindex="-1"></a>The likelihood ratio (LR) test is analogous to the F-test of R-squared change in OLS regression. Its a general workhorse for model building and can be used in other creative ways. For example, the LR test can be used to replace the Wald test in simple logistic regression. To do this, we compare a model with only the intercept to a model with one predictor. And as noted above, the LR test avoids the problems associated with the Wald test, so it is usually preferred when the sample size is smaller ($N &lt; 100$). </span>
<span id="cb66-694"><a href="#cb66-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-695"><a href="#cb66-695" aria-hidden="true" tabindex="-1"></a>The LR tests works as follows:</span>
<span id="cb66-696"><a href="#cb66-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-697"><a href="#cb66-697" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Assume that Model 1 with $K_1$ predictors is nested within Model 2 with $K_2$ predictors (i.e., the predictors in Model 1 are a subset of the predictors in Model 2; see @sec-chap-6 for a refresher on nested models).</span>
<span id="cb66-698"><a href="#cb66-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-699"><a href="#cb66-699" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If it is true that the additional predictors in Model 2 all have regression coefficients equal to zero in the population, then it is also true that the models have equal likelihoods. This leads to the null hypothesis that the ratio of the likelihoods is equal to 1 (this is why its called a likelihood ratio test):    </span>
<span id="cb66-700"><a href="#cb66-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-701"><a href="#cb66-701" aria-hidden="true" tabindex="-1"></a>$$H_0: \frac{L_1}{L_2} = 1$$</span>
<span id="cb66-702"><a href="#cb66-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-703"><a href="#cb66-703" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>When $H_0$ is true, the following test statistic has a chi-square distribution with degrees of freedom equal to $df = K_2 - K_1$. </span>
<span id="cb66-704"><a href="#cb66-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-705"><a href="#cb66-705" aria-hidden="true" tabindex="-1"></a>$$\chi^2 = -2 \log \frac{\hat L_1}{\hat L_2} = 2 (\hat \ell_2 - \hat \ell_1) $$</span>
<span id="cb66-706"><a href="#cb66-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-707"><a href="#cb66-707" aria-hidden="true" tabindex="-1"></a>In this equation, the symbol $\chi$ is the Greek letter for lower-case "x" and is pronounced "chi". The chi-square test is computed as negative 2 times the log of the likelihood ratio, which ends up being the difference of the deviances of the two models (remember, the deviance is just a weird word for -2 times the log likelihood). Also note that the "hats" denote the likelihoods /  log-likelihoods evaluated at the MLEs.</span>
<span id="cb66-708"><a href="#cb66-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-709"><a href="#cb66-709" aria-hidden="true" tabindex="-1"></a>We have not covered the chi-square distribution previously, but it is a lot like the F-distribution. In fact, the F-static is a ratio of two chi-square statistics. Some examples of the chi-square distribution are shown below ($k$ denotes the degrees of freedom). </span>
<span id="cb66-710"><a href="#cb66-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-711"><a href="#cb66-711" aria-hidden="true" tabindex="-1"></a><span class="al">![Chi-square distributions (Source: https://en.wikipedia.org/wiki/Chi-squared_distribution)]( https://upload.wikimedia.org/wikipedia/commons/3/35/Chi-square_pdf.svg)</span></span>
<span id="cb66-712"><a href="#cb66-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-713"><a href="#cb66-713" aria-hidden="true" tabindex="-1"></a>To sum up: </span>
<span id="cb66-714"><a href="#cb66-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-715"><a href="#cb66-715" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The LR test is the analogue of the F-test of R-squared change from linear regression (see @sec-chap-6). It is used to compare two nested models. </span>
<span id="cb66-716"><a href="#cb66-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-717"><a href="#cb66-717" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If LR test is significant, we conclude that model with more predictors does a better job or explaining our data (in the sense of R-squared). In practical terms, this means that we reject the models with fewer predictors in favor of the model with more predictors.  </span>
<span id="cb66-718"><a href="#cb66-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-719"><a href="#cb66-719" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If the LR test is not significant, we conclude that the model with more predictors does not add anything beyond the model with fewer predictors. In practical terms, this means that we reject the models with more predictors and stick with the model with fewer predictors.  </span>
<span id="cb66-720"><a href="#cb66-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-721"><a href="#cb66-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-722"><a href="#cb66-722" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In the case where the two models differ by only one predictor, the LR test is conceptually equivalent to testing whether the regression coefficient of that predictor is equal to zero, and hence the LR test can replace the Wald test reported in R's <span class="in">`summary()`</span> output. This is again analogous to testing R-squared change in linear regression (see @sec-chap-6). </span>
<span id="cb66-723"><a href="#cb66-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-724"><a href="#cb66-724" aria-hidden="true" tabindex="-1"></a>Using our example, let's conduct an LR test of the model with age as a predictor against a model without this predictor (i.e., the model with only the intercept). To do this, we can use the <span class="in">`lrtest`</span> function of the <span class="in">`lmtest`</span> package which produces the following output: </span>
<span id="cb66-725"><a href="#cb66-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-728"><a href="#cb66-728" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-729"><a href="#cb66-729" aria-hidden="true" tabindex="-1"></a><span class="co"># The model with no predictors</span></span>
<span id="cb66-730"><a href="#cb66-730" aria-hidden="true" tabindex="-1"></a>mod0 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> <span class="dv">1</span>, <span class="at">family =</span> <span class="st">"binomial"</span>, <span class="at">data =</span> chd.data)</span>
<span id="cb66-731"><a href="#cb66-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-732"><a href="#cb66-732" aria-hidden="true" tabindex="-1"></a><span class="co"># The LR test (enter the smaller model first)</span></span>
<span id="cb66-733"><a href="#cb66-733" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">lrtest</span>(mod0, mod2)</span>
<span id="cb66-734"><a href="#cb66-734" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-735"><a href="#cb66-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-736"><a href="#cb66-736" aria-hidden="true" tabindex="-1"></a>In APA format, we could write this output as $\chi^2(1) = 29.31, p &lt; .001$.</span>
<span id="cb66-737"><a href="#cb66-737" aria-hidden="true" tabindex="-1"></a>**Please write down your interpretation of the LR test for the CHD example. Your interpretation should mention which two models were compared, whether the LR test was significant or not, and your conclusion about which model provided a better explanation of the data.**</span>
<span id="cb66-738"><a href="#cb66-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-739"><a href="#cb66-739" aria-hidden="true" tabindex="-1"></a><span class="fu">## (Pseudo) R-squared {#sec-pseudo-rsquared-10}</span></span>
<span id="cb66-740"><a href="#cb66-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-741"><a href="#cb66-741" aria-hidden="true" tabindex="-1"></a>As we have just discussed, the likelihood ratio test in logistic regression is analogous to the F-test of R-squared change in linear regression. But we have not yet discussed how to compute R-squared for logistic regression. That is what this section is about. </span>
<span id="cb66-742"><a href="#cb66-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-743"><a href="#cb66-743" aria-hidden="true" tabindex="-1"></a>There are actually a number of statistics, called *pseudo R-squareds*, that have been developed to play the role of R-squared in logistic regression. The pseudo R-squareds are not computed as proportions of variance, which is why the are called *pseudo*. But they each try to describe the "preditive utility" of the logistic regression model, and each has their own shortcomings and limitations. </span>
<span id="cb66-744"><a href="#cb66-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-745"><a href="#cb66-745" aria-hidden="true" tabindex="-1"></a>There is no clear “victor” in the battle of pseudo R-squareds, so we will just present one widely used exemplar that is straight forward to compute and interpret. You can learn about some other options here: <span class="co">[</span><span class="ot">https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/</span><span class="co">](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/)</span></span>
<span id="cb66-746"><a href="#cb66-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-747"><a href="#cb66-747" aria-hidden="true" tabindex="-1"></a><span class="fu">### McFadden's Pseudo R-squared</span></span>
<span id="cb66-748"><a href="#cb66-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-749"><a href="#cb66-749" aria-hidden="true" tabindex="-1"></a>McFadden’s Pseudo R-squared focuses on how much the likelihood improves when comparing two nested models. This is close to, but not quite the same as, the interpretation of $\Delta R^2$ in linear regression. </span>
<span id="cb66-750"><a href="#cb66-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-751"><a href="#cb66-751" aria-hidden="true" tabindex="-1"></a>McFadden’s Pseudo R-squared takes on values between 0 and 1 and its interpretation can be explained by considering these two extreme values. </span>
<span id="cb66-752"><a href="#cb66-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-753"><a href="#cb66-753" aria-hidden="true" tabindex="-1"></a>For Model 1 nested within Model 2, McFadden’s R-squared is defined as:</span>
<span id="cb66-754"><a href="#cb66-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-755"><a href="#cb66-755" aria-hidden="true" tabindex="-1"></a>$$ R^2 = 1 - \frac{ \ell_2}{\ell_1} $$</span>
<span id="cb66-756"><a href="#cb66-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-757"><a href="#cb66-757" aria-hidden="true" tabindex="-1"></a>Note that if Model 2 does not "add anything" to Model 1, then the two models have the same likelihood (see the discussion of the LR test in @sec-inference-10). In this situation,  </span>
<span id="cb66-758"><a href="#cb66-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-759"><a href="#cb66-759" aria-hidden="true" tabindex="-1"></a>$$\frac{ \ell_2}{ \ell_1} = 1 $$</span>
<span id="cb66-760"><a href="#cb66-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-761"><a href="#cb66-761" aria-hidden="true" tabindex="-1"></a>which implies $R^2 = 0.$</span>
<span id="cb66-762"><a href="#cb66-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-763"><a href="#cb66-763" aria-hidden="true" tabindex="-1"></a>On the other hand, if Model 2 provides perfect predictions for all cases, (i.e., $L_i = 1$ for each $i$), then </span>
<span id="cb66-764"><a href="#cb66-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-765"><a href="#cb66-765" aria-hidden="true" tabindex="-1"></a>$$ \hat \ell_2 = \sum_i \log L_{2i} = \sum_i \log 1 = 0. $$</span>
<span id="cb66-766"><a href="#cb66-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-767"><a href="#cb66-767" aria-hidden="true" tabindex="-1"></a>This implies </span>
<span id="cb66-768"><a href="#cb66-768" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb66-769"><a href="#cb66-769" aria-hidden="true" tabindex="-1"></a>$$ R^2 = 1 - \frac{\hat \ell_2}{\hat \ell_1} = 1 - \frac{0}{\hat \ell_1} = 1. $$ </span>
<span id="cb66-770"><a href="#cb66-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-771"><a href="#cb66-771" aria-hidden="true" tabindex="-1"></a>In summary, McFadden's Rsquared</span>
<span id="cb66-772"><a href="#cb66-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-773"><a href="#cb66-773" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>is equal to zero if Model 2 does not add anything to Model 1, </span>
<span id="cb66-774"><a href="#cb66-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-775"><a href="#cb66-775" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>is equal to 1 if Model 2 provides perfect prediction of each data point (but model 1 does not). </span>
<span id="cb66-776"><a href="#cb66-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-777"><a href="#cb66-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-778"><a href="#cb66-778" aria-hidden="true" tabindex="-1"></a>Because McFadden's  R-squared is not a proportion of variance, we can't interpret it the same way as OLS R-squared. Instead, it is usually interpreted in terms of model *fit*. Here "fit" means how much better a nesting model's predictions are compared to a nested model, with zero meaning "not at all" and one meaning "as good as it gets". </span>
<span id="cb66-779"><a href="#cb66-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-780"><a href="#cb66-780" aria-hidden="true" tabindex="-1"></a>The following output shows McFadden's R-squared for the example data. **Please write down an interpretation of the R-square value and we will discuss your interpretations in class.** </span>
<span id="cb66-781"><a href="#cb66-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-782"><a href="#cb66-782" aria-hidden="true" tabindex="-1"></a>McFadden's R-squared: </span>
<span id="cb66-785"><a href="#cb66-785" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-786"><a href="#cb66-786" aria-hidden="true" tabindex="-1"></a><span class="co"># Using the deviance of the models</span></span>
<span id="cb66-787"><a href="#cb66-787" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> mod2<span class="sc">$</span>deviance<span class="sc">/</span>mod0<span class="sc">$</span>deviance</span>
<span id="cb66-788"><a href="#cb66-788" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-789"><a href="#cb66-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-790"><a href="#cb66-790" aria-hidden="true" tabindex="-1"></a>You can use the "show code" button to see an example interpretation below (but try it by yourself first!)</span>
<span id="cb66-791"><a href="#cb66-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-794"><a href="#cb66-794" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-795"><a href="#cb66-795" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpretation: Relative a model with no predictors, </span></span>
<span id="cb66-796"><a href="#cb66-796" aria-hidden="true" tabindex="-1"></a><span class="co"># the inclusion of age improved model fit by about 21.4% </span></span>
<span id="cb66-797"><a href="#cb66-797" aria-hidden="true" tabindex="-1"></a><span class="co"># as much as a model with perfect predictions. </span></span>
<span id="cb66-798"><a href="#cb66-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-799"><a href="#cb66-799" aria-hidden="true" tabindex="-1"></a><span class="co"># More elliptically: Using McFadden's R-squared the inclusion </span></span>
<span id="cb66-800"><a href="#cb66-800" aria-hidden="true" tabindex="-1"></a><span class="co"># of age improved model fit by about 21.4\%</span></span>
<span id="cb66-801"><a href="#cb66-801" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-802"><a href="#cb66-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-803"><a href="#cb66-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-804"><a href="#cb66-804" aria-hidden="true" tabindex="-1"></a><span class="fu">## Assumption checking {#sec-assumption-checking-10}</span></span>
<span id="cb66-805"><a href="#cb66-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-806"><a href="#cb66-806" aria-hidden="true" tabindex="-1"></a>The final topic we need to cover is assumption checking. As we saw when deriving the binomial likelihood in @sec-estimation-10, there are two main assumptions in play when estimating logistic regression using maximum likelihood.</span>
<span id="cb66-807"><a href="#cb66-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-808"><a href="#cb66-808" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The different respondents (person, units) are independent (i.e., a simple random sample). </span>
<span id="cb66-809"><a href="#cb66-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-810"><a href="#cb66-810" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We have the right model for each individual’s probability of CHD, $p_i$, which in this case is the logistic model. </span>
<span id="cb66-811"><a href="#cb66-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-812"><a href="#cb66-812" aria-hidden="true" tabindex="-1"></a>Assumption 1 is ensured by random sampling. If your data wasn't collected using random sampling, you'll need to use a different modeling approach (e.g, multilevel modeling, see EDUC 935). </span>
<span id="cb66-813"><a href="#cb66-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-814"><a href="#cb66-814" aria-hidden="true" tabindex="-1"></a>Assumption 2 can be checked with the Hosmer-Lemeshow (HL) test. The HL test is a chi-square test that compares the empirical probabilities of the outcome variable to the probabilities implied by the logistic regression model. In other words, we want to compare the two curves depicted below: </span>
<span id="cb66-815"><a href="#cb66-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-816"><a href="#cb66-816" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, HLtest, echo = F, fig.cap = "The idea behind the HL test", fig.align = 'center'}</span></span>
<span id="cb66-817"><a href="#cb66-817" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/HL_test.png"</span>)</span>
<span id="cb66-818"><a href="#cb66-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-819"><a href="#cb66-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-820"><a href="#cb66-820" aria-hidden="true" tabindex="-1"></a>The null hypothesis of the HL test is that these two curves are equivalent representations of the data. So, if we reject the null hypothesis, we reject claim that the model fits the data. </span>
<span id="cb66-821"><a href="#cb66-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-822"><a href="#cb66-822" aria-hidden="true" tabindex="-1"></a>The HL test for the example data is reported below. The R code is shown by default to provide details about how the test is computed.</span>
<span id="cb66-823"><a href="#cb66-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-824"><a href="#cb66-824" aria-hidden="true" tabindex="-1"></a>The <span class="in">`logitgof`</span> function uses 3 arguments:</span>
<span id="cb66-825"><a href="#cb66-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-826"><a href="#cb66-826" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`obs`</span> is the binary outcome variable (chd)</span>
<span id="cb66-827"><a href="#cb66-827" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`exp`</span> is the fitted values (predicted probabilities) from the logistic regression.</span>
<span id="cb66-828"><a href="#cb66-828" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`g`</span> is the number of groups / bins to use. For this example, 5 is a good number. </span>
<span id="cb66-829"><a href="#cb66-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-832"><a href="#cb66-832" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-833"><a href="#cb66-833" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb66-834"><a href="#cb66-834" aria-hidden="true" tabindex="-1"></a><span class="do">## Note: Install package "generalhoslem" if you haven't do so already</span></span>
<span id="cb66-835"><a href="#cb66-835" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("generalhoslem")</span></span>
<span id="cb66-836"><a href="#cb66-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-837"><a href="#cb66-837" aria-hidden="true" tabindex="-1"></a>hl_test <span class="ot">&lt;-</span> generalhoslem<span class="sc">::</span><span class="fu">logitgof</span>(<span class="at">obs =</span> chd, <span class="at">exp =</span> <span class="fu">fitted</span>(mod2), <span class="at">g =</span> <span class="dv">5</span>)</span>
<span id="cb66-838"><a href="#cb66-838" aria-hidden="true" tabindex="-1"></a>hl_test</span>
<span id="cb66-839"><a href="#cb66-839" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-840"><a href="#cb66-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-841"><a href="#cb66-841" aria-hidden="true" tabindex="-1"></a>Using APA notation, we could write the results of the test as $\chi^2(3) = .05, p = .99$. **Please write down your interpretation of the HL test and be prepared to share you answer in class. Your interpretation should mention whether the assumption that the logistic model fits the data is problematic or not.**</span>
<span id="cb66-842"><a href="#cb66-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-843"><a href="#cb66-843" aria-hidden="true" tabindex="-1"></a><span class="fu">### More details on the HL test*</span></span>
<span id="cb66-844"><a href="#cb66-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-845"><a href="#cb66-845" aria-hidden="true" tabindex="-1"></a>The HL statistic is computed using the observed and expected (model-implied) counts in each of the groups. Computational details are given below. The number of groups must be chosen by the researcher and should be based on considerations about sample size (e.g., the number of observation per group should be at least 20 to estimate a proportion).     </span>
<span id="cb66-846"><a href="#cb66-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-849"><a href="#cb66-849" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-850"><a href="#cb66-850" aria-hidden="true" tabindex="-1"></a><span class="co"># Observed and expected counts</span></span>
<span id="cb66-851"><a href="#cb66-851" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(hl_test<span class="sc">$</span>observed, hl_test<span class="sc">$</span>expected)</span>
<span id="cb66-852"><a href="#cb66-852" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-853"><a href="#cb66-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-854"><a href="#cb66-854" aria-hidden="true" tabindex="-1"></a>The test statistic is the built up using the differences between the observed and expected values:</span>
<span id="cb66-855"><a href="#cb66-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-856"><a href="#cb66-856" aria-hidden="true" tabindex="-1"></a>$$HL = \sum_{g = 1}^5 \left(\frac{(\text{y0}_g - \text{yhat0}_g)^2}{\text{yhat0}_g} + \frac{(\text{y1}_g - \text{yhat1}_g)^2}{\text{yhat1}_g}\right)$$</span>
<span id="cb66-857"><a href="#cb66-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-858"><a href="#cb66-858" aria-hidden="true" tabindex="-1"></a>Hosmer and Lemeshow showed that this statistic has a chi-square distribution on $df = N_\text{groups} - 2$ when the null hypothesis is true (i.e., when the model fits the data). </span>
<span id="cb66-859"><a href="#cb66-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-860"><a href="#cb66-860" aria-hidden="true" tabindex="-1"></a><span class="fu">## Workbook </span></span>
<span id="cb66-861"><a href="#cb66-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-862"><a href="#cb66-862" aria-hidden="true" tabindex="-1"></a>This section collects the questions asked in this chapter. The lessons for this chapter will focus on discussing these questions and then working on the exercises in @sec-exercises-10. If you haven't written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc. </span>
<span id="cb66-863"><a href="#cb66-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-864"><a href="#cb66-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-865"><a href="#cb66-865" aria-hidden="true" tabindex="-1"></a>@sec-chd-example-10</span>
<span id="cb66-866"><a href="#cb66-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-867"><a href="#cb66-867" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Please take a moment to write down you conclusions (and rationale) about whether the assumptions of linear regression are met for the CHD data.</span>
<span id="cb66-868"><a href="#cb66-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-869"><a href="#cb66-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-870"><a href="#cb66-870" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = F, fig.cap = "Linear probability model with CHD example", fig.align = 'center', fig.width = 12}</span></span>
<span id="cb66-871"><a href="#cb66-871" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(chd <span class="sc">~</span> age, <span class="at">data =</span> chd.data)</span>
<span id="cb66-872"><a href="#cb66-872" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb66-873"><a href="#cb66-873" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="dv">1</span>)</span>
<span id="cb66-874"><a href="#cb66-874" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="dv">2</span>)</span>
<span id="cb66-875"><a href="#cb66-875" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb66-876"><a href="#cb66-876" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-877"><a href="#cb66-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-878"><a href="#cb66-878" aria-hidden="true" tabindex="-1"></a>@sec-logit-10</span>
<span id="cb66-879"><a href="#cb66-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-880"><a href="#cb66-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-881"><a href="#cb66-881" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Using the Table below, please write down your answers to the following questions and be prepared to share them in class. For each question provide a verbal interpretation of the numerical answer (e.g, odds of 2 to 1 means that for every two people with a trait, there is one without). </span>
<span id="cb66-882"><a href="#cb66-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-883"><a href="#cb66-883" aria-hidden="true" tabindex="-1"></a><span class="ss">  1. </span>What is the probability of a person in their 40s having CHD? </span>
<span id="cb66-884"><a href="#cb66-884" aria-hidden="true" tabindex="-1"></a><span class="ss">  2. </span>What are the odds of a person in their 40s having CHD? </span>
<span id="cb66-885"><a href="#cb66-885" aria-hidden="true" tabindex="-1"></a><span class="ss">  3. </span>What is the probability of someone in their 50s **not** having CHD? </span>
<span id="cb66-886"><a href="#cb66-886" aria-hidden="true" tabindex="-1"></a><span class="ss">  4. </span>What are the odds of someone in their 50s **not** having CHD? </span>
<span id="cb66-887"><a href="#cb66-887" aria-hidden="true" tabindex="-1"></a><span class="ss">  5. </span>What is probability of having CHD in your 40s, compared to your 30s? (e.g., is 3 times higher? 4 times higher?)</span>
<span id="cb66-888"><a href="#cb66-888" aria-hidden="true" tabindex="-1"></a><span class="ss">  6. </span>What are the odds of having CHD in your 40s, compared to your 30s? </span>
<span id="cb66-889"><a href="#cb66-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-890"><a href="#cb66-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-891"><a href="#cb66-891" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = T, fig.cap = "Proportions and odds", fig.align = 'center'}</span></span>
<span id="cb66-892"><a href="#cb66-892" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/odds.png"</span>)</span>
<span id="cb66-893"><a href="#cb66-893" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-894"><a href="#cb66-894" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb66-895"><a href="#cb66-895" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb66-896"><a href="#cb66-896" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Using the Table below, please answer the following questions.</span>
<span id="cb66-897"><a href="#cb66-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-898"><a href="#cb66-898" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If probability of an event is equal to .1, what are the odds the event? What is the logit of the event? </span>
<span id="cb66-899"><a href="#cb66-899" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If odds of an event are 4 to 1, what is the probability of the event? What is the logit of the event? </span>
<span id="cb66-900"><a href="#cb66-900" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If logit &lt; 0 then probability &lt;  ? and odds &lt; ? </span>
<span id="cb66-901"><a href="#cb66-901" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>What is more likely: a event with probability of .9 or an event with odds of .9? </span>
<span id="cb66-902"><a href="#cb66-902" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If a probability of $p$ corresponds to a  $\text{logit}$ of $x$, what is the logit corresponding to $1-p$? (Hint, try some numerical examples from the table). </span>
<span id="cb66-903"><a href="#cb66-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-904"><a href="#cb66-904" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = F, fig.cap = "Logistic, odds, and logit", fig.align = 'center'}</span></span>
<span id="cb66-905"><a href="#cb66-905" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/logit-table.png"</span>)</span>
<span id="cb66-906"><a href="#cb66-906" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-907"><a href="#cb66-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-908"><a href="#cb66-908" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb66-909"><a href="#cb66-909" aria-hidden="true" tabindex="-1"></a>@sec-simple-10</span>
<span id="cb66-910"><a href="#cb66-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-911"><a href="#cb66-911" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Please practice your interpretation of the OR in simple logistic regression using the following examples: </span>
<span id="cb66-912"><a href="#cb66-912" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If $b=0$ what is the OR equal to? What is the percent change in the odds for a one unit increase in $X$? </span>
<span id="cb66-913"><a href="#cb66-913" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If $b=.25$ what is the OR equal to? What is the percent change in the odds for a one unit increase in $X$? </span>
<span id="cb66-914"><a href="#cb66-914" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If $b=-.025$ what is the OR equal to? What is the percent change in the odds for a 10 unit increase in $X$? </span>
<span id="cb66-915"><a href="#cb66-915" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>If the odds increase 100% for a one unit increase in $X$, what $b$ equal to?</span>
<span id="cb66-916"><a href="#cb66-916" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb66-917"><a href="#cb66-917" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb66-918"><a href="#cb66-918" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For the CHD example (model given below), please write down the numerical values of each of the  summaries listed below. You can select any values of age to report the predicted probabilities and risk ratios, and you can "eye ball" the probabilities using @fig-pred-prob. </span>
<span id="cb66-919"><a href="#cb66-919" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>In terms of the logit. </span>
<span id="cb66-920"><a href="#cb66-920" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>In terms of the odds. </span>
<span id="cb66-921"><a href="#cb66-921" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>In terms of predicted probabilities.</span>
<span id="cb66-922"><a href="#cb66-922" aria-hidden="true" tabindex="-1"></a><span class="ss">      * </span>Risk ratios / relative risk for some chosen ages.</span>
<span id="cb66-923"><a href="#cb66-923" aria-hidden="true" tabindex="-1"></a><span class="ss">      * </span>The age at which the probability of having CHD is "50-50".</span>
<span id="cb66-924"><a href="#cb66-924" aria-hidden="true" tabindex="-1"></a><span class="ss">      * </span>The approximate rate of increase (slope) of the probability of having CHD for people close to the "50-50" point.  </span>
<span id="cb66-925"><a href="#cb66-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-926"><a href="#cb66-926" aria-hidden="true" tabindex="-1"></a>$$ \text{logit}(CHD) = -5.31  + .11 (\text{age}). $$</span>
<span id="cb66-927"><a href="#cb66-927" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb66-928"><a href="#cb66-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-929"><a href="#cb66-929" aria-hidden="true" tabindex="-1"></a>@sec-estimation-10</span>
<span id="cb66-930"><a href="#cb66-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-931"><a href="#cb66-931" aria-hidden="true" tabindex="-1"></a>To write the likelihood for person $i$, we use the notation $L_i$</span>
<span id="cb66-932"><a href="#cb66-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-933"><a href="#cb66-933" aria-hidden="true" tabindex="-1"></a>$$ L = p ^ {CHD} \times (1 - p)^{1 - CHD} $$</span>
<span id="cb66-934"><a href="#cb66-934" aria-hidden="true" tabindex="-1"></a>and to write the likelihood for the full sample, we use use $L$ without a subscript:</span>
<span id="cb66-935"><a href="#cb66-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-936"><a href="#cb66-936" aria-hidden="true" tabindex="-1"></a>$$ L = \Pi_{i = 1}^N L_i = L_1 \times  L_2 \times \cdots \times L_N. $$</span>
<span id="cb66-937"><a href="#cb66-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-938"><a href="#cb66-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-939"><a href="#cb66-939" aria-hidden="true" tabindex="-1"></a>If we plug into the last equation using the values of $CHD$ in our sample, this gives us the likelihood of observing the sample. For example, the values of CHD for the first four cases in the example are (1, 1, 0, 1). So, the first four terms in the likelihood are: </span>
<span id="cb66-940"><a href="#cb66-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-941"><a href="#cb66-941" aria-hidden="true" tabindex="-1"></a>$$ L = p_1 \times  p_2 \times (1-p_3) \times p_4 \times\cdots $$</span>
<span id="cb66-942"><a href="#cb66-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-943"><a href="#cb66-943" aria-hidden="true" tabindex="-1"></a>Please take a moment to verify this result for yourself and we will walk through the derivation together in class.</span>
<span id="cb66-944"><a href="#cb66-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-945"><a href="#cb66-945" aria-hidden="true" tabindex="-1"></a>@sec-inference-10</span>
<span id="cb66-946"><a href="#cb66-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-947"><a href="#cb66-947" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The output below shows the odds ratios and confidence intervals for the example data. In APA notation we could write the results for age as $OR = 1.12, 95\% \text{ CI: } <span class="co">[</span><span class="ot">1.07, 1.19</span><span class="co">]</span>$. Please write down your interpretation of the OR for age and its confidence interval. In your interpretation, please mention the relative change in the odds that is associated with each additional year of age and whether or not this is statistically different from zero. </span>
<span id="cb66-948"><a href="#cb66-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-949"><a href="#cb66-949" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, message=F}</span></span>
<span id="cb66-950"><a href="#cb66-950" aria-hidden="true" tabindex="-1"></a>ci.b <span class="ot">&lt;-</span> <span class="fu">confint</span>(mod2)</span>
<span id="cb66-951"><a href="#cb66-951" aria-hidden="true" tabindex="-1"></a>ci.table <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">coefs =</span> <span class="fu">coef</span>(mod2), ci.b)</span>
<span id="cb66-952"><a href="#cb66-952" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(ci.table)</span>
<span id="cb66-953"><a href="#cb66-953" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-954"><a href="#cb66-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-955"><a href="#cb66-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-956"><a href="#cb66-956" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To conduct an LR test of the model with age as a predictor against a model without this predictor (i.e., the model with only the intercept), we can use the <span class="in">`lrtest`</span> function of the <span class="in">`lmtest`</span> package to obtain the following output. In APA format, we could write this output as $\chi^2(1) = 29.31, p &lt; .001$ Please write down your interpretation of the LR test for the example. Your interpretation should mention which two models were compared, whether the LR test was significant or not, and your conclusion about which model provided a better explanation of the data.</span>
<span id="cb66-957"><a href="#cb66-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-960"><a href="#cb66-960" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-961"><a href="#cb66-961" aria-hidden="true" tabindex="-1"></a><span class="co"># The model with no predictors</span></span>
<span id="cb66-962"><a href="#cb66-962" aria-hidden="true" tabindex="-1"></a>mod0 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> <span class="dv">1</span>, <span class="at">family =</span> <span class="st">"binomial"</span>, <span class="at">data =</span> chd.data)</span>
<span id="cb66-963"><a href="#cb66-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-964"><a href="#cb66-964" aria-hidden="true" tabindex="-1"></a><span class="co"># The LR test (enter the smaller model first)</span></span>
<span id="cb66-965"><a href="#cb66-965" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">lrtest</span>(mod0, mod2)</span>
<span id="cb66-966"><a href="#cb66-966" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-967"><a href="#cb66-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-968"><a href="#cb66-968" aria-hidden="true" tabindex="-1"></a>@sec-pseudo-rsquared-10</span>
<span id="cb66-969"><a href="#cb66-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-970"><a href="#cb66-970" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The following output shows McFadden's R-squared for the example data (i.e., comparing a model with age as a predictor to a model with just the intercept). Please write down an interpretation of the R-square value and we will discuss you interpretations in class.</span>
<span id="cb66-971"><a href="#cb66-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-974"><a href="#cb66-974" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-975"><a href="#cb66-975" aria-hidden="true" tabindex="-1"></a><span class="co"># Using the deviance of the models</span></span>
<span id="cb66-976"><a href="#cb66-976" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> mod2<span class="sc">$</span>deviance<span class="sc">/</span>mod0<span class="sc">$</span>deviance</span>
<span id="cb66-977"><a href="#cb66-977" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-978"><a href="#cb66-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-979"><a href="#cb66-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-980"><a href="#cb66-980" aria-hidden="true" tabindex="-1"></a>@sec-assumption-checking-10</span>
<span id="cb66-981"><a href="#cb66-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-982"><a href="#cb66-982" aria-hidden="true" tabindex="-1"></a>The HL test for the example data is reported below. Using APA notation, we could write the results of the test as $\chi^2(3) = .05, p = .99$. Please write down your interpretation of the HL test and be prepared to share you answer in class. Your interpretation should mention whether the assumption that the logistic model fits the data is problematic or not.</span>
<span id="cb66-983"><a href="#cb66-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-986"><a href="#cb66-986" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-987"><a href="#cb66-987" aria-hidden="true" tabindex="-1"></a><span class="do">## Note: Install package "generalhoslem" if you haven't do so already</span></span>
<span id="cb66-988"><a href="#cb66-988" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("generalhoslem")</span></span>
<span id="cb66-989"><a href="#cb66-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-990"><a href="#cb66-990" aria-hidden="true" tabindex="-1"></a>hl_test <span class="ot">&lt;-</span> generalhoslem<span class="sc">::</span><span class="fu">logitgof</span>(<span class="at">obs =</span> chd, <span class="at">exp =</span> <span class="fu">fitted</span>(mod2), <span class="at">g =</span> <span class="dv">5</span>)</span>
<span id="cb66-991"><a href="#cb66-991" aria-hidden="true" tabindex="-1"></a>hl_test</span>
<span id="cb66-992"><a href="#cb66-992" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-993"><a href="#cb66-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-994"><a href="#cb66-994" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises: Multiple logistic regression {#sec-exercise-10}</span></span>
<span id="cb66-995"><a href="#cb66-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-996"><a href="#cb66-996" aria-hidden="true" tabindex="-1"></a>These exercises are a bit different than usual. Rather than repeating the code that produced the previously discussed example, we extend the example to illustrate multiple logistic regression. The general idea is that, everything you can do in OLS linear regression you can also do in logistic regression, but some of the details are different. </span>
<span id="cb66-997"><a href="#cb66-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-998"><a href="#cb66-998" aria-hidden="true" tabindex="-1"></a>We will go through this material in class together, so you don't need to work on it before class (but you can if you want.) Before staring this section, you may find it useful to scroll to the top of the page, click on the "&lt;/&gt; Code" menu, and select "Show All Code."</span>
<span id="cb66-999"><a href="#cb66-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1000"><a href="#cb66-1000" aria-hidden="true" tabindex="-1"></a><span class="fu">### CHD and smoking</span></span>
<span id="cb66-1001"><a href="#cb66-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1002"><a href="#cb66-1002" aria-hidden="true" tabindex="-1"></a>This section works through multiple logistic regression with interactions using the CHD data again. Coronary Heart Disease (<span class="in">`chd`</span>; 1 = yes, 0 = no) is still the outcome, and we predict it using a person's age in years (<span class="in">`age`</span>) as well as an indicator for whether a person has ever (or currently) smokes (<span class="in">`smokes`</span>: 1 = yes, 0 = no).</span>
<span id="cb66-1003"><a href="#cb66-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1006"><a href="#cb66-1006" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-1007"><a href="#cb66-1007" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data, take a look</span></span>
<span id="cb66-1008"><a href="#cb66-1008" aria-hidden="true" tabindex="-1"></a><span class="co">#load("CHD.RData")</span></span>
<span id="cb66-1009"><a href="#cb66-1009" aria-hidden="true" tabindex="-1"></a><span class="co">#attach(chd.data)</span></span>
<span id="cb66-1010"><a href="#cb66-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1011"><a href="#cb66-1011" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">cbind</span>(chd, age, smokes))</span>
<span id="cb66-1012"><a href="#cb66-1012" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-1013"><a href="#cb66-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1014"><a href="#cb66-1014" aria-hidden="true" tabindex="-1"></a>Let's start by regressing CHD on smoking and age, and refreshing our interpretations of the parameter estimates and tests. The regression coefficients are interpreted analogously to multiple regression (e.g., as holding constant or controlling for the other predictors). </span>
<span id="cb66-1015"><a href="#cb66-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1018"><a href="#cb66-1018" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-1019"><a href="#cb66-1019" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress CHD on smokes and age</span></span>
<span id="cb66-1020"><a href="#cb66-1020" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age <span class="sc">+</span> smokes, <span class="at">family =</span> binomial)</span>
<span id="cb66-1021"><a href="#cb66-1021" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span>
<span id="cb66-1022"><a href="#cb66-1022" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-1023"><a href="#cb66-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1024"><a href="#cb66-1024" aria-hidden="true" tabindex="-1"></a>For illustrative purposes, let’s double check that smoking predicts CHD over and above age using the LR test. Recall that this test is analogous to the F-test of R-squared change in OLS regression</span>
<span id="cb66-1025"><a href="#cb66-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1028"><a href="#cb66-1028" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-1029"><a href="#cb66-1029" aria-hidden="true" tabindex="-1"></a><span class="co"># LR test coefficient on age, and age + smokes </span></span>
<span id="cb66-1030"><a href="#cb66-1030" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age, <span class="at">family =</span> binomial)</span>
<span id="cb66-1031"><a href="#cb66-1031" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age <span class="sc">+</span> smokes, <span class="at">family =</span> binomial)</span>
<span id="cb66-1032"><a href="#cb66-1032" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">lrtest</span>(mod1, mod2)</span>
<span id="cb66-1033"><a href="#cb66-1033" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-1034"><a href="#cb66-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1035"><a href="#cb66-1035" aria-hidden="true" tabindex="-1"></a>Both the Wald test and LR test lead to the same overall interpretation for smoking.</span>
<span id="cb66-1036"><a href="#cb66-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1037"><a href="#cb66-1037" aria-hidden="true" tabindex="-1"></a>How should we compute R-squared for the two-predictor model? Two different approaches are shown below. Both of these are defensible, but note the difference in interpretation as we change the "baseline model". </span>
<span id="cb66-1038"><a href="#cb66-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1041"><a href="#cb66-1041" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-1042"><a href="#cb66-1042" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare against model 1</span></span>
<span id="cb66-1043"><a href="#cb66-1043" aria-hidden="true" tabindex="-1"></a>(R_21 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mod2<span class="sc">$</span>deviance<span class="sc">/</span>mod1<span class="sc">$</span>deviance)</span>
<span id="cb66-1044"><a href="#cb66-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1045"><a href="#cb66-1045" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare against model with only the intercept</span></span>
<span id="cb66-1046"><a href="#cb66-1046" aria-hidden="true" tabindex="-1"></a>mod0 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> <span class="dv">1</span>, <span class="at">family =</span> binomial)</span>
<span id="cb66-1047"><a href="#cb66-1047" aria-hidden="true" tabindex="-1"></a>(R_20 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mod2<span class="sc">$</span>deviance<span class="sc">/</span>mod0<span class="sc">$</span>deviance)</span>
<span id="cb66-1048"><a href="#cb66-1048" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-1049"><a href="#cb66-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1050"><a href="#cb66-1050" aria-hidden="true" tabindex="-1"></a>Recall that it is common to report the output using odds ratios rather than logits. Make sure to check your interpretation of the ORs. </span>
<span id="cb66-1051"><a href="#cb66-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1054"><a href="#cb66-1054" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-1055"><a href="#cb66-1055" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(mod2), <span class="fu">confint</span>(mod2)))</span>
<span id="cb66-1056"><a href="#cb66-1056" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-1057"><a href="#cb66-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1058"><a href="#cb66-1058" aria-hidden="true" tabindex="-1"></a><span class="fu">### Predicted probabilities</span></span>
<span id="cb66-1059"><a href="#cb66-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1060"><a href="#cb66-1060" aria-hidden="true" tabindex="-1"></a>We can plot the regression lines for smokers and non-smokers using the same techniques as for linear regression.</span>
<span id="cb66-1061"><a href="#cb66-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1064"><a href="#cb66-1064" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-1065"><a href="#cb66-1065" aria-hidden="true" tabindex="-1"></a><span class="co"># Plots of the regression lines in logit scale</span></span>
<span id="cb66-1066"><a href="#cb66-1066" aria-hidden="true" tabindex="-1"></a>visreg<span class="sc">::</span><span class="fu">visreg</span>(mod2, </span>
<span id="cb66-1067"><a href="#cb66-1067" aria-hidden="true" tabindex="-1"></a>               <span class="at">xvar =</span> <span class="st">"age"</span>, </span>
<span id="cb66-1068"><a href="#cb66-1068" aria-hidden="true" tabindex="-1"></a>               <span class="at">by =</span> <span class="st">"smokes"</span>, </span>
<span id="cb66-1069"><a href="#cb66-1069" aria-hidden="true" tabindex="-1"></a>               <span class="at">scale =</span> <span class="st">"linear"</span>, </span>
<span id="cb66-1070"><a href="#cb66-1070" aria-hidden="true" tabindex="-1"></a>               <span class="at">overlay =</span> T)</span>
<span id="cb66-1071"><a href="#cb66-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1072"><a href="#cb66-1072" aria-hidden="true" tabindex="-1"></a><span class="co"># Plots of the regression lines in probability scale</span></span>
<span id="cb66-1073"><a href="#cb66-1073" aria-hidden="true" tabindex="-1"></a>visreg<span class="sc">::</span><span class="fu">visreg</span>(mod2, </span>
<span id="cb66-1074"><a href="#cb66-1074" aria-hidden="true" tabindex="-1"></a>               <span class="at">xvar =</span> <span class="st">"age"</span>, </span>
<span id="cb66-1075"><a href="#cb66-1075" aria-hidden="true" tabindex="-1"></a>               <span class="at">by =</span> <span class="st">"smokes"</span>, </span>
<span id="cb66-1076"><a href="#cb66-1076" aria-hidden="true" tabindex="-1"></a>               <span class="at">scale =</span> <span class="st">"response"</span>, </span>
<span id="cb66-1077"><a href="#cb66-1077" aria-hidden="true" tabindex="-1"></a>               <span class="at">overlay =</span> T)</span>
<span id="cb66-1078"><a href="#cb66-1078" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-1079"><a href="#cb66-1079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1080"><a href="#cb66-1080" aria-hidden="true" tabindex="-1"></a>We can also test for differences between smokers and non-smokers at specific ages, just like we did for the OLS regression. Below is an example of using <span class="in">`emmeans`</span> to compute the risk ratio associated with smoking at +/- 1SD on age.</span>
<span id="cb66-1081"><a href="#cb66-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1084"><a href="#cb66-1084" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-1085"><a href="#cb66-1085" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1. Make a function to compute +/- SD on age</span></span>
<span id="cb66-1086"><a href="#cb66-1086" aria-hidden="true" tabindex="-1"></a>sd_function <span class="ot">&lt;-</span> <span class="cf">function</span>(x) { <span class="fu">c</span>(<span class="fu">mean</span>(x) <span class="sc">-</span> <span class="fu">sd</span>(x), <span class="fu">mean</span>(x) <span class="sc">+</span> <span class="fu">sd</span>(x)) }</span>
<span id="cb66-1087"><a href="#cb66-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1088"><a href="#cb66-1088" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2. Compute and test the values on the odds scale</span></span>
<span id="cb66-1089"><a href="#cb66-1089" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(emmeans)</span>
<span id="cb66-1090"><a href="#cb66-1090" aria-hidden="true" tabindex="-1"></a>gap_odds <span class="ot">&lt;-</span> <span class="fu">emmeans</span>(mod2, </span>
<span id="cb66-1091"><a href="#cb66-1091" aria-hidden="true" tabindex="-1"></a>                    <span class="at">specs =</span> <span class="st">"smokes"</span>, </span>
<span id="cb66-1092"><a href="#cb66-1092" aria-hidden="true" tabindex="-1"></a>                    <span class="at">by =</span> <span class="st">"age"</span>, </span>
<span id="cb66-1093"><a href="#cb66-1093" aria-hidden="true" tabindex="-1"></a>                    <span class="at">cov.reduce =</span> sd_function, </span>
<span id="cb66-1094"><a href="#cb66-1094" aria-hidden="true" tabindex="-1"></a>                    <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb66-1095"><a href="#cb66-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1096"><a href="#cb66-1096" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert odds to probabilities</span></span>
<span id="cb66-1097"><a href="#cb66-1097" aria-hidden="true" tabindex="-1"></a>gap_prob <span class="ot">&lt;-</span> <span class="fu">regrid</span>(gap_odds, <span class="st">"log"</span>)</span>
<span id="cb66-1098"><a href="#cb66-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1099"><a href="#cb66-1099" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute risk ratios for smoking, at different ages.</span></span>
<span id="cb66-1100"><a href="#cb66-1100" aria-hidden="true" tabindex="-1"></a><span class="fu">contrast</span>(gap_prob, <span class="at">method =</span> <span class="st">"revpairwise"</span>)</span>
<span id="cb66-1101"><a href="#cb66-1101" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-1102"><a href="#cb66-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1103"><a href="#cb66-1103" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interactions</span></span>
<span id="cb66-1104"><a href="#cb66-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1105"><a href="#cb66-1105" aria-hidden="true" tabindex="-1"></a>Next, let's consider the interaction between age (centered) and smoking. This interaction addresses whether the relationship between age and CHD changes as a function of smoking status. </span>
<span id="cb66-1106"><a href="#cb66-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1107"><a href="#cb66-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1110"><a href="#cb66-1110" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-1111"><a href="#cb66-1111" aria-hidden="true" tabindex="-1"></a><span class="co"># Interacting smoking with age (centered)</span></span>
<span id="cb66-1112"><a href="#cb66-1112" aria-hidden="true" tabindex="-1"></a>age_centered <span class="ot">&lt;-</span> age <span class="sc">-</span> <span class="fu">mean</span>(age)</span>
<span id="cb66-1113"><a href="#cb66-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1114"><a href="#cb66-1114" aria-hidden="true" tabindex="-1"></a>mod4 <span class="ot">&lt;-</span> <span class="fu">glm</span>(chd <span class="sc">~</span> age_centered<span class="sc">*</span>smokes, <span class="at">family =</span> binomial)</span>
<span id="cb66-1115"><a href="#cb66-1115" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="at">OR =</span> <span class="fu">coef</span>(mod4), <span class="fu">confint</span>(mod4)))</span>
<span id="cb66-1116"><a href="#cb66-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1117"><a href="#cb66-1117" aria-hidden="true" tabindex="-1"></a>visreg<span class="sc">::</span><span class="fu">visreg</span>(mod4, </span>
<span id="cb66-1118"><a href="#cb66-1118" aria-hidden="true" tabindex="-1"></a>               <span class="at">xvar =</span> <span class="st">"age_centered"</span>, </span>
<span id="cb66-1119"><a href="#cb66-1119" aria-hidden="true" tabindex="-1"></a>               <span class="at">by =</span> <span class="st">"smokes"</span>, </span>
<span id="cb66-1120"><a href="#cb66-1120" aria-hidden="true" tabindex="-1"></a>               <span class="at">scale =</span> <span class="st">"response"</span>, </span>
<span id="cb66-1121"><a href="#cb66-1121" aria-hidden="true" tabindex="-1"></a>               <span class="at">overlay =</span> T)</span>
<span id="cb66-1122"><a href="#cb66-1122" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(chd.data)</span>
<span id="cb66-1123"><a href="#cb66-1123" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-1124"><a href="#cb66-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-1125"><a href="#cb66-1125" aria-hidden="true" tabindex="-1"></a>We will talk about the interpretation of this model in class. </span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>