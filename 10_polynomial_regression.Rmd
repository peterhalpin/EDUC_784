# Polynomial regression, etc {#chapter-10}

<!-- this should be called nonlinearity (and heteroskedasticity) and come before log-linear modeling --> 

```{r, echo = F}
# Clean up check
rm(list = ls())


button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```

This chapter collects some other useful techniques for addressing assumption violations in linear regression.  

One approach involves transforming the $X$-variable(s), which can be done in addition to (or instead of) transforming the $Y$ variable. This approach is used to address violations of the assumption of linearity, and this chapter will cover two widely-used techniques 

* Polynomial regression, which means raising $X$-variables to a power (e.g., $X^2$ and $X^3$), and 
* Piecewise or segmented regression, which involves using different regression lines over different ranges of a predictor. 

Both polynomial and piecewise regression are very useful in practice and lead to advanced topics like splines and semi-parametric regression. They also turn out to be special cases of interactions, so we have already covered a lot of the technical details in Chapter \@ref(chapter-6) -- phew! 

We also consider how to deal with heteroskedasticity. The short version is that heteroskedasticity affects the standard errors of regression coefficients, and, consequently, the t-values, p-values, and confidence intervals. There are lots of ways to make standard errors robust to heteroskedasticity, and we will focus on one widely used procedure called heteroskedasticity-corrected (HC) standard errors. 

## Polynomial regression {#polynomial-10}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Polynomial regression means that we regress $Y$ on a polynomial function of $X$:

\[ \widehat Y = b_0 + b_1 X + b_2 X^2 + b_3 X^3 + ....\]

Your first thought might be, "doesn’t this contradict the assumption that regression is linear?" The answer here is a bit subtle. 

As with regular linear regression, the polynomial model is linear in the coefficients -- we don’t raise the regression coefficients to a power (e.g., $b_1^2$), or multiply coefficients together (e.g, $b_1 \times b_2$). This is the technical sense in which polynomial regression is still just linear regression, despite its name. 

Polynomial regression does use nonlinear functions of the predictor(s), but the model is agnostic to what you do with your data. The situation here is a lot like when we worked with interactions in Chapter \@ref(chapter-6). In order to model interactions, we computed the product of two predictors and entered the product into the model as a third predictor. Well, $X^2$ is the product of a predictor with itself, so, in this sense, the quadratic term in a polynomial regression is just a special case of an interaction between two variables. 

Although we did not cover interactions among more than two variables in this course, they are computed in the same way -- e.g., a "three-way" interaction is just the product of 3 predictors. Similarly, $X^3$ is just the three-fold product of a variable with itself. In general, interactions and polynomial regression are related in the same way as multiplication and exponentiation. 

Although polynomial regression is formally similar to interactions, it is used for a different purpose. Interactions address how the relationship between two variables changes as a function of a third. Their inclusion in a model is  usually motivated by a specific research question that is formulated before doing the data analysis (see Chapter \@ref(chapter-6)). 

By contrast, polynomial regression is used to address a non-linear relationship between $Y$ and $X$, and is usually motivated by a preliminary examination of data that indicates the presence of such a relationship (e.g., a scatter plot of $Y$ versus $X$; a residual versus fitted plot). While it is possible to formulate research questions about polynomial terms in a regression model, this is not necessarily or even usually the case when polynomial regression is used -- often its just used to address violations of the linearity assumption. 

### Recap of polynomials

In general, a polynomial of degree $n$ (i.e., highest power of $n$) produces a curve that can have up to $n-1$ bends (minima and maxima). Some examples are illustrated in Figure \@ref(fig:poly) below.

* The (orange) linear function of $X$ is a polynomial of degree 1 and has zero bends.
* The (green) quadratic function of $X$ is a polynomial of degree 2 and has 1 bend (a minimum at $X = 0$; this is also called a parabola).
* etc.

```{r, poly, echo = F, fig.cap = "Examples of Polynomials", fig.align = 'center'}
knitr::include_graphics("images/poly.png", dpi = 150)
```

As we can see, this is a very flexible approach to capturing non-linear relationships between two variables. In fact, it can be too flexible!

### Polynomials and curve fitting

Figure \@ref(fig:overfit) shows three different regression models fitted to the same bivariate data. 

* In the left panel, a standard linear regression model is used, and we can see that the model does not capture the nonlinear (quadratic) trend in the data. 

* The middle panel uses a quadratic model (i.e., includes $X^2$ as a predictor, as well as $X$), and fits the data quite well. 

* The right panel uses a 16-degree polynomial to fit the data. We can see that is has a higher R-squared than the quadratic model. But there is also something fishy about this model, don't you agree? 

**Before moving on please take a moment to write down your intuitions about what is going in the right-hand panel of Figure \@ref(fig:overfit) and whether this model really is better than the one in the middle panel. I will ask you to share your thoughts in class.**

```{r, overfit, echo = F, fig.cap = "Polynomial Regression Examples", fig.align = 'center', fig.width = 12}
# Generate data
set.seed(1)
X <- sort(runif(20, -2, 2))
e <- rnorm(20, 0, .5)
Y <- 2 + X - X^2 + e

# Three regression models
mod1 <- lm(Y ~ X)
mod2 <- lm(Y ~ poly(X, 2))
mod3 <- lm(Y ~ poly(X, 16))

# Plots
par(mfrow = c(1, 3)) 

title1 <- paste0("R-squared = ", round(summary(mod1)$r.square, 3))
plot(X, Y, col = "#4B9CD3", pch = 10, lwd = 5, main = title1)
points(X, fitted(mod1), col = 1, type = "l", lwd = 2)

title2 <- paste0("R-squared = ", round(summary(mod2)$r.square, 3))
plot(X, Y, col = "#4B9CD3", pch = 10, lwd = 5, main = title2)
points(X, fitted(mod2), col = 2, type = "l", lwd = 2)

title3 <- paste0("R-squared = ", round(summary(mod3)$r.square, 3))
plot(X, Y, col = "#4B9CD3", pch = 10, lwd = 5, main = title3)
points(X, fitted(mod3), col = 3, type = "l", lwd = 2)
```

To help formulate your intuitions, you might find it useful to consider the plots below. In these plots a second sample was drawn from the same population model, and the regression lines from Figure \@ref(fig:overfit) were added to the plots. Note that the regression parameters were not re-estimated using the second data set. The model parameters from the first data set were used to produce the regression lines for the second data set. 

```{r, overfit2, echo = F, fig.cap = "Polynomial Regression Examples (With New Data)", fig.align = 'center', fig.width = 12}
# Generate data
set.seed(4)
e <- rnorm(20, 0, .5)
Y1 <- 2 + X - X^2 + e

# Three R-squared values
R.squared1 <- 1 - sum((Y1 - fitted(mod1))^2) / var(Y1) / 19
R.squared2 <- 1 - sum((Y1 - fitted(mod2))^2) / var(Y1) / 19
R.squared3 <- 1 - sum((Y1 - fitted(mod3))^2) / var(Y1) / 19
# Plots

par(mfrow = c(1, 3)) 
title1 <- paste0("R-squared = ", round(R.squared1, 3))
plot(X, Y1, col = "#4B9CD3", pch = 10, lwd = 5, main = title1)
points(X, fitted(mod1), col = 1, type = "l", lwd = 2)

title2 <- paste0("R-squared = ", round(R.squared2, 3))
plot(X, Y1, col = "#4B9CD3", pch = 10, lwd = 5, main = title2)
points(X, fitted(mod2), col = 2, type = "l", lwd = 2)

title3 <- paste0("R-squared = ", round(R.squared3, 3))
plot(X, Y1, col = "#4B9CD3", pch = 10, lwd = 5, main = title3)
points(X, fitted(mod3), col = 3, type = "l", lwd = 2)
```
### Interpreting the model

As mentioned, polynomial terms are often added into a model as a way to address nonlinearity. When this is the case, the polynomial terms themselves are not of much substantive interest, and are just added to "patch up" the model after assumption checking. 

We saw an example of this in Section \@ref(worked-example-7). In that example, a linear term and quadratic term were entered into the model in the same block of predictors. The R-squared was interpreted for the entire block, but the interpretation of the regression coefficient for the quadratic term was not addressed. This is a pretty common way of using polynomial regression -- the polynomial terms are included so that the model assumptions (linearity) are met, but they are not necessarily interpreted beyond this.  

However, we can interpret the terms in a polynomial regression if we want to. This section addresses the interpretation of a quadratic regression, but a similar approach applies to models with higher-order terms. 

A classic example of a substantively interesting quadratic relationship is the Yerkes-Dodson law relating physiological arousal (i.e., stress) to task performance, represented in Figure \@ref(fig:YD). One way to interpret the law is in terms of the overall shape of the relationship. As stress goes up, so does performance -- but only up to a point, after which more stress leads to a deterioration in performance. 

This exemplifies the basic interpretation of a quadratic relationship: 

* A U-shaped curve corresponds to a *positive* regression coefficient on $X^2$. 
* An inverted-U-shaped curve corresponds to a *negative* regression coefficient on $X^2$.

```{r, YD, echo = F, fig.cap = "Yerkes-Dodson Law (Source: Wikipedia)", fig.align = 'center'}
knitr::include_graphics("images/YerkesDodson.png", dpi = 75)
```

Beyond the overall shape of the relationship, we might also want to know what level of stress corresponds to the optimal level of performance -- i.e., where the maximum of the curve is. This exemplifies a more complicated interpretation of a quadratic relationship, and it requires some calculus (see Section \@ref(deriviation-10), which is optional). The main result is that for the quadratic regression model

\[ \widehat Y = b_0 +b_1X + b_2X^2, \]

the value of $X$ that corresponds to the maximum (or minumum) of the quadratic curve is

\[ X = \frac{-b_1}{2 b_2} \]

Here is a hint for the following question: In the Yerkes-Dodson law, when the $X$ variable is centered, the regression coefficient for the linear term is not statistically significant. 

**Based on this  discussion, please use both the "basic" and "more complicated" interpretation of a quadratic relationship to describe the Yerkes-Dodson Law.** 

### Model building

Up to this point we have discussed the interpretation of polynomials. Now we consider how to build polynomial regression models in practice. A typical model-building process for polynomial regression might proceed as follows. 

1. Enter the linear terms into the model and then examine a residual versus fitted plot. 

2. If there is evidence of non-linearity, look at the scatter plots between the outcome variable and each individual predictor to sort out which predictors are potentially causing the non-linearity. 

3. Add a quadratic term for a predictor of interest and examine whether there is a statistically significant increase in R-squared (See Section \@ref(delta-rsquared-7)). If not, the quadratic term is not improving the model fit, so remove it from the model and try again. If so, re-check the residual versus fitted plot to see whether the linearity assumption is still problematic. 

4. Keep adding polynomial terms one at a time until the model assumptions looks reasonable.  

This overall approach is illustrated in the next section. However, there are a couple of important details to point out first. 


* Just like with interactions, higher-order polynomial terms are often highly correlated with lower-order terms (e.g., $X$ and $X^2$ will be highly correlated if $X$ takes on strictly positive values). Recall that if two predictors are highly correlated, this can affect their regression coefficients (see Section \@ref(ols-4)) as well as their standard errors (see Section \@ref(too-many-predictors-7)). There are few things that can be done about this. 

    * Interpret $\Delta R^2$ values rather than the individual regression coefficients. This is the easiest thing to do. 
    * Center the predictors before computing higher order terms. This is the same approach we discussed for interactions (Section \@ref(binary-continuous-interaction-6)).
    * Use "orthogonal polynomials" which ensure the different polynomial terms are uncorrelated. This is the default approach in `R`, but it definitely leans more towards curve-fitting than substantive interpretations of the polynomial terms.


* We can't conclude that there is a quadratic relationship between two variables unless we have controlled for their linear relationship. As we just discussed, higher-order terms can be highly correlated with lower-order terms. Thus, higher-order terms will reflect their specific degree of curvature (e.g., quadratic) only if all lower-order (e.g., linear) terms are partialled out. In practice, this means that all lower-order terms should be included in the model in order for the higher-order terms to have a clear interpretation. 

* Finally, a warning: Making good use of polynomial regression requires walking a fine line between curve fitting and parsimony (see Figure \@ref(fig:overfit)). Sometimes, adding polynomial terms can provide an elegant and intuitive interpretation of the relationship between two variables. But, if you find yourself adding more than a couple of polynomial terms into a model and still have unresolved issues with nonlinearity, it is probably best to consider another approach (such as piece-wise regression, see Section \@ref(piecewise-10)) 

## A worked example {#worked-example-10}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

In Section \@ref(worked-example-9) we saw that applying a log-transform to the `Wages.Rdata` example addressed non-normality of the residuals but did not do much to address nonlinearity. The diagnostic plots for the log-linear regression of wages on education are presented below.    

```{r}
# Load the data and take a look
load("Wages.RData")
attach(wages)

# Create log transform of wage
log_wage <- log(wage + 1)

# Regress it on educ
mod1 <- lm(log_wage ~ educ)

# Check out model fit
par(mfrow = c(1,2))
plot(educ, log_wage, col = "#4B9CD3")
abline(mod1)
plot(mod1, 1, col = "#4B9CD3")
detach(wages)
```

Because there is one prominent bend in our residual vs fitted plot of the log-linear model (at $\hat Y \approx 2.1$), let's see if adding a quadratic term to the model can improve the model fit. 

The `poly` function in `R` makes it  easy to do polynomial regression, without having to hard-code new variables like `educ^2` into our dataset. This function automatically uses orthogonal (uncorrelated) polynomials, so we don't have to worry about centering, either. The basic interpretation of the model coefficients in an orthogonal polynomial regression is the same as discussed in Section \@ref(polynomial-10), but the "more complicated" interpretation of the model parameters is not straightforward. To find out more, use `help(poly)`.

The diagnostic plots for the log-linear model with a quadratic term included for education is shown below, along with the model summary. In the output, `poly(educ, 2)n` is the $n$-th degree term in the polynomial. 

```{r}
attach(wages)
mod2 <- lm(log_wage ~ poly(educ, 2))
par(mfrow = c(1,2))
plot(educ, log_wage, col = "#4B9CD3")

# To plot the trend we need to we first need to order the data and the predicted values ... 
sort_educ <- educ[order(educ)]
sort_fitted<- fitted(mod2)[order(educ)]
points(sort_educ, sort_fitted, type = "l")
plot(mod2, 1, col = "#4B9CD3")
summary(mod2)
detach(wages)
```

**What is your interpretation of the log-linear model with the quadratic term for education included? Does this address the non-linear relationship observed in the log-linear model? What would you do next? For your reference, the model with a cubic term for education is reported below.**


```{r}
attach(wages)
mod3 <- lm(log_wage ~ poly(educ, 3))
par(mfrow = c(1,2))
plot(educ, log_wage, col = "#4B9CD3")

# To plot the trend we need to we first need to order the data and the predicted values ... 
sort_educ <- educ[order(educ)]
sort_fitted<- fitted(mod3)[order(educ)]
points(sort_educ, sort_fitted, type = "l")
plot(mod3, 1, col = "#4B9CD3")
summary(mod3)
detach(wages)
```

## Piecewise regression {#piecewise-10}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Piecewise or segmented regression is another approach to dealing with nonlinearity. Like polynomial regression, it is  mathematically similar to interaction. Also like polynomial regression, it has a special interpretation and application that makes it practically distinct from interaction. 

In the simplest case, piecewise regression involves interacting a predictor variable with a binary re-coding of itself. To illustrate how the approach works, let’s again consider our wages and education example. The scatter plot of log-wages versus education is presented again below for reference. 

```{r, piecewise1, echo = F, fig.cap = "The Wages Example", fig.align = 'center'}
attach(wages)
plot(educ, log(wage + 1), col = "#4B9CD3")
detach(wages)
```

Consider the following reasoning about the example: 

* For people with 12 or less years of education (i.e., who did not obtain post-secondary education) the apparent relationship with wage is quite weak. This seems plausible, because if a job doesn't require a college degree, education probably isn’t a big factor in determining wages.

* For people with more than 12 years of education, the relationship with wage seems to be stronger. This also seems plausible: for jobs that require post secondary education, more education is usually associated with higher wages. 

* To restate this as an interaction: the relationship between wage and education appears different for people who have a post-secondary education versus those who do not. 

To represent this reasoning visually we can modify Figure \@ref(fig:piecewise1) as shown in Figure \@ref(fig:piecewise2). This captures the basic idea behind piecewise regression -- we have different regression lines over different ranges of the predictor, and the overall regression is piecewise or segmented. 

```{r, piecewise2, echo = F, fig.cap = "The wages example", fig.align = 'center'}
attach(wages)

# Create a dummy variable indicating if education is at least 12 years or more
educ12 <- (educ > 12)*1
# Interact the dummy with educ
mod4 <- lm(log(wage + 1) ~ educ*educ12) 

# Add fitted values to dataset
wages$fitted <- fitted(mod4)

# Sort data on educ
wages <- wages[order(educ), ]

# Plot
plot(educ, log(wage + 1), col = "#4B9CD3")

# Change color for the points with educ ≤ 12
with(subset(wages, educ <= 12), points(educ, log(wage + 1)))

# Plot the predicted values for educ > 12
with(subset(wages, educ > 12), lines(educ, fitted, col = "#4B9CD3"))

# Plot the predicted values for educ ≤ 12
with(subset(wages, educ <= 12), lines(educ, fitted))
detach(wages)
```

### The piecewise model

We have reasoned that the relationship between wages and education might change as depending on whether people have a post-secondary education. We also noted that this sounds a lot like an interaction (because it is!), which is the basic approach we can use to create piecewise models. 

In order to run our basic piecewise regression, we need to create a dummy coded version of education that indicates whether the participant had more than 12 years education: 

\[ \text{educ12} = \left\{ \begin{matrix}  
                     1 & \text{if educ } > 12\\ 
                    0 & \text{if educ } \leq 12 
                \end{matrix} \right.
\]

The we just create an interaction model for `educ` and the dummy-coded indicator

\[ \widehat Y = b_0 + b_1\text{educ} + b_2\text{educ12} + b_3 (\text{educ} \times \text{educ12}) \]

**Please take a moment to work out the interpretation of $b_1$ and $b_3$ in the model above, using the same approach as Section \@ref(binary-continuous-interaction-6). (The interpretation of $b_0$ and $b_2$ is not very interesting but you can work them out too if you like.)**

Note that there are more complex approaches that will search for breakpoints, or can be used to smoothly connect polynomial regressions rather than linear regressions, etc. We won't cover these more complex approaches here, but check out the following resource if you are interested and feel free to ask questions in class: https://rpubs.com/MarkusLoew/12164


### Back to the example

Below is the summary output for the piecewise model, as well as the diagnostic plots. You should be able to come up with a viable interpretation of the model output, based on 

  * What you know about interactions between a binary and continuous variable (Section \@ref(binary-continuous-interaction-6) and above) 
  
  * What you know about the interpretation of regression coefficient log-linear models (Section \@ref(interpretation-9)) 
  
You can also use the  `emtrends` functions to test the simple slopes (Section \@ref(inference-for-interactions-6)). Note that the intercept and main effect of the binary variable `educ12` are not of much interest in this application. 

**Please take a moment to write down your interpretation of the results, both the diagnostics and the parameter estimates, and be prepared to share your answers in class.**

```{r, piecewise3, echo = F, fig.cap = "The Wages Example", fig.align = 'center'}

summary(mod4)
emmeans::emtrends(mod4, specs = "educ12", var = "educ")
par(mfrow = c(1, 2))
plot(mod4, 1, col = "#4B9CD3")
plot(mod4, 2, col = "#4B9CD3")
```

## Heteroskedasticity {#heteroskedasticity-10}

Heteroskedasticity in linear regression, and corrections thereof, is a pretty big topic in the methodological literature (see Cite:fox).  In this section we are just going to discuss one widely used solution, and how to implement it in `R`. 

In terms of our example, we can see in the residual versus fitted plot in Figure \@ref(fig:piecewise3) that the residuals are less spread out for lower ranges of the fitted values (i.e., for $<12$ years of educ). As mentioned previously, heteroskedasticity will affect the standard errors of the regression coefficients, and consequently the t-tests, p-values, and confidence intervals. What this means is that the p-values for the regression coefficients will be wrong (usually too small) if the data are heteroskedastic but we mistakenly assume they are homoskedastic. Note that heteroskedasticity won't affect the estimated values of the coefficients (i.e., the $\widehat{b}$'s, and also doesn't affect R-squared or its F-test. 


"Heteroskedasticity-corrected" (HC) standard errors do not assume the data homoskedastic. Technically, they can be used regardless of whether the homoskedasticity assumption is met. But, when the data are homoskedastic, the "regular" OLS standard errors are more efficient (i.e., precise). So, we usually don't want to make the correction unless there is evidence of heteroskedasticity in the data. HC standard errors are also sometimes called heteroskedasticity-consistent, heteroskedastcicity-robust, or just robust.  

There are many different version of HC standard errors, but they are all equivalent with "large" samples. The simplest version is (see cite:Wooldridge)

\[ \text{HCSE}_{\hat{b}_j} = \sqrt{\frac{\sum_{i=1}^N (X_{ij} - \widehat{X}_{ij})^2 (Y_i-\widehat{Y}_i)^2} {\sum_{i=1}^N (X_{ij} - \bar X_j)^2 (1 - R^2_j)}}
(\#eq:se-10)
\] 

In this equation, $\widehat{X}_{ij}$ is the predicted value that results from regressing $X_j$ on the remaining $J-1$ predictors. Comparing this equation to Equation  \@ref(eq:se-4), the main difference is that the $(1 - R^2)$ term for the $Y$ variable is now replaced with a crossproduct of residuals for the $X_j$ and $Y$ variables. The equation is not very intuitive to look at, but the general idea is that it can be derived without assuming homoskedasticity. 

The procedure for using HC standard errors in R has two steps. First, we estimate the HC standard errors. Then, we use the HC standard errors to compute the correct t-tests and p-values (or confidence intervals).  The following example shows how to implement HC standard errors in R, using the piecewise regression from Section \@ref(piecewise-10) wages example.

The code is shown by default, because the main thing about this example is to see how it works in R -- you already know how to interpret SEs, t-tests, p-values, etc. Your will need two packages installed -- `car` and `lmtest` -- for this example to work. 

```{r}
## Make sure the required pacakges are installed
# install.packages("car")
# install.packages("lmtest")

# Step 1. Use "hccm" to get the HC SEs for our piecewise model 
hcse <- car::hccm(mod4)

# Step 2. Use "coeftest" to compute t-tests with the HC SEs
lmtest::coeftest(mod4, hcse)
```

This example is a bit unusual because the HC standard errors were actually a bit smaller than the regular OLS standard errors (see the output in Section \@ref(piecewise-10)) -- more often the opposite is true. Nonetheless, you should have no problems interpreting the output above.  

## Workbook

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

This section collects the questions asked in this chapter. We will discuss these questions in class. If you haven't written down / thought about the answers to these questions  before class, the lesson will not be very useful for you! So, please engage with each question by writing down one or more answers, asking clarifying questions, posing follow up questions, etc. 

**Section \@ref(polynomial-10)**

* Please take a moment to write down your intuitions about what is going in the right-hand panel of the figure below and whether this model really is better than the one in the middle panel. I will ask you to share your thoughts in class.**

```{r, echo = F, fig.cap = "Polynomial Regression Examples", fig.align = 'center', fig.width = 12}
# Generate data
set.seed(1)
X <- sort(runif(20, -2, 2))
e <- rnorm(20, 0, .5)
Y <- 2 + X - X^2 + e

# Three regression models
mod1 <- lm(Y ~ X)
mod2 <- lm(Y ~ poly(X, 2))
mod3 <- lm(Y ~ poly(X, 16))

# Plots
par(mfrow = c(1, 3)) 

title1 <- paste0("R-squared = ", round(summary(mod1)$r.square, 3))
plot(X, Y, col = "#4B9CD3", pch = 10, lwd = 5, main = title1)
points(X, fitted(mod1), col = 1, type = "l", lwd = 2)

title2 <- paste0("R-squared = ", round(summary(mod2)$r.square, 3))
plot(X, Y, col = "#4B9CD3", pch = 10, lwd = 5, main = title2)
points(X, fitted(mod2), col = 2, type = "l", lwd = 2)

title3 <- paste0("R-squared = ", round(summary(mod3)$r.square, 3))
plot(X, Y, col = "#4B9CD3", pch = 10, lwd = 5, main = title3)
points(X, fitted(mod3), col = 3, type = "l", lwd = 2)
```

* Please use both the "basic" and "more complicated" interpretation of a quadratic relationship to describe the Yerkes-Dodson Law.

```{r, echo = F, fig.cap = "Yerkes-Dodson Law (Source: Wikipedia)", fig.align = 'center'}
knitr::include_graphics("images/YerkesDodson.png", dpi = 75)
```

**Section \@ref(worked-example-10)**

* What is your interpretation of the log-linear model with the quadratic term for education included (below)? Does this address the non-linear relationship observed in the log-linear model? What would you do next?

```{r}
attach(wages)
log_wage <- log(wage + 1)
mod2 <- lm(log_wage ~ poly(educ, 2))
par(mfrow = c(1,2))
plot(educ, log_wage, col = "#4B9CD3")

# To plot the trend we need to we first need to order the data and the predicted values ... 
sort_educ <- educ[order(educ)]
sort_fitted<- fitted(mod2)[order(educ)]
points(sort_educ, sort_fitted, type = "l")

plot(mod2, 1, col = "#4B9CD3")
summary(mod2)
detach(wages)
```

**Section \@ref(piecewise-10)**

* Please take a moment to work out the interpretation of $b_1$ and $b_3$ in the model below, using the same approach as Section \@ref(binary-continuous-interaction-6). (The interpretation of $b_0$ and $b_2$ is not very interesting but you can work them out too if you like.)

\[ \text{educ12} = \left\{ \begin{matrix}  
                     1 & \text{if educ } > 12\\ 
                    0 & \text{if educ } \leq 12 
                \end{matrix} \right.
\]

\[ \widehat Y = b_0 + b_1\text{educ} + b_2\text{educ12} + b_3 (\text{educ} \times \text{educ12}) \]


* The diagnostics plots and parameter estimates for the piecewise model are presented below. Please provide an overall interpretation of the results and be prepared to share your answers in class. 

```{r}

summary(mod4)
emmeans::emtrends(mod4, specs = "educ12", var = "educ")
par(mfrow = c(1, 2))
plot(mod4, 1, col = "#4B9CD3")
plot(mod4, 2, col = "#4B9CD3")
```

## Exercises

This section collects the code from Section \@ref(worked-example-10) and Section \@ref(piecewise-10) and your can refer to those sections for more details on interpretation. 

You'll see that some of the plots below require a lot of fiddling about, especially for the piecewise regression model. We will cover some tricks and shortcuts for producing these types plots during the open lab sessions for Assignment 4. So don't worry too much about the complicated-looking coded for the plots at this point! 

Let's start with the "Vanella" log-linear model for the wages examples 

```{r}
# Load the data and take a look
load("Wages.RData")
attach(wages)

# Create log transform of wage
log_wage <- log(wage + 1)

# Regress it on educ
mod1 <- lm(log_wage ~ educ)

# Check out model fit
par(mfrow = c(1,2))
plot(educ, log_wage, col = "#4B9CD3")
abline(mod1)
plot(mod1, 1, col = "#4B9CD3")
```

Because there is one prominent bend in our residual vs fitted plot of the log-linear model (at $\hat Y \approx 2.1$), let's see if adding a quadratic term to the model can improve the model fit. 

The `poly` function in `R` makes it  easy to do polynomial regression, without having to hard-code new variables like `educ^2` into our dataset. This function automatically uses orthogonal (uncorrelated) polynomials, so we don't have to worry about centering, either. The basic interpretation of the model coefficients in an orthogonal polynomial regression is the same as discussed in Section \@ref(polynomial-10), but the "more complicated" interpretation of the model parameters is not straightforward. To find out more, use `help(poly)`.

The diagnostic plots for the log-linear model with a quadratic term included for education is shown below, along with the model summary. In the output, `poly(educ, 2)n` is the $n$-th degree term in the polynomial. 

```{r}
# Regress log_wage on a quadratic function of eduction 
mod2 <- lm(log_wage ~ poly(educ, 2))

# Model output
summary(mod2)

# Diagnostic plots
par(mfrow = c(1,2))

# scatter plot with trend
plot(educ, log_wage, col = "#4B9CD3")
# order the data and the predicted values ... 
sort_educ <- educ[order(educ)]
sort_fitted <- fitted(mod2)[order(educ)]
points(sort_educ, sort_fitted, type = "l")

# residual versus fitted
plot(mod2, 1, col = "#4B9CD3")
```

Using the same approach for the cubic model: 


```{r}
mod3 <- lm(log_wage ~ poly(educ, 3))
summary(mod3)

# Same plots as above, reusing variable names here
par(mfrow = c(1,2))
plot(educ, log_wage, col = "#4B9CD3")
sort_fitted <- fitted(mod3)[order(educ)]
points(sort_educ, sort_fitted, type = "l")
plot(mod3, 1, col = "#4B9CD3")
```

Moving on, let's consider the piecewise model from Section \@ref(piecewise-10)


```{r, fig.cap = "The wages example", fig.align = 'center'}
# Create a dummy variable indicating if education is at least 12 years or more
educ12 <- (educ > 12)*1

# Interact the dummy with educ
mod4 <- lm(log(wage + 1) ~ educ*educ12) 

# The model output
summary(mod4)

# The simple trends
emmeans::emtrends(mod4, specs = "educ12", var = "educ")

# The diagnostic plots
par(mfrow = c(1, 2))
plot(mod4, 1, col = "#4B9CD3")
plot(mod4, 2, col = "#4B9CD3")
```

We can still see some evidence of heteroskedasticity in the residual versus fitted plot, so the last step is to use heteroskedasticity-corrected standard errors to ensure we are making the right conclusions about statistical significance

```{r}
## Make sure the required pacakges are installed
# install.packages("car")
# install.packages("lmtest")

# Step 1. Use "hccm" to get the HC SEs for our piecewise model 
hcse <- car::hccm(mod4)

# Step 2. Use "coeftest" to compute t-tests with the HC SEs
lmtest::coeftest(mod4, hcse)
```

The next bit is optional. It shows how to produce the piecewise regression plot, which takes quite a bit of messing about with R...Let me know if you find an easier way to do this (in base R). 

```{r}
# Building the piecewise regression plot -- yeeesh

# Add fitted values to dataset
wages$fitted <- fitted(mod4)

# Sort data on educ
wages <- wages[order(educ), ]

# Plot
par(mfrow = c(1, 1))
plot(educ, log(wage + 1), col = "#4B9CD3")

# Change color for the points with educ ≤ 12
with(subset(wages, educ <= 12), points(educ, log(wage + 1)))

# Plot the predicted values for educ > 12
with(subset(wages, educ > 12), lines(educ, fitted, col = "#4B9CD3"))

# Plot the predicted values for educ ≤ 12
with(subset(wages, educ <= 12), lines(educ, fitted))
detach(wages)
```