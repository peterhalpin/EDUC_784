<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>EDUC 784 - 2&nbsp; Simple regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ch3_two_predictors.html" rel="next">
<link href="./ch1_review.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch2_simple_regression.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Simple regression</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">EDUC 784</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch1_review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2_simple_regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Simple regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3_two_predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Two predictors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4_categorical_predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Categorical predictors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5_interactions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Interactions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch6_model_building.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Model building</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch7_assumption_checking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Assumption checking</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-example-2" id="toc-sec-example-2" class="nav-link active" data-scroll-target="#sec-example-2"><span class="header-section-number">2.1</span> An example from NELS</a></li>
  <li><a href="#sec-regression-line-2" id="toc-sec-regression-line-2" class="nav-link" data-scroll-target="#sec-regression-line-2"><span class="header-section-number">2.2</span> The regression line</a></li>
  <li><a href="#sec-ols-2" id="toc-sec-ols-2" class="nav-link" data-scroll-target="#sec-ols-2"><span class="header-section-number">2.3</span> OLS</a>
  <ul class="collapse">
  <li><a href="#correlation-and-regression" id="toc-correlation-and-regression" class="nav-link" data-scroll-target="#correlation-and-regression"><span class="header-section-number">2.3.1</span> Correlation and regression</a></li>
  </ul></li>
  <li><a href="#sec-rsquared-2" id="toc-sec-rsquared-2" class="nav-link" data-scroll-target="#sec-rsquared-2"><span class="header-section-number">2.4</span> R-squared</a></li>
  <li><a href="#sec-population-model-2" id="toc-sec-population-model-2" class="nav-link" data-scroll-target="#sec-population-model-2"><span class="header-section-number">2.5</span> The population model</a></li>
  <li><a href="#sec-notation-2" id="toc-sec-notation-2" class="nav-link" data-scroll-target="#sec-notation-2"><span class="header-section-number">2.6</span> Clarifying notation</a></li>
  <li><a href="#sec-inference-2" id="toc-sec-inference-2" class="nav-link" data-scroll-target="#sec-inference-2"><span class="header-section-number">2.7</span> Inference</a>
  <ul class="collapse">
  <li><a href="#sec-inference-for-coefficients-2" id="toc-sec-inference-for-coefficients-2" class="nav-link" data-scroll-target="#sec-inference-for-coefficients-2"><span class="header-section-number">2.7.1</span> Inference for coefficients</a></li>
  <li><a href="#sec-inference-for-rsquared-2" id="toc-sec-inference-for-rsquared-2" class="nav-link" data-scroll-target="#sec-inference-for-rsquared-2"><span class="header-section-number">2.7.2</span> Inference for R-squared</a></li>
  <li><a href="#the-nels-example" id="toc-the-nels-example" class="nav-link" data-scroll-target="#the-nels-example"><span class="header-section-number">2.7.3</span> The NELS example</a></li>
  </ul></li>
  <li><a href="#sec-power-2" id="toc-sec-power-2" class="nav-link" data-scroll-target="#sec-power-2"><span class="header-section-number">2.8</span> Power analysis*</a></li>
  <li><a href="#sec-properties-2" id="toc-sec-properties-2" class="nav-link" data-scroll-target="#sec-properties-2"><span class="header-section-number">2.9</span> Properties of OLS*</a>
  <ul class="collapse">
  <li><a href="#residuals" id="toc-residuals" class="nav-link" data-scroll-target="#residuals"><span class="header-section-number">2.9.1</span> Residuals</a></li>
  <li><a href="#sec-multiple-r-2" id="toc-sec-multiple-r-2" class="nav-link" data-scroll-target="#sec-multiple-r-2"><span class="header-section-number">2.9.2</span> Multiple correlation (<span class="math inline">\(R\)</span>)</a></li>
  </ul></li>
  <li><a href="#workbook-2" id="toc-workbook-2" class="nav-link" data-scroll-target="#workbook-2"><span class="header-section-number">2.10</span> Workbook</a></li>
  <li><a href="#sec-exercises-2" id="toc-sec-exercises-2" class="nav-link" data-scroll-target="#sec-exercises-2"><span class="header-section-number">2.11</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#the-lm-function" id="toc-the-lm-function" class="nav-link" data-scroll-target="#the-lm-function"><span class="header-section-number">2.11.1</span> The <code>lm</code> function</a></li>
  <li><a href="#variance-explained" id="toc-variance-explained" class="nav-link" data-scroll-target="#variance-explained"><span class="header-section-number">2.11.2</span> Variance explained</a></li>
  <li><a href="#predicted-values-and-residuals" id="toc-predicted-values-and-residuals" class="nav-link" data-scroll-target="#predicted-values-and-residuals"><span class="header-section-number">2.11.3</span> Predicted values and residuals</a></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference"><span class="header-section-number">2.11.4</span> Inference</a></li>
  <li><a href="#writing-up-results" id="toc-writing-up-results" class="nav-link" data-scroll-target="#writing-up-results"><span class="header-section-number">2.11.5</span> Writing up results</a></li>
  <li><a href="#additional-exercises" id="toc-additional-exercises" class="nav-link" data-scroll-target="#additional-exercises"><span class="header-section-number">2.11.6</span> Additional exercises</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-chap-2" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Simple regression</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The focus of this course is linear regression with multiple predictors (AKA <em>multiple regression</em>), but we start by reviewing regression with one predictor (AKA <em>simple regression</em>). Most of this material should be familiar from EDUC 710.</p>
<section id="sec-example-2" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-example-2"><span class="header-section-number">2.1</span> An example from NELS</h2>
<p>Let’s begin by considering an example. <a href="#fig-nels-2">Figure&nbsp;<span>2.1</span></a> shows the relationship between Grade 8 Math Achievement (percent correct on a math test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see <a href="https://nces.ed.gov/surveys/nels88/" class="uri">https://nces.ed.gov/surveys/nels88/</a>).</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and attach the NELS88 data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"NELS.RData"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(NELS)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter plot</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ses, </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> achmat08, </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Math Achievement (Grade 8)"</span>, </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"SES"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the regression model</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(achmat08 <span class="sc">~</span> ses)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the regression line to the plot</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-nels-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch2_simple_regression_files/figure-html/fig-nels-2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.1: Math Achievement and SES (NELS88).</figcaption>
</figure>
</div>
</div>
</div>
<p>The strength and direction of the linear relationship between the two variables is summarized by their correlation. In this sample, the value of the correlation is:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(achmat08, ses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3182484</code></pre>
</div>
</div>
<p>This is a moderate, positive correlation between Math Achievement and SES. This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in math (higher Math Achievement).</p>
<p>The relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., <a href="https://www.apa.org/pi/ses/resources/publications/education" class="uri">https://www.apa.org/pi/ses/resources/publications/education</a>). <strong>Please look over this web page and be prepared to share your thoughts/questions about the relationship between SES and academic achievement and its relevance for education research.</strong></p>
</section>
<section id="sec-regression-line-2" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-regression-line-2"><span class="header-section-number">2.2</span> The regression line</h2>
<p>The section presents three interchangeable ways of writing the regression line in <a href="#fig-nels-2">Figure&nbsp;<span>2.1</span></a>. You should be familiar with all three ways of presenting regression equations and you are welcome to use whichever approach you like best in your writing for this class.</p>
<p>The regression line in <a href="#fig-nels-2">Figure&nbsp;<span>2.1</span></a> can be represented mathematically as</p>
<p><span id="eq-yhat"><span class="math display">\[
\widehat Y = a + b X
\tag{2.1}\]</span></span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(Y\)</span> denotes Math Achievement</li>
<li><span class="math inline">\(X\)</span> denotes SES</li>
<li><span class="math inline">\(a\)</span> represents the regression intercept (the value of <span class="math inline">\(\widehat Y\)</span> when <span class="math inline">\(X = 0\)</span>)</li>
<li><span class="math inline">\(b\)</span> represents the regression slope (how much <span class="math inline">\(\widehat Y\)</span> changes for each unit of increase in <span class="math inline">\(X\)</span>)</li>
</ul>
<p>In this equation, the symbol <span class="math inline">\(\widehat Y\)</span> represents the <em>predicted value</em> of Math Achievement for a given value of SES. In <a href="#fig-nels-2">Figure&nbsp;<span>2.1</span></a>, the predicted values are represented by the regression line. The <em>observed values</em> of Math Achievement are denoted as <span class="math inline">\(Y\)</span>. In <a href="#fig-nels-2">Figure&nbsp;<span>2.1</span></a>, these values are represented by the points in the scatter plot.</p>
<p>Some more terminology: the <span class="math inline">\(Y\)</span> variable is often referred to as the <em>outcome</em> or the <em>dependent variable.</em> The <span class="math inline">\(X\)</span> variable is often referred to as the <em>predictor</em>, <em>independent variable</em>, <em>explanatory variable</em>, or <em>covariate.</em> Different areas of research have different conventions about terminology for regression. We talk more about “big picture” interpretations of regression in <a href="ch3_two_predictors.html"><span>Chapter&nbsp;3</span></a>.</p>
<p>The difference between an observed value <span class="math inline">\(Y\)</span> and its predicted value <span class="math inline">\(\widehat Y\)</span> is called a <em>residual</em>. Residuals are denoted as <span class="math inline">\(e = Y - \widehat Y\)</span>. The residuals for a subset of the data points in <a href="#fig-nels-2">Figure&nbsp;<span>2.1</span></a> are shown in pink in <a href="#fig-resid-2">Figure&nbsp;<span>2.2</span></a></p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get predicted values from regression model</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> mod<span class="sc">$</span>fitted.values</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># select a subset of the data</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">sample.int</span>(<span class="dv">500</span>, <span class="dv">30</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot again</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ses[index], </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> achmat08[index], </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Math Achievement (Grade 8)"</span>, </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"SES"</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Add pink lines</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> ses[index], </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>         <span class="at">y0 =</span> yhat[index], </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>         <span class="at">x1 =</span> ses[index], </span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>         <span class="at">y1 =</span> achmat08[index], </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="dv">6</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Overwrite dots to make it look at bit better</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> ses[index], </span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> achmat08[index], </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-resid-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch2_simple_regression_files/figure-html/fig-resid-2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.2: Residuals for a Subsample of the Example.</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that <span class="math inline">\(Y = \widehat Y + e\)</span> by definition:</p>
<p><span class="math display">\[
Y = \widehat Y + e = \widehat Y + (Y - \widehat Y ) = Y.
\]</span></p>
<p>This leads to a second way of writing out a regression model:</p>
<p><span id="eq-y"><span class="math display">\[
Y = a + bX + e.  
\tag{2.2}\]</span></span></p>
<p>The difference between <a href="#eq-yhat">Equation&nbsp;<span>2.1</span></a> and <a href="#eq-y">Equation&nbsp;<span>2.2</span></a> is that the former lets us talk about the predicted values (<span class="math inline">\(\hat Y\)</span>), whereas the latter lets us talk about the observed data points (<span class="math inline">\(Y\)</span>).</p>
<p>A third way to write out the model is using the variable names (or abbreviations) in place of the more generic “X, Y” notation. For example,</p>
<p><span id="eq-math"><span class="math display">\[MATH = a + b(SES) + e \tag{2.3}\]</span></span></p>
<p>This notation is useful when talking about a specific example, because we don’t have to remember what <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> stand for. But this notation is more clunky and doesn’t lend itself talking about regression in general or writing other mathematical expressions related to regression.</p>
<p>You should be familiar with all three ways of presenting regression equations (<a href="#eq-yhat">Equation&nbsp;<span>2.1</span></a>, <a href="#eq-y">Equation&nbsp;<span>2.2</span></a>, and <a href="#eq-math">Equation&nbsp;<span>2.3</span></a>) and you are welcome to use whichever approach you like best in this class.</p>
</section>
<section id="sec-ols-2" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-ols-2"><span class="header-section-number">2.3</span> OLS</h2>
<p>This section talks about how to estimate the regression intercept (denoted as <span class="math inline">\(a\)</span> in <a href="#eq-yhat">Equation&nbsp;<span>2.1</span></a>) and the regression slope (denoted as <span class="math inline">\(b\)</span> in <a href="#eq-yhat">Equation&nbsp;<span>2.1</span></a>). The intercept and slope are collectively referred to as the <em>parameters</em> of the regression line. They are also referred to as <em>regression coefficients.</em></p>
<p>Our overall goal in this section is to “fit a line to the data” – i.e., we want to select the values of the regression coefficients that best represent our data. An intuitive way to approach this problem is by minimizing the residuals – i.e., minimizing the total amount of pink in <a href="#fig-resid-2">Figure&nbsp;<span>2.2</span></a>. We can operationalize this intuitive idea by minimizing the sum of squared residuals:</p>
<p><span class="math display">\[
SS_{\text{res}} = \sum_{i=1}^{N} e_i^2 = \sum_{i=1}^{N} (Y_i - a - b X_i)^2
\]</span></p>
<p>where <span class="math inline">\(i = 1 \dots N\)</span> indexes the respondents in the sample. When we estimate the regression coefficients by minimizing <span class="math inline">\(SS_{\text{res}}\)</span>, this is called ordinary least squares (OLS) regression. OLS is very widely used and is the main focus of this course, although we will visit some other approaches in the second half of the course.</p>
<p>The values of the regression coefficients that minimize <span class="math inline">\(SS_{\text{res}}\)</span> can be found using calculus (i.e., compute the derivatives of <span class="math inline">\(SS_{\text{res}}\)</span> and set them to zero). This approach leads to the following equations for the regression coefficients:</p>
<p><span id="eq-reg-coeffs"><span class="math display">\[
a = \bar Y - b \bar X \quad \quad \quad \quad b = \frac{\text{cov}(X, Y)}{s^2_X} = \text{cor}(X, Y) \frac{s_Y}{s_X}
\tag{2.4}\]</span></span></p>
<p>(If you aren’t familiar with the symbols in these equations, check out the review materials in <a href="ch1_review.html"><span>Chapter&nbsp;1</span></a> for a refresher.)</p>
<p>The formulas in <a href="#eq-reg-coeffs">Equation&nbsp;<span>2.4</span></a> tell us how to compute the regression coefficients using our sample data. However, on face value, these formulas don’t tell us much about how to interpret the regression coefficients. For interpreting the regression coefficients, it is more straightforward to refer to <a href="#eq-yhat">Equation&nbsp;<span>2.1</span></a>.</p>
<p>To clarify:</p>
<ul>
<li><p>To <em>interpret</em> the regression intercept, use <a href="#eq-yhat">Equation&nbsp;<span>2.1</span></a>: It is the value of <span class="math inline">\(\hat Y\)</span> when <span class="math inline">\(X = 0\)</span>. Similarly, the regression slope is how much <span class="math inline">\(\hat Y\)</span> changes for a one-unit increase in <span class="math inline">\(X\)</span>.</p></li>
<li><p>To <em>compute</em> the regression coefficients, use <a href="#eq-reg-coeffs">Equation&nbsp;<span>2.4</span></a>. These formulas are not very intuitive – they are just what we get when we fit a line to the data using OLS.</p></li>
</ul>
<p>It is important to emphasize that the formulas in <a href="#eq-reg-coeffs">Equation&nbsp;<span>2.4</span></a> do lead to some useful mathematical results about regression. <a href="#sec-properties-2"><span>Section&nbsp;2.9</span></a>, which is optional, derives some of the main results. If you want a deeper mathematical understanding of regression, make sure to check out this section. If you prefer to skip the math and just learn about the results as they become relevant, that is OK too.</p>
<section id="correlation-and-regression" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="correlation-and-regression"><span class="header-section-number">2.3.1</span> Correlation and regression</h3>
<p>Before moving on, it is worth noting something that we can learn from <a href="#eq-reg-coeffs">Equation&nbsp;<span>2.4</span></a> without too much math: the regression slope is just a re-packaging of the correlation coefficient. In particular, if we assume that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are z-scores (i.e., they are standardized to have mean of zero and variance of one), then <a href="#eq-reg-coeffs">Equation&nbsp;<span>2.4</span></a> reduces to:</p>
<ul>
<li><span class="math inline">\(a = 0\)</span></li>
<li><span class="math inline">\(b = \text{cov}(X, Y) = \text{cor}(X, Y)\)</span></li>
</ul>
<p>There are two important things to note here.</p>
<p>First, the difference between correlation and simple regression depends on the scale of the variables. Otherwise stated, if we standardize both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, then regression is just correlation. In particular, if the correlation is equal to zero, then the regression slope is also equal to zero – these are just two equivalent ways of saying that the variables are not (linearly) related.</p>
<p>Second, this relationship between correlation and regression holds only for simple regression (i.e., one predictor). When we get to multiple regression, we will see that relationship between regression and correlation (and covariance) gets more complicated.</p>
<p>For the NELS example in <a href="#fig-nels-2">Figure&nbsp;<span>2.1</span></a>, the regression intercept and slope are, respectively:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)         ses 
 48.6780338   0.4292604 </code></pre>
</div>
</div>
<p><strong>Please write down an interpretation of these numbers and be prepared to share your answers in class. How would your interpretation change if, rather than the value of the slope shown above, we had <span class="math inline">\(b = 0\)</span>?</strong></p>
</section>
</section>
<section id="sec-rsquared-2" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-rsquared-2"><span class="header-section-number">2.4</span> R-squared</h2>
<p>In this section we introduce another statistic that is commonly used in regression, called “R-squared” (in symbols: <span class="math inline">\(R^2\)</span>). First we will talk about its interpretation, then we will show how it is computed.</p>
<p>R-squared is the proportion of variance in the outcome variable that is associated with, or “explained by”, the predictor variable. In terms of the NELS example, the variance of the outcome can be interpreted in terms of individual differences in Math Achievement – i.e., how students deviate from, or vary around, the mean level of Math Achievement. R-squared tells us the extent to which these individual differences in Math Achievement are associated with, or explained by, individual differences in SES.</p>
<p>As mentioned, R-squared is a proportion. Because it is a proportion, it takes on values between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. If <span class="math inline">\(R^2 = 0\)</span> then a student’s SES doesn’t tell us anything about their Math Achievement – this is the same as saying the two variables aren’t correlated, or that there is no (linear) relationship between Math Achievement and SES. If <span class="math inline">\(R^2 = 1\)</span>, then all of the data points fall exactly on the regression line, and we can perfectly predict each student’s Math Achievement from their SES.</p>
<p>You might be asking – why do we need R-squared? We already have the regression coefficient (which is just a repackaging of the correlation), so why do we need yet another way of describing the relationship between Math Achievement and SES? This is very true for simple regression! However, when we move on to multiple regression, we will see that R-squared lets us talk about the relationship between the outcome and <em>all</em> of the predictors, or any subset of the predictors, whereas the regression coefficient only lets us talk about the relationship with one predictor at a time.</p>
<p>To see how R-squared is computed for the NELS example, let’s consider <a href="#fig-rsquared-2">Figure&nbsp;<span>2.3</span></a>. The horizontal grey line denotes the mean of Math Achievement. Recall that the variance of <span class="math inline">\(Y\)</span> is computed using the sum-of-squared deviations from the mean. For each student, these deviations from the mean can be divided into two parts. The Figure shows these two parts for a single student, using black and pink dashed lines:</p>
<ul>
<li><p>The black dashed line represents the extent to which the student’s deviation from the mean level of Math Achievement is explained by the linear relationship between Math Achievement and SES.</p></li>
<li><p>The pink dashed line is the regression residual, which was introduced in <a href="#sec-regression-line-2"><span>Section&nbsp;2.2</span></a>. This is the variation in Math Achievement that is “left over” after considering the linear relationship with SES.</p></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-rsquared-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch2_simple_regression_files/figure-html/fig-rsquared-2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.3: The Idea Behind R-squared.</figcaption>
</figure>
</div>
</div>
</div>
<p>The R-squared statistic averages the variation in Math Achievement associated with SES (i.e., the black dashed line) for all students in the sample, and then divides by the total variation in Math Achievement (i.e., black + pink).</p>
<p>The derivation of the R-squared statistic is not very complicated and provides some useful notation. To simplify the derivation, we can work the numerator of the variance, which is called the “total sum of squares:”</p>
<p><span class="math display">\[SS_{\text{total}} = \sum_{i = 1}^N (Y_i - \bar Y)^2. \]</span></p>
<p>Next we add and subtract the predicted values (that old trick!):</p>
<p><span class="math display">\[SS_{\text{total}} = \sum_{i = 1}^N [(Y_i - \widehat Y_i) + (\widehat Y_i - \bar Y)]^2. \]</span></p>
<p>The right-hand-side can be reduced to two other sums of squares using the rules of summation algebra (see <a href="ch1_review.html#sec-rules-1"><span>Section&nbsp;1.2</span></a> – the derivation is long but not complicated).</p>
<p><span class="math display">\[
SS_{\text{total}} = \sum_{i = 1}^N (Y_i - \widehat Y_i)^2 + \sum_{i = 1}^N (\widehat Y_i - \bar Y)^2.
\]</span></p>
<p>The first term on the right-hand-side is just the sum of squared residuals (<span class="math inline">\(SS_\text{res}\)</span>) from <a href="#sec-ols-2"><span>Section&nbsp;2.3</span></a>. The second term is called the sum of squared regression and denoted <span class="math inline">\(SS_\text{reg}\)</span>. Using this notation we can re-write the previous equation as</p>
<p><span class="math display">\[ SS_{\text{total}} = SS_\text{res} + SS_\text{reg} \]</span></p>
<p>and the R-squared statistic is computed as</p>
<p><span class="math display">\[R^2 = SS_{\text{reg}} / SS_{\text{total}}. \]</span></p>
<p>As discussed above, this quantity can be interpreted as the proportion of variance in <span class="math inline">\(Y\)</span> that is explained by its linear relationship with <span class="math inline">\(X\)</span>.</p>
<p>For the NELS example, the R-squared statistic is:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1012821</code></pre>
</div>
</div>
<p><strong>Please write down an interpretation of this number and be prepared to share your answer in class.</strong> Hint: Instead of talking about proportions, it is often helpful to multiply by 100 and talk about percentages instead.</p>
</section>
<section id="sec-population-model-2" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-population-model-2"><span class="header-section-number">2.5</span> The population model</h2>
<p>Up to this point we have discussed simple linear regression as a way of describing the relationship between two variables in a sample. The next step is to discuss statistical inference. Recall that statistical inference involves generalizing from a sample to the population from which the sample was drawn.</p>
<p>In the NELS example, the population of interest is U.S. eighth graders in 1988. We want to be able to draw conclusions about that population based on the sample of eighth graders that participated in NELS. In order to do that, we make some statistical assumptions about the population, which are collectively referred to as the <em>population model</em>.</p>
<p>The population model for simple linear regression is summarized in <a href="#fig-pop-model">Figure&nbsp;<span>2.4</span></a>. The three assumptions associated with this model are written below. We talk about how to check the plausibility of these assumptions in <span class="quarto-unresolved-ref">?sec-chapter-7</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-pop-model" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="files/images/population_model.png" class="img-fluid figure-img" width="612"></p>
<figcaption class="figure-caption">Figure&nbsp;2.4: The Regression Population Model.</figcaption>
</figure>
</div>
</div>
</div>
<p>The three assumptions:</p>
<ol type="1">
<li>Normality: The values of <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(Y|X\)</span>, are normally distributed. The figure shows these distributions for three values of <span class="math inline">\(X\)</span>. We can write this assumption formally as</li>
</ol>
<p><span class="math display">\[Y | X \sim  N(\mu_{Y | X} , \sigma^2_{Y | X}) \]</span></p>
<ul>
<li>(This notation should be familiar from EDUC 710. In general, we write <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span> to denote that the variable <span class="math inline">\(Y\)</span> has a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.)</li>
</ul>
<ol start="2" type="1">
<li>Homoskedasticity: The conditional distributions have equal variances (also called “homogeneity of variance”, or simply “equal variances”).</li>
</ol>
<p><span class="math display">\[ \sigma^2_{Y| X} = \sigma^2 \]</span></p>
<ol start="3" type="1">
<li>Linearity: The means of the conditional distributions are a linear function of <span class="math inline">\(X\)</span>.</li>
</ol>
<p><span class="math display">\[ \mu_{Y| Χ} = a + bX \]</span></p>
<p>These three assumptions are summarized by writing</p>
<p><span class="math display">\[ Y|X \sim N(a + bX, \sigma^2). \]</span> Sometimes it will be easier to state the assumptions in terms of the population residuals, <span class="math inline">\(\epsilon = Y - \mu_{Y|X}\)</span>. The residuals have distribution <span class="math inline">\(\epsilon \sim N(0, \sigma^2)\)</span>.</p>
<p>Sometimes it will also be easier to write the population regression line using expected values, <span class="math inline">\(E(Y|X)\)</span>, rather than <span class="math inline">\(\mu_{Y|X}\)</span>. Both of these are interpreted the same way – they denote the mean of <span class="math inline">\(Y\)</span> for a given value of <span class="math inline">\(X\)</span>.</p>
<p>An additional assumption is usually made about the data in the sample – that they were obtained as a simple random sample from the population. We will see some ways of dealing with other types of samples later on this course, but for now we can consider this a background assumption that applies to OLS regression.</p>
<p>From a mathematical perspective, these assumptions are important because they can be used to prove (a) that OLS regression provides unbiased estimates of the population regression coefficients and (b) that the OLS estimates are more precise than any other unbiased estimates of the population regression coefficients. There are other variations on these assumptions, which are sometimes called the Gauss-Markov assumptions <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">see https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem</a>.</p>
<p>From a practical perspective, these assumptions are important conditions that we should check when conducting data analyses. If the assumptions are violated – particularly the linearity assumption – then our statistical model may not be a good representation of the population. If the model is not a good representation of the population, then inferences based on the model may provide misleading conclusions about the population.</p>
</section>
<section id="sec-notation-2" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sec-notation-2"><span class="header-section-number">2.6</span> Clarifying notation</h2>
<p>At this point we have used the mathematical symbols for regression (e.g., <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>) in two different ways:</p>
<ul>
<li>In <a href="#sec-regression-line-2"><span>Section&nbsp;2.2</span></a> they denoted sample statistics.</li>
<li>In <a href="#sec-population-model-2"><span>Section&nbsp;2.5</span></a> they denoted population parameters.</li>
</ul>
<p>The population versus sample notation for regression is a bit of a hot mess, but the following conventions are used.</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 30%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Concept</th>
<th style="text-align: center;">Sample statistic</th>
<th style="text-align: center;">Population parameter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>regression line</td>
<td style="text-align: center;"><span class="math inline">\(\widehat Y\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\mu_{Y|X}\)</span> or <span class="math inline">\(E(Y|X)\)</span></td>
</tr>
<tr class="even">
<td>slope</td>
<td style="text-align: center;"><span class="math inline">\(\widehat b\)</span></td>
<td style="text-align: center;"><span class="math inline">\(b\)</span></td>
</tr>
<tr class="odd">
<td>intercept</td>
<td style="text-align: center;"><span class="math inline">\(\widehat a\)</span></td>
<td style="text-align: center;"><span class="math inline">\(a\)</span></td>
</tr>
<tr class="even">
<td>residual</td>
<td style="text-align: center;"><span class="math inline">\(e\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\epsilon\)</span></td>
</tr>
<tr class="odd">
<td>variance explained</td>
<td style="text-align: center;"><span class="math inline">\(\widehat R^2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(R^2\)</span></td>
</tr>
</tbody>
</table>
<p>The “hats” always denote sample quantities, and the Greek letters always denote population quantities, but there is some lack of consistency. For example, why not use <span class="math inline">\(\beta\)</span> instead of <span class="math inline">\(b\)</span> for the population slope? Well, <span class="math inline">\(\beta\)</span> is conventionally used to denote standardized regression coefficients in the <em>sample</em>, so its already taken (more on this in <a href="ch4_categorical_predictors.html"><span>Chapter&nbsp;4</span></a>).</p>
<p>If it is clear from context that we are talking about the sample rather than the population, then the hats are usually omitted from the statistics <span class="math inline">\(\widehat a\)</span>, <span class="math inline">\(\widehat b\)</span>, and <span class="math inline">\(\widehat R^2\)</span>. This doesn’t apply to <span class="math inline">\(\widehat Y\)</span>, because the hat is required to distinguish the predicted values from the data points.</p>
<p>Another thing to note is that while <span class="math inline">\(\widehat Y\)</span> is often called a predicted value, <span class="math inline">\(E(Y|X)\)</span> is not usually referred to this way. It is called the conditional mean function or the conditional expectation function. Using this language, we can say that regression is about estimating the conditional mean function.</p>
<p><strong>Please be prepared for a pop quiz on notation during class!</strong></p>
</section>
<section id="sec-inference-2" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="sec-inference-2"><span class="header-section-number">2.7</span> Inference</h2>
<p>This section reviews the main inferential procedures for regression. The formulas presented in this section are used to produce standard errors, t-tests, p-values, and confidence intervals for the regression coefficients, as well as an F-test for R-squared. It is very unlikely that you will ever need to compute these formulas by hand, so don’t worry about memorizing them.</p>
<p>However, it is important that you can interpret the numerical results in research settings. The interpretations of these procedures were reviewed in <a href="ch1_review.html"><span>Chapter&nbsp;1</span></a> and should be familiar from EDUC 710. This sections documents the formulas for simple regression and then asks you to interpret the results in the context of the NELS example.</p>
<p>It is worth noting that the regression intercept is often not of interest in simple regression. Recall that the intercept is the value of <span class="math inline">\(\widehat Y\)</span> when <span class="math inline">\(X = 0\)</span>. So, unless we have a hypothesis or research question about this particular value of <span class="math inline">\(X\)</span> (e.g., eighth graders with <span class="math inline">\(SES = 0\)</span>), we won’t be interested in a test of the regression intercept. When we get into to multiple regression, we will see some situations where the intercept is of interest.</p>
<section id="sec-inference-for-coefficients-2" class="level3" data-number="2.7.1">
<h3 data-number="2.7.1" class="anchored" data-anchor-id="sec-inference-for-coefficients-2"><span class="header-section-number">2.7.1</span> Inference for coefficients</h3>
<p>When the population model is true, <span class="math inline">\(\widehat b\)</span> is an unbiased estimate of <span class="math inline">\(b\)</span> (in symbols: <span class="math inline">\(E(\hat b) = b\)</span>). The standard error of <span class="math inline">\(\widehat b\)</span> is equal to (see <span class="citation" data-cites="fox-2016">(<a href="#ref-fox-2016" role="doc-biblioref"><strong>fox-2016?</strong></a>)</span>, Section 6.1):</p>
<p><span class="math display">\[ SE(\widehat b) = \frac{s_Y}{s_X} \sqrt{\frac{1-R^2}{N-2}} . \]</span> Using these two results, we can compute t-tests and confidence intervals for the regression slope in the usual way.</p>
<p><strong>t-tests</strong></p>
<p>The null hypothesis <span class="math inline">\(H_0: \widehat b = b_0\)</span> can be tested against the alternative <span class="math inline">\(H_A: \widehat b \neq b_0\)</span> using the test statistic:</p>
<p><span class="math display">\[ t = \frac{\widehat b - b_0}{SE(\widehat b)}, \]</span></p>
<p>which has a t-distribution on <span class="math inline">\(N-2\)</span> degrees of freedom when the null hypothesis is true.</p>
<p>The test assumes that the population model is correct. The null hypothesized value of the parameter is usually chosen to be <span class="math inline">\(b_0 = 0\)</span>, in which case the test is interpreted in terms of the “statistical significance” of the regression slope.</p>
<p><strong>Confidence intervals</strong></p>
<p>For a given Type I Error rate, <span class="math inline">\(\alpha\)</span>, the corresponding <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence interval is</p>
<p><span class="math display">\[ b_0 = \widehat b \pm t_{\alpha/2} \times SE(\widehat b), \]</span></p>
<p>where <span class="math inline">\(t_{\alpha/2}\)</span> denotes the <span class="math inline">\(\alpha/2\)</span> quantile of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(N-2\)</span> degrees of freedom. For example, if <span class="math inline">\(\alpha\)</span> is chosen to be <span class="math inline">\(.05\)</span>, the corresponding <span class="math inline">\(95\%\)</span> confidence interval uses <span class="math inline">\(t_{.025}\)</span>, or the 2.5-th percentile of the t-distribution.</p>
<p>The standard error for the regression intercept, presented below, can be used to compute t-tests and confidence intervals for <span class="math inline">\(\hat a\)</span>:</p>
<p><span class="math display">\[
SE(\widehat a) = \sqrt{\frac{SS_{\text{res}}}{N-2} \left(\frac{1}{N} + \frac{\bar X^2}{(N-1)s^2_X}\right)}.
\]</span></p>
</section>
<section id="sec-inference-for-rsquared-2" class="level3" data-number="2.7.2">
<h3 data-number="2.7.2" class="anchored" data-anchor-id="sec-inference-for-rsquared-2"><span class="header-section-number">2.7.2</span> Inference for R-squared</h3>
<p>The null hypothesis <span class="math inline">\(H_0: R^2 = 0\)</span> can be tested against the alternative <span class="math inline">\(H_A: R^2 \neq 0\)</span> using the F-test:</p>
<p><span class="math display">\[ F = (N-2) \frac{\widehat R^2}{1-\widehat R^2}, \]</span></p>
<p>which has a F-distribution on <span class="math inline">\(1\)</span> and <span class="math inline">\(N – 2\)</span> degrees of freedom when the null is true. The test assumes that the population model is true. Confidence intervals for R-squared are generally not reported.</p>
</section>
<section id="the-nels-example" class="level3" data-number="2.7.3">
<h3 data-number="2.7.3" class="anchored" data-anchor-id="the-nels-example"><span class="header-section-number">2.7.3</span> The NELS example</h3>
<p>For the NELS example, the standard errors, t-test, and p-values of the regression coefficients are shown in the table below (along with the OLS estimates). The F-test appears in the text below the table. Note that the output uses the terminology “multiple R-squared” to refer to R-squared.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = achmat08 ~ ses)

Residuals:
     Min       1Q   Median       3Q      Max 
-20.5995  -6.5519  -0.1475   6.0226  27.6634 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  48.6780     1.1282  43.147  &lt; 2e-16 ***
ses           0.4293     0.0573   7.492 3.13e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 8.863 on 498 degrees of freedom
Multiple R-squared:  0.1013,    Adjusted R-squared:  0.09948 
F-statistic: 56.12 on 1 and 498 DF,  p-value: 3.127e-13</code></pre>
</div>
</div>
<p>The <span class="math inline">\(95\%\)</span> confidence intervals for the regression coefficients are:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                 2.5 %     97.5 %
(Intercept) 46.4614556 50.8946120
ses          0.3166816  0.5418392</code></pre>
</div>
</div>
<p><strong>Please write down your interpretation of the t-tests, confidence intervals, and F-test, and be prepared to share your answers in class.</strong> Hint: state whether the tests are statistically significant and the corresponding conclusions are about the population parameters. For confidence intervals, report the range of values we are “confident” about. For practice, you might want to try using APA notation in your answer (or whatever style conventions are used in your field).</p>
</section>
</section>
<section id="sec-power-2" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="sec-power-2"><span class="header-section-number">2.8</span> Power analysis*</h2>
<p>Statistical power is the probability of rejecting the null hypothesis, when it is indeed false. Rejecting the null hypothesis when it is false is sometimes called a “true positive”, meaning we have correctly inferred that a parameter of interest is not zero. Power analysis is useful for designing studies so that the statistical power / true positive rate is satisfactory.</p>
<p>In practice, statistical power comes down to having a large enough sample size. Consequently, power analysis is important when planning studies (e.g., in research grants proposals). In this class, we will not be planning any studies – rather, we will be working with secondary data analyses. In this context, power analysis is not very interesting, and so we do not mention it much. Nonetheless, power analysis is important for research and so we review the basics here.</p>
<p>Power analysis in regression is very similar to power analysis for the tests we studied last semester. There are five ingredients that go into a power analysis:</p>
<ul>
<li>The desired Type I Error rate, <span class="math inline">\(\alpha\)</span>.</li>
<li>The desired level of statistical power.</li>
<li>The sample size, <span class="math inline">\(N\)</span>.</li>
<li>The number of predictors.</li>
<li>The effect size, which is Cohen’s f-squared statistic (AKA the signal to noise ratio):</li>
</ul>
<p><span class="math display">\[ f^2 = {\frac{R^2}{1-R^2}}. \]</span></p>
<p>In principal, we can plug-in values for any four of these ingredients and then solve for the fifth. But, as mentioned, power analysis is most useful when we solve for <span class="math inline">\(N\)</span> while planning a study. When solving for <span class="math inline">\(N\)</span> “prospectively,” the effect size <span class="math inline">\(f^2\)</span> should be based on reports of R-squared in past research. Power and <span class="math inline">\(\alpha\)</span> are usually chosen to be .8 and .05, respectively.</p>
<p>The example below shows the sample size required to detect an effect size of <span class="math inline">\(R^2 = .1\)</span>. This effect size was based on the NELS example discussed above. Note that the values <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> denote the degrees of freedom in the numerator and denominator of the F-test of R-squared, respectively. The former provides information about the number of predictors in the model, the latter about sample size.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("pwr")</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pwr)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run power analysis</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="fu">pwr.f2.test</span>(<span class="at">u =</span> <span class="dv">1</span>, <span class="at">f2 =</span> .<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> .<span class="dv">1</span>), <span class="at">sig.level =</span> .<span class="dv">05</span>, <span class="at">power =</span> .<span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
     Multiple regression power calculation 

              u = 1
              v = 70.61137
             f2 = 0.1111111
      sig.level = 0.05
          power = 0.8</code></pre>
</div>
</div>
<p>Rounding up, we would require 72 persons in the sample in order to have an 80% chance of detecting an effect size of <span class="math inline">\(R^2 = .1\)</span> with simple regression.</p>
<p>Another use of power analysis is to solve for the effect size. This can be useful when the sample size is constrained by external factors (e.g., budget). In this situation, we can use power analysis to address whether the sample size is sufficient to detect an effect that is “reasonable” (again, based on past research). In the NELS example, we have <span class="math inline">\(N=500\)</span> observations. The output below reports the smallest effect size we can detect with a power of <span class="math inline">\(.8\)</span> and <span class="math inline">\(\alpha = .05\)</span>. This is sometimes called the “minimum detectable effect size” (MDES).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pwr.f2.test</span>(<span class="at">u =</span> <span class="dv">1</span>, <span class="at">v =</span> <span class="dv">498</span>, <span class="at">sig.level =</span> .<span class="dv">05</span>, <span class="at">power =</span> .<span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
     Multiple regression power calculation 

              u = 1
              v = 498
             f2 = 0.01575443
      sig.level = 0.05
          power = 0.8</code></pre>
</div>
</div>
<p>With a sample size of 500, and power of 80%, the MDES for simple regression is <span class="math inline">\(R^2 = f^2 / (1 + f^2) \approx .03\)</span>. Based on this calculation, we can conclude that this sample size is sufficient for applications of simple linear regression in which we expect to explain at least 3% of the variance in the outcome.</p>
</section>
<section id="sec-properties-2" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="sec-properties-2"><span class="header-section-number">2.9</span> Properties of OLS*</h2>
<p>This section summarizes some properties of OLS regression that will be used later on. You can skip this section if you aren’t interested in the math behind regression – the results will be mentioned again when needed.</p>
<p>The derivations in this section follow from the three rules of summation reviewed <a href="ch1_review.html#sec-rules-1"><span>Section&nbsp;1.2</span></a> and make use of the properties of means, variances, and covariances already derived in <a href="ch1_review.html#sec-properties-1"><span>Section&nbsp;1.4</span></a>. If you have any questions about the derivations, I would be happy to address them in class during open lab time.</p>
<section id="residuals" class="level3" data-number="2.9.1">
<h3 data-number="2.9.1" class="anchored" data-anchor-id="residuals"><span class="header-section-number">2.9.1</span> Residuals</h3>
<p>We start with two important implications of <a href="#eq-reg-coeffs">Equation&nbsp;<span>2.4</span></a> for the OLS residuals. In particular, OLS residuals always have mean zero and are uncorrelated with the predictor variable. These properties generalize to multiple regression.</p>
<p>First we show that</p>
<p><span id="eq-ebar"><span class="math display">\[\text{mean} (e) = 0. \tag{2.5}\]</span></span></p>
<p>From <a href="#eq-reg-coeffs">Equation&nbsp;<span>2.4</span></a> we have</p>
<p><span class="math display">\[a = \bar Y - b \bar X \]</span> Solving for <span class="math inline">\(\bar Y\)</span> gives</p>
<p><span class="math display">\[\bar Y = a + b \bar X. \]</span> Since <span class="math inline">\(\hat Y\)</span> is a linear transformation of <span class="math inline">\(X\)</span>, we know from <a href="ch1_review.html#sec-properties-1"><span>Section&nbsp;1.4</span></a> that</p>
<p><span class="math display">\[ \text{mean} (\hat Y) = a + b \bar X. \]</span></p>
<p>The previous two equations imply that <span class="math inline">\(\text{mean} (\hat Y) = \bar Y\)</span>. Consequently,</p>
<p><span class="math display">\[\text{mean}(e) = \text{mean}(Y - \hat Y) = \text{mean}(Y) - \text{mean}(\hat Y) = \bar Y - \bar Y = 0\]</span></p>
<p>Next we show that</p>
<p><span id="eq-covxe"><span class="math display">\[\text{cov}(X, e) = 0.  \tag{2.6}\]</span></span></p>
<p>The derivation is:</p>
<p><span class="math display">\[\begin{align}
\text{cov}(X, e) &amp; = \text{cov}(X, Y - \hat Y) \\
&amp; = \text{cov}(X, Y)  - \text{cov}(X, \hat Y) \\
&amp; = \text{cov}(X, Y)  - \text{cov}(X, a + b X) \\
&amp; = \text{cov}(X, Y)  - b \, \text{var}(X) \\
&amp; = \text{cov}(X, Y)  - \left(\frac{\text{cov}(X, Y)} {\text{var}(X)} \right) \text{var}(X) \\
&amp; = 0
\end{align}\]</span></p>
<p>The second last line uses the expression for the slope in <a href="#eq-reg-coeffs">Equation&nbsp;<span>2.4</span></a>.</p>
</section>
<section id="sec-multiple-r-2" class="level3" data-number="2.9.2">
<h3 data-number="2.9.2" class="anchored" data-anchor-id="sec-multiple-r-2"><span class="header-section-number">2.9.2</span> Multiple correlation (<span class="math inline">\(R\)</span>)</h3>
<p>Above we defined <span class="math inline">\(R^2\)</span> as a proportion of variance. This was a bit lazy. Instead, we can start with the definition of the multiple correlation</p>
<p><span class="math display">\[R = \text{cor}(Y, \hat Y)\]</span></p>
<p>and from this definition derive the result, shown above, that <span class="math inline">\(R^2\)</span> is the proportion of variance in <span class="math inline">\(Y\)</span> associated with <span class="math inline">\(\hat Y\)</span>.</p>
<p>Let’s start by showing that</p>
<p><span id="eq-cov-y-yhat"><span class="math display">\[\text{cov}(Y, \hat Y) = \text{var}(\hat Y) \tag{2.7}\]</span></span></p>
<p>Before deriving this result, note that <a href="#eq-reg-coeffs">Equation&nbsp;<span>2.4</span></a> implies</p>
<p><span class="math display">\[\text{cov}(X, Y) = b \, \text{var}(X),\]</span></p>
<p>and, using the the variance of a linear transformation (<a href="ch1_review.html#sec-properties-1"><span>Section&nbsp;1.4</span></a>), we have</p>
<p><span class="math display">\[ \text{var} (\hat Y) = \text{var}(a + b X) = b^2 \text{var}(X). \]</span></p>
<p>These two results are used on the third and fourth lines of the following derivation, respectively.</p>
<p><span class="math display">\[\begin{align}
\text{cov}(Y, \hat Y)  &amp; = \text{cov}(Y, a + b X) \\
&amp; = b \,\text{cov}(Y, X) \\
&amp; = b^2 \,\text{var}(X) \\
&amp; = \text{var}(\hat Y) \\
\end{align}\]</span></p>
<p>Next we show that <span class="math inline">\(R^2 = \text{var}(\hat Y) / \text{var}(Y)\)</span>:</p>
<p><span class="math display">\[\begin{align}
R^2 &amp; = [\text{cor}(Y, \hat Y)]^2 \\
&amp; = \frac{[\text{cov}(Y, \hat Y)]^2}{\text{var}(Y) \; \text{var}(\hat Y)} \\
&amp; = \frac{[\text{var}(\hat Y)]^2}{\text{var}(Y) \; \text{var}(\hat Y)} \\
&amp; = \frac{\text{var}(\hat Y)}{\text{var}(Y)}. \\
\end{align}\]</span></p>
<p>This derivation is nicer than the one in <a href="#sec-rsquared-2"><span>Section&nbsp;2.4</span></a> because it obtains a result about <span class="math inline">\(R^2\)</span> using the definition of <span class="math inline">\(R\)</span>. However, this derivation does not show that the resulting ratio is a proportion, which requires a second step (which also uses <a href="#eq-cov-y-yhat">Equation&nbsp;<span>2.7</span></a>):</p>
<p><span class="math display">\[\begin{align}
\text{var}(e) &amp; = \text{var}(Y - \hat Y) \\
&amp; = \text{var}(Y) + \text{var}(\hat Y) - 2 \text{cov}(Y, \hat Y) \\
&amp; = \text{var}(Y) + \text{var}(\hat Y) - 2 \text{var}(\hat Y) \\
&amp; = \text{var}(Y) - \text{var}(\hat Y).
\end{align}\]</span></p>
<p>Re-arranging gives</p>
<p><span class="math display">\[ \text{var}(Y) =  \text{var}(\hat Y) + \text{var}(e),\]</span> which shows that <span class="math inline">\(0 \leq \text{var}(\hat Y) \leq \text{var}(Y).\)</span></p>
<p>One last detail concerns the relation between the multiple correlation <span class="math inline">\(R\)</span> and the regular correlation coefficient <span class="math inline">\(r = \text{cor}(Y, X)\)</span>. Using the invariance of the correlation under linear transformation (<a href="ch1_review.html#sec-properties-1"><span>Section&nbsp;1.4</span></a>), we have</p>
<p><span class="math display">\[ R = \text{cor}(Y, \widehat Y) = \text{cor}(Y, a + bX) = \text{cor}(Y, X) = r\]</span></p>
<p>Consequently, in simple regression, <span class="math inline">\(R^2 = r^2\)</span> – i.e., the proportion of variance explained by the predictor is just the squared Pearson product-moment correlation. When we add multiple predictors, this relationship between <span class="math inline">\(R^2\)</span> and <span class="math inline">\(r^2\)</span> no longer holds.</p>
</section>
</section>
<section id="workbook-2" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="workbook-2"><span class="header-section-number">2.10</span> Workbook</h2>
<p>This section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in <a href="#sec-exercises-2"><span>Section&nbsp;2.11</span></a>. The lesson will <strong>not</strong> be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.</p>
<p><a href="#sec-example-2"><span>Section&nbsp;2.1</span></a></p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter plot</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ses, </span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> achmat08, </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Math Achievement (Grade 8)"</span>, </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"SES"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the regression model</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(achmat08 <span class="sc">~</span> ses)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the regression line to the plot</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch2_simple_regression_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Math Achievement and SES (NELS88).</figcaption>
</figure>
</div>
</div>
</div>
<p>The strength and direction of the linear relationship between the two variables is summarized by their correlation. In this sample, the value of the correlation is:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(achmat08, ses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3182484</code></pre>
</div>
</div>
<p>This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in Math (higher Math Achievement). This relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., <a href="https://www.apa.org/pi/ses/resources/publications/education" class="uri">https://www.apa.org/pi/ses/resources/publications/education</a>). Please look over this web page and be prepared to share your thoughts about this relationship.</p>
<p><a href="#sec-ols-2"><span>Section&nbsp;2.3</span></a></p>
<p>For the NELS example, the regression intercept and slope are, respectively:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)         ses 
 48.6780338   0.4292604 </code></pre>
</div>
</div>
<p>Please write down an interpretation of these numbers and be prepared to share your answers in class. How would your interpretation change if, rather than the value of the slope shown above, we had <span class="math inline">\(b = 0\)</span>?</p>
<p><a href="#sec-rsquared-2"><span>Section&nbsp;2.4</span></a></p>
<p>For the NELS example, the R-squared statistic is:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1012821</code></pre>
</div>
</div>
<p>Please write down an interpretation of this number and be prepared to share your answer in class. Hint: Instead of talking about proportions, it is often helpful to multiply by 100 and talk about percentages instead.</p>
<p><a href="#sec-notation-2"><span>Section&nbsp;2.6</span></a></p>
<p>Please be prepared for a pop quiz on notation during class!</p>
<table class="table">
<thead>
<tr class="header">
<th>Concept</th>
<th style="text-align: center;">Sample statistic</th>
<th style="text-align: center;">Population parameter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>regression line</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td>slope</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td>intercept</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td>residual</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td>variance explained</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><a href="#sec-inference-2"><span>Section&nbsp;2.7</span></a></p>
<p>For the NELS example, the standard errors, t-test, and p-values of the regression coefficients are shown in the table below (along with the OLS estimates). The F-test appears in the text below the table. Note that the output uses the terminology “multiple R-squared” to refer to R-squared.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = achmat08 ~ ses)

Residuals:
     Min       1Q   Median       3Q      Max 
-20.5995  -6.5519  -0.1475   6.0226  27.6634 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  48.6780     1.1282  43.147  &lt; 2e-16 ***
ses           0.4293     0.0573   7.492 3.13e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 8.863 on 498 degrees of freedom
Multiple R-squared:  0.1013,    Adjusted R-squared:  0.09948 
F-statistic: 56.12 on 1 and 498 DF,  p-value: 3.127e-13</code></pre>
</div>
</div>
<p>The <span class="math inline">\(95\%\)</span> confidence intervals for the regression coefficients are:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                 2.5 %     97.5 %
(Intercept) 46.4614556 50.8946120
ses          0.3166816  0.5418392</code></pre>
</div>
</div>
<p>Please write down your interpretation of the t-tests, confidence intervals, and F-test, and be prepared to share your answers in class. Hint: state whether the tests are statistically significant and the corresponding conclusions are about the population parameters. For confidence intervals, report the range of values we are “confident” about. For practice, you might want to try using APA notation in your answer (or whatever style conventions are used in your field).</p>
</section>
<section id="sec-exercises-2" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="sec-exercises-2"><span class="header-section-number">2.11</span> Exercises</h2>
<p>These exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)</p>
<p>Before staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”</p>
<section id="the-lm-function" class="level3" data-number="2.11.1">
<h3 data-number="2.11.1" class="anchored" data-anchor-id="the-lm-function"><span class="header-section-number">2.11.1</span> The <code>lm</code> function</h3>
<p>The function<code>lm</code>, short for “linear model”, is used to estimate linear regressions using OLS. It also provides a lot of useful output.</p>
<p>The main argument that the we provides to the <code>lm</code> function is a formula. For the simple regression of Y on X, a formula has the syntax:</p>
<p><code>Y ~ X</code></p>
<p>Here <code>Y</code> denotes the outcome variable and <code>X</code> is the predictor variable. The tilde <code>~</code> just means “equals”, but the equals sign <code>=</code> is already used to for other stuff in R, so <code>~</code> is used instead. We will see more complicated formulas as we go through the course. For more information on R’s formula syntax, see <code>help(formula)</code>.</p>
<p>Let’s take a closer look using the following two variables from the NELS data.</p>
<ul>
<li><p><code>achmat08</code>: eighth grade math achievement (percent correct on a math test)</p></li>
<li><p><code>ses</code>: a composite measure of socio-economic status, on a scale from 0-35</p></li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data. Note that you can click on the .RData file and RStudio will load it</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># load("NELS.RData") #Un-comment this line to run</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Attach the data: will discuss this in class</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co"># attach(NELS) #Un-comment this line to run!</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter plot of math achievement against SES</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ses, <span class="at">y =</span> achmat08, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress math achievement on SES; save output as "mod"</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(achmat08 <span class="sc">~</span> ses)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the regression line to the plot</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch2_simple_regression_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the regression coefficients</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)         ses 
 48.6780338   0.4292604 </code></pre>
</div>
</div>
<p>Let’s do some quick calculations to check that the <code>lm</code> output corresponds the formulas for the slope and intercept in <a href="#sec-ols-2"><span>Section&nbsp;2.3</span></a>:</p>
<p><span class="math display">\[ a = \bar Y - b \bar X \quad \text{and} \quad b = \frac{\text{cov}(X, Y)}{\text{var}(X)}. \]</span></p>
<p>We won’t usually do this kind of “manual” calculation, but it is a good way consolidate knowledge presented in the readings with the output presented by R. It is also useful to refresh our memory about some useful R functions and how the R language works.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the slope as the covariance divided by the variance of X</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>cov_xy <span class="ot">&lt;-</span> <span class="fu">cov</span>(achmat08, ses)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>var_x <span class="ot">&lt;-</span> <span class="fu">var</span>(ses)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> cov_xy <span class="sc">/</span> var_x</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the "manual" calculation to the output from lm. </span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4292604</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the y-intercept using from the two means and the slope</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(ses)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">&lt;-</span> <span class="fu">mean</span>(achmat08)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> ybar <span class="sc">-</span> b <span class="sc">*</span> xbar</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the "manual" calculation to the output from lm. </span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 48.67803</code></pre>
</div>
</div>
<p>Let’s also check our interpretation of the parameters. If the answers to these questions are not clear, please make sure to ask in class!</p>
<ul>
<li><p>What is the predicted value of <code>achmat08</code> when <code>ses</code> is equal to zero?</p></li>
<li><p>How much does the predicted value of <code>achmat08</code> increase for each unit of increase in <code>ses</code>?</p></li>
</ul>
</section>
<section id="variance-explained" class="level3" data-number="2.11.2">
<h3 data-number="2.11.2" class="anchored" data-anchor-id="variance-explained"><span class="header-section-number">2.11.2</span> Variance explained</h3>
<p>Another way to describe the relationship between the two variables is by considering the amount of variation in <span class="math inline">\(Y\)</span> that is associated with (or explained by) its relationship with <span class="math inline">\(X\)</span>. Recall that one way to do this is via the “variance” decomposition</p>
<p><span class="math display">\[ SS_{\text{total}} = SS_{\text{res}} + SS_{\text{reg}}\]</span></p>
<p>from which we can compute the proportion of variation in Y that is associated with the regression model:</p>
<p><span class="math display">\[R^2 = \frac{SS_{\text{reg}}}{SS_{\text{total}}}.\]</span></p>
<p>The R-squared for the example is presented in the output below. You should be able to provide an interpretation of this number, so if it’s not clear make sure to ask in class!</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># R-squared from the example</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1012821</code></pre>
</div>
</div>
<p>As above, let’s compute <span class="math inline">\(R^2\)</span> “by hand” for our example.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the sums of squares</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">&lt;-</span> <span class="fu">mean</span>(achmat08)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>ss_total <span class="ot">&lt;-</span> <span class="fu">sum</span>((achmat08 <span class="sc">-</span> ybar)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>ss_reg <span class="ot">&lt;-</span> <span class="fu">sum</span>((yhat <span class="sc">-</span> ybar)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>ss_res <span class="ot">&lt;-</span>  <span class="fu">sum</span>((achmat08 <span class="sc">-</span> yhat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Check that SS_total = SS_reg + SS_res</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>ss_total</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 43526.91</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>ss_reg <span class="sc">+</span> ss_res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 43526.91</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute R-squared (compare to value from lm)</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>ss_reg<span class="sc">/</span>ss_total</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1012821</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Also check that R-squared is really equal to the square of the PPMC</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(achmat08, ses)<span class="sc">^</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1012821</code></pre>
</div>
</div>
</section>
<section id="predicted-values-and-residuals" class="level3" data-number="2.11.3">
<h3 data-number="2.11.3" class="anchored" data-anchor-id="predicted-values-and-residuals"><span class="header-section-number">2.11.3</span> Predicted values and residuals</h3>
<p>The <code>lm</code> function returns the predicted values <span class="math inline">\(\widehat{Y_i}\)</span> and residuals <span class="math inline">\(e_i\)</span> and which we can access using the <code>$</code> operator. These are useful for various reasons, especially model diagnostics, which we discuss later in the course. For now, lets just take a look at the residual vs fitted plot to illustrate the code.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> mod<span class="sc">$</span>fitted.values</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> mod<span class="sc">$</span>resid</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(yhat, res, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch2_simple_regression_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Also note that the residuals values have mean zero and are uncorrelated with the predictor – this is always the case in OLS (See <a href="#sec-properties-2"><span>Section&nbsp;2.9</span></a>})</p>
<pre><code>mean(res)
cor(yhat, res)</code></pre>
</section>
<section id="inference" class="level3" data-number="2.11.4">
<h3 data-number="2.11.4" class="anchored" data-anchor-id="inference"><span class="header-section-number">2.11.4</span> Inference</h3>
<p>Next let’s address statistical inference, or how we can make conclusions about a population based on a sample from that population.</p>
<p>We can use the <code>summary</code> function to test the coefficients in our model.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = achmat08 ~ ses)

Residuals:
     Min       1Q   Median       3Q      Max 
-20.5995  -6.5519  -0.1475   6.0226  27.6634 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  48.6780     1.1282  43.147  &lt; 2e-16 ***
ses           0.4293     0.0573   7.492 3.13e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 8.863 on 498 degrees of freedom
Multiple R-squared:  0.1013,    Adjusted R-squared:  0.09948 
F-statistic: 56.12 on 1 and 498 DF,  p-value: 3.127e-13</code></pre>
</div>
</div>
<p>In the table, the t-test and p-values are for the null hypothesis that the corresponding coefficient is zero in the population. We can see that the intercept and slope are both significantly different from zero at the .05 level. However, the test of the intercept is not very meaningful (why?).</p>
<p>The text below the table summarizes the output for R-squared, including its F-test, it’s degrees of freedom, and the p-value. (We will talk about adjusted R-square in <a href="ch4_categorical_predictors.html"><span>Chapter&nbsp;4</span></a>)</p>
<p>We can also use the <code>confint</code> function to obtain confidence intervals for the regression coefficients. Use <code>help</code> to find out more about the <code>confint</code> function.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                 2.5 %     97.5 %
(Intercept) 46.4614556 50.8946120
ses          0.3166816  0.5418392</code></pre>
</div>
</div>
<p>Be sure to remember the correct interpretation of confidence intervals: <em>there is a 95% chance that the interval includes the true parameter value</em> (not: there is a 95% chance that the parameter falls in the interval). For example, there is a 95% chance that the interval [.32, .54] includes the true regression coefficient for SES.</p>
</section>
<section id="writing-up-results" class="level3" data-number="2.11.5">
<h3 data-number="2.11.5" class="anchored" data-anchor-id="writing-up-results"><span class="header-section-number">2.11.5</span> Writing up results</h3>
<p>We could write up the results from this analysis in APA format as follows. You should practice doing this kind of thing, because it is important to be able to write up the results of your analyses in a way that people in your area of research will understand.</p>
<p>In this analysis, we considered the relationship between Math Achievement in Grade 8 (percent correct on a math test) and SES (a composite on a scale from <span class="math inline">\(0-35\)</span>). Regressing Math Achievement on SES, the relationship was positive and statistically significant at the <span class="math inline">\(.05\)</span> level (<span class="math inline">\(b = 0.43\)</span>, <span class="math inline">\(t(498) = 7.49\)</span>, <span class="math inline">\(p &lt; .001\)</span>, <span class="math inline">\(95\% \text{ CI: } [0.32, 0.54]\)</span>). SES explained about <span class="math inline">\(10\%\)</span> of the variation in Math Achievement (<span class="math inline">\(R^2 = .10\)</span>, <span class="math inline">\(F(1, 498) = 56.12\)</span>, <span class="math inline">\(p &lt; .001\)</span>).</p>
</section>
<section id="additional-exercises" class="level3" data-number="2.11.6">
<h3 data-number="2.11.6" class="anchored" data-anchor-id="additional-exercises"><span class="header-section-number">2.11.6</span> Additional exercises</h3>
<p>If time permits, we will address these additional exercises in class.</p>
<p>These exercises replace <code>achmat08</code> with</p>
<ul>
<li><code>achrdg08</code>: eighth grade Reading Achievement (percent correct on a reading test)</li>
</ul>
<p>Please answer the following questions using R.</p>
<ul>
<li><p>Plot <code>achrdg08</code> against <code>ses</code>.</p></li>
<li><p>What is the correlation between <code>achrdg08</code> and <code>ses</code>? How does it compare to the correlation with Math and SES?</p></li>
<li><p>How much variation in Reading is explained by SES? Is the proportion of variance explained significant at the .05 level?</p></li>
<li><p>How much do predicted Reading scores increase for a one unit of increase in SES? Is this a statistically significant at the .05 level?</p></li>
<li><p>What are your overall conclusions about the relationship between Academic Achievement and SES in the NELS data? Write up your results using APA formatting or whatever conventions are used in your area of research.</p></li>
</ul>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch1_review.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Review</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ch3_two_predictors.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Two predictors</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb51" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> true</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="an">editor:</span><span class="co"> </span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co">  markdown: </span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="co">    wrap: 72</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="fu"># Simple regression {#sec-chap-2}</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>The focus of this course is linear regression with multiple predictors (AKA *multiple regression*), but we start by reviewing regression with one predictor (AKA *simple regression*). Most of this material should be familiar from EDUC 710. </span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## An example from NELS {#sec-example-2}</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>Let's begin by considering an example. @fig-nels-2 shows the relationship between Grade 8 Math Achievement (percent correct on a math test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see <span class="ot">&lt;https://nces.ed.gov/surveys/nels88/&gt;</span>).</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fig-nels-2, fig.cap = 'Math Achievement and SES (NELS88).', fig.align = 'center'}</span></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and attach the NELS88 data</span></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"NELS.RData"</span>)</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(NELS)</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter plot</span></span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ses, </span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> achmat08, </span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, </span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Math Achievement (Grade 8)"</span>, </span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"SES"</span>)</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the regression model</span></span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(achmat08 <span class="sc">~</span> ses)</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the regression line to the plot</span></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod) </span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>The strength and direction of the linear relationship between the two variables is summarized by their correlation. In this sample, the value of the correlation is:</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(achmat08, ses)</span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a>This is a moderate, positive correlation between Math Achievement and SES. This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in math (higher Math Achievement).</span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a>The relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., <span class="ot">&lt;https://www.apa.org/pi/ses/resources/publications/education&gt;</span>). **Please look over this web page and be prepared to share your thoughts/questions about the relationship between SES and academic achievement and its relevance for education research.**  </span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true" tabindex="-1"></a><span class="fu">## The regression line {#sec-regression-line-2}</span></span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true" tabindex="-1"></a>The section presents three interchangeable ways of writing the regression line in @fig-nels-2. You should be familiar with all three ways of presenting regression equations and you are welcome to use whichever approach you like best in your writing for this class.</span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-51"><a href="#cb51-51" aria-hidden="true" tabindex="-1"></a>The regression line in @fig-nels-2 can be represented mathematically as</span>
<span id="cb51-52"><a href="#cb51-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-53"><a href="#cb51-53" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb51-54"><a href="#cb51-54" aria-hidden="true" tabindex="-1"></a>\widehat Y = a + b X</span>
<span id="cb51-55"><a href="#cb51-55" aria-hidden="true" tabindex="-1"></a>$$ {#eq-yhat}</span>
<span id="cb51-56"><a href="#cb51-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-57"><a href="#cb51-57" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb51-58"><a href="#cb51-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-59"><a href="#cb51-59" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$Y$ denotes Math Achievement </span>
<span id="cb51-60"><a href="#cb51-60" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$X$ denotes SES</span>
<span id="cb51-61"><a href="#cb51-61" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$a$ represents the regression intercept (the value of $\widehat Y$ when $X = 0$)</span>
<span id="cb51-62"><a href="#cb51-62" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$b$ represents the regression slope (how much $\widehat Y$ changes for each unit of increase in $X$)</span>
<span id="cb51-63"><a href="#cb51-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-64"><a href="#cb51-64" aria-hidden="true" tabindex="-1"></a>In this equation, the symbol $\widehat Y$ represents the *predicted value* of Math Achievement for a given value of SES. In @fig-nels-2, the predicted values are represented by the regression line. The *observed values* of Math Achievement are denoted as $Y$. In @fig-nels-2, these values are represented by the points in the scatter plot.  </span>
<span id="cb51-65"><a href="#cb51-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-66"><a href="#cb51-66" aria-hidden="true" tabindex="-1"></a>Some more terminology: the $Y$ variable is often referred to as the *outcome* or the *dependent variable.* The $X$ variable is often referred to as the *predictor*, *independent variable*, *explanatory variable*, or *covariate.* Different areas of research have different conventions about terminology for regression. We talk more about "big picture" interpretations of regression in @sec-chap-3. </span>
<span id="cb51-67"><a href="#cb51-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-68"><a href="#cb51-68" aria-hidden="true" tabindex="-1"></a>The difference between an observed value $Y$ and its predicted value $\widehat Y$ is called a *residual*. Residuals are denoted as $e = Y - \widehat Y$. The residuals for a subset of the data points in @fig-nels-2 are shown in pink in @fig-resid-2 </span>
<span id="cb51-69"><a href="#cb51-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-70"><a href="#cb51-70" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fig-resid-2, fig.cap = 'Residuals for a Subsample of the Example.', fig.align = 'center'}</span></span>
<span id="cb51-71"><a href="#cb51-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Get predicted values from regression model</span></span>
<span id="cb51-72"><a href="#cb51-72" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> mod<span class="sc">$</span>fitted.values</span>
<span id="cb51-73"><a href="#cb51-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-74"><a href="#cb51-74" aria-hidden="true" tabindex="-1"></a><span class="co"># select a subset of the data</span></span>
<span id="cb51-75"><a href="#cb51-75" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">10</span>)</span>
<span id="cb51-76"><a href="#cb51-76" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">sample.int</span>(<span class="dv">500</span>, <span class="dv">30</span>)</span>
<span id="cb51-77"><a href="#cb51-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-78"><a href="#cb51-78" aria-hidden="true" tabindex="-1"></a><span class="co"># plot again</span></span>
<span id="cb51-79"><a href="#cb51-79" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ses[index], </span>
<span id="cb51-80"><a href="#cb51-80" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> achmat08[index], </span>
<span id="cb51-81"><a href="#cb51-81" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Math Achievement (Grade 8)"</span>, </span>
<span id="cb51-82"><a href="#cb51-82" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"SES"</span>)</span>
<span id="cb51-83"><a href="#cb51-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-84"><a href="#cb51-84" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod)</span>
<span id="cb51-85"><a href="#cb51-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-86"><a href="#cb51-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Add pink lines</span></span>
<span id="cb51-87"><a href="#cb51-87" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> ses[index], </span>
<span id="cb51-88"><a href="#cb51-88" aria-hidden="true" tabindex="-1"></a>         <span class="at">y0 =</span> yhat[index], </span>
<span id="cb51-89"><a href="#cb51-89" aria-hidden="true" tabindex="-1"></a>         <span class="at">x1 =</span> ses[index], </span>
<span id="cb51-90"><a href="#cb51-90" aria-hidden="true" tabindex="-1"></a>         <span class="at">y1 =</span> achmat08[index], </span>
<span id="cb51-91"><a href="#cb51-91" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="dv">6</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span>
<span id="cb51-92"><a href="#cb51-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-93"><a href="#cb51-93" aria-hidden="true" tabindex="-1"></a><span class="co"># Overwrite dots to make it look at bit better</span></span>
<span id="cb51-94"><a href="#cb51-94" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> ses[index], </span>
<span id="cb51-95"><a href="#cb51-95" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> achmat08[index], </span>
<span id="cb51-96"><a href="#cb51-96" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, </span>
<span id="cb51-97"><a href="#cb51-97" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb51-98"><a href="#cb51-98" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-99"><a href="#cb51-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-100"><a href="#cb51-100" aria-hidden="true" tabindex="-1"></a>Notice that $Y = \widehat Y + e$ by definition: </span>
<span id="cb51-101"><a href="#cb51-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-102"><a href="#cb51-102" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb51-103"><a href="#cb51-103" aria-hidden="true" tabindex="-1"></a>Y = \widehat Y + e = \widehat Y + (Y - \widehat Y ) = Y.</span>
<span id="cb51-104"><a href="#cb51-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb51-105"><a href="#cb51-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-106"><a href="#cb51-106" aria-hidden="true" tabindex="-1"></a>This leads to a second way of writing out a regression model:</span>
<span id="cb51-107"><a href="#cb51-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-108"><a href="#cb51-108" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb51-109"><a href="#cb51-109" aria-hidden="true" tabindex="-1"></a>Y = a + bX + e.  </span>
<span id="cb51-110"><a href="#cb51-110" aria-hidden="true" tabindex="-1"></a>$$ {#eq-y}</span>
<span id="cb51-111"><a href="#cb51-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-112"><a href="#cb51-112" aria-hidden="true" tabindex="-1"></a>The difference between @eq-yhat and @eq-y is that the former lets us talk about the predicted values ($\hat Y$), whereas the latter lets us talk about the observed data points ($Y$). </span>
<span id="cb51-113"><a href="#cb51-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-114"><a href="#cb51-114" aria-hidden="true" tabindex="-1"></a>A third way to write out the model is using the variable names (or abbreviations) in place of the more generic "X, Y" notation. For example,</span>
<span id="cb51-115"><a href="#cb51-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-116"><a href="#cb51-116" aria-hidden="true" tabindex="-1"></a>$$MATH = a + b(SES) + e$$ {#eq-math}</span>
<span id="cb51-117"><a href="#cb51-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-118"><a href="#cb51-118" aria-hidden="true" tabindex="-1"></a>This notation is useful when talking about a specific example, because we don't have to remember what $Y$ and $X$ stand for. But this notation is more clunky and doesn't lend itself talking about regression in general or writing other mathematical expressions related to regression. </span>
<span id="cb51-119"><a href="#cb51-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-120"><a href="#cb51-120" aria-hidden="true" tabindex="-1"></a>You should be familiar with all three ways of presenting regression equations (@eq-yhat, @eq-y, and @eq-math) and you are welcome to use whichever approach you like best in this class.</span>
<span id="cb51-121"><a href="#cb51-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-122"><a href="#cb51-122" aria-hidden="true" tabindex="-1"></a><span class="fu">## OLS {#sec-ols-2}</span></span>
<span id="cb51-123"><a href="#cb51-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-124"><a href="#cb51-124" aria-hidden="true" tabindex="-1"></a>This section talks about how to estimate the regression intercept (denoted as $a$ in @eq-yhat) and the regression slope (denoted as $b$ in @eq-yhat). The intercept and slope are collectively referred to as the *parameters* of the regression line. They are also referred to as *regression coefficients.* </span>
<span id="cb51-125"><a href="#cb51-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-126"><a href="#cb51-126" aria-hidden="true" tabindex="-1"></a>Our overall goal in this section is to  "fit a line to the data" -- i.e., we want to select the values of the regression coefficients that best represent our data. An intuitive way to approach this problem is by minimizing the residuals -- i.e., minimizing the total amount of pink in @fig-resid-2. We can operationalize this intuitive idea by minimizing the sum of squared residuals:</span>
<span id="cb51-127"><a href="#cb51-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-128"><a href="#cb51-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb51-129"><a href="#cb51-129" aria-hidden="true" tabindex="-1"></a>SS_{\text{res}} = \sum_{i=1}^{N} e_i^2 = \sum_{i=1}^{N} (Y_i - a - b X_i)^2 </span>
<span id="cb51-130"><a href="#cb51-130" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb51-131"><a href="#cb51-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-132"><a href="#cb51-132" aria-hidden="true" tabindex="-1"></a>where $i = 1 \dots N$ indexes the respondents in the sample. When we estimate the regression coefficients by minimizing $SS_{\text{res}}$, this is called ordinary least squares (OLS) regression. OLS is very widely used and is the main focus of this course, although we will visit some other approaches in the second half of the course.</span>
<span id="cb51-133"><a href="#cb51-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-134"><a href="#cb51-134" aria-hidden="true" tabindex="-1"></a>The values of the regression coefficients that minimize $SS_{\text{res}}$ can be found using calculus (i.e., compute the derivatives of $SS_{\text{res}}$ and set them to zero). This approach leads to the following equations for the regression coefficients:</span>
<span id="cb51-135"><a href="#cb51-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-136"><a href="#cb51-136" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb51-137"><a href="#cb51-137" aria-hidden="true" tabindex="-1"></a>a = \bar Y - b \bar X \quad \quad \quad \quad b = \frac{\text{cov}(X, Y)}{s^2_X} = \text{cor}(X, Y) \frac{s_Y}{s_X}</span>
<span id="cb51-138"><a href="#cb51-138" aria-hidden="true" tabindex="-1"></a>$$ {#eq-reg-coeffs}</span>
<span id="cb51-139"><a href="#cb51-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-140"><a href="#cb51-140" aria-hidden="true" tabindex="-1"></a>(If you aren't familiar with the symbols in these equations, check out the review materials in @sec-chap-1 for a refresher.)</span>
<span id="cb51-141"><a href="#cb51-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-142"><a href="#cb51-142" aria-hidden="true" tabindex="-1"></a>The formulas in @eq-reg-coeffs tell us how to compute the regression coefficients using our sample data. However, on face value, these formulas don't tell us much about how to interpret the regression coefficients. For interpreting the regression coefficients, it is more straightforward to refer to @eq-yhat. </span>
<span id="cb51-143"><a href="#cb51-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-144"><a href="#cb51-144" aria-hidden="true" tabindex="-1"></a>To clarify: </span>
<span id="cb51-145"><a href="#cb51-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-146"><a href="#cb51-146" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To *interpret* the regression intercept, use @eq-yhat: It is the value of $\hat Y$ when $X = 0$. Similarly, the regression slope is how much $\hat Y$ changes for a one-unit increase in $X$. </span>
<span id="cb51-147"><a href="#cb51-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-148"><a href="#cb51-148" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To *compute* the regression coefficients, use @eq-reg-coeffs. These formulas are not very intuitive -- they are just what we get when we fit a line to the data using OLS. </span>
<span id="cb51-149"><a href="#cb51-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-150"><a href="#cb51-150" aria-hidden="true" tabindex="-1"></a>It is important to emphasize that the formulas in @eq-reg-coeffs do lead to some useful mathematical results about regression. @sec-properties-2, which is optional, derives some of the main results. If you want a deeper mathematical understanding of regression, make sure to check out this section. If you prefer to skip the math and just learn about the results as they become relevant, that is OK too.</span>
<span id="cb51-151"><a href="#cb51-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-152"><a href="#cb51-152" aria-hidden="true" tabindex="-1"></a><span class="fu">### Correlation and regression</span></span>
<span id="cb51-153"><a href="#cb51-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-154"><a href="#cb51-154" aria-hidden="true" tabindex="-1"></a>Before moving on, it is worth noting something that we can learn from @eq-reg-coeffs without too much math: the regression slope is just a re-packaging of the correlation coefficient. In particular, if we assume that $X$ and $Y$ are z-scores (i.e., they are standardized to have mean of zero and variance of one), then @eq-reg-coeffs reduces to:</span>
<span id="cb51-155"><a href="#cb51-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-156"><a href="#cb51-156" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$a = 0$</span>
<span id="cb51-157"><a href="#cb51-157" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$b = \text{cov}(X, Y) = \text{cor}(X, Y)$</span>
<span id="cb51-158"><a href="#cb51-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-159"><a href="#cb51-159" aria-hidden="true" tabindex="-1"></a>There are two important things to note here. </span>
<span id="cb51-160"><a href="#cb51-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-161"><a href="#cb51-161" aria-hidden="true" tabindex="-1"></a>First, the difference between correlation and simple regression depends on the scale of the variables. Otherwise stated, if we standardize both $Y$ and $X$, then regression is just correlation. In particular, if the correlation is equal to zero, then the regression slope is also equal to zero -- these are just two equivalent ways of saying that the variables are not (linearly) related. </span>
<span id="cb51-162"><a href="#cb51-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-163"><a href="#cb51-163" aria-hidden="true" tabindex="-1"></a>Second, this relationship between correlation and regression holds only for simple regression (i.e., one predictor). When we get to multiple regression, we will see that relationship between regression and correlation (and covariance) gets more complicated.</span>
<span id="cb51-164"><a href="#cb51-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-165"><a href="#cb51-165" aria-hidden="true" tabindex="-1"></a>For the NELS example in @fig-nels-2, the regression intercept and slope are, respectively:</span>
<span id="cb51-166"><a href="#cb51-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-169"><a href="#cb51-169" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-170"><a href="#cb51-170" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod)</span>
<span id="cb51-171"><a href="#cb51-171" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-172"><a href="#cb51-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-173"><a href="#cb51-173" aria-hidden="true" tabindex="-1"></a>**Please write down an interpretation of these numbers and be prepared to share your answers in class. How would your interpretation change if, rather than the value of the slope shown above, we had $b = 0$?**   </span>
<span id="cb51-174"><a href="#cb51-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-175"><a href="#cb51-175" aria-hidden="true" tabindex="-1"></a><span class="fu">## R-squared {#sec-rsquared-2}</span></span>
<span id="cb51-176"><a href="#cb51-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-177"><a href="#cb51-177" aria-hidden="true" tabindex="-1"></a>In this section we introduce another statistic that is commonly used in regression, called "R-squared" (in symbols: $R^2$). First we will talk about its interpretation, then we will show how it is computed. </span>
<span id="cb51-178"><a href="#cb51-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-179"><a href="#cb51-179" aria-hidden="true" tabindex="-1"></a>R-squared is the proportion of variance in the outcome variable that is associated with, or "explained by", the predictor variable. In terms of the NELS example, the variance of the outcome can be interpreted in terms of individual differences in Math Achievement -- i.e., how students deviate from, or vary around, the mean level of Math Achievement. R-squared tells us the extent to which these individual differences in Math Achievement are associated with, or explained by, individual differences in SES.  </span>
<span id="cb51-180"><a href="#cb51-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-181"><a href="#cb51-181" aria-hidden="true" tabindex="-1"></a>As mentioned, R-squared is a proportion. Because it is a proportion, it takes on values between $0$ and $1$. If $R^2 = 0$ then a student's SES doesn't tell us anything about their Math Achievement -- this is the same as saying the two variables aren't correlated, or that there is no (linear) relationship between Math Achievement and SES. If $R^2 = 1$, then all of the data points fall exactly on the regression line, and we can perfectly predict each student's Math Achievement from their SES. </span>
<span id="cb51-182"><a href="#cb51-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-183"><a href="#cb51-183" aria-hidden="true" tabindex="-1"></a>You might be asking -- why do we need R-squared? We already have the regression coefficient (which is just a repackaging of the correlation), so why do we need yet another way of describing the relationship between Math Achievement and SES? This is very true for simple regression! However, when we move on to multiple regression, we will see that R-squared lets us talk about the relationship between the outcome and *all* of the predictors, or any subset of the predictors, whereas the regression coefficient only lets us talk about the relationship with one predictor at a time. </span>
<span id="cb51-184"><a href="#cb51-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-185"><a href="#cb51-185" aria-hidden="true" tabindex="-1"></a>To see how R-squared is computed for the NELS example, let's consider @fig-rsquared-2. The horizontal grey line denotes the mean of Math Achievement. Recall that the variance of $Y$ is computed using the sum-of-squared deviations from the mean. For each student, these deviations from the mean can be divided into two parts. The Figure shows these two parts for a single student, using black and pink dashed lines:</span>
<span id="cb51-186"><a href="#cb51-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-187"><a href="#cb51-187" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The black dashed line represents the extent to which the student's deviation from the mean level of Math Achievement is explained by the linear relationship between Math Achievement and SES. </span>
<span id="cb51-188"><a href="#cb51-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-189"><a href="#cb51-189" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The pink dashed line is the regression residual, which was introduced in  @sec-regression-line-2. This is the variation in Math Achievement that is "left over" after considering the linear relationship with SES.</span>
<span id="cb51-190"><a href="#cb51-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-191"><a href="#cb51-191" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fig-rsquared-2, fig.cap = 'The Idea Behind R-squared.', fig.align = 'center', echo = F}</span></span>
<span id="cb51-192"><a href="#cb51-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-193"><a href="#cb51-193" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ses[index], </span>
<span id="cb51-194"><a href="#cb51-194" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> achmat08[index], </span>
<span id="cb51-195"><a href="#cb51-195" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Math Achievement (Grade 8)"</span>, </span>
<span id="cb51-196"><a href="#cb51-196" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"SES"</span>)</span>
<span id="cb51-197"><a href="#cb51-197" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod)</span>
<span id="cb51-198"><a href="#cb51-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-199"><a href="#cb51-199" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">&lt;-</span> <span class="fu">mean</span>(achmat08)</span>
<span id="cb51-200"><a href="#cb51-200" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> ybar, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb51-201"><a href="#cb51-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-202"><a href="#cb51-202" aria-hidden="true" tabindex="-1"></a><span class="co"># Add pink lines for case 6</span></span>
<span id="cb51-203"><a href="#cb51-203" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> ses[index[<span class="dv">22</span>]], </span>
<span id="cb51-204"><a href="#cb51-204" aria-hidden="true" tabindex="-1"></a>         <span class="at">y0 =</span> yhat[index[<span class="dv">22</span>]], </span>
<span id="cb51-205"><a href="#cb51-205" aria-hidden="true" tabindex="-1"></a>         <span class="at">x1 =</span> ses[index[<span class="dv">22</span>]], </span>
<span id="cb51-206"><a href="#cb51-206" aria-hidden="true" tabindex="-1"></a>         <span class="at">y1 =</span> achmat08[index[<span class="dv">22</span>]], </span>
<span id="cb51-207"><a href="#cb51-207" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="dv">6</span>, <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb51-208"><a href="#cb51-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-209"><a href="#cb51-209" aria-hidden="true" tabindex="-1"></a><span class="co"># Add black lines </span></span>
<span id="cb51-210"><a href="#cb51-210" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="at">x0 =</span> ses[index[<span class="dv">22</span>]], </span>
<span id="cb51-211"><a href="#cb51-211" aria-hidden="true" tabindex="-1"></a>         <span class="at">y0 =</span> yhat[index[<span class="dv">22</span>]], </span>
<span id="cb51-212"><a href="#cb51-212" aria-hidden="true" tabindex="-1"></a>         <span class="at">x1 =</span> ses[index[<span class="dv">22</span>]], </span>
<span id="cb51-213"><a href="#cb51-213" aria-hidden="true" tabindex="-1"></a>         <span class="at">y1 =</span> ybar, </span>
<span id="cb51-214"><a href="#cb51-214" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb51-215"><a href="#cb51-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-216"><a href="#cb51-216" aria-hidden="true" tabindex="-1"></a><span class="co"># Overwrite dots to make it look at bit better</span></span>
<span id="cb51-217"><a href="#cb51-217" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="at">x =</span> ses[index],</span>
<span id="cb51-218"><a href="#cb51-218" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> achmat08[index], </span>
<span id="cb51-219"><a href="#cb51-219" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb51-220"><a href="#cb51-220" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-221"><a href="#cb51-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-222"><a href="#cb51-222" aria-hidden="true" tabindex="-1"></a>The R-squared statistic averages the variation in Math Achievement associated with SES (i.e., the black dashed line) for all students in the sample, </span>
<span id="cb51-223"><a href="#cb51-223" aria-hidden="true" tabindex="-1"></a>and then divides by the total variation in Math Achievement (i.e., black + pink). </span>
<span id="cb51-224"><a href="#cb51-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-225"><a href="#cb51-225" aria-hidden="true" tabindex="-1"></a>The derivation of the R-squared statistic is not very complicated and provides some useful notation. To simplify the derivation, we can work the numerator of the variance, which is called the "total sum of squares:"</span>
<span id="cb51-226"><a href="#cb51-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-227"><a href="#cb51-227" aria-hidden="true" tabindex="-1"></a>$$SS_{\text{total}} = \sum_{i = 1}^N (Y_i - \bar Y)^2. $$</span>
<span id="cb51-228"><a href="#cb51-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-229"><a href="#cb51-229" aria-hidden="true" tabindex="-1"></a>Next we add and subtract the predicted values (that old trick!):</span>
<span id="cb51-230"><a href="#cb51-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-231"><a href="#cb51-231" aria-hidden="true" tabindex="-1"></a>$$SS_{\text{total}} = \sum_{i = 1}^N <span class="co">[</span><span class="ot">(Y_i - \widehat Y_i) + (\widehat Y_i - \bar Y)</span><span class="co">]</span>^2. $$</span>
<span id="cb51-232"><a href="#cb51-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-233"><a href="#cb51-233" aria-hidden="true" tabindex="-1"></a>The right-hand-side can be reduced to two other sums of squares using the rules of summation algebra (see @sec-rules-1 -- the derivation is long but not complicated). </span>
<span id="cb51-234"><a href="#cb51-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-235"><a href="#cb51-235" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb51-236"><a href="#cb51-236" aria-hidden="true" tabindex="-1"></a>SS_{\text{total}} = \sum_{i = 1}^N (Y_i - \widehat Y_i)^2 + \sum_{i = 1}^N (\widehat Y_i - \bar Y)^2. </span>
<span id="cb51-237"><a href="#cb51-237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb51-238"><a href="#cb51-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-239"><a href="#cb51-239" aria-hidden="true" tabindex="-1"></a>The first term on the right-hand-side is just the sum of squared residuals ($SS_\text{res}$) from @sec-ols-2. The second term is called the sum of squared regression and denoted $SS_\text{reg}$. Using this notation we can re-write the previous equation as</span>
<span id="cb51-240"><a href="#cb51-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-241"><a href="#cb51-241" aria-hidden="true" tabindex="-1"></a>$$ SS_{\text{total}} = SS_\text{res} + SS_\text{reg} $$</span>
<span id="cb51-242"><a href="#cb51-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-243"><a href="#cb51-243" aria-hidden="true" tabindex="-1"></a>and the R-squared statistic is computed as</span>
<span id="cb51-244"><a href="#cb51-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-245"><a href="#cb51-245" aria-hidden="true" tabindex="-1"></a>$$R^2 = SS_{\text{reg}} / SS_{\text{total}}. $$</span>
<span id="cb51-246"><a href="#cb51-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-247"><a href="#cb51-247" aria-hidden="true" tabindex="-1"></a>As discussed above, this quantity can be interpreted as the proportion of variance in $Y$ that is explained by its linear relationship with $X$.</span>
<span id="cb51-248"><a href="#cb51-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-249"><a href="#cb51-249" aria-hidden="true" tabindex="-1"></a>For the NELS example, the R-squared statistic is:</span>
<span id="cb51-250"><a href="#cb51-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-253"><a href="#cb51-253" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-254"><a href="#cb51-254" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared</span>
<span id="cb51-255"><a href="#cb51-255" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-256"><a href="#cb51-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-257"><a href="#cb51-257" aria-hidden="true" tabindex="-1"></a>**Please write down an interpretation of this number and be prepared to share your answer in class.** Hint: Instead of talking about proportions, it is often helpful to multiply by 100 and talk about percentages instead. </span>
<span id="cb51-258"><a href="#cb51-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-259"><a href="#cb51-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## The population model {#sec-population-model-2}</span></span>
<span id="cb51-260"><a href="#cb51-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-261"><a href="#cb51-261" aria-hidden="true" tabindex="-1"></a>Up to this point we have discussed simple linear regression as a way of describing the relationship between two variables in a sample. The next step is to discuss statistical inference. Recall that statistical inference involves generalizing from a sample to the population from which the sample was drawn. </span>
<span id="cb51-262"><a href="#cb51-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-263"><a href="#cb51-263" aria-hidden="true" tabindex="-1"></a>In the NELS example, the population of interest is U.S. eighth graders in 1988. We want to be able to draw conclusions about that population based on the sample of eighth graders that participated in NELS. In order to do that, we make some statistical assumptions about the population, which are collectively referred to as the *population model*. </span>
<span id="cb51-264"><a href="#cb51-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-265"><a href="#cb51-265" aria-hidden="true" tabindex="-1"></a>The population model for simple linear regression is summarized in @fig-pop-model. The three assumptions associated with this model are written below. We talk about how to check the plausibility of these assumptions in @sec-chapter-7.</span>
<span id="cb51-266"><a href="#cb51-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-267"><a href="#cb51-267" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-pop-model, echo = F, fig.cap = "The Regression Population Model.", fig.align = 'center'}</span></span>
<span id="cb51-268"><a href="#cb51-268" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/population_model.png"</span>)</span>
<span id="cb51-269"><a href="#cb51-269" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-270"><a href="#cb51-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-271"><a href="#cb51-271" aria-hidden="true" tabindex="-1"></a>The three assumptions:</span>
<span id="cb51-272"><a href="#cb51-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-273"><a href="#cb51-273" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Normality: The values of $Y$ conditional on $X$, denoted $Y|X$, are normally distributed. The figure shows these distributions for three values of $X$. We can write this assumption formally as </span>
<span id="cb51-274"><a href="#cb51-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-275"><a href="#cb51-275" aria-hidden="true" tabindex="-1"></a>$$Y | X \sim  N(\mu_{Y | X} , \sigma^2_{Y | X}) $$</span>
<span id="cb51-276"><a href="#cb51-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-277"><a href="#cb51-277" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>(This notation should be familiar from EDUC 710. In general, we write $Y \sim   N(\mu, \sigma^2)$ to denote that the variable $Y$ has a normal distribution with mean $\mu$ and variance $\sigma^2$.)</span>
<span id="cb51-278"><a href="#cb51-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-279"><a href="#cb51-279" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Homoskedasticity: The conditional distributions have equal variances (also called "homogeneity of variance", or simply "equal variances").</span>
<span id="cb51-280"><a href="#cb51-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-281"><a href="#cb51-281" aria-hidden="true" tabindex="-1"></a>$$ \sigma^2_{Y| X} = \sigma^2 $$</span>
<span id="cb51-282"><a href="#cb51-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-283"><a href="#cb51-283" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Linearity: The means of the conditional distributions are a linear function of $X$.</span>
<span id="cb51-284"><a href="#cb51-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-285"><a href="#cb51-285" aria-hidden="true" tabindex="-1"></a>$$ \mu_{Y| Χ} = a + bX $$</span>
<span id="cb51-286"><a href="#cb51-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-287"><a href="#cb51-287" aria-hidden="true" tabindex="-1"></a>These three assumptions are summarized by writing</span>
<span id="cb51-288"><a href="#cb51-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-289"><a href="#cb51-289" aria-hidden="true" tabindex="-1"></a>$$ Y|X \sim N(a + bX, \sigma^2). $$</span>
<span id="cb51-290"><a href="#cb51-290" aria-hidden="true" tabindex="-1"></a>Sometimes it will be easier to state the assumptions in terms of the population residuals, $\epsilon = Y - \mu_{Y|X}$. The residuals have distribution $\epsilon \sim N(0, \sigma^2)$.</span>
<span id="cb51-291"><a href="#cb51-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-292"><a href="#cb51-292" aria-hidden="true" tabindex="-1"></a>Sometimes it will also be easier to write the population regression line using expected values, $E(Y|X)$, rather than $\mu_{Y|X}$. Both of these are interpreted the same way -- they denote the mean of $Y$ for a given value of $X$. </span>
<span id="cb51-293"><a href="#cb51-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-294"><a href="#cb51-294" aria-hidden="true" tabindex="-1"></a>An additional assumption is usually made about the data in the sample -- that they were obtained as a simple random sample from the population. We will see some ways of dealing with other types of samples later on this course, but for now we can consider this a background assumption that applies to OLS regression.  </span>
<span id="cb51-295"><a href="#cb51-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-296"><a href="#cb51-296" aria-hidden="true" tabindex="-1"></a>From a mathematical perspective, these assumptions are important because they can be used to prove (a) that OLS regression provides unbiased estimates of the population regression coefficients and (b) that the OLS estimates are more precise than any other unbiased estimates of the population regression coefficients. There are other variations on these assumptions, which are sometimes called the Gauss-Markov assumptions <span class="co">[</span><span class="ot">see https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem</span><span class="co">](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem)</span>. </span>
<span id="cb51-297"><a href="#cb51-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-298"><a href="#cb51-298" aria-hidden="true" tabindex="-1"></a>From a practical perspective, these assumptions are important conditions that we should check when conducting data analyses. If the assumptions are violated -- particularly the linearity assumption -- then our statistical model may not be a good representation of the population. If the model is not a good representation of the population, then inferences based on the model may provide misleading conclusions about the population.   </span>
<span id="cb51-299"><a href="#cb51-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-300"><a href="#cb51-300" aria-hidden="true" tabindex="-1"></a><span class="fu">## Clarifying notation {#sec-notation-2}</span></span>
<span id="cb51-301"><a href="#cb51-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-302"><a href="#cb51-302" aria-hidden="true" tabindex="-1"></a>At this point we have used the mathematical symbols for regression (e.g., $a$, $b$) in two different ways:</span>
<span id="cb51-303"><a href="#cb51-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-304"><a href="#cb51-304" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>In @sec-regression-line-2 they denoted sample statistics.</span>
<span id="cb51-305"><a href="#cb51-305" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>In @sec-population-model-2 they denoted population parameters.</span>
<span id="cb51-306"><a href="#cb51-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-307"><a href="#cb51-307" aria-hidden="true" tabindex="-1"></a>The population versus sample notation for regression is a bit of a hot mess, but the following conventions are used.</span>
<span id="cb51-308"><a href="#cb51-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-309"><a href="#cb51-309" aria-hidden="true" tabindex="-1"></a>| Concept            | Sample statistic | Population parameter |</span>
<span id="cb51-310"><a href="#cb51-310" aria-hidden="true" tabindex="-1"></a>|--------------------|:----------------:|:--------------------:|</span>
<span id="cb51-311"><a href="#cb51-311" aria-hidden="true" tabindex="-1"></a>| regression line    |   $\widehat Y$   |     $\mu_{Y|X}$  or $E(Y|X)$    |</span>
<span id="cb51-312"><a href="#cb51-312" aria-hidden="true" tabindex="-1"></a>| slope              |   $\widehat b$   |         $b$          |</span>
<span id="cb51-313"><a href="#cb51-313" aria-hidden="true" tabindex="-1"></a>| intercept          |   $\widehat a$   |         $a$          |</span>
<span id="cb51-314"><a href="#cb51-314" aria-hidden="true" tabindex="-1"></a>| residual           |       $e$        |      $\epsilon$      |</span>
<span id="cb51-315"><a href="#cb51-315" aria-hidden="true" tabindex="-1"></a>| variance explained |  $\widehat R^2$  |        $R^2$         |</span>
<span id="cb51-316"><a href="#cb51-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-317"><a href="#cb51-317" aria-hidden="true" tabindex="-1"></a>The "hats" always denote sample quantities, and the Greek letters  always denote population quantities, but there is some lack of consistency. For example, why not use $\beta$ instead of $b$ for the population slope? Well, $\beta$ is conventionally used to denote standardized regression coefficients in the *sample*, so its already taken (more on this in @sec-chap-4).</span>
<span id="cb51-318"><a href="#cb51-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-319"><a href="#cb51-319" aria-hidden="true" tabindex="-1"></a>If it is clear from context that we are talking about the sample rather than the population, then the hats are usually omitted from the statistics $\widehat a$, $\widehat b$, and $\widehat R^2$. This doesn't apply to $\widehat Y$, because the hat is required to distinguish the predicted values from the data points.</span>
<span id="cb51-320"><a href="#cb51-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-321"><a href="#cb51-321" aria-hidden="true" tabindex="-1"></a>Another thing to note is that while $\widehat Y$ is often called a predicted value, $E(Y|X)$ is not usually referred to this way. It is called the conditional mean function or the conditional expectation function. Using this language, we can say that regression is about estimating the conditional mean function. </span>
<span id="cb51-322"><a href="#cb51-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-323"><a href="#cb51-323" aria-hidden="true" tabindex="-1"></a>**Please be prepared for a pop quiz on notation during class!**</span>
<span id="cb51-324"><a href="#cb51-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-325"><a href="#cb51-325" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference {#sec-inference-2}</span></span>
<span id="cb51-326"><a href="#cb51-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-327"><a href="#cb51-327" aria-hidden="true" tabindex="-1"></a>This section reviews the main inferential procedures for regression. The formulas presented in this section are used to produce standard errors, t-tests, p-values, and confidence intervals for the regression coefficients, as well as an F-test for R-squared. It is very unlikely that you will ever need to compute these formulas by hand, so don't worry about memorizing them. </span>
<span id="cb51-328"><a href="#cb51-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-329"><a href="#cb51-329" aria-hidden="true" tabindex="-1"></a>However, it is important that you can interpret the numerical results in research settings. The interpretations of these procedures were reviewed in @sec-chap-1 and should be familiar from EDUC 710. This sections documents the formulas for simple regression and then asks you to interpret the results in the context of the NELS example. </span>
<span id="cb51-330"><a href="#cb51-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-331"><a href="#cb51-331" aria-hidden="true" tabindex="-1"></a>It is worth noting that the regression intercept is often not of interest in simple regression. Recall that the intercept is the value of $\widehat Y$ when $X = 0$. So, unless we have a hypothesis or research question about this particular value of $X$ (e.g., eighth graders with $SES = 0$), we won't be interested in a test of the regression intercept. When we get into to multiple regression, we will see some situations where the intercept is of interest. </span>
<span id="cb51-332"><a href="#cb51-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-333"><a href="#cb51-333" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference for coefficients {#sec-inference-for-coefficients-2}</span></span>
<span id="cb51-334"><a href="#cb51-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-335"><a href="#cb51-335" aria-hidden="true" tabindex="-1"></a>When the population model is true, $\widehat b$ is an unbiased estimate of $b$ (in symbols: $E(\hat b) = b$). The standard error of $\widehat b$ is equal to (see @fox-2016, Section 6.1):</span>
<span id="cb51-336"><a href="#cb51-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-337"><a href="#cb51-337" aria-hidden="true" tabindex="-1"></a>$$ SE(\widehat b) = \frac{s_Y}{s_X} \sqrt{\frac{1-R^2}{N-2}} . $$</span>
<span id="cb51-338"><a href="#cb51-338" aria-hidden="true" tabindex="-1"></a>Using these two results, we can compute t-tests and confidence intervals for the regression slope in the usual way.</span>
<span id="cb51-339"><a href="#cb51-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-340"><a href="#cb51-340" aria-hidden="true" tabindex="-1"></a>**t-tests**</span>
<span id="cb51-341"><a href="#cb51-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-342"><a href="#cb51-342" aria-hidden="true" tabindex="-1"></a>The null hypothesis $H_0: \widehat b = b_0$ can be tested against the alternative $H_A: \widehat b \neq b_0$ using the test statistic:</span>
<span id="cb51-343"><a href="#cb51-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-344"><a href="#cb51-344" aria-hidden="true" tabindex="-1"></a>$$ t = \frac{\widehat b - b_0}{SE(\widehat b)}, $$</span>
<span id="cb51-345"><a href="#cb51-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-346"><a href="#cb51-346" aria-hidden="true" tabindex="-1"></a>which has a t-distribution on $N-2$ degrees of freedom when the null hypothesis is true.</span>
<span id="cb51-347"><a href="#cb51-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-348"><a href="#cb51-348" aria-hidden="true" tabindex="-1"></a>The test assumes that the population model is correct. The null hypothesized value of the parameter is usually chosen to be $b_0 = 0$, in which case the test is interpreted in terms of the "statistical significance" of the regression slope.</span>
<span id="cb51-349"><a href="#cb51-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-350"><a href="#cb51-350" aria-hidden="true" tabindex="-1"></a>**Confidence intervals**</span>
<span id="cb51-351"><a href="#cb51-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-352"><a href="#cb51-352" aria-hidden="true" tabindex="-1"></a>For a given Type I Error rate, $\alpha$, the corresponding $(1-\alpha) \times 100\%$ confidence interval is</span>
<span id="cb51-353"><a href="#cb51-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-354"><a href="#cb51-354" aria-hidden="true" tabindex="-1"></a>$$ b_0 = \widehat b \pm t_{\alpha/2} \times SE(\widehat b), $$</span>
<span id="cb51-355"><a href="#cb51-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-356"><a href="#cb51-356" aria-hidden="true" tabindex="-1"></a>where $t_{\alpha/2}$ denotes the $\alpha/2$ quantile of the $t$-distribution with $N-2$ degrees of freedom. For example, if $\alpha$ is chosen to be $.05$, the corresponding $95\%$ confidence interval uses $t_{.025}$, or the 2.5-th percentile of the t-distribution.</span>
<span id="cb51-357"><a href="#cb51-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-358"><a href="#cb51-358" aria-hidden="true" tabindex="-1"></a>The standard error for the regression intercept, presented below, can be used to compute t-tests and confidence intervals for $\hat a$: </span>
<span id="cb51-359"><a href="#cb51-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-360"><a href="#cb51-360" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb51-361"><a href="#cb51-361" aria-hidden="true" tabindex="-1"></a>SE(\widehat a) = \sqrt{\frac{SS_{\text{res}}}{N-2} \left(\frac{1}{N} + \frac{\bar X^2}{(N-1)s^2_X}\right)}.</span>
<span id="cb51-362"><a href="#cb51-362" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb51-363"><a href="#cb51-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-364"><a href="#cb51-364" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference for R-squared {#sec-inference-for-rsquared-2}</span></span>
<span id="cb51-365"><a href="#cb51-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-366"><a href="#cb51-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-367"><a href="#cb51-367" aria-hidden="true" tabindex="-1"></a>The null hypothesis $H_0: R^2 = 0$ can be tested against the alternative $H_A: R^2 \neq 0$ using the F-test:</span>
<span id="cb51-368"><a href="#cb51-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-369"><a href="#cb51-369" aria-hidden="true" tabindex="-1"></a>$$ F = (N-2) \frac{\widehat R^2}{1-\widehat R^2}, $$</span>
<span id="cb51-370"><a href="#cb51-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-371"><a href="#cb51-371" aria-hidden="true" tabindex="-1"></a>which has a F-distribution on $1$ and $N – 2$ degrees of freedom when the null is true. The test assumes that the population model is true. Confidence intervals for R-squared are generally not reported.</span>
<span id="cb51-372"><a href="#cb51-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-373"><a href="#cb51-373" aria-hidden="true" tabindex="-1"></a><span class="fu">### The NELS example</span></span>
<span id="cb51-374"><a href="#cb51-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-375"><a href="#cb51-375" aria-hidden="true" tabindex="-1"></a>For the NELS example, the standard errors, t-test, and p-values of the regression coefficients are shown in the table below (along with the OLS estimates). The F-test appears in the text below the table. Note that the output uses the terminology "multiple R-squared" to refer to R-squared.</span>
<span id="cb51-376"><a href="#cb51-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-379"><a href="#cb51-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-380"><a href="#cb51-380" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span>
<span id="cb51-381"><a href="#cb51-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-382"><a href="#cb51-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-383"><a href="#cb51-383" aria-hidden="true" tabindex="-1"></a>The $95\%$ confidence intervals for the regression coefficients are: </span>
<span id="cb51-384"><a href="#cb51-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-387"><a href="#cb51-387" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-388"><a href="#cb51-388" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(mod)</span>
<span id="cb51-389"><a href="#cb51-389" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-390"><a href="#cb51-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-391"><a href="#cb51-391" aria-hidden="true" tabindex="-1"></a>**Please write down your interpretation of the t-tests, confidence intervals, and F-test, and be prepared to share your answers in class.** Hint: state whether the tests are statistically significant and the corresponding conclusions are about the population parameters. For confidence intervals, report the range of values we are "confident" about. For practice, you might want to try using APA notation in your answer (or whatever style conventions are used in your field). </span>
<span id="cb51-392"><a href="#cb51-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-393"><a href="#cb51-393" aria-hidden="true" tabindex="-1"></a><span class="fu">## Power analysis* {#sec-power-2}</span></span>
<span id="cb51-394"><a href="#cb51-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-395"><a href="#cb51-395" aria-hidden="true" tabindex="-1"></a>Statistical power is the probability of rejecting the null hypothesis, when it is indeed false. Rejecting the null hypothesis when it is false is sometimes called a "true positive", meaning we have correctly inferred that a parameter of interest is not zero. Power analysis is useful for designing studies so that the statistical power / true positive rate is satisfactory. </span>
<span id="cb51-396"><a href="#cb51-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-397"><a href="#cb51-397" aria-hidden="true" tabindex="-1"></a>In practice, statistical power comes down to having a large enough sample size. Consequently, power analysis is important when planning studies (e.g., in research grants proposals). In this class, we will not be planning any studies -- rather, we will be working with secondary data analyses. In this context, power analysis is not very interesting, and so we do not mention it much. Nonetheless, power analysis is important for research and so we review the basics here. </span>
<span id="cb51-398"><a href="#cb51-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-399"><a href="#cb51-399" aria-hidden="true" tabindex="-1"></a>Power analysis in regression is very similar to power analysis for the tests we studied last semester. There are five ingredients that go into a power analysis:</span>
<span id="cb51-400"><a href="#cb51-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-401"><a href="#cb51-401" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The desired Type I Error rate, $\alpha$.</span>
<span id="cb51-402"><a href="#cb51-402" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The desired level of statistical power.</span>
<span id="cb51-403"><a href="#cb51-403" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The sample size, $N$.</span>
<span id="cb51-404"><a href="#cb51-404" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The number of predictors.</span>
<span id="cb51-405"><a href="#cb51-405" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The effect size, which is Cohen's f-squared statistic (AKA the signal to noise ratio):</span>
<span id="cb51-406"><a href="#cb51-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-407"><a href="#cb51-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-408"><a href="#cb51-408" aria-hidden="true" tabindex="-1"></a>$$ f^2 = {\frac{R^2}{1-R^2}}. $$</span>
<span id="cb51-409"><a href="#cb51-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-410"><a href="#cb51-410" aria-hidden="true" tabindex="-1"></a>In principal, we can plug-in values for any four of these ingredients and then solve for the fifth. But, as mentioned, power analysis is most useful when we solve for $N$ while planning a study. When solving for $N$ "prospectively," the effect size $f^2$ should be based on reports of R-squared in past research. Power and $\alpha$ are usually chosen to be .8 and .05, respectively.</span>
<span id="cb51-411"><a href="#cb51-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-412"><a href="#cb51-412" aria-hidden="true" tabindex="-1"></a>The example below shows the sample size required to detect an effect size of $R^2 = .1$. This effect size was based on the NELS example discussed above. Note that the values $u$ and $v$ denote the degrees of freedom in the numerator and denominator of the F-test of R-squared, respectively. The former provides information about the number of predictors in the model, the latter about sample size. </span>
<span id="cb51-413"><a href="#cb51-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-416"><a href="#cb51-416" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-417"><a href="#cb51-417" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb51-418"><a href="#cb51-418" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("pwr")</span></span>
<span id="cb51-419"><a href="#cb51-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-420"><a href="#cb51-420" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb51-421"><a href="#cb51-421" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pwr)</span>
<span id="cb51-422"><a href="#cb51-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-423"><a href="#cb51-423" aria-hidden="true" tabindex="-1"></a><span class="co"># Run power analysis</span></span>
<span id="cb51-424"><a href="#cb51-424" aria-hidden="true" tabindex="-1"></a><span class="fu">pwr.f2.test</span>(<span class="at">u =</span> <span class="dv">1</span>, <span class="at">f2 =</span> .<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> .<span class="dv">1</span>), <span class="at">sig.level =</span> .<span class="dv">05</span>, <span class="at">power =</span> .<span class="dv">8</span>)</span>
<span id="cb51-425"><a href="#cb51-425" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-426"><a href="#cb51-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-427"><a href="#cb51-427" aria-hidden="true" tabindex="-1"></a>Rounding up, we would require 72 persons in the sample in order to have an 80\% chance of detecting an effect size of $R^2 = .1$ with simple regression.  </span>
<span id="cb51-428"><a href="#cb51-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-429"><a href="#cb51-429" aria-hidden="true" tabindex="-1"></a>Another use of power analysis is to solve for the effect size. This can be useful when the sample size is constrained by external factors (e.g., budget). In this situation, we can use power analysis to address whether the sample size is sufficient to detect an effect that is "reasonable" (again, based on past research). In the NELS example, we have $N=500$ observations. The output below reports the smallest effect size we can detect with a power of $.8$ and $\alpha = .05$. This is sometimes called the "minimum detectable effect size" (MDES). </span>
<span id="cb51-430"><a href="#cb51-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-433"><a href="#cb51-433" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-434"><a href="#cb51-434" aria-hidden="true" tabindex="-1"></a><span class="fu">pwr.f2.test</span>(<span class="at">u =</span> <span class="dv">1</span>, <span class="at">v =</span> <span class="dv">498</span>, <span class="at">sig.level =</span> .<span class="dv">05</span>, <span class="at">power =</span> .<span class="dv">8</span>)</span>
<span id="cb51-435"><a href="#cb51-435" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-436"><a href="#cb51-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-437"><a href="#cb51-437" aria-hidden="true" tabindex="-1"></a>With a sample size of 500, and power of 80\%, the MDES for simple regression is $R^2 = f^2 / (1 + f^2) \approx .03$. Based on this calculation, we can conclude that this sample size is sufficient for applications of simple linear regression in which we expect to explain at least 3\% of the variance in the outcome. </span>
<span id="cb51-438"><a href="#cb51-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-439"><a href="#cb51-439" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of OLS* {#sec-properties-2}</span></span>
<span id="cb51-440"><a href="#cb51-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-441"><a href="#cb51-441" aria-hidden="true" tabindex="-1"></a>This section summarizes some properties of OLS regression that will be used later on. You can skip this section if you aren't interested in the math behind regression -- the results will be mentioned again when needed.  </span>
<span id="cb51-442"><a href="#cb51-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-443"><a href="#cb51-443" aria-hidden="true" tabindex="-1"></a>The derivations in this section follow from the three rules of summation reviewed @sec-rules-1 and make use of the properties of means, variances, and covariances already derived in @sec-properties-1. If you have any questions about the derivations, I would be happy to address them in class during open lab time. </span>
<span id="cb51-444"><a href="#cb51-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-445"><a href="#cb51-445" aria-hidden="true" tabindex="-1"></a><span class="fu">### Residuals</span></span>
<span id="cb51-446"><a href="#cb51-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-447"><a href="#cb51-447" aria-hidden="true" tabindex="-1"></a>We start with two important implications of @eq-reg-coeffs for the OLS residuals. In particular, OLS residuals always have mean zero and are uncorrelated with the predictor variable. These properties generalize to multiple regression. </span>
<span id="cb51-448"><a href="#cb51-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-449"><a href="#cb51-449" aria-hidden="true" tabindex="-1"></a>First we show that </span>
<span id="cb51-450"><a href="#cb51-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-451"><a href="#cb51-451" aria-hidden="true" tabindex="-1"></a>$$\text{mean} (e) = 0.$$ {#eq-ebar}</span>
<span id="cb51-452"><a href="#cb51-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-453"><a href="#cb51-453" aria-hidden="true" tabindex="-1"></a>From @eq-reg-coeffs we have</span>
<span id="cb51-454"><a href="#cb51-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-455"><a href="#cb51-455" aria-hidden="true" tabindex="-1"></a>$$a = \bar Y - b \bar X $$</span>
<span id="cb51-456"><a href="#cb51-456" aria-hidden="true" tabindex="-1"></a>Solving for $\bar Y$ gives</span>
<span id="cb51-457"><a href="#cb51-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-458"><a href="#cb51-458" aria-hidden="true" tabindex="-1"></a>$$\bar Y = a + b \bar X. $$</span>
<span id="cb51-459"><a href="#cb51-459" aria-hidden="true" tabindex="-1"></a>Since $\hat Y$ is a linear transformation of $X$, we know from @sec-properties-1 that </span>
<span id="cb51-460"><a href="#cb51-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-461"><a href="#cb51-461" aria-hidden="true" tabindex="-1"></a>$$ \text{mean} (\hat Y) = a + b \bar X. $$ </span>
<span id="cb51-462"><a href="#cb51-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-463"><a href="#cb51-463" aria-hidden="true" tabindex="-1"></a>The previous two equations imply that $\text{mean} (\hat Y) = \bar Y$. Consequently, </span>
<span id="cb51-464"><a href="#cb51-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-465"><a href="#cb51-465" aria-hidden="true" tabindex="-1"></a>$$\text{mean}(e) = \text{mean}(Y - \hat Y) = \text{mean}(Y) - \text{mean}(\hat Y) = \bar Y - \bar Y = 0$$</span>
<span id="cb51-466"><a href="#cb51-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-467"><a href="#cb51-467" aria-hidden="true" tabindex="-1"></a>Next we show that </span>
<span id="cb51-468"><a href="#cb51-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-469"><a href="#cb51-469" aria-hidden="true" tabindex="-1"></a>$$\text{cov}(X, e) = 0. $$ {#eq-covxe}</span>
<span id="cb51-470"><a href="#cb51-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-471"><a href="#cb51-471" aria-hidden="true" tabindex="-1"></a>The derivation is: </span>
<span id="cb51-472"><a href="#cb51-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-473"><a href="#cb51-473" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb51-474"><a href="#cb51-474" aria-hidden="true" tabindex="-1"></a>\text{cov}(X, e) &amp; = \text{cov}(X, Y - \hat Y) <span class="sc">\\</span> </span>
<span id="cb51-475"><a href="#cb51-475" aria-hidden="true" tabindex="-1"></a>&amp; = \text{cov}(X, Y)  - \text{cov}(X, \hat Y) <span class="sc">\\</span> </span>
<span id="cb51-476"><a href="#cb51-476" aria-hidden="true" tabindex="-1"></a>&amp; = \text{cov}(X, Y)  - \text{cov}(X, a + b X) <span class="sc">\\</span> </span>
<span id="cb51-477"><a href="#cb51-477" aria-hidden="true" tabindex="-1"></a>&amp; = \text{cov}(X, Y)  - b \, \text{var}(X) <span class="sc">\\</span> </span>
<span id="cb51-478"><a href="#cb51-478" aria-hidden="true" tabindex="-1"></a>&amp; = \text{cov}(X, Y)  - \left(\frac{\text{cov}(X, Y)} {\text{var}(X)} \right) \text{var}(X) <span class="sc">\\</span> </span>
<span id="cb51-479"><a href="#cb51-479" aria-hidden="true" tabindex="-1"></a>&amp; = 0</span>
<span id="cb51-480"><a href="#cb51-480" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb51-481"><a href="#cb51-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-482"><a href="#cb51-482" aria-hidden="true" tabindex="-1"></a>The second last line uses the expression for the slope in @eq-reg-coeffs. </span>
<span id="cb51-483"><a href="#cb51-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-484"><a href="#cb51-484" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multiple correlation ($R$) {#sec-multiple-r-2}</span></span>
<span id="cb51-485"><a href="#cb51-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-486"><a href="#cb51-486" aria-hidden="true" tabindex="-1"></a>Above we defined $R^2$ as a proportion of variance. This was a bit lazy. Instead, we can start with the definition of the multiple correlation</span>
<span id="cb51-487"><a href="#cb51-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-488"><a href="#cb51-488" aria-hidden="true" tabindex="-1"></a>$$R = \text{cor}(Y, \hat Y)$$ </span>
<span id="cb51-489"><a href="#cb51-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-490"><a href="#cb51-490" aria-hidden="true" tabindex="-1"></a>and from this definition derive the result, shown above, that $R^2$ is the proportion of variance in $Y$ associated with $\hat Y$. </span>
<span id="cb51-491"><a href="#cb51-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-492"><a href="#cb51-492" aria-hidden="true" tabindex="-1"></a>Let's start by showing that </span>
<span id="cb51-493"><a href="#cb51-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-494"><a href="#cb51-494" aria-hidden="true" tabindex="-1"></a>$$\text{cov}(Y, \hat Y) = \text{var}(\hat Y)$$ {#eq-cov-y-yhat}  </span>
<span id="cb51-495"><a href="#cb51-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-496"><a href="#cb51-496" aria-hidden="true" tabindex="-1"></a>Before deriving this result, note that @eq-reg-coeffs implies</span>
<span id="cb51-497"><a href="#cb51-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-498"><a href="#cb51-498" aria-hidden="true" tabindex="-1"></a>$$\text{cov}(X, Y) = b \, \text{var}(X),$$</span>
<span id="cb51-499"><a href="#cb51-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-500"><a href="#cb51-500" aria-hidden="true" tabindex="-1"></a>and, using the the variance of a linear transformation (@sec-properties-1), we have  </span>
<span id="cb51-501"><a href="#cb51-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-502"><a href="#cb51-502" aria-hidden="true" tabindex="-1"></a>$$ \text{var} (\hat Y) = \text{var}(a + b X) = b^2 \text{var}(X). $$ </span>
<span id="cb51-503"><a href="#cb51-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-504"><a href="#cb51-504" aria-hidden="true" tabindex="-1"></a>These two results are used on the third and fourth lines of the following derivation, respectively. </span>
<span id="cb51-505"><a href="#cb51-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-506"><a href="#cb51-506" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb51-507"><a href="#cb51-507" aria-hidden="true" tabindex="-1"></a>\text{cov}(Y, \hat Y)  &amp; = \text{cov}(Y, a + b X) <span class="sc">\\</span> </span>
<span id="cb51-508"><a href="#cb51-508" aria-hidden="true" tabindex="-1"></a> &amp; = b \,\text{cov}(Y, X) <span class="sc">\\</span> </span>
<span id="cb51-509"><a href="#cb51-509" aria-hidden="true" tabindex="-1"></a> &amp; = b^2 \,\text{var}(X) <span class="sc">\\</span> </span>
<span id="cb51-510"><a href="#cb51-510" aria-hidden="true" tabindex="-1"></a> &amp; = \text{var}(\hat Y) <span class="sc">\\</span> </span>
<span id="cb51-511"><a href="#cb51-511" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb51-512"><a href="#cb51-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-513"><a href="#cb51-513" aria-hidden="true" tabindex="-1"></a>Next we show that $R^2 = \text{var}(\hat Y) / \text{var}(Y)$:</span>
<span id="cb51-514"><a href="#cb51-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-515"><a href="#cb51-515" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb51-516"><a href="#cb51-516" aria-hidden="true" tabindex="-1"></a>R^2 &amp; = <span class="co">[</span><span class="ot">\text{cor}(Y, \hat Y)</span><span class="co">]</span>^2 <span class="sc">\\</span> </span>
<span id="cb51-517"><a href="#cb51-517" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{<span class="co">[</span><span class="ot">\text{cov}(Y, \hat Y)</span><span class="co">]</span>^2}{\text{var}(Y) \; \text{var}(\hat Y)} <span class="sc">\\</span> </span>
<span id="cb51-518"><a href="#cb51-518" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{<span class="co">[</span><span class="ot">\text{var}(\hat Y)</span><span class="co">]</span>^2}{\text{var}(Y) \; \text{var}(\hat Y)} <span class="sc">\\</span> </span>
<span id="cb51-519"><a href="#cb51-519" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\text{var}(\hat Y)}{\text{var}(Y)}. <span class="sc">\\</span> </span>
<span id="cb51-520"><a href="#cb51-520" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb51-521"><a href="#cb51-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-522"><a href="#cb51-522" aria-hidden="true" tabindex="-1"></a>This derivation is nicer than the one in @sec-rsquared-2 because it obtains a result about $R^2$ using the definition of $R$. However, this derivation does not show that the resulting ratio is a proportion, which requires a second step (which also uses @eq-cov-y-yhat): </span>
<span id="cb51-523"><a href="#cb51-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-524"><a href="#cb51-524" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb51-525"><a href="#cb51-525" aria-hidden="true" tabindex="-1"></a>\text{var}(e) &amp; = \text{var}(Y - \hat Y) <span class="sc">\\</span> </span>
<span id="cb51-526"><a href="#cb51-526" aria-hidden="true" tabindex="-1"></a>&amp; = \text{var}(Y) + \text{var}(\hat Y) - 2 \text{cov}(Y, \hat Y) <span class="sc">\\</span> </span>
<span id="cb51-527"><a href="#cb51-527" aria-hidden="true" tabindex="-1"></a>&amp; = \text{var}(Y) + \text{var}(\hat Y) - 2 \text{var}(\hat Y) <span class="sc">\\</span> </span>
<span id="cb51-528"><a href="#cb51-528" aria-hidden="true" tabindex="-1"></a>&amp; = \text{var}(Y) - \text{var}(\hat Y). </span>
<span id="cb51-529"><a href="#cb51-529" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb51-530"><a href="#cb51-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-531"><a href="#cb51-531" aria-hidden="true" tabindex="-1"></a>Re-arranging gives</span>
<span id="cb51-532"><a href="#cb51-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-533"><a href="#cb51-533" aria-hidden="true" tabindex="-1"></a>$$ \text{var}(Y) =  \text{var}(\hat Y) + \text{var}(e),$$ </span>
<span id="cb51-534"><a href="#cb51-534" aria-hidden="true" tabindex="-1"></a>which shows that $0 \leq \text{var}(\hat Y) \leq  \text{var}(Y).$</span>
<span id="cb51-535"><a href="#cb51-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-536"><a href="#cb51-536" aria-hidden="true" tabindex="-1"></a>One last detail concerns the relation between the multiple correlation $R$ and the regular correlation coefficient $r = \text{cor}(Y, X)$. Using the invariance of the correlation under linear transformation (@sec-properties-1), we have</span>
<span id="cb51-537"><a href="#cb51-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-538"><a href="#cb51-538" aria-hidden="true" tabindex="-1"></a>$$ R = \text{cor}(Y, \widehat Y) = \text{cor}(Y, a + bX) = \text{cor}(Y, X) = r$$</span>
<span id="cb51-539"><a href="#cb51-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-540"><a href="#cb51-540" aria-hidden="true" tabindex="-1"></a>Consequently, in simple regression, $R^2 = r^2$ -- i.e., the proportion of variance explained by the predictor is just the squared Pearson product-moment correlation. When we add multiple predictors, this relationship between $R^2$ and $r^2$ no longer holds. </span>
<span id="cb51-541"><a href="#cb51-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-542"><a href="#cb51-542" aria-hidden="true" tabindex="-1"></a><span class="fu">## Workbook {#workbook-2}</span></span>
<span id="cb51-543"><a href="#cb51-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-544"><a href="#cb51-544" aria-hidden="true" tabindex="-1"></a>This section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in @sec-exercises-2. The lesson will **not** be a lecture that reviews all of the material in the chapter! So, if you haven't written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc. </span>
<span id="cb51-545"><a href="#cb51-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-546"><a href="#cb51-546" aria-hidden="true" tabindex="-1"></a>@sec-example-2</span>
<span id="cb51-547"><a href="#cb51-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-548"><a href="#cb51-548" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.cap = 'Math Achievement and SES (NELS88).', fig.align = 'center'}</span></span>
<span id="cb51-549"><a href="#cb51-549" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter plot</span></span>
<span id="cb51-550"><a href="#cb51-550" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ses, </span>
<span id="cb51-551"><a href="#cb51-551" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> achmat08, </span>
<span id="cb51-552"><a href="#cb51-552" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, </span>
<span id="cb51-553"><a href="#cb51-553" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Math Achievement (Grade 8)"</span>, </span>
<span id="cb51-554"><a href="#cb51-554" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"SES"</span>)</span>
<span id="cb51-555"><a href="#cb51-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-556"><a href="#cb51-556" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the regression model</span></span>
<span id="cb51-557"><a href="#cb51-557" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(achmat08 <span class="sc">~</span> ses)</span>
<span id="cb51-558"><a href="#cb51-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-559"><a href="#cb51-559" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the regression line to the plot</span></span>
<span id="cb51-560"><a href="#cb51-560" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod) </span>
<span id="cb51-561"><a href="#cb51-561" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-562"><a href="#cb51-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-563"><a href="#cb51-563" aria-hidden="true" tabindex="-1"></a>The strength and direction of the linear relationship between the two variables is summarized by their correlation. In this sample, the value of the correlation is:</span>
<span id="cb51-564"><a href="#cb51-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-567"><a href="#cb51-567" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-568"><a href="#cb51-568" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(achmat08, ses)</span>
<span id="cb51-569"><a href="#cb51-569" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-570"><a href="#cb51-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-571"><a href="#cb51-571" aria-hidden="true" tabindex="-1"></a>This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in Math (higher Math Achievement). This relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., <span class="ot">&lt;https://www.apa.org/pi/ses/resources/publications/education&gt;</span>). Please look over this web page and be prepared to share your thoughts about this relationship.</span>
<span id="cb51-572"><a href="#cb51-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-573"><a href="#cb51-573" aria-hidden="true" tabindex="-1"></a>@sec-ols-2</span>
<span id="cb51-574"><a href="#cb51-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-575"><a href="#cb51-575" aria-hidden="true" tabindex="-1"></a>For the NELS example, the regression intercept and slope are, respectively:</span>
<span id="cb51-576"><a href="#cb51-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-579"><a href="#cb51-579" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-580"><a href="#cb51-580" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod)</span>
<span id="cb51-581"><a href="#cb51-581" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-582"><a href="#cb51-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-583"><a href="#cb51-583" aria-hidden="true" tabindex="-1"></a>Please write down an interpretation of these numbers and be prepared to share your answers in class. How would your interpretation change if, rather than the value of the slope shown above, we had $b = 0$?  </span>
<span id="cb51-584"><a href="#cb51-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-585"><a href="#cb51-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-586"><a href="#cb51-586" aria-hidden="true" tabindex="-1"></a>@sec-rsquared-2</span>
<span id="cb51-587"><a href="#cb51-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-588"><a href="#cb51-588" aria-hidden="true" tabindex="-1"></a>For the NELS example, the R-squared statistic is:</span>
<span id="cb51-589"><a href="#cb51-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-592"><a href="#cb51-592" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-593"><a href="#cb51-593" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared</span>
<span id="cb51-594"><a href="#cb51-594" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-595"><a href="#cb51-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-596"><a href="#cb51-596" aria-hidden="true" tabindex="-1"></a>Please write down an interpretation of this number and be prepared to share your answer in class. Hint: Instead of talking about proportions, it is often helpful to multiply by 100 and talk about percentages instead. </span>
<span id="cb51-597"><a href="#cb51-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-598"><a href="#cb51-598" aria-hidden="true" tabindex="-1"></a>@sec-notation-2</span>
<span id="cb51-599"><a href="#cb51-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-600"><a href="#cb51-600" aria-hidden="true" tabindex="-1"></a>Please be prepared for a pop quiz on notation during class!</span>
<span id="cb51-601"><a href="#cb51-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-602"><a href="#cb51-602" aria-hidden="true" tabindex="-1"></a>| Concept            | Sample statistic | Population parameter |</span>
<span id="cb51-603"><a href="#cb51-603" aria-hidden="true" tabindex="-1"></a>|--------------------|:----------------:|:--------------------:|</span>
<span id="cb51-604"><a href="#cb51-604" aria-hidden="true" tabindex="-1"></a>| regression line    |                  |                      |</span>
<span id="cb51-605"><a href="#cb51-605" aria-hidden="true" tabindex="-1"></a>| slope              |                  |                      |</span>
<span id="cb51-606"><a href="#cb51-606" aria-hidden="true" tabindex="-1"></a>| intercept          |                  |                      |</span>
<span id="cb51-607"><a href="#cb51-607" aria-hidden="true" tabindex="-1"></a>| residual           |                  |                      |</span>
<span id="cb51-608"><a href="#cb51-608" aria-hidden="true" tabindex="-1"></a>| variance explained |                  |                      |</span>
<span id="cb51-609"><a href="#cb51-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-610"><a href="#cb51-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-611"><a href="#cb51-611" aria-hidden="true" tabindex="-1"></a>@sec-inference-2</span>
<span id="cb51-612"><a href="#cb51-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-613"><a href="#cb51-613" aria-hidden="true" tabindex="-1"></a>For the NELS example, the standard errors, t-test, and p-values of the regression coefficients are shown in the table below (along with the OLS estimates). The F-test appears in the text below the table. Note that the output uses the terminology "multiple R-squared" to refer to R-squared.</span>
<span id="cb51-614"><a href="#cb51-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-617"><a href="#cb51-617" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-618"><a href="#cb51-618" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span>
<span id="cb51-619"><a href="#cb51-619" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-620"><a href="#cb51-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-621"><a href="#cb51-621" aria-hidden="true" tabindex="-1"></a>The $95\%$ confidence intervals for the regression coefficients are: </span>
<span id="cb51-622"><a href="#cb51-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-625"><a href="#cb51-625" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-626"><a href="#cb51-626" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(mod)</span>
<span id="cb51-627"><a href="#cb51-627" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-628"><a href="#cb51-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-629"><a href="#cb51-629" aria-hidden="true" tabindex="-1"></a>Please write down your interpretation of the t-tests, confidence intervals, and F-test, and be prepared to share your answers in class. Hint: state whether the tests are statistically significant and the corresponding conclusions are about the population parameters. For confidence intervals, report the range of values we are "confident" about. For practice, you might want to try using APA notation in your answer (or whatever style conventions are used in your field). </span>
<span id="cb51-630"><a href="#cb51-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-631"><a href="#cb51-631" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises {#sec-exercises-2}</span></span>
<span id="cb51-632"><a href="#cb51-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-633"><a href="#cb51-633" aria-hidden="true" tabindex="-1"></a>These exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don't need to work on it before class (but you can if you want.) </span>
<span id="cb51-634"><a href="#cb51-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-635"><a href="#cb51-635" aria-hidden="true" tabindex="-1"></a>Before staring this section, you may find it useful to scroll to the top of the page, click on the "&lt;/&gt; Code" menu, and select "Show All Code."</span>
<span id="cb51-636"><a href="#cb51-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-637"><a href="#cb51-637" aria-hidden="true" tabindex="-1"></a><span class="fu">### The `lm` function</span></span>
<span id="cb51-638"><a href="#cb51-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-639"><a href="#cb51-639" aria-hidden="true" tabindex="-1"></a>The function<span class="in">`lm`</span>, short for "linear model", is used to estimate linear regressions using OLS. It also provides a lot of useful output.</span>
<span id="cb51-640"><a href="#cb51-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-641"><a href="#cb51-641" aria-hidden="true" tabindex="-1"></a>The main argument that the we  provides to the <span class="in">`lm`</span> function is a formula. For the simple regression of Y on X, a formula has the syntax:</span>
<span id="cb51-642"><a href="#cb51-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-643"><a href="#cb51-643" aria-hidden="true" tabindex="-1"></a><span class="in">`Y ~ X`</span></span>
<span id="cb51-644"><a href="#cb51-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-645"><a href="#cb51-645" aria-hidden="true" tabindex="-1"></a>Here <span class="in">`Y`</span> denotes the outcome variable and <span class="in">`X`</span> is the predictor variable. The tilde <span class="in">`~`</span> just means "equals", but the equals sign <span class="in">`=`</span> is already used to for other stuff in R, so <span class="in">`~`</span> is used instead. We will see more complicated formulas as we go through the course. For more information on R's formula syntax, see <span class="in">`help(formula)`</span>.</span>
<span id="cb51-646"><a href="#cb51-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-647"><a href="#cb51-647" aria-hidden="true" tabindex="-1"></a>Let's take a closer look using the following two variables from the NELS data.</span>
<span id="cb51-648"><a href="#cb51-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-649"><a href="#cb51-649" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`achmat08`</span>: eighth grade math achievement (percent correct on a math test)</span>
<span id="cb51-650"><a href="#cb51-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-651"><a href="#cb51-651" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`ses`</span>: a composite measure of socio-economic status, on a scale from 0-35</span>
<span id="cb51-652"><a href="#cb51-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-655"><a href="#cb51-655" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-656"><a href="#cb51-656" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data. Note that you can click on the .RData file and RStudio will load it</span></span>
<span id="cb51-657"><a href="#cb51-657" aria-hidden="true" tabindex="-1"></a><span class="co"># load("NELS.RData") #Un-comment this line to run</span></span>
<span id="cb51-658"><a href="#cb51-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-659"><a href="#cb51-659" aria-hidden="true" tabindex="-1"></a><span class="co"># Attach the data: will discuss this in class</span></span>
<span id="cb51-660"><a href="#cb51-660" aria-hidden="true" tabindex="-1"></a><span class="co"># attach(NELS) #Un-comment this line to run!</span></span>
<span id="cb51-661"><a href="#cb51-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-662"><a href="#cb51-662" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter plot of math achievement against SES</span></span>
<span id="cb51-663"><a href="#cb51-663" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> ses, <span class="at">y =</span> achmat08, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb51-664"><a href="#cb51-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-665"><a href="#cb51-665" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress math achievement on SES; save output as "mod"</span></span>
<span id="cb51-666"><a href="#cb51-666" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(achmat08 <span class="sc">~</span> ses)</span>
<span id="cb51-667"><a href="#cb51-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-668"><a href="#cb51-668" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the regression line to the plot</span></span>
<span id="cb51-669"><a href="#cb51-669" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod)</span>
<span id="cb51-670"><a href="#cb51-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-671"><a href="#cb51-671" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the regression coefficients</span></span>
<span id="cb51-672"><a href="#cb51-672" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod)</span>
<span id="cb51-673"><a href="#cb51-673" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-674"><a href="#cb51-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-675"><a href="#cb51-675" aria-hidden="true" tabindex="-1"></a>Let's do some quick calculations to check that the <span class="in">`lm`</span> output corresponds the formulas for the slope and intercept in @sec-ols-2:</span>
<span id="cb51-676"><a href="#cb51-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-677"><a href="#cb51-677" aria-hidden="true" tabindex="-1"></a>$$ a = \bar Y - b \bar X \quad \text{and} \quad b = \frac{\text{cov}(X, Y)}{\text{var}(X)}. $$ </span>
<span id="cb51-678"><a href="#cb51-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-679"><a href="#cb51-679" aria-hidden="true" tabindex="-1"></a>We won't usually do this kind of "manual" calculation, but it is a good way consolidate knowledge presented in the readings with the output presented by R. It is also useful to refresh our memory about some useful R functions and how the R language works.</span>
<span id="cb51-680"><a href="#cb51-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-683"><a href="#cb51-683" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-684"><a href="#cb51-684" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the slope as the covariance divided by the variance of X</span></span>
<span id="cb51-685"><a href="#cb51-685" aria-hidden="true" tabindex="-1"></a>cov_xy <span class="ot">&lt;-</span> <span class="fu">cov</span>(achmat08, ses)</span>
<span id="cb51-686"><a href="#cb51-686" aria-hidden="true" tabindex="-1"></a>var_x <span class="ot">&lt;-</span> <span class="fu">var</span>(ses)</span>
<span id="cb51-687"><a href="#cb51-687" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> cov_xy <span class="sc">/</span> var_x</span>
<span id="cb51-688"><a href="#cb51-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-689"><a href="#cb51-689" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the "manual" calculation to the output from lm. </span></span>
<span id="cb51-690"><a href="#cb51-690" aria-hidden="true" tabindex="-1"></a>b</span>
<span id="cb51-691"><a href="#cb51-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-692"><a href="#cb51-692" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the y-intercept using from the two means and the slope</span></span>
<span id="cb51-693"><a href="#cb51-693" aria-hidden="true" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(ses)</span>
<span id="cb51-694"><a href="#cb51-694" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">&lt;-</span> <span class="fu">mean</span>(achmat08)</span>
<span id="cb51-695"><a href="#cb51-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-696"><a href="#cb51-696" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> ybar <span class="sc">-</span> b <span class="sc">*</span> xbar</span>
<span id="cb51-697"><a href="#cb51-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-698"><a href="#cb51-698" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the "manual" calculation to the output from lm. </span></span>
<span id="cb51-699"><a href="#cb51-699" aria-hidden="true" tabindex="-1"></a>a</span>
<span id="cb51-700"><a href="#cb51-700" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-701"><a href="#cb51-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-702"><a href="#cb51-702" aria-hidden="true" tabindex="-1"></a>Let's also check our interpretation of the parameters. If the answers to these questions are not clear, please make sure to ask in class!</span>
<span id="cb51-703"><a href="#cb51-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-704"><a href="#cb51-704" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>What is the predicted value of <span class="in">`achmat08`</span> when <span class="in">`ses`</span> is equal to zero?</span>
<span id="cb51-705"><a href="#cb51-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-706"><a href="#cb51-706" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>How much does the predicted value of <span class="in">`achmat08`</span> increase for each unit of increase in <span class="in">`ses`</span>?</span>
<span id="cb51-707"><a href="#cb51-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-708"><a href="#cb51-708" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variance explained</span></span>
<span id="cb51-709"><a href="#cb51-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-710"><a href="#cb51-710" aria-hidden="true" tabindex="-1"></a>Another way to describe the relationship between the two variables is by considering the amount of variation in $Y$ that is associated with (or explained by) its relationship with $X$. Recall that one way to do this is via the "variance" decomposition</span>
<span id="cb51-711"><a href="#cb51-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-712"><a href="#cb51-712" aria-hidden="true" tabindex="-1"></a>$$ SS_{\text{total}} = SS_{\text{res}} + SS_{\text{reg}}$$</span>
<span id="cb51-713"><a href="#cb51-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-714"><a href="#cb51-714" aria-hidden="true" tabindex="-1"></a>from which we can compute the proportion of variation in Y that is associated with the regression model:</span>
<span id="cb51-715"><a href="#cb51-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-716"><a href="#cb51-716" aria-hidden="true" tabindex="-1"></a>$$R^2 = \frac{SS_{\text{reg}}}{SS_{\text{total}}}.$$</span>
<span id="cb51-717"><a href="#cb51-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-718"><a href="#cb51-718" aria-hidden="true" tabindex="-1"></a>The R-squared for the example is presented in the output below. You should be able to provide an interpretation of this number, so if it's not clear make sure to ask in class!</span>
<span id="cb51-719"><a href="#cb51-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-722"><a href="#cb51-722" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-723"><a href="#cb51-723" aria-hidden="true" tabindex="-1"></a><span class="co"># R-squared from the example</span></span>
<span id="cb51-724"><a href="#cb51-724" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared</span>
<span id="cb51-725"><a href="#cb51-725" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-726"><a href="#cb51-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-727"><a href="#cb51-727" aria-hidden="true" tabindex="-1"></a>As above, let's compute $R^2$ "by hand" for our example.</span>
<span id="cb51-728"><a href="#cb51-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-731"><a href="#cb51-731" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-732"><a href="#cb51-732" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the sums of squares</span></span>
<span id="cb51-733"><a href="#cb51-733" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">&lt;-</span> <span class="fu">mean</span>(achmat08)</span>
<span id="cb51-734"><a href="#cb51-734" aria-hidden="true" tabindex="-1"></a>ss_total <span class="ot">&lt;-</span> <span class="fu">sum</span>((achmat08 <span class="sc">-</span> ybar)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb51-735"><a href="#cb51-735" aria-hidden="true" tabindex="-1"></a>ss_reg <span class="ot">&lt;-</span> <span class="fu">sum</span>((yhat <span class="sc">-</span> ybar)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb51-736"><a href="#cb51-736" aria-hidden="true" tabindex="-1"></a>ss_res <span class="ot">&lt;-</span>  <span class="fu">sum</span>((achmat08 <span class="sc">-</span> yhat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb51-737"><a href="#cb51-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-738"><a href="#cb51-738" aria-hidden="true" tabindex="-1"></a><span class="co"># Check that SS_total = SS_reg + SS_res</span></span>
<span id="cb51-739"><a href="#cb51-739" aria-hidden="true" tabindex="-1"></a>ss_total</span>
<span id="cb51-740"><a href="#cb51-740" aria-hidden="true" tabindex="-1"></a>ss_reg <span class="sc">+</span> ss_res</span>
<span id="cb51-741"><a href="#cb51-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-742"><a href="#cb51-742" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute R-squared (compare to value from lm)</span></span>
<span id="cb51-743"><a href="#cb51-743" aria-hidden="true" tabindex="-1"></a>ss_reg<span class="sc">/</span>ss_total</span>
<span id="cb51-744"><a href="#cb51-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-745"><a href="#cb51-745" aria-hidden="true" tabindex="-1"></a><span class="co"># Also check that R-squared is really equal to the square of the PPMC</span></span>
<span id="cb51-746"><a href="#cb51-746" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(achmat08, ses)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb51-747"><a href="#cb51-747" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-748"><a href="#cb51-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-749"><a href="#cb51-749" aria-hidden="true" tabindex="-1"></a><span class="fu">### Predicted values and residuals</span></span>
<span id="cb51-750"><a href="#cb51-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-751"><a href="#cb51-751" aria-hidden="true" tabindex="-1"></a>The <span class="in">`lm`</span> function returns the predicted values $\widehat{Y_i}$ and residuals $e_i$ and which we can access using the <span class="in">`$`</span> operator. These are useful for various reasons, especially model diagnostics, which we discuss later in the course. For now, lets just take a look at the residual vs fitted plot to illustrate the code.</span>
<span id="cb51-752"><a href="#cb51-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-755"><a href="#cb51-755" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-756"><a href="#cb51-756" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> mod<span class="sc">$</span>fitted.values</span>
<span id="cb51-757"><a href="#cb51-757" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> mod<span class="sc">$</span>resid</span>
<span id="cb51-758"><a href="#cb51-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-759"><a href="#cb51-759" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(yhat, res, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb51-760"><a href="#cb51-760" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-761"><a href="#cb51-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-762"><a href="#cb51-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-763"><a href="#cb51-763" aria-hidden="true" tabindex="-1"></a>Also note that the residuals values have mean zero and are uncorrelated with the predictor -- this is always the case in OLS (See @sec-properties-2})</span>
<span id="cb51-764"><a href="#cb51-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-765"><a href="#cb51-765" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-766"><a href="#cb51-766" aria-hidden="true" tabindex="-1"></a><span class="in">mean(res)</span></span>
<span id="cb51-767"><a href="#cb51-767" aria-hidden="true" tabindex="-1"></a><span class="in">cor(yhat, res)</span></span>
<span id="cb51-768"><a href="#cb51-768" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-769"><a href="#cb51-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-770"><a href="#cb51-770" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference</span></span>
<span id="cb51-771"><a href="#cb51-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-772"><a href="#cb51-772" aria-hidden="true" tabindex="-1"></a>Next let's address statistical inference, or how we can make conclusions about a population based on a sample from that population.</span>
<span id="cb51-773"><a href="#cb51-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-774"><a href="#cb51-774" aria-hidden="true" tabindex="-1"></a>We can use the <span class="in">`summary`</span> function to test the coefficients in our model.</span>
<span id="cb51-775"><a href="#cb51-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-778"><a href="#cb51-778" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-779"><a href="#cb51-779" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span>
<span id="cb51-780"><a href="#cb51-780" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-781"><a href="#cb51-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-782"><a href="#cb51-782" aria-hidden="true" tabindex="-1"></a>In the table, the t-test and p-values are for the null hypothesis that the corresponding coefficient is zero in the population. We can see that the intercept and slope are both significantly different from zero at the .05 level. However, the test of the intercept is not very meaningful (why?).</span>
<span id="cb51-783"><a href="#cb51-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-784"><a href="#cb51-784" aria-hidden="true" tabindex="-1"></a>The text below the table summarizes the output for R-squared, including its F-test, it's degrees of freedom, and the p-value. (We will talk about adjusted R-square in @sec-chap-4)</span>
<span id="cb51-785"><a href="#cb51-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-786"><a href="#cb51-786" aria-hidden="true" tabindex="-1"></a>We can also use the <span class="in">`confint`</span> function to obtain confidence intervals for the regression coefficients. Use <span class="in">`help`</span> to find out more about the <span class="in">`confint`</span> function.</span>
<span id="cb51-787"><a href="#cb51-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-790"><a href="#cb51-790" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb51-791"><a href="#cb51-791" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(mod)</span>
<span id="cb51-792"><a href="#cb51-792" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb51-793"><a href="#cb51-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-794"><a href="#cb51-794" aria-hidden="true" tabindex="-1"></a>Be sure to remember the correct interpretation of confidence intervals: *there is a 95% chance that the interval includes the true parameter value* (not: there is a 95% chance that the parameter falls in the interval). For example, there is a 95% chance that the interval <span class="co">[</span><span class="ot">.32, .54</span><span class="co">]</span> includes the true regression coefficient for SES.</span>
<span id="cb51-795"><a href="#cb51-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-796"><a href="#cb51-796" aria-hidden="true" tabindex="-1"></a><span class="fu">### Writing up results</span></span>
<span id="cb51-797"><a href="#cb51-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-798"><a href="#cb51-798" aria-hidden="true" tabindex="-1"></a>We could write up the results from this analysis in APA format as follows. You should practice doing this kind of thing, because it is important to be able to write up the results of your analyses in a way that people in your area of research will understand. </span>
<span id="cb51-799"><a href="#cb51-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-800"><a href="#cb51-800" aria-hidden="true" tabindex="-1"></a>In this analysis, we considered the relationship between Math Achievement in Grade 8 (percent correct on a math test) and SES (a composite on a scale from $0-35$). Regressing Math Achievement on SES, the relationship was positive and statistically significant at the $.05$ level ($b = 0.43$, $t(498) = 7.49$, $p &lt; .001$, $95\% \text{ CI: } <span class="co">[</span><span class="ot">0.32, 0.54</span><span class="co">]</span>$). SES explained about $10\%$ of the variation in Math Achievement ($R^2 = .10$, $F(1, 498) = 56.12$, $p &lt; .001$). </span>
<span id="cb51-801"><a href="#cb51-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-802"><a href="#cb51-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-803"><a href="#cb51-803" aria-hidden="true" tabindex="-1"></a><span class="fu">### Additional exercises</span></span>
<span id="cb51-804"><a href="#cb51-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-805"><a href="#cb51-805" aria-hidden="true" tabindex="-1"></a>If time permits, we will address these additional exercises in class.</span>
<span id="cb51-806"><a href="#cb51-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-807"><a href="#cb51-807" aria-hidden="true" tabindex="-1"></a>These exercises replace <span class="in">`achmat08`</span> with</span>
<span id="cb51-808"><a href="#cb51-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-809"><a href="#cb51-809" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`achrdg08`</span>: eighth grade Reading Achievement (percent correct on a reading test)</span>
<span id="cb51-810"><a href="#cb51-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-811"><a href="#cb51-811" aria-hidden="true" tabindex="-1"></a>Please answer the following questions using R.</span>
<span id="cb51-812"><a href="#cb51-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-813"><a href="#cb51-813" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Plot <span class="in">`achrdg08`</span> against <span class="in">`ses`</span>. </span>
<span id="cb51-814"><a href="#cb51-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-815"><a href="#cb51-815" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>What is the correlation between <span class="in">`achrdg08`</span> and <span class="in">`ses`</span>? How does it compare to the correlation with Math and SES?</span>
<span id="cb51-816"><a href="#cb51-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-817"><a href="#cb51-817" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>How much variation in Reading is explained by SES?  Is the proportion of variance explained significant at the .05 level?</span>
<span id="cb51-818"><a href="#cb51-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-819"><a href="#cb51-819" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>How much do predicted Reading scores increase for a one unit of increase in SES? Is this a statistically significant at the .05 level?</span>
<span id="cb51-820"><a href="#cb51-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-821"><a href="#cb51-821" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>What are your overall conclusions about the relationship between Academic Achievement and SES in the NELS data? Write up your results using APA formatting or whatever conventions are used in your area of research. </span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>