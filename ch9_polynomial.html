<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>EDUC 784 - 9&nbsp; Polynomial regression, etc</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ch10_logistic.html" rel="next">
<link href="./ch8_loglinear.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch9_polynomial.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Polynomial regression, etc</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">EDUC 784</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch1_review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2_simple_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Simple regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3_two_predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Two predictors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4_categorical_predictors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Categorical predictors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5_interactions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Interactions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch6_model_building.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Model building</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch7_assumption_checking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Assumption checking</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch8_loglinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Log-linear regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch9_polynomial.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Polynomial regression, etc</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch10_logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Logistic regression</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-polynomial-9" id="toc-sec-polynomial-9" class="nav-link active" data-scroll-target="#sec-polynomial-9"><span class="header-section-number">9.1</span> Polynomial regression</a>
  <ul class="collapse">
  <li><a href="#recap-of-polynomials" id="toc-recap-of-polynomials" class="nav-link" data-scroll-target="#recap-of-polynomials"><span class="header-section-number">9.1.1</span> Recap of polynomials</a></li>
  <li><a href="#sec-overfitting-9" id="toc-sec-overfitting-9" class="nav-link" data-scroll-target="#sec-overfitting-9"><span class="header-section-number">9.1.2</span> “Over-fitting” the data</a></li>
  <li><a href="#sec-interpreting-the-model-9" id="toc-sec-interpreting-the-model-9" class="nav-link" data-scroll-target="#sec-interpreting-the-model-9"><span class="header-section-number">9.1.3</span> Interpreting the model</a></li>
  <li><a href="#sec-deriviation-9" id="toc-sec-deriviation-9" class="nav-link" data-scroll-target="#sec-deriviation-9"><span class="header-section-number">9.1.4</span> Analysing polynomials*</a></li>
  <li><a href="#model-building-vs-curve-fitting" id="toc-model-building-vs-curve-fitting" class="nav-link" data-scroll-target="#model-building-vs-curve-fitting"><span class="header-section-number">9.1.5</span> Model building vs curve fitting</a></li>
  </ul></li>
  <li><a href="#sec-worked-example-9" id="toc-sec-worked-example-9" class="nav-link" data-scroll-target="#sec-worked-example-9"><span class="header-section-number">9.2</span> Worked Example</a>
  <ul class="collapse">
  <li><a href="#orthogonal-polynomails" id="toc-orthogonal-polynomails" class="nav-link" data-scroll-target="#orthogonal-polynomails"><span class="header-section-number">9.2.1</span> Orthogonal polynomails*</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">9.2.2</span> Summary</a></li>
  </ul></li>
  <li><a href="#sec-piecewise-9" id="toc-sec-piecewise-9" class="nav-link" data-scroll-target="#sec-piecewise-9"><span class="header-section-number">9.3</span> Piecewise regression</a>
  <ul class="collapse">
  <li><a href="#the-piecewise-model" id="toc-the-piecewise-model" class="nav-link" data-scroll-target="#the-piecewise-model"><span class="header-section-number">9.3.1</span> The piecewise model</a></li>
  <li><a href="#back-to-the-example" id="toc-back-to-the-example" class="nav-link" data-scroll-target="#back-to-the-example"><span class="header-section-number">9.3.2</span> Back to the example</a></li>
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1"><span class="header-section-number">9.3.3</span> Summary</a></li>
  </ul></li>
  <li><a href="#sec-workbook-9" id="toc-sec-workbook-9" class="nav-link" data-scroll-target="#sec-workbook-9"><span class="header-section-number">9.4</span> Workbook</a></li>
  <li><a href="#sec-exercises-9" id="toc-sec-exercises-9" class="nav-link" data-scroll-target="#sec-exercises-9"><span class="header-section-number">9.5</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression"><span class="header-section-number">9.5.1</span> Polynomial regression</a></li>
  <li><a href="#write-up" id="toc-write-up" class="nav-link" data-scroll-target="#write-up"><span class="header-section-number">9.5.2</span> Write up</a></li>
  <li><a href="#piecewise-regression" id="toc-piecewise-regression" class="nav-link" data-scroll-target="#piecewise-regression"><span class="header-section-number">9.5.3</span> Piecewise regression</a></li>
  <li><a href="#write-up-1" id="toc-write-up-1" class="nav-link" data-scroll-target="#write-up-1"><span class="header-section-number">9.5.4</span> Write up</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-chap-9" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Polynomial regression, etc</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This chapter will cover two widely-used techniques for addressing violations of the assumption of linearity:</p>
<ul>
<li>Polynomial regression, which means raising <span class="math inline">\(X\)</span>-variables to a power (e.g., <span class="math inline">\(X^2\)</span> and <span class="math inline">\(X^3\)</span>), and</li>
<li>Piecewise or segmented regression, which involves using different regression lines over different ranges of a predictor. These techniques involve transforming the <span class="math inline">\(X\)</span>-variable(s), which can be done in addition to (or instead of) transforming the <span class="math inline">\(Y\)</span> variable (see <a href="ch8_loglinear.html"><span>Chapter&nbsp;8</span></a>).</li>
</ul>
<p>Both polynomial and piecewise regression are useful in practice and lead to advanced topics like splines and semi-parametric regression. They also turn out to be special cases of interactions, so we have already covered a lot of the technical details in <a href="ch5_interactions.html"><span>Chapter&nbsp;5</span></a> – phew!</p>
<section id="sec-polynomial-9" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="sec-polynomial-9"><span class="header-section-number">9.1</span> Polynomial regression</h2>
<p>Polynomial regression means that we regress <span class="math inline">\(Y\)</span> on a polynomial function of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[ \widehat Y = b_0 + b_1 X + b_2 X^2 + b_3 X^3 + ....\]</span></p>
<p>Your first thought might be, “doesn’t this contradict the assumption that regression is linear?” The answer here is a bit subtle.</p>
<p>As with regular linear regression, the polynomial model is linear in the coefficients – we don’t raise the regression coefficients to a power (e.g., <span class="math inline">\(b_1^2\)</span>), or multiply coefficients together (e.g, <span class="math inline">\(b_1 \times b_2\)</span>). This is the technical sense in which polynomial regression is still just linear regression, despite its name.</p>
<p>Polynomial regression does use nonlinear functions of the predictor(s), but the model is agnostic to what you do with your data. The situation here is a lot like when we worked with interactions in <a href="ch5_interactions.html"><span>Chapter&nbsp;5</span></a>. In order to model interactions, we computed the product of two predictors and entered the product into the model as a third predictor. Well, <span class="math inline">\(X^2\)</span> is the product of a predictor with itself, so, in this sense, the quadratic term in a polynomial regression is just a special case of an interaction between two variables.</p>
<p>Although we did not cover interactions among more than two variables in this course, they are computed in the same way – e.g., a “three-way” interaction is just the product of 3 predictors. Similarly, <span class="math inline">\(X^3\)</span> is just the three-fold product of a variable with itself.</p>
<p>While polynomial regression is formally similar to interactions, it is used for a different purpose. Interactions address how the relationship between two variables changes as a function of a third. Their inclusion in a model is usually motivated by a specific research question that is formulated before doing the data analysis (see <a href="ch6_model_building.html"><span>Chapter&nbsp;6</span></a>).</p>
<p>By contrast, polynomial regression is used to address a non-linear relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, and is usually motivated by a preliminary examination of data that indicates the presence of such a relationship (e.g., a scatter plot of <span class="math inline">\(Y\)</span> versus <span class="math inline">\(X\)</span>; a residual versus fitted plot). While it is possible to formulate research questions about polynomial terms in a regression model, this is not necessarily or even usually the case when polynomial regression is used – often its just used to address violations of the linearity assumption.</p>
<section id="recap-of-polynomials" class="level3" data-number="9.1.1">
<h3 data-number="9.1.1" class="anchored" data-anchor-id="recap-of-polynomials"><span class="header-section-number">9.1.1</span> Recap of polynomials</h3>
<p>In general, a polynomial of degree <span class="math inline">\(n\)</span> (i.e., highest power of <span class="math inline">\(n\)</span>) produces a curve that can have up to <span class="math inline">\(n-1\)</span> bends (minima and maxima). Some examples are illustrated in Figure @ref(fig:poly) below.</p>
<ul>
<li>The (orange) linear function of <span class="math inline">\(X\)</span> is a polynomial of degree 1 and has zero bends.</li>
<li>The (green) quadratic function of <span class="math inline">\(X\)</span> is a polynomial of degree 2 and has 1 bend (a minimum at <span class="math inline">\(X = 0\)</span>; this is also called a parabola).</li>
<li>etc.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="files/images/poly.png" class="img-fluid figure-img" width="479"></p>
<figcaption class="figure-caption">Examples of Polynomials</figcaption>
</figure>
</div>
</div>
</div>
<p>As we can see, this is a very flexible approach to capturing non-linear relationships between two variables. In fact, it can be too flexible! This is the topic of the next section.</p>
</section>
<section id="sec-overfitting-9" class="level3" data-number="9.1.2">
<h3 data-number="9.1.2" class="anchored" data-anchor-id="sec-overfitting-9"><span class="header-section-number">9.1.2</span> “Over-fitting” the data</h3>
<p><a href="#fig-overfit">Figure&nbsp;<span>9.1</span></a> shows three different regression models fitted to the same (simulated) bivariate data.</p>
<ul>
<li><p>In the left panel, a standard linear regression model is used, and we can see that the model does not capture the nonlinear (quadratic) trend in the data.</p></li>
<li><p>The middle panel uses a quadratic model (i.e., includes <span class="math inline">\(X^2\)</span> as a predictor, as well as <span class="math inline">\(X\)</span>), and fits the data quite well.</p></li>
<li><p>The right panel uses a 16-degree polynomial to fit the data. We can see that is has a higher R-squared than the quadratic model. But there is also something fishy about this model, don’t you agree?</p></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-overfit" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch9_polynomial_files/figure-html/fig-overfit-1.png" class="img-fluid figure-img" width="1152"></p>
<figcaption class="figure-caption">Figure&nbsp;9.1: Polynomial Regression Examples</figcaption>
</figure>
</div>
</div>
</div>
<p>To help compare these three models, let’s simulate a second sample from the same population. In the plots below, the regression lines from <a href="#fig-overfit">Figure&nbsp;<span>9.1</span></a> were added to the plots from a second sample. Note that the regression parameters were not re-estimated using the second data set. The model parameters from the first data set were used to produce the regression lines for the second data set, so the regression lines are the same as in <a href="#fig-overfit">Figure&nbsp;<span>9.1</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-overfit2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch9_polynomial_files/figure-html/fig-overfit2-1.png" class="img-fluid figure-img" width="1152"></p>
<figcaption class="figure-caption">Figure&nbsp;9.2: Polynomial Regression Examples (With New Data)</figcaption>
</figure>
</div>
</div>
</div>
<p>This procedure, which is called out-of-sample-prediction or cross-validation, is one widely used method for comparing the quality of predictions from different models. The R-squared values of the different models in the second sample are provided to help summarize the quality of predictions.</p>
<p>We will talk more about this example in class. <strong>Before moving on, please take a moment to write down your intuitions about what is going in <a href="#fig-overfit">Figure&nbsp;<span>9.1</span></a> and <a href="#fig-overfit2">Figure&nbsp;<span>9.2</span></a>. In particular, you might consider whether the model in the right-hand panel is better than the one in the middle panel. I will ask you to share your thoughts in class.</strong></p>
</section>
<section id="sec-interpreting-the-model-9" class="level3" data-number="9.1.3">
<h3 data-number="9.1.3" class="anchored" data-anchor-id="sec-interpreting-the-model-9"><span class="header-section-number">9.1.3</span> Interpreting the model</h3>
<p>As mentioned, polynomial terms are often added into a model as a way to address nonlinearity. When this is the case, the polynomial terms themselves are not necessarily of substantive interest – they can be added just to “patch up” the model after assumption checking.</p>
<p>We saw an example of this in <a href="ch6_model_building.html#sec-worked-example-6"><span>Section&nbsp;6.2</span></a>. In that example, linear and quadratic terms for SES were entered into the model in the first block. The R-squared was interpreted for the entire block, but the interpretation of the regression coefficient for the quadratic term was not addressed. This is a pretty common way of using polynomial regression – the polynomial terms are included so that the model assumptions (linearity) are met, but they are not necessarily interpreted beyond this.</p>
<p>However, we <em>can</em> interpret the regression slopes on the polynomial terms if we want to. This section addresses the interpretation of quadratic polynomials, which have the equation:</p>
<p><span class="math display">\[ \widehat Y = b_0 +b_1X + b_2X^2.\]</span></p>
<p>A similar approach applies to models with higher-order terms as well (cubics, etc.).</p>
<p>Let’s start with a classic example of a quadratic relationship: the Yerkes-Dodson law relating physiological arousal (“stress”) to task performance, which is represented in <a href="#fig-YD">Figure&nbsp;<span>9.3</span></a>. One way to interpret the law is in terms of the overall shape of the relationship. As stress goes up, so does performance – but only up to a point, after which more stress leads to a deterioration in performance.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-YD" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="files/images/YerkesDodson.png" class="img-fluid figure-img" width="563"></p>
<figcaption class="figure-caption">Figure&nbsp;9.3: Yerkes-Dodson Law (Source: Wikipedia)</figcaption>
</figure>
</div>
</div>
</div>
<p>The overall shape of the trend depends on only the <em>sign</em> of the regression slope on the quadratic term:</p>
<ul>
<li><p>A U-shaped curve corresponds to a <em>positive</em> regression coefficient on <span class="math inline">\(X^2\)</span> (think of a parabola from high school math)</p></li>
<li><p>An inverted-U-shaped curve corresponds to a <em>negative</em> regression coefficient on <span class="math inline">\(X^2\)</span></p></li>
</ul>
<p>Beyond the overall shape of the relationship, we might also want to know what level of stress corresponds to the “optimal” level of performance – i.e., where the maximum of the curve is. This exemplifies a more complicated interpretation of a quadratic relationship, and it requires some calculus (see <a href="#sec-deriviation-9"><span>Section&nbsp;9.1.4</span></a>, which is optional). The value of <span class="math inline">\(X\)</span> that corresponds to the maximum (or minumum) of the quadratic curve is</p>
<p><span id="eq-xmax"><span class="math display">\[ X = \frac{-b_1}{2 b_2}. \tag{9.1}\]</span></span></p>
<p><strong>Based on this discussion, please answer the following questions bout the Yerkes-Dodson Law, using the following model parameters. Make sure to explain how you know the answer! </strong></p>
<p><span class="math display">\[ \widehat Y = 20 + 1 X - 2X^2.\]</span></p>
<ul>
<li><p><strong>What is the overall shape of the relationship: U or inverted-U? </strong></p></li>
<li><p><strong>What is the value of stress at which predicted performance reaches a maximum?</strong></p></li>
<li><p><strong>Bonus: what is the maximum value of predicted performance? </strong></p></li>
</ul>
</section>
<section id="sec-deriviation-9" class="level3" data-number="9.1.4">
<h3 data-number="9.1.4" class="anchored" data-anchor-id="sec-deriviation-9"><span class="header-section-number">9.1.4</span> Analysing polynomials*</h3>
<p>This sections shows how we get <a href="#eq-xmax">Equation&nbsp;<span>9.1</span></a> and some related details. Recall from intro calculus that the extrema (i.e., minima and maxima) of a function occur when the derivative of the function is equal to zero. The derivative of the quadratic regression equation is</p>
<p><span class="math display">\[
\frac{d}{dX} \hat Y = \frac{d}{dX} (b_0 + b_1X + b_2X^2) = b_1 + 2b_2X.
\]</span></p>
<p>Setting the derivative to zero <span class="math display">\[
b_1 + 2b_2X = 0
\]</span></p>
<p>and solving for <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[ X = -\frac{b_1}{2b_2} \]</span></p>
<p>gives the value of <span class="math inline">\(X\)</span> at which the <span class="math inline">\(\hat Y\)</span> reaches its minimum (or maximum) value. Let’s call this value of <span class="math inline">\(X^*\)</span>. Plugging <span class="math inline">\(X^*\)</span> into the original equation tells us the minimum (or maximum) of <span class="math inline">\(\hat Y\)</span>.</p>
<p>We can use the second derivative rule to determine whether <span class="math inline">\(X^*\)</span> is a minimum or maximum of <span class="math inline">\(\hat Y\)</span>.</p>
<p><span class="math display">\[
\frac{d^2}{dX^2} \hat Y = \frac{d}{dX} (b_1 + 2b_2X) = 2b_2
\]</span></p>
<p>If this value is positive (i.e., if <span class="math inline">\(b_2 &gt;0\)</span>), then the second derivative rule tells that <span class="math inline">\(X^*\)</span> is minimum, hence the curve is “U-shaped”. If the value is negative (i.e., if <span class="math inline">\(b_2 &lt; 0\)</span>) then its a maximum, and hence the curve is “inverted-U-shaped”.</p>
</section>
<section id="model-building-vs-curve-fitting" class="level3" data-number="9.1.5">
<h3 data-number="9.1.5" class="anchored" data-anchor-id="model-building-vs-curve-fitting"><span class="header-section-number">9.1.5</span> Model building vs curve fitting</h3>
<p>Up to this point, we have discussed the use and interpretation of polynomials. In this section we consider how to build polynomial regression models in practice.</p>
<p>A typical model-building process for polynomial regression might proceed as follows.</p>
<ol type="1">
<li><p>Enter just the linear terms into the model and examine a residual versus fitted plot.</p></li>
<li><p>If there is evidence of non-linearity, look at the scatter plots between the outcome variable and each individual predictor to make a guess about which predictor(s) may be causing the non-linearity.</p></li>
<li><p>Add a quadratic term for a predictor of interest and examine whether there is a statistically significant increase in R-squared (see <a href="ch6_model_building.html#sec-delta-rsquared-6"><span>Section&nbsp;6.1.4</span></a>). If there is, you have found a source of non-linearity! If not, the quadratic term is not explaining variance in the outcome variable, so you can remove it from the model.</p></li>
<li><p>Keep adding polynomial terms (quadratic terms for other predictors; higher-order terms for the same predictor) one at a time until the model assumptions looks reasonable. This might take a bit of trial and error.</p></li>
</ol>
<p>This overall approach is illustrated in the next section. However, there are a couple of important points to mention first.</p>
<ul>
<li><p>Making good use of polynomial regression requires walking a fine line between curve-fitting and theory-based modeling (see <a href="#fig-overfit">Figure&nbsp;<span>9.1</span></a>). Sometimes, adding polynomial terms can provide an elegant and intuitive interpretation of the relationship between two variables. But, if you find yourself adding more than a couple of polynomial terms into a model and still have unresolved issues with nonlinearity, it is probably best to consider another approach (such as piecewise regression, coming up in <a href="#sec-piecewise-9"><span>Section&nbsp;9.3</span></a>)</p></li>
<li><p>Just like with interactions, higher-order polynomial terms are often highly correlated with lower-order terms (e.g., if <span class="math inline">\(X\)</span> takes on strictly positive values, <span class="math inline">\(X\)</span> and <span class="math inline">\(X^2\)</span> will be highly correlated). Recall that if two predictors are highly correlated, this can affect their regression coefficients (<a href="ch3_two_predictors.html#sec-ols-3"><span>Section&nbsp;3.4</span></a>) as well as their standard errors (<a href="ch6_model_building.html#sec-too-many-predictors-6"><span>Section&nbsp;6.3</span></a>). In the context of polynomial regression, there are a couple of things that can be done about this.</p>
<ul>
<li><p>Interpret <span class="math inline">\(\Delta R^2\)</span> values rather than the individual regression coefficients and their <span class="math inline">\(p\)</span>-values. This is the easiest thing to do, conceptually.</p></li>
<li><p>Use “orthogonal polynomials”, which are designed to ensure the different polynomial terms for the same predictor are uncorrelated (orthogonal just means uncorrelated). The result of this approach is that numerical values of the regression coefficients are not directly interpretable beyond their sign, but the t-tests of the regression coefficients can be interpreted as testing the <span class="math inline">\(\Delta R^2\)</span> for each term in the polynomial. This is conceptually more complicated than first option, but leads to the same overall conclusions.</p></li>
</ul></li>
</ul>
<p>Both approaches are illustrated in the next section.</p>
</section>
</section>
<section id="sec-worked-example-9" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="sec-worked-example-9"><span class="header-section-number">9.2</span> Worked Example</h2>
<p>In <a href="ch8_loglinear.html#sec-worked-example-8"><span>Section&nbsp;8.6</span></a> we saw that applying a log-transform to the <code>Wages.Rdata</code> example addressed non-normality of the residuals but did not do much to address nonlinearity. The summary output and diagnostic plots for the log-linear regression of wages on education are presented again below. <strong>We will go through this example in class together, so please note any questions you have about the procedures or their interpretation and be prepared to ask them in class. </strong></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data and take a look</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"Wages.RData"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(wages)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create log transform of wage</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>log_wage <span class="ot">&lt;-</span> <span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress log_wages on educ</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> educ)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out model fit</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod1)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch9_polynomial_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log_wage ~ educ)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.07475 -0.35961  0.02614  0.31244  1.18163 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 1.188730   0.102328   11.62   &lt;2e-16 ***
educ        0.074802   0.007263   10.30   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4234 on 398 degrees of freedom
Multiple R-squared:  0.2104,    Adjusted R-squared:  0.2085 
F-statistic: 106.1 on 1 and 398 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Because there is one prominent bend in our residual vs fitted plot (at <span class="math inline">\(\hat Y \approx 2.1\)</span>), let’s see if adding a quadratic term to the model can improve the model fit.</p>
<p>The <code>poly</code> function in <code>R</code> makes it easy to do polynomial regression, without having to hard-code new variables like <code>educ^2</code> into our dataset. In the summary output below, <code>poly(...)n</code> denote’s the <span class="math inline">\(n\)</span>-th term in the polynomial. The diagnostic plots for the log-linear model with a quadratic term are also shown below.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> <span class="fu">poly</span>(educ, <span class="dv">2</span>, <span class="at">raw =</span> T))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># To plot the trend we need to we first need to order the data and the predicted values ... </span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>sort_educ <span class="ot">&lt;-</span> educ[<span class="fu">order</span>(educ)]</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>sort_fitted<span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod2)[<span class="fu">order</span>(educ)]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(sort_educ, sort_fitted, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod2, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch9_polynomial_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log_wage ~ poly(educ, 2, raw = T))

Residuals:
     Min       1Q   Median       3Q      Max 
-1.04723 -0.38939  0.01877  0.31820  1.14129 

Coefficients:
                         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              1.862958   0.406587   4.582 6.18e-06 ***
poly(educ, 2, raw = T)1 -0.031492   0.062468  -0.504   0.6144    
poly(educ, 2, raw = T)2  0.003985   0.002326   1.713   0.0875 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4224 on 397 degrees of freedom
Multiple R-squared:  0.2162,    Adjusted R-squared:  0.2123 
F-statistic: 54.77 on 2 and 397 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Writing the R output in terms of the regression model, we have:</p>
<p><span class="math display">\[ \widehat{\log(WAGES)} = 1.862958 - 0.031492 (EDUC) + 0.003985 (EDUC)^2.\]</span></p>
<p>Let’s start by interpreting the plots. Based on the left-hand panel, it looks like a quadratic relationship provides a reasonable representation of the data. Based on the right-hand panel, I would conclude that the apparent non-linearity in the residual vs fitted plot has been sufficiently reduced. There is still a blip at <span class="math inline">\(\hat Y = 2.3\)</span>, but there are 5 data points there so I am not to worried about it.</p>
<p>Turning to the summary output, there are three main take-aways:</p>
<ol type="1">
<li><p>As discussed in the previous section, the sign of the quadratic term tells us something about the overall shape of the relationship (do you remember what that is?). However, interpreting the numerical values of the regression coefficients in a polynomial regression is not always useful. For example, using the approach to interpreting quadratic regression from <a href="#sec-interpreting-the-model-9"><span>Section&nbsp;9.1.3</span></a>, it turns out that the minimum predicted wages occur for someone with 3.95 years of education. The lowest level of education in the sample is 6 years, so this interpretation isn’t super relevant for our example. Consequently, rather than focusing on the interpretation of the regression coefficients, it is often sufficient to focus on whether the two predictors (i.e., <span class="math inline">\(EDUC\)</span> and <span class="math inline">\(EDUC^2\)</span>) together explained a significant proportion of variation in the outcome variable. This information is provided by the R-squared statistic and the F-test of R-squared in the summary output above.</p></li>
<li><p>There are two ways to test whether the addition of the quadratic term (<code>poly(educ, 2)2 = 0.003985</code>) improves the model. First, we can examine its test of significance. This test tells us that, controlling for the linear relationship between log-wages and education, the quadratic term is statistically significant at the .1 level (it is not statistically significant at the .05 level). Recall from <a href="ch7_assumption_checking.html"><span>Chapter&nbsp;7</span></a> that this same information could be obtained by setting up a heirarhical model (Block 1 = linear term; Block 2 = quadratic term) and testing the change in R-squared. For the example, the F-test of R-squared change is</p></li>
</ol>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(mod1, mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: log_wage ~ educ
Model 2: log_wage ~ poly(educ, 2, raw = T)
  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
1    398 71.363                              
2    397 70.840  1   0.52368 2.9348 0.08747 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Note that the p-value is exactly the same as the t-test of regression coefficient reported above. In both cases, it seems that we don’t really need the quadratic term, based on the .05 level of significance (more on this <a href="#sec-piecewise-9"><span>Section&nbsp;9.3</span></a>)</p>
<ol start="3" type="1">
<li>Finally, compared to the model without the quadratic term, we can see the linear term (<code>poly(educ, 2)1 = - 0.031492</code> ) is now longer statistically significant. This can happen when we add higher order terms into a model. In this example, the linear and quadratic terms are highly correlated (the correlation is over .99 in the example!). Due to this correlation, the linear terms is not statistically significant, and the quadratic term is only “marginally” significant, even though the F-test of R-squared in the summary output is fatalistically significant with <span class="math inline">\(p &lt; .001\)</span>. In the next section, we will see how to avoid this issue of having highly correlated predictors in polynomial regression.</li>
</ol>
<p>** If you have any questions about the interpretation of the model results discussed in this section, please list them now and I will be happy to address the in class.**</p>
<section id="orthogonal-polynomails" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="orthogonal-polynomails"><span class="header-section-number">9.2.1</span> Orthogonal polynomails*</h3>
<p>Before moving on, a quick (and optional) note on orthogonal vs.&nbsp;raw polynomials. Orthogonal polynomials are the default approach in <code>R</code>, and they make life easier, so they are worth knowing about.</p>
<p>When using orthogonal polynomials, the different polynomial terms (e.g., <span class="math inline">\(X, X^2, X^3\)</span>) are transformed so that they are uncorrelated. This means that the <span class="math inline">\(t\)</span>-test of each regression coefficient can be interpreted as testing the proportion of variance associated uniquely with that term of the polynomial. Basically, using orthogonal polynomials means that we don’t need to do the model building stuff (e.g., sequential blocks, adding in each term one at a time)– it’s already built into the coefficients.</p>
<p>The downside of orthogonal polynomials is that, beyond their sign, the regression coefficients are complicated to interpret. But, these coefficients aren’t easy to interpret anyway, and we often don’t care much about their exact values. If you are in a situation where you don’t really care about the interpretation of the coefficients beyond the overall shape of the relationship, the orthogonal polynomials are definitely a good choice!</p>
<p>The use of orthogonal polynomials is illustrated below. You’ll see that most of the output is the same as in the previous section, except the numerical value and associated tests of the regression coefficients in the summary table. In particular, the linear trend is statistically significant in the output below, because it is no longer correlated with the quadratic trend. In fact the t-test and p-value are exactly the same as the first model we fit to our example data above (the model with just the linear trend, and no quadratic trend).</p>
<p>In summary, orthogonal polynomials provide a shortcut to hierarchical model buiding with polynomials. By transforming the data so that the different terms of the polynomial are uncorrelated, we get the similar information from a single model using orthogonal polynomials as we would if we fitted a series of hierarchical models, adding each additional term into the model one at a time.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>mod2a <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> <span class="fu">poly</span>(educ, <span class="dv">2</span>, <span class="at">raw =</span> F))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># To plot the trend we need to we first need to order the data and the predicted values ... </span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>sort_educ <span class="ot">&lt;-</span> educ[<span class="fu">order</span>(educ)]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>sort_fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod2a)[<span class="fu">order</span>(educ)]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(sort_educ, sort_fitted, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod2a, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch9_polynomial_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log_wage ~ poly(educ, 2, raw = F))

Residuals:
     Min       1Q   Median       3Q      Max 
-1.04723 -0.38939  0.01877  0.31820  1.14129 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              2.21987    0.02112 105.103   &lt;2e-16 ***
poly(educ, 2, raw = F)1  4.36133    0.42242  10.325   &lt;2e-16 ***
poly(educ, 2, raw = F)2  0.72366    0.42242   1.713   0.0875 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4224 on 397 degrees of freedom
Multiple R-squared:  0.2162,    Adjusted R-squared:  0.2123 
F-statistic: 54.77 on 2 and 397 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>To find out more, use <code>help(poly)</code>. A good discussion of this point is also available on StatExchange: <a href="https://stats.stackexchange.com/questions/258307/raw-or-orthogonal-polynomial-regression?r=SearchResults&amp;s=2%7C87.5473">https://stats.stackexchange.com/questions/258307/raw-or-orthogonal-polynomial-regression?r=SearchResults&amp;s=2%7C87.5473</a></p>
</section>
<section id="summary" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="summary"><span class="header-section-number">9.2.2</span> Summary</h3>
<p>This section has addressed how to use, interpret, and implement polynomial regression. Some key points:</p>
<ul>
<li><p>We don’t want to overfit the data by adding too many higher-order terms. If a quadratic or cubic polynomial doesn’t sort out any issues with linearity (as diagnosed by the residual vs fitted plots), then you probably want to try something else (see next section).</p></li>
<li><p>Often the overall shape of a polynomial regression is of interest. This is communicated by the sign of the regression slopes on the higher-order terms. However we aren’t interested in a more specific interpretation of the regression slopes – it can be done (#sec-interpreting-the-model-9), but it is not very common.</p></li>
<li><p>Instead, we usually approach polynomial regression from the perspective of hierarchical model building – if the higher order terms lead to a significant increase in the variance explained (i.e., R-squared change), we keep them in the model.</p></li>
<li><p>Orthogonal polynomials provide a shortcut to doing heirarhical model building with polynomials. The make our life easier, but they aren’t doing anything different than the hierarchical.</p></li>
<li><p>In the worked example, it turned out that despite the apparent issue with linearity in the original model, and despite the apparently better fit of the quadratic model in terms of linearity, the statistical tests actually suggested we don’t need the quadratic term (using <span class="math inline">\(\alpha = .05\)</span>). The next example provides another perspective on this modelling issue.</p></li>
</ul>
</section>
</section>
<section id="sec-piecewise-9" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="sec-piecewise-9"><span class="header-section-number">9.3</span> Piecewise regression</h2>
<p>Piecewise or segmented regression is another approach to dealing with nonlinearity. Like polynomial regression, it is mathematically similar to interaction. Also like polynomial regression, it has a special interpretation and application that make it practically distinct from interaction.</p>
<p>In the simplest case, piecewise regression involves interacting a predictor variable with a binary re-coding of itself. To illustrate how the approach works, let’s again consider our wages and education example. The scatter plot of log-wages versus education is presented again below for reference.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-piecewise1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch9_polynomial_files/figure-html/fig-piecewise1-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;9.4: The Wages Example</figcaption>
</figure>
</div>
</div>
</div>
<p>Consider the following reasoning about the example:</p>
<ul>
<li><p>For people with 12 or less years of education (i.e., who did not obtain post-secondary education) the apparent relationship with wage is quite weak. This seems plausible, because if a job doesn’t require a college degree, education probably isn’t a big factor in determining wages.</p></li>
<li><p>For people with more than 12 years of education, the relationship with wage seems to be stronger. This also seems plausible: for jobs that require post secondary education, more education is usually associated with higher wages.</p></li>
<li><p>To restate this as an interaction: the relationship between wage and education appears different for people who have a post-secondary education versus those who do not.</p></li>
</ul>
<p>To represent this reasoning visually we can modify <a href="#fig-piecewise1">Figure&nbsp;<span>9.4</span></a> as shown in <a href="#fig-piecewise2">Figure&nbsp;<span>9.5</span></a>. This captures the basic idea behind piecewise regression – we have different regression lines over different ranges of the predictor, and the overall regression is piecewise or segmented. The next section shows how to build this model.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-piecewise2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch9_polynomial_files/figure-html/fig-piecewise2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;9.5: The wages example</figcaption>
</figure>
</div>
</div>
</div>
<section id="the-piecewise-model" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="the-piecewise-model"><span class="header-section-number">9.3.1</span> The piecewise model</h3>
<p>We have reasoned that the relationship between wages and education might depend on whether people have post-secondary education. We also noted that this sounds a lot like an interaction (because it is!), which is the basic approach we can use to create piecewise models.</p>
<p>In order to run our piecewise regression, first we need to create a dummy-coded version of education that indicates whether a person had more than 12 years education:</p>
<p><span class="math display">\[ EDUC_{12} = \left\{ \begin{matrix}  
                     1 &amp; \text{if } EDUC  &gt; 12\\
                    0 &amp; \text{if } EDUC  \leq 12
                \end{matrix} \right.
\]</span></p>
<p>Then, we enter the original variable, the dummy-coded indicator, and their interaction into the model:</p>
<p><span class="math display">\[ \widehat{\log(WAGES)} = b_0 + b_1 (EDUC) + b_2 (EDUC_{12}) + b_3 (EDUC \times EDUC_{12}) \]</span></p>
<p>As we can see, the resulting model is a special case of an interaction between a continuous predictor (<span class="math inline">\(EDUC\)</span>) and binary predictor (<span class="math inline">\(EDUC_{12}\)</span>).</p>
<p>While the above model conveys the overall idea of piecewise regression, there are also more complex approaches that will search for breakpoints, smoothly connect the lines at the breakpoints, use nonlinear functions (e.g., polynomials) for the segments, etc. We won’t cover these more complex approaches here, but check out the following resource if you are interested and feel free to ask questions in class: <a href="https://rpubs.com/MarkusLoew/12164">https://rpubs.com/MarkusLoew/12164</a></p>
</section>
<section id="back-to-the-example" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="back-to-the-example"><span class="header-section-number">9.3.2</span> Back to the example</h3>
<p>The output for the example is provided below. Following the output, some questions are posed about the interpretation of the model.</p>
<ul>
<li>Diagnostic plots for the piecewise model:</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-piecewise3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch9_polynomial_files/figure-html/fig-piecewise3-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;9.6: The Wages Example</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li>Summary output and estimated model (the model doesn’t fit nicely on one line!):</li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log_wage ~ educ * educ12)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.98101 -0.36398  0.02055  0.30687  1.10305 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.78426    0.23531   7.583 2.43e-13 ***
educ         0.01736    0.02159   0.804   0.4219    
educ12      -0.64255    0.35573  -1.806   0.0716 .  
educ:educ12  0.06144    0.02737   2.245   0.0253 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4203 on 396 degrees of freedom
Multiple R-squared:  0.2261,    Adjusted R-squared:  0.2202 
F-statistic: 38.56 on 3 and 396 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p><span class="math display">\[
\begin{align} \widehat{\log(WAGES)} = &amp; 1.78426  + 0.01736 (EDUC) \\ &amp; - 0.64255 (EDUC_{12})
+ 0.06144 (EDUC \times EDUC_{12})
\end{align}
\]</span></p>
<ul>
<li>Simple trends using of the <code>emtrends</code> function (see <a href="ch5_interactions.html#sec-inference-for-interactions-5"><span>Section&nbsp;5.4</span></a>).</li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>emmeans<span class="sc">::</span><span class="fu">emtrends</span>(mod4, <span class="at">specs =</span> <span class="st">"educ12"</span>, <span class="at">var =</span> <span class="st">"educ"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> educ12 educ.trend     SE  df lower.CL upper.CL
      0     0.0174 0.0216 396  -0.0251   0.0598
      1     0.0788 0.0168 396   0.0457   0.1119

Confidence level used: 0.95 </code></pre>
</div>
</div>
<p>We will discuss this model together in class. It should feel a lot like deja vu from <a href="ch5_interactions.html"><span>Chapter&nbsp;5</span></a>, but even more complicated due to the interpretation of <span class="math inline">\(EDUC_{12}\)</span> and the fact that the outcome is log-transformed. Fun!!! I’ll ask questions below in class:</p>
<p>Note that the intercept and main effect of the binary variable <code>educ12</code> are not of much interest in this application.</p>
<ul>
<li><p><strong>Using the 2-step approach from <a href="ch5_interactions.html#sec-binary-continuous-interaction-5"><span>Section&nbsp;5.3</span></a>, please take a moment to work out the interpretation of main effect on <span class="math inline">\(EDUC\)</span> <span class="math inline">\(b_1 = 0.01736\)</span> and the interaction <span class="math inline">\(b_3 = 0.06144\)</span> in the model above. It might help to draw a plot like <a href="#fig-piecewise2">Figure&nbsp;<span>9.5</span></a> and label it accordingly. (The interpretation of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_2\)</span> is not very interesting but you can work them out too if you like.)</strong></p></li>
<li><p><strong>Please take a moment to write down your interpretation of the results of the piecewise regression, addressing the diagnostic plots, the parameter estimates, and the simple slopes.</strong></p></li>
</ul>
</section>
<section id="summary-1" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="summary-1"><span class="header-section-number">9.3.3</span> Summary</h3>
<p>Piecewise regression is another approach to dealing with nonlinearity. It can be especially powerful when we can conceptualize the nonlinearity in as an interaction between a variable and categorical encoding of itself (e.g., the relationship between years of education and wages depends on whether you went to college.) The overall interpretation and implementation of the model is also based on the material we already covered in <a href="ch5_interactions.html#sec-binary-continuous-interaction-5"><span>Section&nbsp;5.3</span></a>, so take a look at the summary of that section for additional pointers.</p>
</section>
</section>
<section id="sec-workbook-9" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="sec-workbook-9"><span class="header-section-number">9.4</span> Workbook</h2>
<p>This section collects the questions asked in this chapter. The lessons for this chapter will focus on discussing these questions and then working on the exercises in <a href="#sec-exercises-9"><span>Section&nbsp;9.5</span></a>. The lesson will <strong>not</strong> be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.</p>
<p><a href="#sec-overfitting-9"><span>Section&nbsp;9.1.2</span></a></p>
<ul>
<li>Please take a moment to write down your intuitions about what is going in <a href="#fig-overfit">Figure&nbsp;<span>9.1</span></a> and <a href="#fig-overfit2">Figure&nbsp;<span>9.2</span></a>. In particular, you might consider whether the model in the right-hand panel is better than the one in the middle panel. I will ask you to share your thoughts in class.</li>
</ul>
<p><a href="#sec-interpreting-the-model-9"><span>Section&nbsp;9.1.3</span></a></p>
<ul>
<li>Please answer the following questions bout the Yerkes-Dodson Law, using the following model parameters. Make sure to explain how you know the answer.</li>
</ul>
<p><span class="math display">\[ \widehat Y = 20 + 1 X - 2X^2.\]</span></p>
<ul>
<li><p>What is the overall shape of the relationship: U or inverted-U?</p></li>
<li><p>What is the value of stress at which predicted performance reaches a maximum?</p></li>
<li><p>Bonus: what is the maximum value of predicted performance?</p></li>
</ul>
<p><a href="#sec-worked-example-9"><span>Section&nbsp;9.2</span></a></p>
<ul>
<li>We will go through the example in class together, so please note any questions you have about the procedures or their interpretation and be prepared to ask them in class.</li>
</ul>
<p><a href="#sec-piecewise-9"><span>Section&nbsp;9.3</span></a></p>
<ul>
<li><p>The output for the example is provided below. Following the output, some questions are posed about the interpretation of the model,</p></li>
<li><p>Diagnostic plots for the piecewise model:</p></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch9_polynomial_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">The Wages Example</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li>Summary output and estimated model:</li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log_wage ~ educ * educ12)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.98101 -0.36398  0.02055  0.30687  1.10305 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.78426    0.23531   7.583 2.43e-13 ***
educ         0.01736    0.02159   0.804   0.4219    
educ12      -0.64255    0.35573  -1.806   0.0716 .  
educ:educ12  0.06144    0.02737   2.245   0.0253 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4203 on 396 degrees of freedom
Multiple R-squared:  0.2261,    Adjusted R-squared:  0.2202 
F-statistic: 38.56 on 3 and 396 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p><span class="math display">\[
\begin{align} \widehat{\log(WAGES)} = &amp; 1.78426  + 0.01736 (EDUC) \\ &amp; - 0.64255 (EDUC_{12})
+ 0.06144 (EDUC \times EDUC_{12})
\end{align}
\]</span></p>
<ul>
<li>Simple trends using of the <code>emtrends</code> function (see <a href="ch5_interactions.html#sec-inference-for-interactions-5"><span>Section&nbsp;5.4</span></a>).</li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>emmeans<span class="sc">::</span><span class="fu">emtrends</span>(mod4, <span class="at">specs =</span> <span class="st">"educ12"</span>, <span class="at">var =</span> <span class="st">"educ"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> educ12 educ.trend     SE  df lower.CL upper.CL
      0     0.0174 0.0216 396  -0.0251   0.0598
      1     0.0788 0.0168 396   0.0457   0.1119

Confidence level used: 0.95 </code></pre>
</div>
</div>
<ul>
<li><p>Using the 2-step approach from <a href="ch5_interactions.html#sec-binary-continuous-interaction-5"><span>Section&nbsp;5.3</span></a>, please take a moment to work out the interpretation of main effect on <span class="math inline">\(EDUC\)</span> <span class="math inline">\(b_1 = 0.01736\)</span> and the interaction <span class="math inline">\(b_3 = 0.06144\)</span> in the model above. It might help to draw a plot like <a href="#fig-piecewise2">Figure&nbsp;<span>9.5</span></a> and label it accordingly. (The interpretation of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_2\)</span> is not very interesting but you can work them out too if you like.)</p></li>
<li><p>Please take a moment to write down your interpretation of the results of the piecewise regression, addressing the diagnostic plots, the parameter estimates, and the simple slopes.</p></li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># clean up!</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(wages)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="sec-exercises-9" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="sec-exercises-9"><span class="header-section-number">9.5</span> Exercises</h2>
<p>There isn’t much new in terms of R code in this chapter, but the workflows for the two types of model are pretty complicated so we review them here. You’ll see that some of the plots require a lot of fiddling about, especially for the piecewise regression model. We will cover some tricks and shortcuts for producing these types plots during the open lab sessions for Assignment 4. So don’t worry too much about the complicated-looking coded for the plots at this point!</p>
<p>We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.) Before staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”</p>
<section id="polynomial-regression" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="polynomial-regression"><span class="header-section-number">9.5.1</span> Polynomial regression</h3>
<p>Let’s start with the “Vanella” log-linear model for the wages examples</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data and take a look</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"Wages.RData"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(wages)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create log transform of wage</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>log_wage <span class="ot">&lt;-</span> <span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress it on educ</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> educ)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out model fit</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod1)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch9_polynomial_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Because there is one prominent bend in our residual vs fitted plot of the log-linear model (at <span class="math inline">\(\hat Y \approx 2.1\)</span>), let’s see if adding a quadratic term to the model can improve the model fit.</p>
<p>The <code>poly</code> function in <code>R</code> makes it easy to do polynomial regression, without having to hard-code new variables like <code>EDUC^2</code> into our dataset. This function automatically uses orthogonal (uncorrelated) polynomials, so we don’t have to worry about centering, either. The basic interpretation of the model coefficients in an orthogonal polynomial regression is the same as discussed in #sec-polynomial-9, but the “more complicated” interpretation of the model parameters is not straightforward. To find out more, use <code>help(poly)</code>.</p>
<p>The diagnostic plots for the log-linear model with a quadratic term included for education is shown below, along with the model summary. In the output, <code>poly(educ, 2)n</code> is the <span class="math inline">\(n\)</span>-th degree term in the polynomial. Orthogonal polynomials were used.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress log_wage on a quadratic function of eduction </span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> <span class="fu">poly</span>(educ, <span class="dv">2</span>))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model output</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log_wage ~ poly(educ, 2))

Residuals:
     Min       1Q   Median       3Q      Max 
-1.04723 -0.38939  0.01877  0.31820  1.14129 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     2.21987    0.02112 105.103   &lt;2e-16 ***
poly(educ, 2)1  4.36133    0.42242  10.325   &lt;2e-16 ***
poly(educ, 2)2  0.72366    0.42242   1.713   0.0875 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4224 on 397 degrees of freedom
Multiple R-squared:  0.2162,    Adjusted R-squared:  0.2123 
F-statistic: 54.77 on 2 and 397 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostic plots</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># scatter plot with trend</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co"># order the data and the predicted values ... </span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>sort_educ <span class="ot">&lt;-</span> educ[<span class="fu">order</span>(educ)]</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>sort_fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod2)[<span class="fu">order</span>(educ)]</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(sort_educ, sort_fitted, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># residual versus fitted</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod2, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch9_polynomial_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The F-test of R-squared change between the first and second models (not really required since we used orhtogonal polynomials, but for illustrative purposes):</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(mod1, mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: log_wage ~ educ
Model 2: log_wage ~ poly(educ, 2)
  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
1    398 71.363                              
2    397 70.840  1   0.52368 2.9348 0.08747 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Illustrating the same overall approach for the cubic model:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> <span class="fu">poly</span>(educ, <span class="dv">3</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log_wage ~ poly(educ, 3))

Residuals:
    Min      1Q  Median      3Q     Max 
-1.0344 -0.3734  0.0203  0.3107  1.1612 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      2.2199     0.0211 105.213   &lt;2e-16 ***
poly(educ, 3)1   4.3613     0.4220  10.336   &lt;2e-16 ***
poly(educ, 3)2   0.7237     0.4220   1.715   0.0871 .  
poly(educ, 3)3  -0.5717     0.4220  -1.355   0.1763    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.422 on 396 degrees of freedom
Multiple R-squared:  0.2199,    Adjusted R-squared:  0.2139 
F-statistic:  37.2 on 3 and 396 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Same plots as above, reusing variable names here</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>sort_fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod3)[<span class="fu">order</span>(educ)]</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(sort_educ, sort_fitted, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod3, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch9_polynomial_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(mod1, mod2, mod3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: log_wage ~ educ
Model 2: log_wage ~ poly(educ, 2)
Model 3: log_wage ~ poly(educ, 3)
  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
1    398 71.363                              
2    397 70.840  1   0.52368 2.9410 0.08714 .
3    396 70.513  1   0.32682 1.8354 0.17626  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
</section>
<section id="write-up" class="level3" data-number="9.5.2">
<h3 data-number="9.5.2" class="anchored" data-anchor-id="write-up"><span class="header-section-number">9.5.2</span> Write up</h3>
<p>This section illustrates the write-up for the quadratic regression based on heirarhical modeling and based on orthogonal polynomials. In practice, just pick one of these.</p>
<ul>
<li><p><em>Based on heirachical modeling.</em> Starting with a model containing only the linear term, years of education explained about 21.6% of the variance in log-wages (<span class="math inline">\(R^2 = .216, F(2, 397) = 54.77, p &lt; .001\)</span>). Adding a quadratic term to address potential nonlinearity, it was found that only an additional .5% of variance was explained (<span class="math inline">\(\Delta R^2 = .005, F(1, 293) = 2.93, p = 0.088\)</span>).</p></li>
<li><p><em>Based on orthogonal polynomials</em> Regressing log wages on a second-degree orthogonal polynomial function of wages, it was found that the linear term was statistically significant at the .05 level (<span class="math inline">\(b = 4.36, t(396) = 10.33, p &lt; .001\)</span>), but the quadratic term was not (<span class="math inline">\(b = 0.72, t(396) = 1.71, p = 0.088\)</span>).</p></li>
</ul>
</section>
<section id="piecewise-regression" class="level3" data-number="9.5.3">
<h3 data-number="9.5.3" class="anchored" data-anchor-id="piecewise-regression"><span class="header-section-number">9.5.3</span> Piecewise regression</h3>
<p>Moving on, let’s consider the piecewise model from <a href="#sec-piecewise-9"><span>Section&nbsp;9.3</span></a></p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dummy variable indicating if education is at least 12 years or more</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>educ12 <span class="ot">&lt;-</span> (educ <span class="sc">&gt;</span> <span class="dv">12</span>)<span class="sc">*</span><span class="dv">1</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Interact the dummy with educ</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>mod4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">~</span> educ<span class="sc">*</span>educ12) </span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The model output</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(wage + 1) ~ educ * educ12)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.98101 -0.36398  0.02055  0.30687  1.10305 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.78426    0.23531   7.583 2.43e-13 ***
educ         0.01736    0.02159   0.804   0.4219    
educ12      -0.64255    0.35573  -1.806   0.0716 .  
educ:educ12  0.06144    0.02737   2.245   0.0253 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4203 on 396 degrees of freedom
Multiple R-squared:  0.2261,    Adjusted R-squared:  0.2202 
F-statistic: 38.56 on 3 and 396 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The simple trends</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>trends <span class="ot">&lt;-</span> emmeans<span class="sc">::</span><span class="fu">emtrends</span>(mod4, <span class="at">specs =</span> <span class="st">"educ12"</span>, <span class="at">var =</span> <span class="st">"educ"</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>emmeans<span class="sc">::</span><span class="fu">test</span>(trends)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> educ12 educ.trend     SE  df t.ratio p.value
      0     0.0174 0.0216 396   0.804  0.4219
      1     0.0788 0.0168 396   4.686  &lt;.0001</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The diagnostic plots</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod4, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod4, <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ch9_polynomial_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">The wages example</figcaption>
</figure>
</div>
</div>
</div>
<p>We can still see some evidence of heteroskedasticity in the residual versus fitted plot, so the last step is to use heteroskedasticity-corrected standard errors to ensure we are making the right conclusions about statistical significance</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Make sure the required pacakges are installed</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("car")</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("lmtest")</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1. Use "hccm" to get the HC SEs for our piecewise model </span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>hcse <span class="ot">&lt;-</span> car<span class="sc">::</span><span class="fu">hccm</span>(mod4)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2. Use "coeftest" to compute t-tests with the HC SEs</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">coeftest</span>(mod4, hcse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
t test of coefficients:

             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.784262   0.188861  9.4475  &lt; 2e-16 ***
educ         0.017362   0.017859  0.9722  0.33156    
educ12      -0.642555   0.323851 -1.9841  0.04793 *  
educ:educ12  0.061436   0.024565  2.5010  0.01279 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The next bit is optional. It shows how to produce the piecewise regression plot, which takes quite a bit of messing about with R…Let me know if you find an easier way to do this (in base R).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Building the piecewise regression plot -- yeeesh</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Add fitted values to dataset</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>wages<span class="sc">$</span>fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod4)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort data on educ</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>wages <span class="ot">&lt;-</span> wages[<span class="fu">order</span>(educ), ]</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, <span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>), <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Change color for the points with educ ≤ 12</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(wages, educ <span class="sc">&lt;=</span> <span class="dv">12</span>), <span class="fu">points</span>(educ, <span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>)))</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted values for educ &gt; 12</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(wages, educ <span class="sc">&gt;</span> <span class="dv">12</span>), <span class="fu">lines</span>(educ, fitted, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>))</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted values for educ ≤ 12</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(wages, educ <span class="sc">&lt;=</span> <span class="dv">12</span>), <span class="fu">lines</span>(educ, fitted))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch9_polynomial_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="write-up-1" class="level3" data-number="9.5.4">
<h3 data-number="9.5.4" class="anchored" data-anchor-id="write-up-1"><span class="header-section-number">9.5.4</span> Write up</h3>
<p>Using a piecewise regression model with heteroskedasticity consistent standard errors, it was found that the relationship between log-wages and years of education depended on whether a person attended post-secondary education (<span class="math inline">\(b = 0.061, t(396) = 2.50, p = .013\)</span>). Using simple trends, it was found for people with 12 or less years of education, the relationship between log-wages and years of education was not significant (<span class="math inline">\(b = 0.017, t = (396) = .804, p = .42\)</span>), whereas the relationship was significant for people with at least some post-secondary education (<span class="math inline">\(b = 0.078, t = (396) = 4.68, p &lt; .001\)</span>). The results indicate that, simply for those with some post-secondary education, each additional year of education was associated with a 8.1% increase in hourly wages (<span class="math inline">\(\exp(0.078) - 1 = .081\)</span>). Due to a software limitation, the simple trends did not utlize heteroskedastic consistent standard errors.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch8_loglinear.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Log-linear regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ch10_logistic.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Logistic regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb40" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="an">fold:</span><span class="co"> true</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="an">editor:</span><span class="co"> </span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co">  markdown: </span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co">    wrap: 72</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="fu"># Polynomial regression, etc {#sec-chap-9}</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>This chapter will cover two widely-used techniques for addressing violations of the assumption of linearity: </span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Polynomial regression, which means raising $X$-variables to a power (e.g., $X^2$ and $X^3$), and </span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Piecewise or segmented regression, which involves using different regression lines over different ranges of a predictor. </span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>These techniques involve transforming the $X$-variable(s), which can be done in addition to (or instead of) transforming the $Y$ variable (see @sec-chap-8). </span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>Both polynomial and piecewise regression are useful in practice and lead to advanced topics like splines and semi-parametric regression. They also turn out to be special cases of interactions, so we have already covered a lot of the technical details in @sec-chap-5 -- phew! </span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## Polynomial regression {#sec-polynomial-9}</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>Polynomial regression means that we regress $Y$ on a polynomial function of $X$:</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>$$ \widehat Y = b_0 + b_1 X + b_2 X^2 + b_3 X^3 + ....$$</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>Your first thought might be, "doesn’t this contradict the assumption that regression is linear?" The answer here is a bit subtle. </span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>As with regular linear regression, the polynomial model is linear in the coefficients -- we don’t raise the regression coefficients to a power (e.g., $b_1^2$), or multiply coefficients together (e.g, $b_1 \times b_2$). This is the technical sense in which polynomial regression is still just linear regression, despite its name. </span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>Polynomial regression does use nonlinear functions of the predictor(s), but the model is agnostic to what you do with your data. The situation here is a lot like when we worked with interactions in @sec-chap-5. In order to model interactions, we computed the product of two predictors and entered the product into the model as a third predictor. Well, $X^2$ is the product of a predictor with itself, so, in this sense, the quadratic term in a polynomial regression is just a special case of an interaction between two variables. </span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>Although we did not cover interactions among more than two variables in this course, they are computed in the same way -- e.g., a "three-way" interaction is just the product of 3 predictors. Similarly, $X^3$ is just the three-fold product of a variable with itself. </span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>While polynomial regression is formally similar to interactions, it is used for a different purpose. Interactions address how the relationship between two variables changes as a function of a third. Their inclusion in a model is usually motivated by a specific research question that is formulated before doing the data analysis (see @sec-chap-6). </span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>By contrast, polynomial regression is used to address a non-linear relationship between $Y$ and $X$, and is usually motivated by a preliminary examination of data that indicates the presence of such a relationship (e.g., a scatter plot of $Y$ versus $X$; a residual versus fitted plot). While it is possible to formulate research questions about polynomial terms in a regression model, this is not necessarily or even usually the case when polynomial regression is used -- often its just used to address violations of the linearity assumption. </span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a><span class="fu">### Recap of polynomials</span></span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>In general, a polynomial of degree $n$ (i.e., highest power of $n$) produces a curve that can have up to $n-1$ bends (minima and maxima). Some examples are illustrated in Figure \@ref(fig:poly) below.</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The (orange) linear function of $X$ is a polynomial of degree 1 and has zero bends.</span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The (green) quadratic function of $X$ is a polynomial of degree 2 and has 1 bend (a minimum at $X = 0$; this is also called a parabola).</span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>etc.</span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, poly, echo = F, fig.cap = "Examples of Polynomials", fig.align = 'center'}</span></span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/poly.png"</span>, <span class="at">dpi =</span> <span class="dv">150</span>)</span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>As we can see, this is a very flexible approach to capturing non-linear relationships between two variables. In fact, it can be too flexible! This is the topic of the next section. </span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a><span class="fu">### "Over-fitting" the data {#sec-overfitting-9}</span></span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a>@fig-overfit shows three different regression models fitted to the same (simulated) bivariate data. </span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In the left panel, a standard linear regression model is used, and we can see that the model does not capture the nonlinear (quadratic) trend in the data. </span>
<span id="cb40-54"><a href="#cb40-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-55"><a href="#cb40-55" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The middle panel uses a quadratic model (i.e., includes $X^2$ as a predictor, as well as $X$), and fits the data quite well. </span>
<span id="cb40-56"><a href="#cb40-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-57"><a href="#cb40-57" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The right panel uses a 16-degree polynomial to fit the data. We can see that is has a higher R-squared than the quadratic model. But there is also something fishy about this model, don't you agree? </span>
<span id="cb40-58"><a href="#cb40-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-59"><a href="#cb40-59" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-overfit, echo = F, fig.cap = "Polynomial Regression Examples", fig.align = 'center', fig.width = 12}</span></span>
<span id="cb40-60"><a href="#cb40-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb40-61"><a href="#cb40-61" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb40-62"><a href="#cb40-62" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(<span class="dv">20</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb40-63"><a href="#cb40-63" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>, <span class="dv">0</span>, .<span class="dv">5</span>)</span>
<span id="cb40-64"><a href="#cb40-64" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">+</span> X <span class="sc">-</span> X<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> e</span>
<span id="cb40-65"><a href="#cb40-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-66"><a href="#cb40-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Three regression models</span></span>
<span id="cb40-67"><a href="#cb40-67" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X)</span>
<span id="cb40-68"><a href="#cb40-68" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> <span class="fu">poly</span>(X, <span class="dv">2</span>))</span>
<span id="cb40-69"><a href="#cb40-69" aria-hidden="true" tabindex="-1"></a>mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> <span class="fu">poly</span>(X, <span class="dv">16</span>))</span>
<span id="cb40-70"><a href="#cb40-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-71"><a href="#cb40-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb40-72"><a href="#cb40-72" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>)) </span>
<span id="cb40-73"><a href="#cb40-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-74"><a href="#cb40-74" aria-hidden="true" tabindex="-1"></a>title1 <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"R-squared = "</span>, <span class="fu">round</span>(<span class="fu">summary</span>(mod1)<span class="sc">$</span>r.square, <span class="dv">3</span>))</span>
<span id="cb40-75"><a href="#cb40-75" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, Y, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">pch =</span> <span class="dv">10</span>, <span class="at">lwd =</span> <span class="dv">5</span>, <span class="at">main =</span> title1)</span>
<span id="cb40-76"><a href="#cb40-76" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X, <span class="fu">fitted</span>(mod1), <span class="at">col =</span> <span class="dv">1</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb40-77"><a href="#cb40-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-78"><a href="#cb40-78" aria-hidden="true" tabindex="-1"></a>title2 <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"R-squared = "</span>, <span class="fu">round</span>(<span class="fu">summary</span>(mod2)<span class="sc">$</span>r.square, <span class="dv">3</span>))</span>
<span id="cb40-79"><a href="#cb40-79" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, Y, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">pch =</span> <span class="dv">10</span>, <span class="at">lwd =</span> <span class="dv">5</span>, <span class="at">main =</span> title2)</span>
<span id="cb40-80"><a href="#cb40-80" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X, <span class="fu">fitted</span>(mod2), <span class="at">col =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb40-81"><a href="#cb40-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-82"><a href="#cb40-82" aria-hidden="true" tabindex="-1"></a>title3 <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"R-squared = "</span>, <span class="fu">round</span>(<span class="fu">summary</span>(mod3)<span class="sc">$</span>r.square, <span class="dv">3</span>))</span>
<span id="cb40-83"><a href="#cb40-83" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, Y, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">pch =</span> <span class="dv">10</span>, <span class="at">lwd =</span> <span class="dv">5</span>, <span class="at">main =</span> title3)</span>
<span id="cb40-84"><a href="#cb40-84" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X, <span class="fu">fitted</span>(mod3), <span class="at">col =</span> <span class="dv">3</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb40-85"><a href="#cb40-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-86"><a href="#cb40-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-87"><a href="#cb40-87" aria-hidden="true" tabindex="-1"></a>To help compare these three models, let's simulate a second sample from the same population. In the plots below, the regression lines from @fig-overfit were added to the plots from a second sample. Note that the regression parameters were not re-estimated using the second data set. The model parameters from the first data set were used to produce the regression lines for the second data set, so the regression lines are the same as in @fig-overfit.  </span>
<span id="cb40-88"><a href="#cb40-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-89"><a href="#cb40-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-90"><a href="#cb40-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-overfit2, echo = F, fig.cap = "Polynomial Regression Examples (With New Data)", fig.align = 'center', fig.width = 12}</span></span>
<span id="cb40-91"><a href="#cb40-91" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb40-92"><a href="#cb40-92" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4</span>)</span>
<span id="cb40-93"><a href="#cb40-93" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">20</span>, <span class="dv">0</span>, .<span class="dv">5</span>)</span>
<span id="cb40-94"><a href="#cb40-94" aria-hidden="true" tabindex="-1"></a>Y1 <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">+</span> X <span class="sc">-</span> X<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> e</span>
<span id="cb40-95"><a href="#cb40-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-96"><a href="#cb40-96" aria-hidden="true" tabindex="-1"></a><span class="co"># Three R-squared values</span></span>
<span id="cb40-97"><a href="#cb40-97" aria-hidden="true" tabindex="-1"></a>R.squared1 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>((Y1 <span class="sc">-</span> <span class="fu">fitted</span>(mod1))<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">var</span>(Y1) <span class="sc">/</span> <span class="dv">19</span></span>
<span id="cb40-98"><a href="#cb40-98" aria-hidden="true" tabindex="-1"></a>R.squared2 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>((Y1 <span class="sc">-</span> <span class="fu">fitted</span>(mod2))<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">var</span>(Y1) <span class="sc">/</span> <span class="dv">19</span></span>
<span id="cb40-99"><a href="#cb40-99" aria-hidden="true" tabindex="-1"></a>R.squared3 <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>((Y1 <span class="sc">-</span> <span class="fu">fitted</span>(mod3))<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">var</span>(Y1) <span class="sc">/</span> <span class="dv">19</span></span>
<span id="cb40-100"><a href="#cb40-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Plots</span></span>
<span id="cb40-101"><a href="#cb40-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-102"><a href="#cb40-102" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>)) </span>
<span id="cb40-103"><a href="#cb40-103" aria-hidden="true" tabindex="-1"></a>title1 <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"R-squared = "</span>, <span class="fu">round</span>(R.squared1, <span class="dv">3</span>))</span>
<span id="cb40-104"><a href="#cb40-104" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, Y1, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">pch =</span> <span class="dv">10</span>, <span class="at">lwd =</span> <span class="dv">5</span>, <span class="at">main =</span> title1)</span>
<span id="cb40-105"><a href="#cb40-105" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X, <span class="fu">fitted</span>(mod1), <span class="at">col =</span> <span class="dv">1</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb40-106"><a href="#cb40-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-107"><a href="#cb40-107" aria-hidden="true" tabindex="-1"></a>title2 <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"R-squared = "</span>, <span class="fu">round</span>(R.squared2, <span class="dv">3</span>))</span>
<span id="cb40-108"><a href="#cb40-108" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, Y1, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">pch =</span> <span class="dv">10</span>, <span class="at">lwd =</span> <span class="dv">5</span>, <span class="at">main =</span> title2)</span>
<span id="cb40-109"><a href="#cb40-109" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X, <span class="fu">fitted</span>(mod2), <span class="at">col =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb40-110"><a href="#cb40-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-111"><a href="#cb40-111" aria-hidden="true" tabindex="-1"></a>title3 <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"R-squared = "</span>, <span class="fu">round</span>(R.squared3, <span class="dv">3</span>))</span>
<span id="cb40-112"><a href="#cb40-112" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, Y1, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>, <span class="at">pch =</span> <span class="dv">10</span>, <span class="at">lwd =</span> <span class="dv">5</span>, <span class="at">main =</span> title3)</span>
<span id="cb40-113"><a href="#cb40-113" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(X, <span class="fu">fitted</span>(mod3), <span class="at">col =</span> <span class="dv">3</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb40-114"><a href="#cb40-114" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-115"><a href="#cb40-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-116"><a href="#cb40-116" aria-hidden="true" tabindex="-1"></a>This procedure, which is called out-of-sample-prediction or cross-validation, is one widely used method for comparing the quality of predictions from different models. The R-squared values of the different models in the second sample are provided to help summarize the quality of predictions.  </span>
<span id="cb40-117"><a href="#cb40-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-118"><a href="#cb40-118" aria-hidden="true" tabindex="-1"></a>We will talk more about this example in class. **Before moving on, please take a moment to write down your intuitions about what is going in  @fig-overfit and @fig-overfit2. In particular, you might consider whether the model in the right-hand panel is better than the one in the middle panel. I will ask you to share your thoughts in class.**</span>
<span id="cb40-119"><a href="#cb40-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-120"><a href="#cb40-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting the model {#sec-interpreting-the-model-9}</span></span>
<span id="cb40-121"><a href="#cb40-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-122"><a href="#cb40-122" aria-hidden="true" tabindex="-1"></a>As mentioned, polynomial terms are often added into a model as a way to address nonlinearity. When this is the case, the polynomial terms themselves are not necessarily of substantive interest -- they can be added just to "patch up" the model after assumption checking. </span>
<span id="cb40-123"><a href="#cb40-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-124"><a href="#cb40-124" aria-hidden="true" tabindex="-1"></a>We saw an example of this in @sec-worked-example-6. In that example, linear and quadratic terms for SES were entered into the model in the first block. The R-squared was interpreted for the entire block, but the interpretation of the regression coefficient for the quadratic term was not addressed. This is a pretty common way of using polynomial regression -- the polynomial terms are included so that the model assumptions (linearity) are met, but they are not necessarily interpreted beyond this.  </span>
<span id="cb40-125"><a href="#cb40-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-126"><a href="#cb40-126" aria-hidden="true" tabindex="-1"></a>However, we *can* interpret the regression slopes on the polynomial terms if we want to. This section addresses the interpretation of quadratic polynomials, which have the equation: </span>
<span id="cb40-127"><a href="#cb40-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-128"><a href="#cb40-128" aria-hidden="true" tabindex="-1"></a>$$ \widehat Y = b_0 +b_1X + b_2X^2.$$</span>
<span id="cb40-129"><a href="#cb40-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-130"><a href="#cb40-130" aria-hidden="true" tabindex="-1"></a>A similar approach applies to models with higher-order terms as well (cubics, etc.). </span>
<span id="cb40-131"><a href="#cb40-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-132"><a href="#cb40-132" aria-hidden="true" tabindex="-1"></a>Let's start with a classic example of a quadratic relationship: the Yerkes-Dodson law relating physiological arousal ("stress") to task performance, which is represented in @fig-YD. One way to interpret the law is in terms of the overall shape of the relationship. As stress goes up, so does performance -- but only up to a point, after which more stress leads to a deterioration in performance. </span>
<span id="cb40-133"><a href="#cb40-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-134"><a href="#cb40-134" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-YD, echo = F, fig.cap = "Yerkes-Dodson Law (Source: Wikipedia)", fig.align = 'center'}</span></span>
<span id="cb40-135"><a href="#cb40-135" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/YerkesDodson.png"</span>, <span class="at">dpi =</span> <span class="dv">75</span>)</span>
<span id="cb40-136"><a href="#cb40-136" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-137"><a href="#cb40-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-138"><a href="#cb40-138" aria-hidden="true" tabindex="-1"></a>The overall shape of the trend depends on only the *sign* of the regression slope on the quadratic term: </span>
<span id="cb40-139"><a href="#cb40-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-140"><a href="#cb40-140" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A U-shaped curve corresponds to a *positive* regression coefficient on $X^2$ (think of a parabola from high school math) </span>
<span id="cb40-141"><a href="#cb40-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-142"><a href="#cb40-142" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>An inverted-U-shaped curve corresponds to a *negative* regression coefficient on $X^2$</span>
<span id="cb40-143"><a href="#cb40-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-144"><a href="#cb40-144" aria-hidden="true" tabindex="-1"></a>Beyond the overall shape of the relationship, we might also want to know what level of stress corresponds to the "optimal" level of performance -- i.e., where the maximum of the curve is. This exemplifies a more complicated interpretation of a quadratic relationship, and it requires some calculus (see @sec-deriviation-9, which is optional). The value of $X$ that corresponds to the maximum (or minumum) of the quadratic curve is</span>
<span id="cb40-145"><a href="#cb40-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-146"><a href="#cb40-146" aria-hidden="true" tabindex="-1"></a>$$ X = \frac{-b_1}{2 b_2}.$$ {#eq-xmax}</span>
<span id="cb40-147"><a href="#cb40-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-148"><a href="#cb40-148" aria-hidden="true" tabindex="-1"></a>**Based on this discussion, please answer the following questions bout the Yerkes-Dodson Law, using the following model parameters. Make sure to explain how you know the answer! **</span>
<span id="cb40-149"><a href="#cb40-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-150"><a href="#cb40-150" aria-hidden="true" tabindex="-1"></a>$$ \widehat Y = 20 + 1 X - 2X^2.$$</span>
<span id="cb40-151"><a href="#cb40-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-152"><a href="#cb40-152" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**What is the overall shape of the relationship: U or inverted-U? ** </span>
<span id="cb40-153"><a href="#cb40-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-154"><a href="#cb40-154" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**What is the value of stress at which predicted performance reaches a maximum?**</span>
<span id="cb40-155"><a href="#cb40-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-156"><a href="#cb40-156" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Bonus: what is the maximum value of predicted performance? **</span>
<span id="cb40-157"><a href="#cb40-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-158"><a href="#cb40-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-159"><a href="#cb40-159" aria-hidden="true" tabindex="-1"></a><span class="fu">### Analysing polynomials* {#sec-deriviation-9}</span></span>
<span id="cb40-160"><a href="#cb40-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-161"><a href="#cb40-161" aria-hidden="true" tabindex="-1"></a>This sections shows how we get @eq-xmax and some related details. Recall from intro calculus that the extrema (i.e., minima and maxima) of a function occur when the derivative of the function is equal to zero. The derivative of the quadratic regression equation is</span>
<span id="cb40-162"><a href="#cb40-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-163"><a href="#cb40-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb40-164"><a href="#cb40-164" aria-hidden="true" tabindex="-1"></a>\frac{d}{dX} \hat Y = \frac{d}{dX} (b_0 + b_1X + b_2X^2) = b_1 + 2b_2X.</span>
<span id="cb40-165"><a href="#cb40-165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb40-166"><a href="#cb40-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-167"><a href="#cb40-167" aria-hidden="true" tabindex="-1"></a>Setting the derivative to zero</span>
<span id="cb40-168"><a href="#cb40-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb40-169"><a href="#cb40-169" aria-hidden="true" tabindex="-1"></a>b_1 + 2b_2X = 0</span>
<span id="cb40-170"><a href="#cb40-170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb40-171"><a href="#cb40-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-172"><a href="#cb40-172" aria-hidden="true" tabindex="-1"></a>and solving for $X$ </span>
<span id="cb40-173"><a href="#cb40-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-174"><a href="#cb40-174" aria-hidden="true" tabindex="-1"></a>$$ X = -\frac{b_1}{2b_2} $$</span>
<span id="cb40-175"><a href="#cb40-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-176"><a href="#cb40-176" aria-hidden="true" tabindex="-1"></a>gives the value of $X$ at which the $\hat Y$ reaches its minimum (or maximum) value. Let's call this value of $X^*$. Plugging $X^*$ into the original equation tells us the minimum (or maximum) of $\hat Y$.</span>
<span id="cb40-177"><a href="#cb40-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-178"><a href="#cb40-178" aria-hidden="true" tabindex="-1"></a>We can use the second derivative rule to determine whether $X^*$ is a minimum or maximum of $\hat Y$. </span>
<span id="cb40-179"><a href="#cb40-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-180"><a href="#cb40-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb40-181"><a href="#cb40-181" aria-hidden="true" tabindex="-1"></a>\frac{d^2}{dX^2} \hat Y = \frac{d}{dX} (b_1 + 2b_2X) = 2b_2</span>
<span id="cb40-182"><a href="#cb40-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb40-183"><a href="#cb40-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-184"><a href="#cb40-184" aria-hidden="true" tabindex="-1"></a>If this value is positive (i.e., if $b_2 &gt;0$), then the second derivative rule tells that $X^*$ is minimum, hence the curve is "U-shaped". If the value is negative (i.e., if $b_2 &lt; 0$) then its a maximum, and hence the curve is "inverted-U-shaped". </span>
<span id="cb40-185"><a href="#cb40-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-186"><a href="#cb40-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-187"><a href="#cb40-187" aria-hidden="true" tabindex="-1"></a><span class="fu">### Model building vs curve fitting</span></span>
<span id="cb40-188"><a href="#cb40-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-189"><a href="#cb40-189" aria-hidden="true" tabindex="-1"></a>Up to this point, we have discussed the use and interpretation of polynomials. In this section we consider how to build polynomial regression models in practice. </span>
<span id="cb40-190"><a href="#cb40-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-191"><a href="#cb40-191" aria-hidden="true" tabindex="-1"></a>A typical model-building process for polynomial regression might proceed as follows. </span>
<span id="cb40-192"><a href="#cb40-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-193"><a href="#cb40-193" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Enter just the linear terms into the model and examine a residual versus fitted plot. </span>
<span id="cb40-194"><a href="#cb40-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-195"><a href="#cb40-195" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>If there is evidence of non-linearity, look at the scatter plots between the outcome variable and each individual predictor to make a guess about which predictor(s) may be causing the non-linearity. </span>
<span id="cb40-196"><a href="#cb40-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-197"><a href="#cb40-197" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Add a quadratic term for a predictor of interest and examine whether there is a statistically significant increase in R-squared (see @sec-delta-rsquared-6). If there is, you have found a source of non-linearity! If not, the quadratic term is not explaining variance in the outcome variable, so you can remove it from the model. </span>
<span id="cb40-198"><a href="#cb40-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-199"><a href="#cb40-199" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Keep adding polynomial terms (quadratic terms for other predictors; higher-order terms for the same predictor) one at a time until the model assumptions looks reasonable. This might take a bit of trial and error. </span>
<span id="cb40-200"><a href="#cb40-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-201"><a href="#cb40-201" aria-hidden="true" tabindex="-1"></a>This overall approach is illustrated in the next section. However, there are a couple of important points to mention first. </span>
<span id="cb40-202"><a href="#cb40-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-203"><a href="#cb40-203" aria-hidden="true" tabindex="-1"></a><span class="ss">*  </span>Making good use of polynomial regression requires walking a fine line between curve-fitting and theory-based modeling (see @fig-overfit). Sometimes, adding polynomial terms can provide an elegant and intuitive interpretation of the relationship between two variables. But, if you find yourself adding more than a couple of polynomial terms into a model and still have unresolved issues with nonlinearity, it is probably best to consider another approach (such as piecewise regression, coming up in @sec-piecewise-9) </span>
<span id="cb40-204"><a href="#cb40-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-205"><a href="#cb40-205" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Just like with interactions, higher-order polynomial terms are often highly correlated with lower-order terms (e.g., if $X$ takes on strictly positive values, $X$ and $X^2$ will be highly correlated). Recall that if two predictors are highly correlated, this can affect their regression coefficients (@sec-ols-3) as well as their standard errors (@sec-too-many-predictors-6). In the context of polynomial regression, there are a couple of things that can be done about this.</span>
<span id="cb40-206"><a href="#cb40-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-207"><a href="#cb40-207" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Interpret $\Delta R^2$ values rather than the individual regression coefficients and their $p$-values. This is the easiest thing to do, conceptually. </span>
<span id="cb40-208"><a href="#cb40-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-209"><a href="#cb40-209" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Use "orthogonal polynomials", which are designed to ensure the different polynomial terms for the same predictor are uncorrelated (orthogonal just means uncorrelated). The result of this approach is that numerical values of the regression coefficients are not directly interpretable beyond their sign, but the t-tests of the regression coefficients can be interpreted as testing the $\Delta R^2$ for each term in the polynomial. This is conceptually more complicated than first option, but leads to the same overall conclusions. </span>
<span id="cb40-210"><a href="#cb40-210" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-211"><a href="#cb40-211" aria-hidden="true" tabindex="-1"></a>Both approaches are illustrated in the next section. </span>
<span id="cb40-212"><a href="#cb40-212" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-213"><a href="#cb40-213" aria-hidden="true" tabindex="-1"></a><span class="fu">## Worked Example {#sec-worked-example-9}</span></span>
<span id="cb40-214"><a href="#cb40-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-215"><a href="#cb40-215" aria-hidden="true" tabindex="-1"></a>In @sec-worked-example-8 we saw that applying a log-transform to the <span class="in">`Wages.Rdata`</span> example addressed non-normality of the residuals but did not do much to address nonlinearity. The summary output and diagnostic plots for the log-linear regression of wages on education are presented again below. **We will go through this example in class together, so please note any questions you have about the procedures or their interpretation and be prepared to ask them in class. **  </span>
<span id="cb40-216"><a href="#cb40-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-219"><a href="#cb40-219" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-220"><a href="#cb40-220" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data and take a look</span></span>
<span id="cb40-221"><a href="#cb40-221" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"Wages.RData"</span>)</span>
<span id="cb40-222"><a href="#cb40-222" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(wages)</span>
<span id="cb40-223"><a href="#cb40-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-224"><a href="#cb40-224" aria-hidden="true" tabindex="-1"></a><span class="co"># Create log transform of wage</span></span>
<span id="cb40-225"><a href="#cb40-225" aria-hidden="true" tabindex="-1"></a>log_wage <span class="ot">&lt;-</span> <span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb40-226"><a href="#cb40-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-227"><a href="#cb40-227" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress log_wages on educ</span></span>
<span id="cb40-228"><a href="#cb40-228" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> educ)</span>
<span id="cb40-229"><a href="#cb40-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-230"><a href="#cb40-230" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out model fit</span></span>
<span id="cb40-231"><a href="#cb40-231" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb40-232"><a href="#cb40-232" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-233"><a href="#cb40-233" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod1)</span>
<span id="cb40-234"><a href="#cb40-234" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-235"><a href="#cb40-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-236"><a href="#cb40-236" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb40-237"><a href="#cb40-237" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-238"><a href="#cb40-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-239"><a href="#cb40-239" aria-hidden="true" tabindex="-1"></a>Because there is one prominent bend in our residual vs fitted plot (at $\hat Y \approx 2.1$), let's see if adding a quadratic term to the model can improve the model fit. </span>
<span id="cb40-240"><a href="#cb40-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-241"><a href="#cb40-241" aria-hidden="true" tabindex="-1"></a>The <span class="in">`poly`</span> function in <span class="in">`R`</span> makes it  easy to do polynomial regression, without having to hard-code new variables like <span class="in">`educ^2`</span> into our dataset. In the summary output below, <span class="in">`poly(...)n`</span> denote's the $n$-th term in the polynomial. The diagnostic plots for the log-linear model with a quadratic term are also shown below.</span>
<span id="cb40-242"><a href="#cb40-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-245"><a href="#cb40-245" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-246"><a href="#cb40-246" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> <span class="fu">poly</span>(educ, <span class="dv">2</span>, <span class="at">raw =</span> T))</span>
<span id="cb40-247"><a href="#cb40-247" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb40-248"><a href="#cb40-248" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-249"><a href="#cb40-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-250"><a href="#cb40-250" aria-hidden="true" tabindex="-1"></a><span class="co"># To plot the trend we need to we first need to order the data and the predicted values ... </span></span>
<span id="cb40-251"><a href="#cb40-251" aria-hidden="true" tabindex="-1"></a>sort_educ <span class="ot">&lt;-</span> educ[<span class="fu">order</span>(educ)]</span>
<span id="cb40-252"><a href="#cb40-252" aria-hidden="true" tabindex="-1"></a>sort_fitted<span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod2)[<span class="fu">order</span>(educ)]</span>
<span id="cb40-253"><a href="#cb40-253" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(sort_educ, sort_fitted, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb40-254"><a href="#cb40-254" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod2, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-255"><a href="#cb40-255" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span>
<span id="cb40-256"><a href="#cb40-256" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-257"><a href="#cb40-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-258"><a href="#cb40-258" aria-hidden="true" tabindex="-1"></a>Writing the R output in terms of the regression model, we have:</span>
<span id="cb40-259"><a href="#cb40-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-260"><a href="#cb40-260" aria-hidden="true" tabindex="-1"></a>$$ \widehat{\log(WAGES)} = 1.862958 - 0.031492 (EDUC) + 0.003985 (EDUC)^2.$$</span>
<span id="cb40-261"><a href="#cb40-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-262"><a href="#cb40-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-263"><a href="#cb40-263" aria-hidden="true" tabindex="-1"></a>Let's start by interpreting the plots. Based on the left-hand panel, it looks like a quadratic relationship provides a reasonable representation of the data. Based on the right-hand panel, I would conclude that the apparent non-linearity in the residual vs fitted plot has been sufficiently reduced. There is still a blip at $\hat Y = 2.3$, but there are 5 data points there so I am not to worried about it. </span>
<span id="cb40-264"><a href="#cb40-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-265"><a href="#cb40-265" aria-hidden="true" tabindex="-1"></a>Turning to the summary output, there are three main take-aways: </span>
<span id="cb40-266"><a href="#cb40-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-267"><a href="#cb40-267" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>As discussed in the previous section, the sign of the quadratic term  tells us something  about the overall shape of the relationship (do you remember what that is?). However, interpreting the numerical values of the regression coefficients in a polynomial regression is not always useful. For example, using the approach to interpreting quadratic regression from @sec-interpreting-the-model-9, it turns out that the minimum predicted wages occur for someone with 3.95 years of education. The lowest level of education in the sample is 6 years, so this interpretation isn't super relevant for our example. Consequently, rather than focusing on the interpretation of the regression coefficients, it is often sufficient to focus on whether the two predictors (i.e., $EDUC$ and $EDUC^2$) together explained a significant proportion of variation in the outcome variable. This information is provided by the R-squared statistic and the F-test of R-squared in the summary output above. </span>
<span id="cb40-268"><a href="#cb40-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-269"><a href="#cb40-269" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>There are two ways to test whether the addition of the quadratic term (<span class="in">`poly(educ, 2)2 = 0.003985`</span>) improves the model. First, we can examine its test of significance. This test tells us that, controlling for the linear relationship between log-wages and education, the quadratic term is statistically significant at the .1 level (it is not statistically significant at the .05 level). Recall from @sec-chap-7 that this same information could be obtained by setting up a heirarhical model (Block 1 = linear term;  Block 2 = quadratic term) and testing the change in R-squared. For the example, the F-test of R-squared change is </span>
<span id="cb40-270"><a href="#cb40-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-273"><a href="#cb40-273" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-274"><a href="#cb40-274" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(mod1, mod2)</span>
<span id="cb40-275"><a href="#cb40-275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-276"><a href="#cb40-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-277"><a href="#cb40-277" aria-hidden="true" tabindex="-1"></a>Note that the p-value is exactly the same as the t-test of regression coefficient reported above. In both cases, it seems that we don't really need the quadratic term, based on the .05 level of significance (more on this @sec-piecewise-9)</span>
<span id="cb40-278"><a href="#cb40-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-279"><a href="#cb40-279" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Finally, compared to the model without the quadratic term, we can see the linear term (<span class="in">`poly(educ, 2)1 = - 0.031492`</span> ) is now longer statistically significant. This can happen when we add higher order terms into a model. In this example, the linear and quadratic terms are highly correlated (the correlation is over .99 in the example!). Due to this correlation, the linear terms is not statistically significant, and the quadratic term is only "marginally" significant, even though the F-test of R-squared in the summary output is fatalistically significant with $p &lt; .001$. In the next section, we will see how to avoid this issue of having highly correlated predictors in polynomial regression. </span>
<span id="cb40-280"><a href="#cb40-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-281"><a href="#cb40-281" aria-hidden="true" tabindex="-1"></a>** If you have any questions about the interpretation of the model results discussed in this section, please list them now and I will be happy to address the in class.** </span>
<span id="cb40-282"><a href="#cb40-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-283"><a href="#cb40-283" aria-hidden="true" tabindex="-1"></a><span class="fu">### Orthogonal polynomails*</span></span>
<span id="cb40-284"><a href="#cb40-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-285"><a href="#cb40-285" aria-hidden="true" tabindex="-1"></a>Before moving on, a quick (and optional) note on orthogonal vs. raw polynomials. Orthogonal polynomials are the default approach in <span class="in">`R`</span>, and they make life easier, so they are worth knowing about. </span>
<span id="cb40-286"><a href="#cb40-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-287"><a href="#cb40-287" aria-hidden="true" tabindex="-1"></a>When using orthogonal polynomials, the different polynomial terms (e.g., $X, X^2, X^3$) are transformed so that they are uncorrelated. This means that the $t$-test of each regression coefficient can be interpreted as testing the proportion of variance associated uniquely with that term of the polynomial. Basically, using orthogonal polynomials means that we don't need to do the model building stuff (e.g., sequential blocks, adding in each term one at a time)-- it's already built into the coefficients. </span>
<span id="cb40-288"><a href="#cb40-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-289"><a href="#cb40-289" aria-hidden="true" tabindex="-1"></a>The downside of orthogonal polynomials is that, beyond their sign, the regression coefficients are complicated to interpret. But, these coefficients aren't easy to interpret anyway, and we often don't care much about their exact values. If you are in a situation where you don't really care about the interpretation of the coefficients beyond the overall shape of the relationship, the orthogonal polynomials are definitely a good choice!  </span>
<span id="cb40-290"><a href="#cb40-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-291"><a href="#cb40-291" aria-hidden="true" tabindex="-1"></a>The use of orthogonal polynomials is illustrated below. You'll see that most of the output is the same as in the previous section, except the numerical value and associated tests of the regression coefficients in the summary table. In particular, the linear trend is statistically significant in the output below, because it is no longer correlated with the quadratic trend. In fact the t-test and p-value are exactly the same as the first model we fit to our example data above (the model with just the linear trend, and no quadratic trend). </span>
<span id="cb40-292"><a href="#cb40-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-293"><a href="#cb40-293" aria-hidden="true" tabindex="-1"></a>In summary, orthogonal polynomials provide a shortcut to  hierarchical model buiding with polynomials. By transforming the data so that the different terms of the polynomial are uncorrelated, we get the similar information from a single model using orthogonal polynomials as we would if we fitted a series of hierarchical models, adding each additional term into the model one at a time. </span>
<span id="cb40-294"><a href="#cb40-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-297"><a href="#cb40-297" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-298"><a href="#cb40-298" aria-hidden="true" tabindex="-1"></a>mod2a <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> <span class="fu">poly</span>(educ, <span class="dv">2</span>, <span class="at">raw =</span> F))</span>
<span id="cb40-299"><a href="#cb40-299" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb40-300"><a href="#cb40-300" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-301"><a href="#cb40-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-302"><a href="#cb40-302" aria-hidden="true" tabindex="-1"></a><span class="co"># To plot the trend we need to we first need to order the data and the predicted values ... </span></span>
<span id="cb40-303"><a href="#cb40-303" aria-hidden="true" tabindex="-1"></a>sort_educ <span class="ot">&lt;-</span> educ[<span class="fu">order</span>(educ)]</span>
<span id="cb40-304"><a href="#cb40-304" aria-hidden="true" tabindex="-1"></a>sort_fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod2a)[<span class="fu">order</span>(educ)]</span>
<span id="cb40-305"><a href="#cb40-305" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(sort_educ, sort_fitted, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb40-306"><a href="#cb40-306" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod2a, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-307"><a href="#cb40-307" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2a)</span>
<span id="cb40-308"><a href="#cb40-308" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-309"><a href="#cb40-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-310"><a href="#cb40-310" aria-hidden="true" tabindex="-1"></a>To find out more, use <span class="in">`help(poly)`</span>. A good discussion of this point is also available on StatExchange: <span class="co">[</span><span class="ot">https://stats.stackexchange.com/questions/258307/raw-or-orthogonal-polynomial-regression?r=SearchResults&amp;s=2%7C87.5473</span><span class="co">](https://stats.stackexchange.com/questions/258307/raw-or-orthogonal-polynomial-regression?r=SearchResults&amp;s=2%7C87.5473)</span></span>
<span id="cb40-311"><a href="#cb40-311" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb40-312"><a href="#cb40-312" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summary</span></span>
<span id="cb40-313"><a href="#cb40-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-314"><a href="#cb40-314" aria-hidden="true" tabindex="-1"></a>This section has addressed how to use, interpret, and implement polynomial regression. Some key points: </span>
<span id="cb40-315"><a href="#cb40-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-316"><a href="#cb40-316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We don't want to overfit the data by adding too many higher-order terms. If a quadratic or cubic polynomial doesn't sort out any issues with linearity (as diagnosed by the residual vs fitted plots), then you probably want to try something else (see next section). </span>
<span id="cb40-317"><a href="#cb40-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-318"><a href="#cb40-318" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often the overall shape of a polynomial regression is of interest. This is communicated by the sign of the regression slopes on the higher-order terms. However we aren't interested in a more specific interpretation of the regression slopes -- it can be done (#sec-interpreting-the-model-9), but it is not very common. </span>
<span id="cb40-319"><a href="#cb40-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-320"><a href="#cb40-320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Instead, we usually approach polynomial regression from the perspective of hierarchical model building -- if the higher order terms lead to a significant increase in the variance explained (i.e., R-squared change), we keep them in the model. </span>
<span id="cb40-321"><a href="#cb40-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-322"><a href="#cb40-322" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Orthogonal polynomials provide a shortcut to doing heirarhical model building with polynomials. The make our life easier, but they aren't doing anything different than the hierarchical. </span>
<span id="cb40-323"><a href="#cb40-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-324"><a href="#cb40-324" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the worked example, it turned out that despite the apparent issue with linearity in the original model, and despite the apparently better fit of the quadratic model in terms of linearity, the statistical tests actually suggested we don't need the quadratic term (using $\alpha = .05$). The next example provides another perspective on this modelling issue. </span>
<span id="cb40-325"><a href="#cb40-325" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-326"><a href="#cb40-326" aria-hidden="true" tabindex="-1"></a><span class="fu">## Piecewise regression {#sec-piecewise-9}</span></span>
<span id="cb40-327"><a href="#cb40-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-328"><a href="#cb40-328" aria-hidden="true" tabindex="-1"></a>Piecewise or segmented regression is another approach to dealing with nonlinearity. Like polynomial regression, it is  mathematically similar to interaction. Also like polynomial regression, it has a special interpretation and application that make it practically distinct from interaction. </span>
<span id="cb40-329"><a href="#cb40-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-330"><a href="#cb40-330" aria-hidden="true" tabindex="-1"></a>In the simplest case, piecewise regression involves interacting a predictor variable with a binary re-coding of itself. To illustrate how the approach works, let’s again consider our wages and education example. The scatter plot of log-wages versus education is presented again below for reference. </span>
<span id="cb40-331"><a href="#cb40-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-332"><a href="#cb40-332" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-piecewise1, echo = F, fig.cap = "The Wages Example", fig.align = 'center'}</span></span>
<span id="cb40-333"><a href="#cb40-333" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-334"><a href="#cb40-334" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-335"><a href="#cb40-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-336"><a href="#cb40-336" aria-hidden="true" tabindex="-1"></a>Consider the following reasoning about the example: </span>
<span id="cb40-337"><a href="#cb40-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-338"><a href="#cb40-338" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For people with 12 or less years of education (i.e., who did not obtain post-secondary education) the apparent relationship with wage is quite weak. This seems plausible, because if a job doesn't require a college degree, education probably isn’t a big factor in determining wages.</span>
<span id="cb40-339"><a href="#cb40-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-340"><a href="#cb40-340" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For people with more than 12 years of education, the relationship with wage seems to be stronger. This also seems plausible: for jobs that require post secondary education, more education is usually associated with higher wages. </span>
<span id="cb40-341"><a href="#cb40-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-342"><a href="#cb40-342" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To restate this as an interaction: the relationship between wage and education appears different for people who have a post-secondary education versus those who do not. </span>
<span id="cb40-343"><a href="#cb40-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-344"><a href="#cb40-344" aria-hidden="true" tabindex="-1"></a>To represent this reasoning visually we can modify @fig-piecewise1 as shown in @fig-piecewise2. This captures the basic idea behind piecewise regression -- we have different regression lines over different ranges of the predictor, and the overall regression is piecewise or segmented. The next section shows how to build this model.</span>
<span id="cb40-345"><a href="#cb40-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-346"><a href="#cb40-346" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-piecewise2, echo = F, fig.cap = "The wages example", fig.align = 'center'}</span></span>
<span id="cb40-347"><a href="#cb40-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-348"><a href="#cb40-348" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dummy variable indicating if education is at least 12 years or more</span></span>
<span id="cb40-349"><a href="#cb40-349" aria-hidden="true" tabindex="-1"></a>educ12 <span class="ot">&lt;-</span> (educ <span class="sc">&gt;</span> <span class="dv">12</span>)<span class="sc">*</span><span class="dv">1</span></span>
<span id="cb40-350"><a href="#cb40-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-351"><a href="#cb40-351" aria-hidden="true" tabindex="-1"></a><span class="co"># Interact the dummy with educ</span></span>
<span id="cb40-352"><a href="#cb40-352" aria-hidden="true" tabindex="-1"></a>mod4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> educ<span class="sc">*</span>educ12) </span>
<span id="cb40-353"><a href="#cb40-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-354"><a href="#cb40-354" aria-hidden="true" tabindex="-1"></a><span class="co"># Add fitted values to dataset</span></span>
<span id="cb40-355"><a href="#cb40-355" aria-hidden="true" tabindex="-1"></a>wages<span class="sc">$</span>fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod4)</span>
<span id="cb40-356"><a href="#cb40-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-357"><a href="#cb40-357" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort data on educ</span></span>
<span id="cb40-358"><a href="#cb40-358" aria-hidden="true" tabindex="-1"></a>wages <span class="ot">&lt;-</span> wages[<span class="fu">order</span>(educ), ]</span>
<span id="cb40-359"><a href="#cb40-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-360"><a href="#cb40-360" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb40-361"><a href="#cb40-361" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-362"><a href="#cb40-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-363"><a href="#cb40-363" aria-hidden="true" tabindex="-1"></a><span class="co"># Change color for the points with educ ≤ 12</span></span>
<span id="cb40-364"><a href="#cb40-364" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(wages, educ <span class="sc">&lt;=</span> <span class="dv">12</span>), <span class="fu">points</span>(educ, <span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>)))</span>
<span id="cb40-365"><a href="#cb40-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-366"><a href="#cb40-366" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted values for educ &gt; 12</span></span>
<span id="cb40-367"><a href="#cb40-367" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(wages, educ <span class="sc">&gt;</span> <span class="dv">12</span>), <span class="fu">lines</span>(educ, fitted, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>))</span>
<span id="cb40-368"><a href="#cb40-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-369"><a href="#cb40-369" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted values for educ ≤ 12</span></span>
<span id="cb40-370"><a href="#cb40-370" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(wages, educ <span class="sc">&lt;=</span> <span class="dv">12</span>), <span class="fu">lines</span>(educ, fitted))</span>
<span id="cb40-371"><a href="#cb40-371" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-372"><a href="#cb40-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-373"><a href="#cb40-373" aria-hidden="true" tabindex="-1"></a><span class="fu">### The piecewise model</span></span>
<span id="cb40-374"><a href="#cb40-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-375"><a href="#cb40-375" aria-hidden="true" tabindex="-1"></a>We have reasoned that the relationship between wages and education might depend on whether people have post-secondary education. We also noted that this sounds a lot like an interaction (because it is!), which is the basic approach we can use to create piecewise models. </span>
<span id="cb40-376"><a href="#cb40-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-377"><a href="#cb40-377" aria-hidden="true" tabindex="-1"></a>In order to run our piecewise regression, first we need to create a dummy-coded version of education that indicates whether a person had more than 12 years education: </span>
<span id="cb40-378"><a href="#cb40-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-379"><a href="#cb40-379" aria-hidden="true" tabindex="-1"></a>$$ EDUC_{12} = \left<span class="sc">\{</span> \begin{matrix}  </span>
<span id="cb40-380"><a href="#cb40-380" aria-hidden="true" tabindex="-1"></a>                     1 &amp; \text{if } EDUC  &gt; 12<span class="sc">\\</span> </span>
<span id="cb40-381"><a href="#cb40-381" aria-hidden="true" tabindex="-1"></a>                    0 &amp; \text{if } EDUC  \leq 12 </span>
<span id="cb40-382"><a href="#cb40-382" aria-hidden="true" tabindex="-1"></a>                \end{matrix} \right.</span>
<span id="cb40-383"><a href="#cb40-383" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb40-384"><a href="#cb40-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-385"><a href="#cb40-385" aria-hidden="true" tabindex="-1"></a>Then, we enter the original variable, the dummy-coded indicator, and their interaction into the model: </span>
<span id="cb40-386"><a href="#cb40-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-387"><a href="#cb40-387" aria-hidden="true" tabindex="-1"></a>$$ \widehat{\log(WAGES)} = b_0 + b_1 (EDUC) + b_2 (EDUC_{12}) + b_3 (EDUC \times EDUC_{12}) $$</span>
<span id="cb40-388"><a href="#cb40-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-389"><a href="#cb40-389" aria-hidden="true" tabindex="-1"></a>As we can see, the resulting model is a special case of an interaction between a continuous predictor ($EDUC$) and binary predictor ($EDUC_{12}$).</span>
<span id="cb40-390"><a href="#cb40-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-391"><a href="#cb40-391" aria-hidden="true" tabindex="-1"></a>While the above model conveys the overall idea of piecewise regression, there are also more complex approaches that will search for breakpoints, smoothly connect the lines at the breakpoints, use nonlinear functions (e.g., polynomials) for the segments, etc. We won't cover these more complex approaches here, but check out the following resource if you are interested and feel free to ask questions in class: <span class="co">[</span><span class="ot">https://rpubs.com/MarkusLoew/12164</span><span class="co">](https://rpubs.com/MarkusLoew/12164)</span></span>
<span id="cb40-392"><a href="#cb40-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-393"><a href="#cb40-393" aria-hidden="true" tabindex="-1"></a><span class="fu">### Back to the example</span></span>
<span id="cb40-394"><a href="#cb40-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-395"><a href="#cb40-395" aria-hidden="true" tabindex="-1"></a>The output for the example is provided below. Following the output, some questions are posed about the interpretation of the model.</span>
<span id="cb40-396"><a href="#cb40-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-397"><a href="#cb40-397" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Diagnostic plots for the piecewise model: </span>
<span id="cb40-398"><a href="#cb40-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-399"><a href="#cb40-399" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-piecewise3, echo = F, fig.cap = "The Wages Example", fig.align = 'center'}</span></span>
<span id="cb40-400"><a href="#cb40-400" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb40-401"><a href="#cb40-401" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod4, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-402"><a href="#cb40-402" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod4, <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-403"><a href="#cb40-403" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-404"><a href="#cb40-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-405"><a href="#cb40-405" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Summary output and estimated model (the model doesn't fit nicely on one line!): </span>
<span id="cb40-406"><a href="#cb40-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-409"><a href="#cb40-409" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-410"><a href="#cb40-410" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod4)</span>
<span id="cb40-411"><a href="#cb40-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-412"><a href="#cb40-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-413"><a href="#cb40-413" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb40-414"><a href="#cb40-414" aria-hidden="true" tabindex="-1"></a>\begin{align} \widehat{\log(WAGES)} = &amp; 1.78426  + 0.01736 (EDUC) <span class="sc">\\</span> &amp; - 0.64255 (EDUC_{12}) </span>
<span id="cb40-415"><a href="#cb40-415" aria-hidden="true" tabindex="-1"></a><span class="ss"> + </span>0.06144 (EDUC \times EDUC_{12})</span>
<span id="cb40-416"><a href="#cb40-416" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb40-417"><a href="#cb40-417" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb40-418"><a href="#cb40-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-419"><a href="#cb40-419" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Simple trends  using of the <span class="in">`emtrends`</span> function (see @sec-inference-for-interactions-5). </span>
<span id="cb40-420"><a href="#cb40-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-423"><a href="#cb40-423" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-424"><a href="#cb40-424" aria-hidden="true" tabindex="-1"></a>emmeans<span class="sc">::</span><span class="fu">emtrends</span>(mod4, <span class="at">specs =</span> <span class="st">"educ12"</span>, <span class="at">var =</span> <span class="st">"educ"</span>)</span>
<span id="cb40-425"><a href="#cb40-425" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-426"><a href="#cb40-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-427"><a href="#cb40-427" aria-hidden="true" tabindex="-1"></a>We will discuss this model together in class. It should feel a lot like deja vu from @sec-chap-5, but even more complicated due to the interpretation of $EDUC_{12}$ and the fact that the outcome is log-transformed. Fun!!! I'll ask questions below in class: </span>
<span id="cb40-428"><a href="#cb40-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-429"><a href="#cb40-429" aria-hidden="true" tabindex="-1"></a>  Note that the intercept and main effect of the binary variable <span class="in">`educ12`</span> are not of much interest in this application. </span>
<span id="cb40-430"><a href="#cb40-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-431"><a href="#cb40-431" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Using the 2-step approach from @sec-binary-continuous-interaction-5, please take a moment to work out the interpretation of main effect on $EDUC$ $b_1 = 0.01736$ and the interaction $b_3 = 0.06144$ in the model above. It might help to draw a plot like @fig-piecewise2 and label it accordingly. (The interpretation of $b_0$ and $b_2$ is not very interesting but you can work them out too if you like.)** </span>
<span id="cb40-432"><a href="#cb40-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-433"><a href="#cb40-433" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Please take a moment to write down your interpretation of the results of the piecewise regression, addressing the diagnostic plots, the parameter estimates, and the simple slopes.**</span>
<span id="cb40-434"><a href="#cb40-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-435"><a href="#cb40-435" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summary</span></span>
<span id="cb40-436"><a href="#cb40-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-437"><a href="#cb40-437" aria-hidden="true" tabindex="-1"></a>Piecewise regression is another approach to dealing with nonlinearity. It can be especially powerful when we can conceptualize the nonlinearity in as an interaction between a variable and categorical encoding of itself (e.g., the relationship between years of education and wages depends on whether you went to college.) The overall interpretation and implementation of the model is also based on the material we already covered in @sec-binary-continuous-interaction-5, so take a look at the summary of that section for additional pointers. </span>
<span id="cb40-438"><a href="#cb40-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-439"><a href="#cb40-439" aria-hidden="true" tabindex="-1"></a><span class="fu">## Workbook {#sec-workbook-9}</span></span>
<span id="cb40-440"><a href="#cb40-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-441"><a href="#cb40-441" aria-hidden="true" tabindex="-1"></a>This section collects the questions asked in this chapter. The lessons for this chapter will focus on discussing these questions and then working on the exercises in @sec-exercises-9. The lesson will **not** be a lecture that reviews all of the material in the chapter! So, if you haven't written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc. </span>
<span id="cb40-442"><a href="#cb40-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-443"><a href="#cb40-443" aria-hidden="true" tabindex="-1"></a>@sec-overfitting-9</span>
<span id="cb40-444"><a href="#cb40-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-445"><a href="#cb40-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-446"><a href="#cb40-446" aria-hidden="true" tabindex="-1"></a><span class="ss"> * </span>Please take a moment to write down your intuitions about what is going in  @fig-overfit and @fig-overfit2. In particular, you might consider whether the model in the right-hand panel is better than the one in the middle panel. I will ask you to share your thoughts in class.</span>
<span id="cb40-447"><a href="#cb40-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-448"><a href="#cb40-448" aria-hidden="true" tabindex="-1"></a>@sec-interpreting-the-model-9</span>
<span id="cb40-449"><a href="#cb40-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-450"><a href="#cb40-450" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Please answer the following questions bout the Yerkes-Dodson Law, using the following model parameters. Make sure to explain how you know the answer. </span>
<span id="cb40-451"><a href="#cb40-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-452"><a href="#cb40-452" aria-hidden="true" tabindex="-1"></a>$$ \widehat Y = 20 + 1 X - 2X^2.$$</span>
<span id="cb40-453"><a href="#cb40-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-454"><a href="#cb40-454" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>What is the overall shape of the relationship: U or inverted-U? </span>
<span id="cb40-455"><a href="#cb40-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-456"><a href="#cb40-456" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>What is the value of stress at which predicted performance reaches a maximum?</span>
<span id="cb40-457"><a href="#cb40-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-458"><a href="#cb40-458" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Bonus: what is the maximum value of predicted performance? </span>
<span id="cb40-459"><a href="#cb40-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-460"><a href="#cb40-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-461"><a href="#cb40-461" aria-hidden="true" tabindex="-1"></a>@sec-worked-example-9</span>
<span id="cb40-462"><a href="#cb40-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-463"><a href="#cb40-463" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We will go through the example in class together, so please note any questions you have about the procedures or their interpretation and be prepared to ask them in class.  </span>
<span id="cb40-464"><a href="#cb40-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-465"><a href="#cb40-465" aria-hidden="true" tabindex="-1"></a>@sec-piecewise-9</span>
<span id="cb40-466"><a href="#cb40-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-467"><a href="#cb40-467" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The output for the example is provided below. Following the output, some questions are posed about the interpretation of the model, </span>
<span id="cb40-468"><a href="#cb40-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-469"><a href="#cb40-469" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Diagnostic plots for the piecewise model: </span>
<span id="cb40-470"><a href="#cb40-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-471"><a href="#cb40-471" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = F, fig.cap = "The Wages Example", fig.align = 'center'}</span></span>
<span id="cb40-472"><a href="#cb40-472" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb40-473"><a href="#cb40-473" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod4, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-474"><a href="#cb40-474" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod4, <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-475"><a href="#cb40-475" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-476"><a href="#cb40-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-477"><a href="#cb40-477" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Summary output and estimated model: </span>
<span id="cb40-478"><a href="#cb40-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-481"><a href="#cb40-481" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-482"><a href="#cb40-482" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod4)</span>
<span id="cb40-483"><a href="#cb40-483" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-484"><a href="#cb40-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-485"><a href="#cb40-485" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb40-486"><a href="#cb40-486" aria-hidden="true" tabindex="-1"></a>\begin{align} \widehat{\log(WAGES)} = &amp; 1.78426  + 0.01736 (EDUC) <span class="sc">\\</span> &amp; - 0.64255 (EDUC_{12}) </span>
<span id="cb40-487"><a href="#cb40-487" aria-hidden="true" tabindex="-1"></a><span class="ss"> + </span>0.06144 (EDUC \times EDUC_{12})</span>
<span id="cb40-488"><a href="#cb40-488" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb40-489"><a href="#cb40-489" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb40-490"><a href="#cb40-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-491"><a href="#cb40-491" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Simple trends  using of the <span class="in">`emtrends`</span> function (see @sec-inference-for-interactions-5). </span>
<span id="cb40-492"><a href="#cb40-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-495"><a href="#cb40-495" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-496"><a href="#cb40-496" aria-hidden="true" tabindex="-1"></a>emmeans<span class="sc">::</span><span class="fu">emtrends</span>(mod4, <span class="at">specs =</span> <span class="st">"educ12"</span>, <span class="at">var =</span> <span class="st">"educ"</span>)</span>
<span id="cb40-497"><a href="#cb40-497" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-498"><a href="#cb40-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-499"><a href="#cb40-499" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Using the 2-step approach from @sec-binary-continuous-interaction-5, please take a moment to work out the interpretation of main effect on $EDUC$ $b_1 = 0.01736$ and the interaction $b_3 = 0.06144$ in the model above. It might help to draw a plot like @fig-piecewise2 and label it accordingly. (The interpretation of $b_0$ and $b_2$ is not very interesting but you can work them out too if you like.)</span>
<span id="cb40-500"><a href="#cb40-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-501"><a href="#cb40-501" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Please take a moment to write down your interpretation of the results of the piecewise regression, addressing the diagnostic plots, the parameter estimates, and the simple slopes.</span>
<span id="cb40-502"><a href="#cb40-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-505"><a href="#cb40-505" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-506"><a href="#cb40-506" aria-hidden="true" tabindex="-1"></a><span class="co"># clean up!</span></span>
<span id="cb40-507"><a href="#cb40-507" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(wages)</span>
<span id="cb40-508"><a href="#cb40-508" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-509"><a href="#cb40-509" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-510"><a href="#cb40-510" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises {#sec-exercises-9}</span></span>
<span id="cb40-511"><a href="#cb40-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-512"><a href="#cb40-512" aria-hidden="true" tabindex="-1"></a>There isn't much new in terms of R code in this chapter, but the workflows for the two types of model are pretty complicated so we review them here. You'll see that some of the plots require a lot of fiddling about, especially for the piecewise regression model. We will cover some tricks and shortcuts for producing these types plots during the open lab sessions for Assignment 4. So don't worry too much about the complicated-looking coded for the plots at this point! </span>
<span id="cb40-513"><a href="#cb40-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-514"><a href="#cb40-514" aria-hidden="true" tabindex="-1"></a>We will go through this material in class together, so you don't need to work on it before class (but you can if you want.) Before staring this section, you may find it useful to scroll to the top of the page, click on the "&lt;/&gt; Code" menu, and select "Show All Code."</span>
<span id="cb40-515"><a href="#cb40-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-516"><a href="#cb40-516" aria-hidden="true" tabindex="-1"></a><span class="fu">### Polynomial regression</span></span>
<span id="cb40-517"><a href="#cb40-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-518"><a href="#cb40-518" aria-hidden="true" tabindex="-1"></a>Let's start with the "Vanella" log-linear model for the wages examples </span>
<span id="cb40-519"><a href="#cb40-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-522"><a href="#cb40-522" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-523"><a href="#cb40-523" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the data and take a look</span></span>
<span id="cb40-524"><a href="#cb40-524" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"Wages.RData"</span>)</span>
<span id="cb40-525"><a href="#cb40-525" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(wages)</span>
<span id="cb40-526"><a href="#cb40-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-527"><a href="#cb40-527" aria-hidden="true" tabindex="-1"></a><span class="co"># Create log transform of wage</span></span>
<span id="cb40-528"><a href="#cb40-528" aria-hidden="true" tabindex="-1"></a>log_wage <span class="ot">&lt;-</span> <span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb40-529"><a href="#cb40-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-530"><a href="#cb40-530" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress it on educ</span></span>
<span id="cb40-531"><a href="#cb40-531" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> educ)</span>
<span id="cb40-532"><a href="#cb40-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-533"><a href="#cb40-533" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out model fit</span></span>
<span id="cb40-534"><a href="#cb40-534" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb40-535"><a href="#cb40-535" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-536"><a href="#cb40-536" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod1)</span>
<span id="cb40-537"><a href="#cb40-537" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod1, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-538"><a href="#cb40-538" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-539"><a href="#cb40-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-540"><a href="#cb40-540" aria-hidden="true" tabindex="-1"></a>Because there is one prominent bend in our residual vs fitted plot of the log-linear model (at $\hat Y \approx 2.1$), let's see if adding a quadratic term to the model can improve the model fit. </span>
<span id="cb40-541"><a href="#cb40-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-542"><a href="#cb40-542" aria-hidden="true" tabindex="-1"></a>The <span class="in">`poly`</span> function in <span class="in">`R`</span> makes it  easy to do polynomial regression, without having to hard-code new variables like <span class="in">`EDUC^2`</span> into our dataset. This function automatically uses orthogonal (uncorrelated) polynomials, so we don't have to worry about centering, either. The basic interpretation of the model coefficients in an orthogonal polynomial regression is the same as discussed in #sec-polynomial-9, but the "more complicated" interpretation of the model parameters is not straightforward. To find out more, use <span class="in">`help(poly)`</span>.</span>
<span id="cb40-543"><a href="#cb40-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-544"><a href="#cb40-544" aria-hidden="true" tabindex="-1"></a>The diagnostic plots for the log-linear model with a quadratic term included for education is shown below, along with the model summary. In the output, <span class="in">`poly(educ, 2)n`</span> is the $n$-th degree term in the polynomial. Orthogonal polynomials were used. </span>
<span id="cb40-545"><a href="#cb40-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-548"><a href="#cb40-548" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-549"><a href="#cb40-549" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress log_wage on a quadratic function of eduction </span></span>
<span id="cb40-550"><a href="#cb40-550" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> <span class="fu">poly</span>(educ, <span class="dv">2</span>))</span>
<span id="cb40-551"><a href="#cb40-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-552"><a href="#cb40-552" aria-hidden="true" tabindex="-1"></a><span class="co"># Model output</span></span>
<span id="cb40-553"><a href="#cb40-553" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span>
<span id="cb40-554"><a href="#cb40-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-555"><a href="#cb40-555" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostic plots</span></span>
<span id="cb40-556"><a href="#cb40-556" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb40-557"><a href="#cb40-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-558"><a href="#cb40-558" aria-hidden="true" tabindex="-1"></a><span class="co"># scatter plot with trend</span></span>
<span id="cb40-559"><a href="#cb40-559" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-560"><a href="#cb40-560" aria-hidden="true" tabindex="-1"></a><span class="co"># order the data and the predicted values ... </span></span>
<span id="cb40-561"><a href="#cb40-561" aria-hidden="true" tabindex="-1"></a>sort_educ <span class="ot">&lt;-</span> educ[<span class="fu">order</span>(educ)]</span>
<span id="cb40-562"><a href="#cb40-562" aria-hidden="true" tabindex="-1"></a>sort_fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod2)[<span class="fu">order</span>(educ)]</span>
<span id="cb40-563"><a href="#cb40-563" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(sort_educ, sort_fitted, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb40-564"><a href="#cb40-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-565"><a href="#cb40-565" aria-hidden="true" tabindex="-1"></a><span class="co"># residual versus fitted</span></span>
<span id="cb40-566"><a href="#cb40-566" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod2, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-567"><a href="#cb40-567" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-568"><a href="#cb40-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-569"><a href="#cb40-569" aria-hidden="true" tabindex="-1"></a>The F-test of R-squared change between the first and second models (not really required since we used orhtogonal polynomials, but for illustrative purposes): </span>
<span id="cb40-570"><a href="#cb40-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-573"><a href="#cb40-573" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-574"><a href="#cb40-574" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(mod1, mod2)</span>
<span id="cb40-575"><a href="#cb40-575" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-576"><a href="#cb40-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-577"><a href="#cb40-577" aria-hidden="true" tabindex="-1"></a>Illustrating the same overall approach for the cubic model: </span>
<span id="cb40-578"><a href="#cb40-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-579"><a href="#cb40-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-582"><a href="#cb40-582" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-583"><a href="#cb40-583" aria-hidden="true" tabindex="-1"></a>mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(log_wage <span class="sc">~</span> <span class="fu">poly</span>(educ, <span class="dv">3</span>))</span>
<span id="cb40-584"><a href="#cb40-584" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod3)</span>
<span id="cb40-585"><a href="#cb40-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-586"><a href="#cb40-586" aria-hidden="true" tabindex="-1"></a><span class="co"># Same plots as above, reusing variable names here</span></span>
<span id="cb40-587"><a href="#cb40-587" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb40-588"><a href="#cb40-588" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, log_wage, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-589"><a href="#cb40-589" aria-hidden="true" tabindex="-1"></a>sort_fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod3)[<span class="fu">order</span>(educ)]</span>
<span id="cb40-590"><a href="#cb40-590" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(sort_educ, sort_fitted, <span class="at">type =</span> <span class="st">"l"</span>)</span>
<span id="cb40-591"><a href="#cb40-591" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod3, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-592"><a href="#cb40-592" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-593"><a href="#cb40-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-594"><a href="#cb40-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-597"><a href="#cb40-597" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-598"><a href="#cb40-598" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(mod1, mod2, mod3)</span>
<span id="cb40-599"><a href="#cb40-599" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-600"><a href="#cb40-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-601"><a href="#cb40-601" aria-hidden="true" tabindex="-1"></a><span class="fu">### Write up</span></span>
<span id="cb40-602"><a href="#cb40-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-603"><a href="#cb40-603" aria-hidden="true" tabindex="-1"></a>This section illustrates the write-up for the quadratic regression based on heirarhical modeling and based on orthogonal polynomials. In practice, just pick one of these. </span>
<span id="cb40-604"><a href="#cb40-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-605"><a href="#cb40-605" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>*Based on heirachical modeling.* Starting with a model containing only the linear term, years of education explained about 21.6% of the variance in log-wages ($R^2 = .216, F(2, 397) = 54.77, p &lt; .001$). Adding a quadratic term to address potential nonlinearity, it was found that only an additional .5% of variance was explained ($\Delta R^2 = .005, F(1, 293) = 2.93, p = 0.088$). </span>
<span id="cb40-606"><a href="#cb40-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-607"><a href="#cb40-607" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>*Based on orthogonal polynomials* Regressing log wages on a  second-degree orthogonal polynomial function of wages, it was found that the linear term was statistically significant at the .05 level ($b = 4.36, t(396) = 10.33, p &lt; .001$), but the quadratic term was not ($b = 0.72, t(396) = 1.71, p = 0.088$).</span>
<span id="cb40-608"><a href="#cb40-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-609"><a href="#cb40-609" aria-hidden="true" tabindex="-1"></a><span class="fu">### Piecewise regression </span></span>
<span id="cb40-610"><a href="#cb40-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-611"><a href="#cb40-611" aria-hidden="true" tabindex="-1"></a>Moving on, let's consider the piecewise model from @sec-piecewise-9</span>
<span id="cb40-612"><a href="#cb40-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-613"><a href="#cb40-613" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig.cap = "The wages example", fig.align = 'center'}</span></span>
<span id="cb40-614"><a href="#cb40-614" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dummy variable indicating if education is at least 12 years or more</span></span>
<span id="cb40-615"><a href="#cb40-615" aria-hidden="true" tabindex="-1"></a>educ12 <span class="ot">&lt;-</span> (educ <span class="sc">&gt;</span> <span class="dv">12</span>)<span class="sc">*</span><span class="dv">1</span></span>
<span id="cb40-616"><a href="#cb40-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-617"><a href="#cb40-617" aria-hidden="true" tabindex="-1"></a><span class="co"># Interact the dummy with educ</span></span>
<span id="cb40-618"><a href="#cb40-618" aria-hidden="true" tabindex="-1"></a>mod4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">~</span> educ<span class="sc">*</span>educ12) </span>
<span id="cb40-619"><a href="#cb40-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-620"><a href="#cb40-620" aria-hidden="true" tabindex="-1"></a><span class="co"># The model output</span></span>
<span id="cb40-621"><a href="#cb40-621" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod4)</span>
<span id="cb40-622"><a href="#cb40-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-623"><a href="#cb40-623" aria-hidden="true" tabindex="-1"></a><span class="co"># The simple trends</span></span>
<span id="cb40-624"><a href="#cb40-624" aria-hidden="true" tabindex="-1"></a>trends <span class="ot">&lt;-</span> emmeans<span class="sc">::</span><span class="fu">emtrends</span>(mod4, <span class="at">specs =</span> <span class="st">"educ12"</span>, <span class="at">var =</span> <span class="st">"educ"</span>)</span>
<span id="cb40-625"><a href="#cb40-625" aria-hidden="true" tabindex="-1"></a>emmeans<span class="sc">::</span><span class="fu">test</span>(trends)</span>
<span id="cb40-626"><a href="#cb40-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-627"><a href="#cb40-627" aria-hidden="true" tabindex="-1"></a><span class="co"># The diagnostic plots</span></span>
<span id="cb40-628"><a href="#cb40-628" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb40-629"><a href="#cb40-629" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod4, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-630"><a href="#cb40-630" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod4, <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-631"><a href="#cb40-631" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-632"><a href="#cb40-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-633"><a href="#cb40-633" aria-hidden="true" tabindex="-1"></a>We can still see some evidence of heteroskedasticity in the residual versus fitted plot, so the last step is to use heteroskedasticity-corrected standard errors to ensure we are making the right conclusions about statistical significance</span>
<span id="cb40-634"><a href="#cb40-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-637"><a href="#cb40-637" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-638"><a href="#cb40-638" aria-hidden="true" tabindex="-1"></a><span class="do">## Make sure the required pacakges are installed</span></span>
<span id="cb40-639"><a href="#cb40-639" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("car")</span></span>
<span id="cb40-640"><a href="#cb40-640" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("lmtest")</span></span>
<span id="cb40-641"><a href="#cb40-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-642"><a href="#cb40-642" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1. Use "hccm" to get the HC SEs for our piecewise model </span></span>
<span id="cb40-643"><a href="#cb40-643" aria-hidden="true" tabindex="-1"></a>hcse <span class="ot">&lt;-</span> car<span class="sc">::</span><span class="fu">hccm</span>(mod4)</span>
<span id="cb40-644"><a href="#cb40-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-645"><a href="#cb40-645" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2. Use "coeftest" to compute t-tests with the HC SEs</span></span>
<span id="cb40-646"><a href="#cb40-646" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">coeftest</span>(mod4, hcse)</span>
<span id="cb40-647"><a href="#cb40-647" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-648"><a href="#cb40-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-649"><a href="#cb40-649" aria-hidden="true" tabindex="-1"></a>The next bit is optional. It shows how to produce the piecewise regression plot, which takes quite a bit of messing about with R...Let me know if you find an easier way to do this (in base R). </span>
<span id="cb40-650"><a href="#cb40-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-653"><a href="#cb40-653" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb40-654"><a href="#cb40-654" aria-hidden="true" tabindex="-1"></a><span class="co"># Building the piecewise regression plot -- yeeesh</span></span>
<span id="cb40-655"><a href="#cb40-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-656"><a href="#cb40-656" aria-hidden="true" tabindex="-1"></a><span class="co"># Add fitted values to dataset</span></span>
<span id="cb40-657"><a href="#cb40-657" aria-hidden="true" tabindex="-1"></a>wages<span class="sc">$</span>fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod4)</span>
<span id="cb40-658"><a href="#cb40-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-659"><a href="#cb40-659" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort data on educ</span></span>
<span id="cb40-660"><a href="#cb40-660" aria-hidden="true" tabindex="-1"></a>wages <span class="ot">&lt;-</span> wages[<span class="fu">order</span>(educ), ]</span>
<span id="cb40-661"><a href="#cb40-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-662"><a href="#cb40-662" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb40-663"><a href="#cb40-663" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb40-664"><a href="#cb40-664" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(educ, <span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>), <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb40-665"><a href="#cb40-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-666"><a href="#cb40-666" aria-hidden="true" tabindex="-1"></a><span class="co"># Change color for the points with educ ≤ 12</span></span>
<span id="cb40-667"><a href="#cb40-667" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(wages, educ <span class="sc">&lt;=</span> <span class="dv">12</span>), <span class="fu">points</span>(educ, <span class="fu">log</span>(wage <span class="sc">+</span> <span class="dv">1</span>)))</span>
<span id="cb40-668"><a href="#cb40-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-669"><a href="#cb40-669" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted values for educ &gt; 12</span></span>
<span id="cb40-670"><a href="#cb40-670" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(wages, educ <span class="sc">&gt;</span> <span class="dv">12</span>), <span class="fu">lines</span>(educ, fitted, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>))</span>
<span id="cb40-671"><a href="#cb40-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-672"><a href="#cb40-672" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted values for educ ≤ 12</span></span>
<span id="cb40-673"><a href="#cb40-673" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(<span class="fu">subset</span>(wages, educ <span class="sc">&lt;=</span> <span class="dv">12</span>), <span class="fu">lines</span>(educ, fitted))</span>
<span id="cb40-674"><a href="#cb40-674" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb40-675"><a href="#cb40-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-676"><a href="#cb40-676" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = F}</span></span>
<span id="cb40-677"><a href="#cb40-677" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span>
<span id="cb40-678"><a href="#cb40-678" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(wages)</span>
<span id="cb40-679"><a href="#cb40-679" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb40-680"><a href="#cb40-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-681"><a href="#cb40-681" aria-hidden="true" tabindex="-1"></a><span class="fu">### Write up</span></span>
<span id="cb40-682"><a href="#cb40-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-683"><a href="#cb40-683" aria-hidden="true" tabindex="-1"></a>Using a piecewise regression model with heteroskedasticity consistent standard errors, it was found that the relationship between log-wages and years of education depended on whether a person attended post-secondary education ($b = 0.061, t(396) = 2.50, p = .013$). Using simple trends, it was found for people with 12 or less years of education, the relationship between log-wages and years of education was not significant ($b = 0.017, t = (396) = .804, p = .42$), whereas the relationship was significant for people with at least some post-secondary education ($b = 0.078, t = (396) = 4.68, p &lt; .001$). The results indicate that, simply for those with some post-secondary education, each additional year of education was associated with a 8.1% increase in hourly wages ($\exp(0.078) - 1 = .081$). Due to a software limitation, the simple trends did not utlize heteroskedastic consistent standard errors. </span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>