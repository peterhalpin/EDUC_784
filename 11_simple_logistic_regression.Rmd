# Logistic Regression {#chapter-11}

<!-- baaaaaah! you need to review ALL of the workbook stuff before first lesson so you remember the numerical values for the examples. And you need to review ALL of the exercise stuff for the second lesson, so you can get through the example in order. --> 

```{r, echo = F}
# Clean up check
rm(list = ls())


button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```

The topic we address in this chapter is logistic regression. Like the previous two chapters, logistic regression is an extension of multiple linear regression to situations in which the "standard" model does not directly apply. This time, the extension is to binary *outcome* variables. 

There are many situations in which the outcome of interest can be thought of as a binary or “yes / no” or “true / false” outcome:

* Death (the original logistic outcome; [Berkson, 1944](https://doi.org/10.2307%2F2280041))
* Onset of disease or condition
* Employment status
* College enrollment
* Passing a course or completing a credential
* Provide a correct response to a test question
* ... 

Just like with linear regression, we often we want to relate these types of binary variables to predictors, such as medical history, family background, personal characteristics, etc. That is what logistic regression does. 

The main theme of this chapter is also an extension of the previous two chapters. We have seen a general strategy for how to deal with data that do not "fit" the assumptions of linear regression: 

1. Transform one or more variables so that the data *do* fit the assumptions linear regression. 
2. Run the analysis as usual.
3. Then work out the interpretation of results in terms of the original variable(s). 

We will see this basic approach again in this chapter. In fact, its kind of a general-purpose hack for quantitative research -- when you are faced with a problem you don't know how to deal with, turn it into something you do know how to deal with. Certainly this is not the most creative approach, but it has the advantage of letting us "port over" many of the tools we have developed in one context (regression with a continuous outcome) into a new context (regression with a binary outcome). 

In terms of statistical modeling, the move to binary outcome variables is a pretty big deal. Everything we have done up until now has focused on OLS regression, using the same basic principles we discussed in Chapters \@ref(chapter-2) and \@ref(chapter-4). However, logistic regression takes us into the wider framework of *generalized linear models* (GLMs), which are estimated using maximum likelihood (ML) rather than OLS. Thus we will need to start at the "ground floor" to build up our knowledge of logistic regression, which then provides a stepping stone to GLMs, which can additionally handle other types of outcome variables (e.g., count data, ordered categorical data). 

Since we are starting with a new modeling approach, let's kick things off with a new example. 

## The CHD example {#chd-example-11}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

As a working example, we will use data contained in the file `CHD.RData` to explore the relationship between age in years ("age") and evidence (absence or presence) of coronary heart disease ("chd").  The data set contains 100 cases. Respondents' ages range from 20 to 69, while evidence of CHD is coded 0 when it is absent and 1 when it is present. A sample of 20 cases is shown  below. (Source: Applied Logistic Regression by David W. Hosmer and Stanley Lemeshow, 1989, John Wiley and Sons.) 

```{r}
load("CHD.RData")
attach(chd.data)
knitr::kable(list(chd.data[sample(1:100, 10),2:3], chd.data[sample(1:100, 10),2:3]), row.names = F, caption = "The CHD example")
```

If we regress CHD on age using linear regression (i.e., the "linear probability model"), our diagnostic plots and summary output look like this: 

```{r, lpm, echo = F, fig.cap = "Linear probability model with CHD example", fig.align = 'center', fig.width = 12}
mod1 <- lm(chd ~ age, data = chd.data)
par(mfrow = c(1, 2))
plot(mod1, 1)
plot(mod1, 2)
summary(mod1)
```

**Before moving on, please take a moment to write down you conclusions (and rationle) about whether the assumptions of linear regression are met for these data.**

I'll note that researchers who have a strong preference for OLS methods (AKA economists) often approach binary data using the linear probability model. As we can see, this approach violates all of the assumptions of linear regression, can lead to predicted probabilities outside of the range [0,1], produces incorrect standard errors for model parameters, and is, in a word, wrong. Yet, despite all this, it works pretty well in some situations and has the benefit of being easier to interpret than logistic regression. We will consider the situations in which the linear probability model is "close enough" at the end of the next section. 


## Logit & logistic functions {#logit-11}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

The general game plan for dealing with a binary outcome is to transform it into a different variable that is easier to work with, run the analysis, and then "reverse-transform" the model coefficients so that they are intepretable in terms of the original binary variable. This strategy should sound familiar from Chapter \@ref(chapter-9) -- it's the same overall approach we used for log-linear regression. Also in common with Chapter \@ref(chapter-9), we are going to use logs and exponents as the main workhorse for this approach (that is where the "log" in logistic comes from).

However, the overall strategy for transforming the $Y$ variable in logistic regression is a bit more complicated than the log-linear model. So, it is helpful to start wit an overall "roadmap".  

* Step 1 (from binary to probability). First, we are going to work with probabilities rather than the original binary variable. In terms of our example, we are going to shift focus from whether or not a person has CHD to the *probability* of a person having CHD. 

* Step 2 (from probability to logistic). The logistic function is widely-used model for probabilities. In terms of our example, we are going to use the logistic function to relate the probability of a person having CHD to their age. 

* Step 3 (from logistic to logit). The logistic function has a nice interpretation, but it is not a linear function of age. So, we are going to transform it into something that is linear in age, which will let us "port over" a lot of what we have learned about linear models. Actually, the reason we choose the logistic function as a model of probability is because this transform is relatively straightforward and can be "undone" afterwards when interpreting the model coefficients, just like with log-linear regression. The transformation two steps:

    * Step3A (probability to odds). First we transform the probability of having CHD into the *odds* of having CHD. If $p$ denotes probability then odds are just $p / (1-p)$. We will spend a while talking about how to interpret odds below. 
    
    * Step 3B (odds to logit). Then we take the log of the odds, which is called the *logit*. The logit turns out to be a linear function of age, so we can model the relationship between age and the logit of CHD in a way that is very similar to regular linear regression. 
    
So, that's the overall approach to dealing with a binary variable in logistic regression. Clear as mud, right? Don't worry, we will walk through each step in the following subsections. If you find yourself getting lost in the details, it can be helpful to refer back to this overall strategy. In short, the overall game plan is: 

\[ \text{binary outcome} \rightarrow \text{probability} \rightarrow \text{logistic} \rightarrow \text{logit (log odds) }  \]
  
Once we have all these concepts in play, we can start doing logistic regression.

### From binary to probability

The following table presents the example data in terms of the proportion of cases with CHD, broken down by age groups. The first column shows the age groups, the second shows the number of cases without CHD, the third shows the number of cases with CHD, and the last column shows the proportion of cases with CHD. 

```{r, props, echo = T, fig.cap = "From a binary variable to proportions", fig.align = 'center'}
knitr::include_graphics("images/props.png")
```

Recall that a proportion is computed as the number of cases of interest over the total number of cases. In terms of the table above: 

\[ p(CHD = 1) = \frac{ N_1}{N_0 + N_1 } 
(#eq:prob)\]

were $N_1$ denotes the number of cases with CDH, and $N_0$ is the number of cases without. 

The number $p(CHD = 1)$ can be interpreted in many ways, which leads to a lot of terminology here. 

* The *proportion* of cases in our sample with CHD.
* If we multiply by 100, it is the *percentage* of cases with CHD (i.e., cases per 100) in our sample.
* If we multiply by a number other than 100 (say 1000), it is the *rate* of CHD (e.g., cases per 1000) in our sample.
* Since a proportion is just the mean of binary variable, it is the *mean* CHD in our sample.
* And finally, since proportions are one interpretation of probability, it is the *probability* of CHD in our sample. 

You might hear all of these terms (i.e., proportion, percentage, rate, mean, probability) used in connection with logistic regression. But, they are all just different ways of interpreting the rightmost column of Figure \@ref(fig:props). I will try to make a point of using all of these terms so you get used to interpreting them in this context :)  

Another concept that will be useful for interpreting our data is *odds*. Odds are closely related to, but not the same as, probability. The figure below adds the odds of having CHD to Figure \@ref(fig:props). 

```{r, odds, echo = T, fig.cap = "Proportions and odds", fig.align = 'center'}
knitr::include_graphics("images/odds.png")
```

As shown in the table, the odds are also a function of the two sample sizes, $N_1$ and $N_0$:

\[ \text{odds}(CHD = 1) = \frac{ N_1}{N_0}. 
(#eq:odds)
\]

Let's take a moment to compare the interpretation of probability versus odds. 

* The first row of the table tells us that the probability of having CHD in your 20's is "1 in 10". Loosely, this means that for every 10 people in their 20s, one of them will have CHD. 

* By contrast, the odds of having CHD in your twenties is "1 to 9". Roughly, this means that for every person in their twenties with CHD, there are nine without CHD. 

Clearly, probabilities and odds are just two different ways of packaging the same information. The following equations shows the relation between odds and probability (you can derive these from Equations \@ref(eq:prob) and \@ref(eq:odds))

\begin{align}
p(CHD = 1) & = \frac{\text{odds}(CHD = 1)}{1 + \text{odds}(CHD = 1)} \\ \\
\text{odds}(CHD = 1) & = \frac{p(CHD = 1)}{1 - p(CHD = 1)} 
(#eq:p2o)
\end{align}

We will see these relations again shortly. But, before moving on, let's get some more practice interpreting odds and probabilities using the data in Figure \@ref(fig:odds). **Please write down your answers to the following questions and be prepared to share them in class. For each question provide a verbal interpretation of the numerical answer (e.g, odds of 2 to 1 means that for every two people with a trait, there is one without.) **

  1. What is the probability of a person in their 40s having CHD? 
  2. What are the odds of a person in their 40s having CHD? 
  3. What is the probability of someone in their 50s **not** having CHD? 
  4. What are the odds of someone in their 50s **not** having CHD? 
  5. What is probability of having CHD in your 40s, compared to your 30s? (e.g., is 3 times higher? 4 times higher?)
  6. What are the odds of having CHD in your 40s, compared to your 30s? 
  
The answers hidden below (use Show Code button), but please try out the questions yourself first! 

```{r}
# 1. .39, so about 40% of people
# 2. 11/17, so for 11 people with CHD there are 17 without
# 3. 1 - .72 = .28, so about 28% of people 
# 4. (18/7)^-1 = 7/18, so 7 out ever 18 people
# 5. .39 / .19 ~= 2, so the probability of having CHD in your 40s is about 2 times higher than the probability of having CHD in your 30s. This is called a relative risk, or a risk ratio. 
#6. (11/17)/(5/22) ~= 2.8, so the odds of having CHD in your 40s is about 2.8 times higher than the ods of having CHD in your 30s. This is called an odds ratio. 
```

### From probability to logistic 

On thing you may have noted about the CHD data is that the proportion of cases with CHD increases with age. This relationship is shown visually in Figure \@ref(fig:chd2).

```{r, chd2, echo = T, fig.cap = "Proportion of cases with CHD as a function of age", fig.align = 'center'}
prop <- tapply(chd, catage, mean)
years <- unique(catage)*10
plot(years, prop, type = "l", lwd = 2, col = "#4B9CD3", ylab = "p(CHD =1)", xlab = "Age categories")
```
Looking at the plot, we might suspect that the relationship between the probability of CHD and age is non-linear. In particular, we know that probabilities cannot take on values outside of the range (0, 1), so the relationship is going to have to "flatten out" in the tails. For example, even if you are a baby, your probability of having CHD cannot be less than 0. And, even if you are centenarian, the probability can't be great than 1. 

Based on this reasoning, we know that the relationship between age and the rate of CHD should take on a sort of  "S-shaped" curve or "sigmoid". This S-shape is hinted at in Figure \@ref(fig:chd2) but is not very clear. Some clearer examples are shown in Figure \@ref(fig:sigmoids). 

```{r, sigmoids, echo = T, fig.cap = "Examples of sigmoids", fig.align = 'center'}
logistic <- function(x, a, b){exp(a*x + b) / (1 + exp(a*x + b))}
x <- seq(-5, 5, by = .1)
plot(x, logistic(x, 1, 0), type = "l", lwd = 2, col = 2, ylab = "logistic")
points(x, logistic(x, .75, -1.5), type = "l", lwd = 2, col = 3, ylab = "logistic")
points(x, logistic(x, 1.5,- 1), type = "l", lwd = 2, col = 4, ylab = "logistic")
points(x, logistic(x, 3, 2), type = "l", lwd = 2, col = 5, ylab = "logistic")
```

The mathematical equation used to create these S-shaped curves is called the logistic function, the namesake of logistic regression. Returning to our example, we can see in Figure \@ref(fig:chd3) that the logistic function provides a reasonable approximation for the relationship between the rate of CHD and age. 

```{r, chd3, echo = T, fig.cap = "Proportion of cases with CHD, data versus logistic", fig.align = 'center'}
par(mfrow = c(1, 2))
plot(years, prop, type = "l", lwd = 2, col = "#4B9CD3", ylab = "p(CHD =1)", xlab = "Age categories")
plot(20:60, logistic(20:60, .12, -5.2), col = "#4B9CD3", ylab = "logistic", xlab = "Age in years")
```

One important thing to notice about Figure \@ref(fig:chd3) is that the plot on the left required re-coding age into a categorical variable and computing the proportion of cases with CHD in each age category (see Figure \@ref(fig:props)). However, the logistic plot on the right did not require categorizing age. So, one advantage of using the logistic function is that we can model the probability of CHD as a function of age "directly", without having to categorize our predictor variables. And, just like with linear regression, we can easily extend this approach to multiple predictor variables.

The take home message of this section is that the logistic function is a nice way to model how a proportion depends on a continuous variable like age. Next, we'll talk about the math of the logistic function in a bit more detail. 


### Logistic to log odds (logit) 

The formula for the logistic function (i.e., the function that produced the curves in Figures \@ref(fig:sigmoids)) is

\[ p = \frac{\exp(x)}{1 + \exp(x)}. 
(#eq:logistic)
\] 


This function maps the variable $x$ onto the interval $(0, 1)$. In Figure \@ref(fig:chd3) we saw that the logistic function can provide a nice model for probabilities. And, because of this, the logistic function is also non-linear function of $x$ (i.e., it is sigmoidal or S-shaped). 

However, a nice thing about the logistic function is that we can transform it into a linear function of $x$. Since we already know how to deal with linear functions (that is what this whole course has been about!), transforming the logistic into a linear function of $x$ will let us port over a lot of what we know about linear regression to situations in which the outcome variable is binary. In fact, this was part of the motivation for choosing the logistic function in the first place, rather than some other S-shaped curve.

So, let's see how to get from our S-shaped logistic function of $x$ to a linear function of $x$. Algebra with \@ref(eq:logistic) shows that we can re-express the logistic function in terms of the odds: 

\[ \frac{p}{1- p} = \exp(x).
(#eq:exp-odds)
\]

Note that Equations \@ref(eq:logistic) and \@ref(eq:exp-odds) directly parallel the two expressions in Equation \@ref(eq:p2o). The only difference is that, in the logistic model, the odds are represented as an exponential function of the variable $x$, which is what Equation \@ref(eq:exp-odds) is telling us. 

In order to turn Equation \@ref(eq:exp-odds) into a linear function of $x$, all we need to do is get rid of the exponent. Do you remember how?? That's right, just take the log (see Section \@ref(math-review-9)): 

\[  \log\left(\frac{p}{1- p}\right) = x
(#eq:logit)
\]

This equation is telling us that the log of the odds is linear in $x$. The log-odds is also called the *logit*. The next section discusses how to interpret the logit. 

### Interpretation 

The relationship between the logistic, odds, and logit is shown in Figure \@ref(fig:logit). 

```{r, logit, echo = T, fig.cap = "Logistic, odds, and logit", fig.align = 'center'}
knitr::include_graphics("images/logit.png")
```

* The left-hand panel shows the logistic function, which is our "intuitive-but-nonlinear" model for probabilities. In terms of our example, this panel is saying that the probability of having CHD is a logistic or S-shaped function of age. 

* The middle panel shows that the odds are an exponential function of $x$. In terms of our example, this means that the odds of having CHD are an exponential function of age. This is the main assumption of the logistic model, and we will revisit this assumption again below. 

<!-- in section \@ref(assumption-checking-11). -->

* Finally, the right-hand panel shows the "not-really-intuitive-but-definitely-linear" model for the logit. In terms of our example, the logit of having CHD is a linear function of age. 

The logit is going to be our workhorse for logistic regression. In Section \@ref(simple-11), we will replace the variable $x$ with a simple regression model $a + bX$ to get simple logistic regression. In Section \@ref(multiple-11) we will extend simple logistic regression to multiple logistic regression, just like we did for multiple linear regression. 

Although the logit is the workhorse, we generally don't want to work with the logit when it comes time to interpret the results. The situation here is a lot like log-linear regression (see Chapter \@ref(chapter-9)). In log-linear regression, we treated $\log(Y)$ as a linear function of our predictor variable(s). However, we didn't want to interpret the model in terms of $\log(Y)$, because, well, who thinks in log units? Instead we wanted an interpretation in terms of the original outcome, $Y$.

Now, you might have already noted that the relationship between the logit (i.e., $\log(\text{odds})$) and $\text{odds}$ in logistic regression is the same as the relationship between $\log(Y)$ and $Y$ in log-linear regression. The parallel between the two model is as follows: 

* In the log-linear model we interpreted a $b$ unit increase in $\log(Y)$ in terms of an $(\exp(b) - 1) \times 100\%$ increase in $Y$ (see Section \@ref(interpretation-9)). 

* In logistic regression we will interpret a $b$ unit increase in $\text{logit}(Y)$ in terms of an $(\exp(b) - 1) \times 100\%$ times increase in $\text{odds}(Y)$. 

So, while we use the logit function for modeling, we typically use the odds for interpretation, with the overall situation being directly parallel to log-linear modeling.

Recently, some authors have argued that people don't really know how to interpret odds properly. These authors suggest that we interpret the logistic model in terms of probabilities, rather than odds. We will discuss how to do this as well.

### Pop quiz 

Before moving, lets nail down the relation between probability, odds, and logits. Figure \@ref(fig:logit-table) presents the relationship in tabular form. 

```{r, logit-table, echo = F, fig.cap = "Logistic, odds, and logit", fig.align = 'center'}
knitr::include_graphics("images/logit-table.png")
```

**I will asks some questions along the following lines in class.**

  * If logit < 0 then probability <  ? and odds < ? 
  * What has a larger logit value: probability of .9 or odds of .9?  
  * Let's assume that a probability of $p$ corresponds to a  $\text{logit}$ of $x$.  What is the logit corresponding to $1-p$? (Hint, try some numerical examples from the table). 
  


### Summary

At this point we have covered the overall logic of how we can model a binary outcome variable like CHD in terms of the logistic function. The overall situation is very similar to, but a bit more complicated than, log-linear regression. The main take aways are

* We use the logit (log-odds) for statistical analysis, because it results in a linear function, and we already know how to deal with linear functions. 

* We use the odds for interpretation, because the logistic model leads to proportional change in the odds, in the same way that the log-linear model leads to proportional change in $Y$. 

* We can also use probabilities for interpretation, but, since the logistic model implies that probabilities are non-linear (sigmoidal), things can get a bit complicated with this approach. 


## Simple logistic regression {#simple-11}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

In this section we move onto logistic regression proper. For the CHD example, the model we are interested in is 

\[ \text{logit}(CHD) = a + b (\text{age}). \]

We are going to skip a few steps and go right into the interpretation of the R output. Once we know how to interpret the output, we will loop back to discuss technicals details of estimation and inference in the following sections. 

The summary R output for the example is below. The focus for now is just the interpretation of the values under the "Estimate" heading. 

```{r}
mod2 <- glm(chd ~ age, family = binomial, data = chd.data)
summary(mod2)
```

Plugging the estimates into our logit model, we have the following equation

\[ \text{logit}(CHD) = -5.03  + .11 (\text{age}). \]

The "literal" interpretation of this equation is: 

* When $\text{age} = 0$, $\text{logit}(CHD) = -5.03$.
* Each unit of increase in age (i.e., each additional year), is associated with a .11 unit increase in $\text{logit}(CHD)$. 

While this interpretation is perfectly correct, most applied audiences are not going to know how to interpret   $\text{logit}(CHD)$. So, instead, we often work with the odds. In particular, the logistic regression model implies

\[ \frac{\text{odds} (X+1)}{\text{odds}(X)} = \exp(b)
(#eq:OR)
\]

where $\text{odds}(X)$ are the odds of the outcome associated with a given value of the predictor $X$. Equation \@ref(eq:OR) is called the *odds ratio* (abbreviated OR) associated with a one-unit increase in $X$. If you refer back to section \@ref(derivation-9), you can see we are using the exact same approach from log-linear regression, but in Equation \@ref(eq:OR) we interpret the regression coefficient in term of the odds rather than the $Y$ variable itself. Specifically, we interpret the exponentiated regression coefficient as the predicted odds ratio corresponding to a one unit change in $X$. 

For the CHD example, the OR is

\[ \exp(b) = \exp(.11) = 1.116278 \] 

This means that each additional year a person ages is associated with an OR of 1.11. 

Just like the log-linear model, we can also report the results of our analysis in terms of proportional (or percentage) change in the odds of CHD associated with each additional year of age: 

* $(\exp(.11) - 1) \times 100 = 11.67\%$ 

This means that the predicted odds of CHD increase 11.67% for each additional year of age. 

Whether you use relative size (i.e.,odds ratio) or relative change (percent change) to report the results of logistic regression is up to you. In many fields, it is conventional to reports the odds ratios in tables, but to use percent change when writing about results in a sentence.

Before moving, **please practice your interpretation of the OR in simple logistic regression using the following examples**

* If $b=0$ what is the OR equal to? What is the percent change in the odds for a one unit increase in $X$? 
* If $b=.25$ what is the OR equal to? What is the percent change in the odds for a one unit increase in $X$? 
* If $b=-.025$ what is the OR equal to? What is the percent change in the odds for a 10 unit increase in $X$? 
* If the odds increase 100% for a one unit increase in $X$, what $b$ equal to? 

Answers hidden below (use Show Code button), but please try out the questions yourself first! 

```{r}
# 1. OR = exp(0) = 1 and percent change equals (exp(0) - 1) X 100 = 0%. So, "no relationship" means OR = 1. 
# 2. OR = exp(.25) = 1.28 and percent change equals (exp(.25) - 1) X 100 = 28% increase
# 3. OR = exp(-.025) = 0.97 and percent change for one unit equals (exp(-.025) - 1) X 100 = -2.47%. For 10 units of change, multiply by 10, which gives 24.69% decrease (negative sign is decrease). 
# 4. exp(b) - 1) X 100 = 100 --> exp(b) = 2 --> b = log(2) = .69
```


### Other interpretations: Predicted probabilities
Another way to interpret the logistic model is in terms of the predicted probabilities, which are plotted below for the example data. 

```{r, pred-prob, echo = T, fig.cap = "Predicted probabilities", fig.align = 'center'}
visreg::visreg(mod2, xvar = "age", scale = "response")
```

Using this plot, we can read off the probability of CHD for any given age. We might also want to report the probability of CHD for two or more chosen ages, which is an example of the MERV approach to marginal effects (see Section  \@ref(inference-for-interactions-6)):  

```{r}
library(emmeans)
emmeans(mod2, specs = "age", at = list(age = c(20, 40)), type = "response")
```

The ratio of two probabilities is often called the *risk ratio* or the *relative risk*. So, we could also say that the risk ratio of CHD for someone who in their 40s as compared to someone who is 20 is .2947 / .0435 = 6.77. Otherwise stated, the risk of having CHD in you are 40s is almost 7 times higher than in your 20s (for this sample)

### Other interpretations: Equal odds

Another interpretation of logistic regression is to report the value of $X$ at which the $\text{odds}$ of the outcome are equal to 1 (equivalently, the probability of the outcome is equal to .5). This idea is illustrated in Figure \@ref(fig:equal-odds). 

```{r, equal-odds, echo = T, fig.cap = "The Equal Odds Interpretation", fig.align = 'center'}
x <- data.frame(age = 20:70)
prob <- predict(mod2, newdata = x, type = "response")
plot(x$age, prob, xlab = "age", ylab = "p(CHD)", type = "l", col = "#4B9CD3")
segments(x0 = 15, y0 = .5, x1 = 48, y1 = .5, lty = 2, lwd = 2)
segments(x0 = 48, y0 = 0, x1 = 48, y1 = .5, lty = 3, lwd = 2)
```

First we find the probability of .5 on the $Y$ axis and then follow the horizontal dashed line to the logistic curve. Then we follow the vertical dashed line down to the value of $X$. This gives use the age at which the probability of CHD is "50-50". Based on the plot we can say that, after your 48th birthday, your chances of having CHD are above 50%. 

The math behind this interpretation is below. Since

\[ \log(.5/.5) = \log(1) = 0 \]

we can solve 

\[ a + b(\text{age}) = 0 \]

to find the age at which someone has equal odds of CHD, leading to

\[ \text{age} = - a/b. \]

For the example data

\[ \text{age} = - a/b = - (-5.30) / .11 = 48.18, \]

which confirms the conclusion we made looking at the plot. 

In summary, another way of interpreting regression coefficients in simple logistic regression is to compute $-a / b$, which gives the value of $X$ at which $p(Y = 1) = .5$. 

### Other interpretations: Rate of change

Yet another interpretation is in terms of the slope of the straight line (tangent) through the point $p(CHD) = .5$. The slope of this line describes the approximate rate of change in the probability of CHD for people who are "close to" the age of equal odds.  

It turns out that the slope of the tangent line is equal to $b/4$. The derivation requires calculus and is omitted, but the line for our example is shown in Figure \@ref(fig:tangent). 

```{r, tangent, echo = T, fig.cap = "The Rate of Change Interpretation", fig.align = 'center'}
x <- data.frame(age = 20:70)
prob <- predict(mod2, newdata = x, type = "response")
plot(x$age, prob, xlab = "age", ylab = "p(CHD)", type = "l", col = "#4B9CD3")
abline(a = -.815, b = .11/4, lty = 2, lwd = 2)
```

For the example data $b / 4 = .11 / 4 = .0275$. So, for every additional year, the predicted probability of CHD increases by .0275. However, this interpretation only applies to people who "around" the age of equal odds  (48 years old in this example). Looking at the plot, we can see that the approximation is pretty good for people between the ages of 40 and 60. 

Before moving on, notice that the logistic function is roughly linear for probabilities in the range $[.2, .8]$. As mentioned in the introduction of this chapter, this is the situation in which using linear regression with a binary outcome works "well enough". So, if you run a linear regression on your binary outcome, and none of the fitted / predicted values are outside the range $[.2, .8]$, you are in a situation where you might prefer to use linear regression (despite it being technically wrong). 


### Summary 

The simple logistic regression model 

\[ \text{logit}(CHD) = a + b (\text{age}) \]

has the following interpretations. 

* In terms of the logit: 
    * The intercept ($a$) is the predicted value of the log-odds of CHD when age = 0. 
    * The slope ($b$) is how much the predicted log-odds of CHD changes for one unit (year) of increase in age.

* In terms of the odds: 
    * The exponent of the regression parameter, $\exp(b)$, is odds ratio associated with a one unit of increase in age.
    * $(\exp(b) - 1) \times 100$ is the percentage change in the odds of CHD for a one unit increase in age. 
    * Also note that $exp(a)$ is the predicted odds of having CHD when $age = 0$. This isn't very useful number in our example, but it can be useful when the predictor(s) are centered.  

* In terms of predicted probabilities
    * The logistic plot provides a visual summary of how the probability of CHD changes as a function of age -- if you are wanting to report in terms of probabilities, this plot is usually a good choice! 
    * Predicted probabilities can be reported for specific values of age (MERVs), and risk ratios / relative risk can be computed to compare the predicted probabilities at specific  ages. 
    * $–a / b$ corresponds the age at which the probability of having CHD is "50-50"
    * $b / 4$ is the approximate rate of increase (slope) of the probability of having CHD for people close to the "50-50" point.  

**Please write down the numerical values of each of the above summaries for the CHD example (except the predicted probability plot). You can select any values of age to report the predicted probabilities and risk ratios, and you can "eye ball" the probabilities using Figure \@ref(fig:pred-prob).**  

## Estimation {#estimation-11} 

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

The previous section discussed the interpretation of the simple logistic regression model. In this section we discuss how to get the numbers in the “Estimate” column of the R summary table. 

The sort version: In logistic regression, we use maximum likelihood (ML) rather than OLS to estimate the model parameters. The resulting estimates are called the maximum likelihood estimates (MLEs). 

By understanding ML estimation, we will also be able to address 

* Testing model parameters for significance, confidence intervals, etc (Section \@ref(inference-11))
* The assumptions of logistic regression, and how to check them (Section \@ref(assumption-checking-11))

### ML vs OLS

Let's first contrast ML with OLS. In linear regression, OLS regression leads to "closed form" expressions for the regression coefficients (e.g., Section \@ref(ols-2). This means we can write down an explicit algebraic equation for the regression coefficients, e.g., 

\[ b = \text{cov}(Y, X) / s^2_X \] 

As we discussed in the introduction, these estimates usually aren't very good when the outcome is binary. So, instead we use ML. 

The first thing to know about ML with logistic regression is that it does not lead to closed form expression for the regression coefficients. In other words, using algebra we can never write down the other side of the equation $b = ?$. Instead, we define the regression parameters *algorithmically* -- whatever value results from maximizing the likelihood is the value we choose for $b$. We discussed the difference between algebraic and algorithmic definitions when we compared the mean and median last semester. For now, the take home message is

* In OLS, we didn’t really have to know much about least squares per se, because we could just work with the formulas for the regression coefficients.  
* In ML, we really do need to know about likelihood, because we don’t get any other formulas for the coefficients.

### The likelihood 

The likelihood is a function that tells us the probability of sample being drawn from a population. For our example data, we have $N = 100$ cases in which there are 43 people with CHD and 57 people without CHD. The likelihood tell us the probability of this sample being drawn from the population. In particular, when the variable of interest is binary, it is called the *binomial* likelihood. 

The building block of the binomial likelihood is the probability of a person having CHD. For person $i$, we use the shorthand notation

\[ p_i = p(CHD_i = 1) \] 

to denote the probability of having CHD. You might be thinking -- don’t we already know if person $i$ has CHD or not? Isn't that is what our data tell us!?

That is true, but recall that we are imaging that person $i$ is sampled from a population, and in that population there are people who are exactly like person $i$ with respect to the predictors we have collected (e.g., of exactly the same age). Some proportion of those people will have CHD, and that is the probability we are interested in -- e.g., the probability of a person of age 48 having CHD in the population.

The likelihood of CHD for person $i$ is 

\[ L_i = p_i \times CHD_i + (1 - p_i) \times (1 - CHD_i) 
(#eq:Li)
\] 

This might look complicated but its just a way of writing the population probability of having CHD, or not having CHD, in one formula. All it says is: 

* The probability of sampling someone with CHD is $p_i$.
    * i.e., plug in $CHD_i = 1$ into Equation \@ref(eq:Li) and the answer is $p_i$.

* The probability of sampling someone without CHD is $1 - p_i$.
    * i.e., plug in $CHD_i = 0$ into Equation \@ref(eq:Li) and the answer is $1 - p_i$.
    
For a sample of $N$ **independent** persons (i.e., a simple random sample), the binomial likelihood is

\[ L = \Pi_{i = 1}^N L_i = L_1 \times  L_2 \times \cdots \times L_N
(#eq:L)
\] 

The symbol $\Pi$ is shorthand for multiplication, just like $\Sigma$ is used for addition. If we plug into Equation \@ref(eq:L) using $CHD = 1$ for 43 people and $CHD = 0$ for 57 people, this will give us the likelihood for example data set. 

Of course, the trick is that we need to know $p_i$. This is where the logistic regression model comes in -- it provides a probability of CHD for each person in our sample. In maximum likelihood estimation, we select the coefficients of the logistic regression model so that the likelihood in Equation \@ref(eq:L) is maximized. This is called the principal of maximum likelihood -- it says "chose the model parameters that make the observed data most likely."

Maximizing the likelihood is complicated stuff, but that is what `R` is for. One complication that we should be aware of is that the likelihood can cause "underflow" issues in computing, because we are multiplying together lots of numbers less than 1, which can lead to very, very small values -- too small for a computer represent. To address this issue, we usually work with the log-likelihood, rather than the likelihood: 

\[ \ell = \log(L) = \log \prod_{i = 1}^N L_i = \sum_{i = 1}^N \log(L_i) 
(#eq:ell)
\] 

The last equality uses the "addition with logs" rule from Section \@ref(math-review-9). For our purposes here, we just need to know that taking the log of the likelihood changes a very small number (the likelihood) into a large negative number (the log-likelihood), which prevents your computer from breaking when doing computations. 

The log-likelihood $ell$ shows up a lot in what follows. In particular, when the log-likelihood is evaluated at the MLEs, $-2 \times \ell$ is called the *deviance.* 

### Summary 

Lets take a look at our summary output for the example data again: 

```{r}
summary(mod2)
```

This section has addressed the question: how do we get the numbers under the "Estimate" column? The answer is maximum likelihood (ML). In short: 

* The likelihood function tells us the probability of sampling a given set of cases from a population – sometimes it is referred to as "the probability of the data". 
   * When the variable of interest is binary, it is called the binomial likelihood. 

* Intuitively, ML is based on the idea that the values of the model parameters that make the data most likely are the “best”
    * This is sometimes called the principal of maximum likelihood. 
    * The resulting values are called the maximum likelihood estimates (MLEs). 

* In logistic regression, ML is accomplished using a computer algorithm -- we can't write down algebraic expressions for the MLEs, we only have the algorithmic definition (FYI: the algorithm R uses is a version of Newton-Raephson called Fisher Scoring). 

To check your knowledge before moving on: **In a few sentences, describe how the model parameters in logistic regression are estimated and how this compares to OLS regression**. 


## Inference {#inference-11}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

There are three main types of inference for logistic regression: 

* *Wald test of regression coefficients*. The Wald test is a z-test that can be used with large samples ($N > 100$). This is what is reported in R's `summary` output for logistic regression. 

* *Confidence intervals for odds ratios*. Odds ratios can be tested using confidence intervals. If the confidence interval includes the value 1, then the odds ratio is not statistically significant at the chosen level of alpha / confidence. 

* *Likelihood ratio test of nested models*. The likelihood ratio test plays the same role as the F-test of $\Delta R^2$ in OLS regression. It allows you to compare nested models. We will talk about the analogue statistic for $\Delta R^2$ in the next section. 

In general, the Wald test should only be used when you have a large sample size. The Wald test is just like a z-test, and so it requires an estimate of the standard error of the coefficient being tests. In logistic regression, the standard errors can be too large in small samples, especially if the coefficient is large in value. So, when you have fewer than 100 cases, it is good practice to use (likelihood-based) confidence intervals on the odds ratio, or the likelihood-ratio test for nested models.  

The rest this section briefly illustrates each of these three methods with our example data. 

### Wald test

MLEs have the property that they obey the central limit theorem. This means that the sampling distribution of an MLE becomes more and more like a normal distribution as the sample size gets large. We say that the MLEs are “approximately” or “asymptotically” normal. 

So, if we divide the MLE by its standard error, we get an approximate z-test of statistical significance. This test is called a Wald test, and it is what R reports in the summary table below. You should have no problem interpreting these tests at this point in the course. 

```{r}
summary(mod2)
```

In summary: The hypotheses pair $H_0: b = b_0$ versus $H_A: b \neq b_0$ can be tested using the statistic 

\[ z = (\hat b - b_0) / s_{\hat b}, \]

which has a standard normal distribution when the null is true. This tests assumes: 

* $\hat b$ is the MLE (requires that model is correct -- more on this in Section \@ref(assumption-checking-11))

* The sample size is "approximately" infinite -- $N > 100$ is a practical lower limit. 

The same test applies to the intercept, or any other MLE. 

### Odds ratio (OR)

Recall that the OR was introduced as a more intuitive way of interpreting logistic regression. But how do we make inferences about this more interpretable quantity? The answer is to use confidence intervals. The reason we use confidence intervals instead of tests of significance is because the $b = 0$ corresponds to 

\[ OR = \exp(b) = \exp(0) = 1. \] 

So, if the regression coefficient is zero (i.e., no relationship), this means the OR is one. In terms of percent change, $OR = 1$ means 0% percent change: 

\[ (OR - 1) \times 100  = (1 - 1) \times 100 = 0. \]

We can get confidence intervals on the OR using the following two steps:

* Compute the confidence intervals for the regression coefficient $b$, say 

\[ [b_{\text{lower}} ,b_{\text{upper}}]. \]

* Then exponentiate the confidence intervals to get a confidence interval on the OR.

\[ [\exp(b_{\text{lower}}) ,\exp(b_{\text{upper}})]. \]

Below is the result of this procedure for our example.

```{r}
ci.b <- confint(mod2)
ci.table <- cbind(coefs = coef(mod2), ci.b)
exp(ci.table)
```

**Please write down your interpretation of the OR and its confidence interval for the example**

Note that the confidence intervals produced by $R$ use the (profile) likelihood rather than the Wald test. So, the problems we discussed with the Wald test don't apply to these confidence intervals. (You can get Waldian confidence intervals form the `confint` function, but this is not the default.)

### Likelihood ratio test

The likelihood ratio tests is analogous to the F-test of R-squared change in OLS regression. If you test a model without any predictors versus a model with one predictor, it can be used to replace the Wald test in simple logistic regression. The LR test is also asymptotic, but it does not require estimating the problematic quantity of the Wald test – i.e., the standard error --  so it is often preferred when the sample size is smaller (< 100).


Like the F-test of R-squared change, the likelihood ratio tests can also be used to compare any two nested models. So, it used quite a lot, not just to replace the Wald test in simple logistic regression. 

 The tests works as follows:

* Assume that model 1 with $K_1$ predictors is nested within model 2 with $K_2$ predictors (i.e., model 1 has fewer predictors than model 2).

* If it is true that the additional predictors in model 2 all have regression coefficients equal to zero in the population, then it is also true that the models have equal likelihoods, so the ratio of the likeihoods  is 1.   

\[H_0: \frac{L_1}{L_2} = 1\]

* When $H_0$ is true, the statistic

\[ -2 \log \frac{\hat L_1}{\hat L_2} = 2 (\hat \ell_2 - \hat \ell_1) \] 

has a chi-square distribution with degrees of freedom equal to $K_2 - K_1$. Note that the "hats" denote the likelihood (or log-likelihood) evaluated at the MLEs. 

The chi-square distribution is a lot like the F-distribution. In fact, the F-static is a ratio of two chi-square statistics. Some examples are shown below ($k$ denotes the degrees of freedom). 

![Chi-square distributions (Source: https://en.wikipedia.org/wiki/Chi-squared_distribution)]( https://upload.wikimedia.org/wikipedia/commons/3/35/Chi-square_pdf.svg)


To conduct an LR test of the model with age as a predictor against a model without this predictor (i.e., the model with only the intercept), we can use the `lrtest` function of the `lmtest` package: 

```{r}
# The model with no predictors
mod0 <- glm(chd ~ 1, family = "binomial", data = chd.data)

# The LR test (enter the smaller model first)
lmtest::lrtest(mod0, mod2)
```
**Please write down your interpretation of the LR test for the example give above.**


## (Pseudo) R-squared {#r-squared-11}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

As we have just discussed, the likelihood ratio test in logistic regression is analogous to the F-test of R-squared change in linear regression. But what is the statistic analogous to R-squared?  

There are actually a number of statistics, called *pseudo R-squareds*, that have been developed to address this question. 
The pseudo R-squareds are not proportions of variance, but they try to come close, each with their own shortcomings and limitations. 

There is no clear “victor” in the battle of pseudo R-squareds, so we will just present one exemplar that is straight forward to compute and interpret

### McFadden's Pseudo R-squared

McFadden’s Pseudo R-squared captures the idea of how much the likelihood improves when adding more predictors into the model. For model 1 nested within model 2, it is computed as:

\[ R^2 = 1 - \frac{\hat \ell_2}{\hat \ell_1} \]

Note that if the model 2 does not "add anything" to model 1, then the two models have the same likelihood (see Section \@ref(inference-11)). Thus 

\[ \frac{\hat \ell_2}{\hat \ell_1} = 1 \] 

which implies $R^2 = 0.$

On the other hand, if model 2 provides perfect predictions for all cases, (i.e., $L_i = 1$ for each $i$), then 

\[ \hat \ell_2 = \sum_i \log L_{2i} = \sum_i \log 1 = 0. \]

Thus 
 
\[ R^2 = 1 - \frac{\hat \ell_2}{\hat \ell_1} = 1 - 0/1 = 1. \] 

In summary, McFadden's Rsquared is equal to zero if model 2 does not add anything to model 1, and is equal to 1 if model 2 provides perfect prediction of each case (but model 1 does not). This is close to, but not quite the same as, the interpretation of $\Delta R^2$ in linear regression. 

Because the McFadden's  R-squared is not about proportion  of variance, we can't interpret it the same way as OLS R-squared. Instead, it is usually interpreted in terms of model *fit*. Here "fit" means how much the model's predictions improve, with zero meaning "not at all" and one meaning "perfect prediction". 

The following output shows McFadden's R-squared for the example data. **Please write down an interpretation of the R-square value** and we will discuss you interpretations in class. You can use the "show code" button to see an example interpretation. 

McFadden's R-squared: 
```{r}
# Using the deviance of the models
1 - mod2$deviance/mod0$deviance
```

Interpretation (try it by yourself first!):

```{r}
# Interpretation: Relative a model with no predictors, 
# the inclusion of age improved model fit by about 21.4% 
# of that expected by a model with perfect predictions. 

# More elliptically: the inclusion of age improved model 
# fit by about 21.4\%
```


## Assumption checking {#assumption-checking-11}

As we saw when deriving the likelihood, there are basically two assumptions when estimating logistic regression by maximum likelihood.

1. The different respondents (person, units) are independent 

2. We have the right model for each individual’s probability of CHD, $p_i$

Assumption 1 is ensured by random sampling and is not an population assumption -- we referred to it as a background assumption last semester. If your data didn't use random sampling, you'll need to use a different modeling approach (e.g, multilevel modeling, see EDUC 935). 

Assumption 2 can be checked with the Hosmer-Lemeshow (HL) test. Intuitively, the HL test compares the empirical probabilities to the model-implied ones. In other words, we want to compare the two curves depicted below: 

```{r, HLtest, echo = F, fig.cap = "The idea behind the HL test", fig.align = 'center'}
knitr::include_graphics("images/HL_test.png")
```

The null hypothesis of the HL test is that these two curves are equivalent representations of the data. If we reject the null, we claim that the model does not fit the data. If we retain the null, we conclude the model does fit the data. 

The HL test for the data is reported below. The R-code is shown by default to provide details about how the test is computed.

The function uses 3 arguments:

* `obs` is the binary variable to compute the proportions from.
* `exp` is the fitted values (predicted probabilities) from the logistic regression.
* `g` is the number of groups / bins to use. For this example, 5 is a good number. 

```{r}
## Note: Install package "generalhoslem" if you haven't do so already
# install.packages("generalhoslem")
HLtest <- generalhoslem::logitgof(obs = chd, exp = fitted(mod2), g = 5)
HLtest
```

**Please write down your interpretation of the HL test and be prepared to share you answer in class.**

### More details on the HL test
The HL statistic is computed using the observed and expected (model-implied) counts in each of the 5 groups. Computational details are given below. 

```{r}
# Observed and expected counts
cbind(HLtest$observed, HLtest$expected)
```

The test statistic is the built up using the differences between the observed and expected values:

$$HL = \sum_{g = 1}^5 \left(\frac{(\text{y0}_g - \text{yhat0}_g)^2}{\text{yhat0}_g} + \frac{(\text{y1}_g - \text{yhat1}_g)^2}{\text{yhat1}_g}\right)$$

Hosmer and Lemeshow showed that this statistic has a chi-square distribution on $df = N_\text{groups} - 2$ when the null hypothesis is true (i.e., when the model fits the data). 

## Workbook

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

This section collects the questions asked in this chapter. We will discuss these questions in class. If you haven't written down / thought about the answers to these questions  before class, the lesson will not be very useful for you! So, please engage with each question by writing down one or more answers, asking clarifying questions, posing follow up questions, etc. 

**Section \@ref(chd-example-11)**

* Please take a moment to write down you conclusions about whether the assumptions of linear regression are met for these data, and any other details you may notice about the linear probability model.

```{r, echo = F, fig.cap = "Linear probability model with CHD example", fig.align = 'center', fig.width = 12}
mod1 <- lm(chd ~ age, data = chd.data)
par(mfrow = c(1, 2))
plot(mod1, 1)
plot(mod1, 2)
summary(mod1)
```

**Section \@ref(logit-11)**

```{r, echo = T, fig.cap = "Proportions and odds", fig.align = 'center'}
knitr::include_graphics("images/odds.png")
```

* Please write down your answers to the following questions using the above table, and be prepared to share them in class. For each question provide both a verbal interpretation of the numerical answer (e.g, odds of 2 to 1 means that for every two people with a trait, there is one without.) The answers hidden below (use Show Code button), but please try out the questions yourself first! 

  1. What is the probability of a person in their 40s having CHD? 
  2. What are the odds of a person in their 40s having CHD? 
  3. What is the probability of someone in their 50s **not** having CHD? 
  4. What are the odds of someone in their 50s **not** having CHD? 
  5. What is probability of having CHD in your 40s, compared to your 30s? (e.g., is 3 times higher? 4 times higher?)
  6. What are the odds of having CHD in your 40s, compared to your 30s? 

```{r}
# 1. .39, so about 40% of people
# 2. 11/17, so for 11 people with CHD there are 17 without
# 3. 1 - .72 = .28, so about 28% of people 
# 4. (18/7)^-1 = 7/18, so 7 out ever 18 people
# 5. .39 / .19 ~= 2, so the probability of having CHD in your 40s is about 2 times higher than the probability of having CHD in your 30s. This is called a relative risk, or a risk ratio. 
#6. (11/17)/(5/22) ~= 2.8, so the odds of having CHD in your 40s is about 2.8 times higher than the ods of having CHD in your 30s. This is called an odds ratio. 
```

* I will asks some questions along the following lines in class (use the following table). 

  * If logit < 0 then probability <  ? and odds < ? 
  * What has a larger logit value: probability of .9 or odds of .9?  
  * Let's assume that a probability of $p$ corresponds to a  $\text{logit}$ of $x$.  What is the logit corresponding to $1-p$? (Hint, try some numerical examples from the table). 
  
  
```{r, echo = F, fig.cap = "Logistic, odds, and logit", fig.align = 'center'}
knitr::include_graphics("images/logit-table.png")
```
  
**Section \@ref(simple-11)**

* Please practice your interpretation of the OR in simple logistic regression using the following examples. Answers hidden below (use Show Code button), but please try out the questions yourself first! 
\[ \text{logit}(Y) = a + b X. \]
  1. If $b=0$ what is the OR equal to? What is the percent change in the odds for a one unit increase in $X$? 
  2. If $b=.25$ what is the OR equal to? What is the percent change in the odds for a one unit increase in $X$? 
  3. If $b=-.025$ what is the OR equal to? What is the percent change in the odds for a 10 unit increase in $X$? 
  4. If the odds increase 100% for a one unit increase in $X$, what $b$ equal to? 

```{r}
# 1. OR = exp(0) = 1 and percent change equals (exp(0) - 1) X 100 = 0%. So, "no relationship" means OR = 1. 
# 2. OR = exp(.25) = 1.28 and percent change equals (exp(.25) - 1) X 100 = 28% increase
# 3. OR = exp(-.025) = 0.97 and percent change for one unit equals (exp(-.025) - 1) X 100 = -2.47%. For 10 units of change, multiply by 10, which gives 24.69% decrease (negative sign is decrease). 
# 4. exp(b) - 1) X 100 = 100 --> exp(b) = 2 --> b = log(2) = .69
```


* Please write down the numerical values all of the following summaries of the simple logistic regression model, using the CHD example. You can select any values of age to report the predicted probabilities and risk ratios, and you can "eye ball" the probabilities using the figure. Answers hidden below (use Show Code button), but please try out the questions yourself first! 
\[ \text{logit}(CHD) = -5.03  + .11 (\text{age}). \]
  1. In terms of the logit: 
      * The intercept ($a$) is the predicted value of the log-odds of CHD when age = 0. 
      * The slope ($b$) is how much the predicted log-odds of CHD changes for one unit (year) of increase in age.

  2. In terms of the odds: 
      * The exponent of the regression parameter, $\exp(b)$, is odds ratio for one unit of increase in age.
      * $(\exp(b) - 1) \times 100$ is the percentage change in the odds of CHD for a one unit increase in age. 

  3. In terms of predicted probabilities
      * Predicted probabilities can be reported for specific values of age (MERVs). 
      * Risk ratios can be computed comparing the predicted probabilities at different ages. 
      * $–a / b$ corresponds the age at which the probability of having CHD is "50-50"
      * $b / 4$ is the approximate rate of increase (slope) of the probability of having CHD for people close to the "50-50" point.  

```{r, echo = T, fig.cap = "Predicted probabilities", fig.align = 'center'}
visreg::visreg(mod2, xvar = "age", scale = "response")
```


**Section \@ref(estimation-11)**

* In a sentence or two, describe how the model parameters in logistic regression are estimated and how this compares to OLS regression

**Section \@ref(inference-11)**

* Please write down your interpretation of the OR and its confidence interval for the example below

```{r}
ci.b <- confint(mod2)
ci.table <- cbind(coefs = coef(mod2), ci.b)
exp(ci.table)
```


* Please write down your interpretation of the LR test given below

```{r}
# The model with no predictors
mod0 <- glm(chd ~ 1, family = "binomial", data = chd.data)

# The LR test (enter the smaller model first)
lmtest::lrtest(mod0, mod2)
```

**Section \@ref(r-squared-11)**

* Please write down an interpretation of McFadden's R-squared  for the example and we will discuss you interpretations in class. 

McFadden's R-squared: 

```{r}
# Using the deviance of the models
1 - mod2$deviance/mod0$deviance
```


**Section \@ref(assumption-checking-11)**

* Please write down your interpretation of the HL test and be prepared to share you answer in class

```{r}
## Note: Install package "generalhoslem" if you haven't do so already
# install.packages("generalhoslem")
HLtest <- generalhoslem::logitgof(obs = chd, exp = fitted(mod2), g = 5)
HLtest
```


## Exercise: Multiple logistic regression {#multiple-11}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "show", style = button)
```

This section works through multiple logistic regression with interactions using the Coronary Heart Disease (`chd`; 1 = yes, 0 = no), and indicator for whether a person has ever (or currently) smokes (`smokes`: 1 = yes, 0 = no) and age in years (`age`). The R code is shown by default for these exercises. 

```{r}
# Load the data, take a look
#load("CHD.RData")
#attach(chd.data)
summary(cbind(chd, age, smokes))
```

Let's start by regressing CHD on smokes and age, and refreshing our interpretations of the parameter estimates and tests. The regression coefficients are interpreted analogously to multiple regression (e.g., as holding constant, controlling for, incremental to, the other predictors). 

```{r}
# Regress CHD on smokes and age
mod <- glm(chd ~ age + smokes, family = binomial)
summary(mod)
```

For illustrative purposes, let’s double check that smoking predicts CHD over and above age using the LR test. Recall that this test is analogous to the F-test of R-squared change in OLS regression

```{r}
# LR test coefficient on age, and age + smokes 
mod1 <- glm(chd ~ age, family = binomial)
mod2 <- glm(chd ~ age + smokes, family = binomial)
lmtest::lrtest(mod1, mod2)
```

Both the Wald test and LR test lead to the same interpretation for smoking (this shouldn't be surprising).

How should we compute R-squared? Two different approaches are shown below. Both of these are defensible, but note the difference in interpretation, as we are changing the "baseline model". 

```{r}
# Compare against model 1
(R_21 <- 1 - mod2$deviance/mod1$deviance)

# Compare against model with only the intercept
mod0 <- glm(chd ~ 1, family = binomial)
(R_20 <- 1 - mod2$deviance/mod0$deviance)
```

Recall that it is common to report the output using odds ratios rather than logits. Make sure to check your interpretation of the ORs. 

```{r}
exp(cbind(OR = coef(mod2), confint(mod2)))
```

### Predicted probabilities

We can plot the regression lines for smokers and non-smokers using the same techniques as for linear regression.

```{r}
# Plots of the regression lines in logit scale
visreg::visreg(mod2, "age", "smokes", scale = "linear", overlay = T)

# Plots of the regression lines in probability scale
visreg::visreg(mod2, "age", "smokes", scale = "response", overlay = T)
```

We can also test for differences between smokers and non-smokers at specific ages, just like we did for the OLS regression. Below is an example of using `emmeans` to test the difference between smokers and non-smokers, +/- 1SD on age.

```{r}
# Step 1. Make a function to compute +/- SD on age
sd_function <- function(x) { c(mean(x) - sd(x), mean(x) + sd(x)) }

# Step 2. Compute and test the values on the probability scale
library(emmeans)
gap_prob <- emmeans(mod2, specs = "smokes", by = "age", cov.reduce = sd_function, type = "response")
summary(gap_prob)
```

### Interactions

Let's consider the interaction between age (centered) and smoking. This interaction addresses whether the relationship between age and CHD changes as a function of smoking status. 


```{r}
# Interacting smoking with age (centered)
age_centered <- age - mean(age)
mod4 <- glm(chd ~ age_centered*smokes, family = binomial)
exp(cbind(OR = coef(mod4), confint(mod4)))
visreg::visreg(mod4, "age_centered", "smokes", scale = "response", overlay = T)
detach(chd.data)
```

We will talk about the interpretation of the output in class. 
