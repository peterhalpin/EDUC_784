[["6-chapter-6.html", "Chapter 6 Interactions", " Chapter 6 Interactions In statistics, the term interaction means that the relationship between two variables depends on a third variable. In the context of regression, we are usually interested in the situation where the relationship between the outcome \\(Y\\) and a predictor \\(X_1\\) depends on the value of another predictor \\(X_2\\). This situation is also referred to as moderation or sometimes as effect heterogeneity. Interactions are a “big picture” idea with a lot conceptual power, especially when describing topics related to social inequality or “gaps”. Some examples of interactions are: The relationship between wages and years of education depends on gender. (https://en.wikipedia.org/wiki/Gender_pay_gap) The relationship between reading achievement and age depends on race (https://cepa.stanford.edu/educational-opportunity-monitoring-project/achievement-gaps/race/) The effect of COVID-19 school shutdowns on academic achievement depends on SES. (https://www.mckinsey.com/industries/education/our-insights/covid-19-and-student-learning-in-the-united-states-the-hurt-could-last-a-lifetime) This chapter starts by considering what happens when both categorical and continuous predictors are used together in a model, and uses this combination of predictors as a way of digging into the math behind interactions. Later sections will consider what happens when we have interactions between two continuous predictors, or two categorical predictors. In this chapter we are exclusively interested in interactions between two predictors at a time – two-way interactions. It is possible to consider “higher-order” interactions as well, e.g., interactions among three predictors or three-way interactions. But we will just focus on the simplest case. Sometimes we will also use the terminology main effect to describe the relationship between each individual predictor and the outcome variable, as distinct from the interactions among the predictors. Up to now, we have be working only with main effects. "],["6.1-example-6.html", "6.1 An example from NELS", " 6.1 An example from NELS There is well-documented gender gap in STEM achievement by the end of high school. The t-test reported below illustrates the gap in Math Achievement using the NELS data. load(&quot;NELS.RData&quot;) t.test(achmat12 ~ gender, var.equal = T, data = NELS) ## ## Two Sample t-test ## ## data: achmat12 by gender ## t = -4.6, df = 498, p-value = 7e-06 ## alternative hypothesis: true difference in means between group Female and group Male is not equal to 0 ## 95 percent confidence interval: ## -4.527 -1.798 ## sample estimates: ## mean in group Female mean in group Male ## 55.47 58.63 In this chapter, our first goal is to use linear regression to better understand this gender gap in Math Achievement. To do this, we consider a third variable, Reading Achievement. The plot below shows the relationship between Math Achievement and Reading Achievement estimated just for males (Blue), and the same relationship estimated just for females (Black). attach(NELS) females &lt;- gender == &quot;Female&quot; males &lt;- gender == &quot;Male&quot; mod1 &lt;- lm(achmat12[females] ~ achrdg12[females]) mod2 &lt;- lm(achmat12[males] ~ achrdg12[males]) # Plot reading and math for females plot(achrdg12[females], achmat12[females], xlab = &quot;Reading&quot;, ylab = &quot;Math&quot;) abline(mod1, lwd = 2) # Add points and line for males points(achrdg12[males], achmat12[males], col = &quot;#4B9CD3&quot;, pch = 2) abline(mod2, col = &quot;#4B9CD3&quot;, lwd = 2) # Add a legend legend(x = &quot;topleft&quot;, legend = levels(gender), pch = c(1, 2), col = c(1, &quot;#4B9CD3&quot;)) Figure 6.1: Math Achievement, Reading Achievement, and Gender. detach(NELS) Take a minute to think about what this plot is telling us about the relationships among Math Achievement, Reading Achievement, and Gender. Is the gender gap in math constant? Is the relationship between math and reading the same for males and females? Can you provide a summary of the plot in terms of what it says about the gender gap in Math Achievement? Please write down your answers to these questions and be prepared to share them in class. Note that in the figure above, we estimated two separate simple regression models, one just for males and one just for females. In the next few sections, we will work our way towards a single multiple regression model that can be used to represent the relationships among these three variables. "],["6.2-binary-continuous-6.html", "6.2 Binary + continuous", " 6.2 Binary + continuous As a first step, let’s consider what happens when we include both Gender and Reading Achievement as predictors of Math Achievement in our usual multiple regression equation: \\[\\widehat Y = b_0 + b_1X_1 + b_2 X_2 \\tag{6.1} \\] where \\(Y\\) is Math Achievement in grade 12 \\(X_1\\) is Reading Achievement in grade 12 \\(X_2\\) is Gender (binary, with Female = 0 and Male = 1) Note that this model does not include an interaction between the two predictors – we are first going to consider what is “missing” from the usual regression model, and then use this to motivate inclusion of another predictor that represents the interaction. To get an initial sense of what is missing, the model in Equation (6.1) is plotted Figure 6.2 – can you spot the difference with Figure 6.1? attach(NELS) mod3 &lt;- lm(achmat12 ~ achrdg12 + gender, data = NELS) a_females &lt;- coef(mod3)[1] b_females &lt;- coef(mod3)[2] a_males &lt;- a_females + coef(mod3)[3] b_males &lt;- b_females # Plot reading and math for females plot(achrdg12[females], achmat12[females], xlab = &quot;reading&quot;, ylab = &quot;math&quot;) abline(a_females, b_females, lwd = 2) # Add points and line for males points(achrdg12[males], achmat12[males], col = &quot;#4B9CD3&quot;, pch = 2) abline(a_males, b_males, col = &quot;#4B9CD3&quot;, lwd = 2) # Add a legend legend(x = &quot;topleft&quot;, legend = levels(gender), pch = c(1, 2), col = c(1, &quot;#4B9CD3&quot;)) Figure 6.2: Math Achievement, Reading Achievement, and Gender (No Interaction). detach(NELS) In order to interpret our multiple regression model, we can take the same two-step approach we used to interpret categorical predictors in Chapter 5. First, we plug in values for the categorical predictor, then we use the resulting equations solve for quantities of interest. In particular, \\[\\begin{align} \\widehat Y (Female) &amp; = b_0 + b_1X_1 + b_2 (0) \\\\ &amp; = b_0 + b_1X_1 \\\\\\\\ \\widehat Y (Male) &amp; = b_0 + b_1X_1 + b_2 (1) \\\\ &amp; = (b_0 + b_2) + b_1 X_1 \\\\\\\\ \\widehat Y (Male) - \\widehat Y (Female) &amp; = b_2 \\end{align}\\] The equations for \\(\\widehat Y (Female)\\) and \\(\\widehat Y (Male)\\) are referred to simple trends or simple slopes. These describe the regression of Math on Reading, simply for Males, or simply for Females. This usage of the term “simple” is related to the simple regression model with only one predictor, but it is also used more widely. The difference between the two simple regression equations is, in this context, the predicted gender gap in Math Achievement. From these equations we can see that: The simple trends for Females and Males have different intercepts, because the regression coefficient for Gender (\\(b_2\\)) is added to the intercept for Males. The simple trends for Females and Males have the same slope, meaning that the regression lines are parallel (see Figure 6.1). The difference between the predicted values (i.e., the predicted gender gap in Math Achievement) is a constant, and is equal to the regression coefficient for Gender (\\(b_2\\)) The regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the simple trends and the gender gap in Math Achievement for the NELS example (Don’t worry about statistical significance, just focus on the meaning of the coefficients.) coef(mod3) ## (Intercept) achrdg12 genderMale ## 19.9812 0.6355 3.5017 6.2.1 Marginal means Before moving on to discuss what is missing from the regression model in Equation (6.1), it is important to note that the interpretation of the regression coefficient on gender has changed from what we discussed in Chapter 5. In Chapter 5, we noted that the regression coefficient on a dummy variables can be interpreted as the mean of the group indicated by the dummy (e.g. the mean of Math Achievement for Males, or for Females). However, when additional predictors are included in the regression model, this relationship no longer holds in general. To see this we can compare the output of the t-test in Section 6.1 with the regression output shown above. In the t-test, the group means for Math Achievement were 55.47 for Females and 58.63 for males, so the mean difference was \\[58.63 - 55.47 58.63 - 55.47 = 3.16 \\] However, the regression coefficient on gender in the multiple regression model above is equal to \\(3.50\\). Why the difference? Remember that in the multiple regression model, the regression coefficient on Gender controls for the relationship between Reading Achievement and Gender. So, the multiple regression coefficient represents the relationship between Gender and Math, after controlling for Reading. The t-test doesnt control for Reading. In order to emphasize the distinction between “raw” group means computed from the data and the predicted group means obtained from a multiple regression model, the latter are referred to as marginal means, or sometimes as adjusted means or least squares means. 6.2.2 Summary In a regression model with one continuous and one binary predictor (and no interaction): The model results in two regression lines, one for each value of the binary predictor. These are called the simple trends. In a standard multiple regression model, the simple trends are parallel but can have a different intercept; the difference in the intercepts is equal to regression coefficient of the binary variable. The difference between the simple trends is often called a “gap”, and the gap is also equal to the regression coefficient of the binary variable. It is important to note that the predicted group means for the binary variable are no longer equal to the “raw” group means computed directly from the data, because the predicted group means now control for the intercorrelation among the predictors. The predicted group means are called marginal means to emphasize this distinction. "],["6.3-binary-continuous-interaction-6.html", "6.3 Binary + continuous + interaction", " 6.3 Binary + continuous + interaction In the previous section, the multiple regression model led us to conclude that the gender gap in Math Achievement is constant and equal to 3.50 (i.e., the regression coefficient on Gender). As we saw in Figure 6.2, this implies that the simple trends are parallel. However, these conclusions do not agree with what we saw in Section 6.1 when we fitted two simple regression model to the NELS data. In this section, we discuss what is missing from the multiple regression model in Equation (6.1): the interaction between Gender and Reading. Mathematically, an interaction is just the product between two variables. Equation (6.2) shows how to include this product in our multiple regression model – we just take the product of the two predictors and add it into the model as a third predictor: \\[\\hat Y = b_0 + b_1X_1 + b_2 X_2 + b_3 (X_1 \\times X_2). \\tag{6.2} \\] For the NELS example, this regression model is depicted in Figure 6.3. Note the the simple trends are no longer parallel and the regression lines agree exactly with what we had in Section 6.1. So, as promised, we have now arrived at a single multiple regression model that captures the relationships among Math Achievement, Reading Achievement, and Gender. attach(NELS) # Interaction via hard coding genderXachrdg12 &lt;- (as.numeric(gender) - 1) * achrdg12 mod4 &lt;- lm(achmat12 ~ achrdg12 + gender + genderXachrdg12) a_females &lt;- coef(mod4)[1] b_females &lt;- coef(mod4)[2] a_males &lt;- a_females + coef(mod4)[3] b_males &lt;- b_females + coef(mod4)[4] # Plot reading and math for females plot(achrdg12[females], achmat12[females], xlab = &quot;reading&quot;, ylab = &quot;math&quot;) abline(a_females, b_females, lwd = 2) # Add points and line for males points(achrdg12[males], achmat12[males], col = &quot;#4B9CD3&quot;, pch = 2) abline(a_males, b_males, col = &quot;#4B9CD3&quot;, lwd = 2) # Add a legend legend(x = &quot;topleft&quot;, legend = levels(gender), pch = c(1, 2), col = c(1, &quot;#4B9CD3&quot;)) Figure 6.3: Math Achievement, Reading Achievement, and Gender (No Interaction). detach(NELS) To see what the model says about the data, let’s work through the model equations using our two-step procedure. As usual, we first plug in values for the categorical predictor, then we use the resulting equations solve for quantities of interest (the simple trends and the gender gap). \\[\\begin{align} \\widehat Y (Female) &amp; = b_0 + b_1X_1 + b_2 (0) + b_3(X_1 \\times 0) \\\\ &amp; = b_0 + b_1X_1 \\\\\\\\ \\widehat Y (Male) &amp; = b_0 + b_1X_1 + b_2 (1) + b_3(X_1 \\times 1)\\\\ &amp; = (b_0 + b_2) + (b_1 + b_3) X_1 \\\\\\\\ \\widehat Y (Male) - \\widehat Y (Female) &amp; = b_2 + b_3 X_1 \\end{align}\\] Similar to the results in Section 6.2, we can that see that The simple trends for Females and Males have different intercepts, because the regression coefficient for Gender (\\(b_2\\)) is added to the intercept for Males. However, in contrast to Section 6.2, The simple trends for Females and Males no longer have same slope, because the regression coefficient for the interaction (\\(b_3\\)) is added to the slope for Males. The difference between the predicted values (i.e., the predicted gender gap in Math Achievement) is no longer constant, but is instead a function of \\(X_1\\). In particular, the gender gap in Math changes by \\(b_3\\) units for each unit of change in Reading. This last point is especially important in the context of our example. The gender gap in Math Achievement is a function of Reading Achievement. This is the mathematical meaning behind the concept of an interaction – the relationship between two variables (Math and Gender) is changing as a function of a third variable (Reading). This is what is means when we say, e.g., that the relationship between Gender and Math depends on Reading. 6.3.1 Choosing the moderator The concept of an interaction is “symmetrical” in the sense that we can chose which variable goes in the “depends on” clause. For example, it is equally valid to say the relationship between Math and Gender depends on Reading, or the relationship between Math and Reading depends on Gender. Whichever variable appears in the “depends on” clause is called the moderator, and the other two variables are called the focal variables. The researcher chooses which variable to treat as the moderator when interpreting an interaction. The overall idea here is to “break down” the interaction in the way that is most compatible with the research question(s). For example, our research question was about the gender gap in STEM Achievement. So, our focal relationship is between Math and Gender, and Reading would be our moderator. Taking this approach, it would make sense to focuses our interpretation on difference \\(\\widehat Y (Male) - \\widehat Y (Female)\\) – i.e., the predicted gender gap. So, we could say that the relationship between Math and Gender depends on Reading, or, more specifically, that the predicted gender gap in Math changes by \\(b_3\\) units for each unit of increase in Reading. By contrast, if we were more interested in the relationship between Math and Reading, then we could treat Gender as the moderator. Here it would make sense to focus our interpretation on the simple trends, and in particular on the slope parameters in these equations. For example, we might say: For females, predicted Math Achievement changed by \\(b_1\\) units for each unit of increase in Reading, whereas for males, the predicted change was (\\(b_1 + b_3\\)) units for each unit of increase in Reading. This might feel less intuitive than talking about the gender gap, but the two interpretations are mathematicaly equivalent. Its just a matter of whether you want to interpret the regression coefficient \\(b_3\\) with reference to the gender gap, or with reference to the simple trends. 6.3.2 Back to the example The regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the interaction between Gender and Reading. (Don’t worry about statistical significance, just focus on the meaning of the coefficients.) coef(mod4) ## (Intercept) achrdg12 genderMale genderXachrdg12 ## 14.8031 0.7282 13.3933 -0.1779 Some potential answers are hidden below, but don’t peak until you have tried it for yourself! # Gender gap # General: The gender gap in Math is smaller for students who are also strong in Reading # Specific: The gender gap in Math Achievement decreases by .18 percentage points for each percentage point increase Reading Achievement # Simple slopes # General: The relationship between Math and Reading is stronger (i.e. has a larger slope) for Females than for Males # Specific: # For Females, Math scores are predicted to increase by .72 percentage points for each percentage point increase in Reading Achievement # For Males, Math scores are predicted to increase by .55 percentage points for each percentage point increase in Reading Achievement 6.3.3 Centering the continuous predictor You may have noticed that the coefficient on gender was wildly different in regression models with and without the interaction. In the model without the interaction (Section 6.2) the coefficient on Gender was 3.50, and in the model with the interaction (above), it was 13.40. So in one model, the “effect” of being Male was a 3.5 percentage points gain on a Math exam, but in the other model, it was a 13.40 percentage point gain. Why this huge difference in the effect of Gender? The answer can be seen in the equation for the gender gap. In the model without the interaction, the gender gap was constant and equal to the regression coefficient on Gender (denoted as \\(b_2\\) in the model): \\[ \\widehat Y (Male) - \\widehat Y (Female) = b_2 \\] But in the regression model with the interaction, the gender gap was a linear function of Reading and the regression coefficient on Gender is actually the intercept for the linear relationship. \\[ \\widehat Y (Male) - \\widehat Y (Female) = b_2 + b_3 X_1 \\] So, in the model with the interaction, \\(b_2\\) is the gender gap for students who score zero on Reading Achievement. Since the lowest score on Reading Achievement was around 35, the intercept in this equation (i.e., the regression coefficient on gender, \\(b_2\\)) is not very meaningful. One way to address this situation is to center the Reading variable so that it has a mean of zero. To do this, let \\[ D_1 = X_1 - \\bar X_1 \\] denote the deviation scores for \\(X_1\\) (i.e., the mean-centered version of \\(X_1\\)). Then we just regress Math Achievement on \\(D_1\\) rather than \\(X_1\\). Working through the equations shows that the gender gap is now \\[ \\widehat Y (Male) - \\widehat Y (Female) = b_2 + b_3 D_1 \\] Since \\(D_1 = 0\\) when \\(X_1 = \\bar X_1\\), the regression coefficient on Gender (\\(b_2\\)) is now interpretable as the gender gap in Math Achievement, for students with average Reading Achievement. This is a much more interpretable number than the coefficient in the original interaction model! In the example, this approach yields the following model parameters: attach(NELS) # compute the deviation scores for reading reading_dev &lt;- achrdg12 - mean(achrdg12, na.rm = T) # Run the interaction model as above genderXreading_dev &lt;- (as.numeric(gender) - 1) * reading_dev mod5 &lt;- lm(achmat12 ~ reading_dev + gender + genderXreading_dev) coef(mod5) ## (Intercept) reading_dev genderMale genderXreading_dev ## 55.2944 0.7282 3.4993 -0.1779 detach(NELS) The regression coefficient for Gender is now pretty close to what it was in the original model without the interaction, but the interpretation is different. Notice that the intercept in the multiple regression model with Reading centered has changed compared to the previous model in which Reading was not centered. However, the centering did not affect the regression coefficient for Reading or the interaction. Please write down your interpretation of the intercept and the regression coefficient for Gender in the above regression output, and be prepared to share your answer in class. 6.3.4 Summary The interaction between two variables is just their product. When this product is added as a predictor in a multiple regression model with one continuous and one binary predictor: The model again results in two regression lines, one for each value of the binary predictor, but we get regression lines with different intercepts and different slopes. The difference in slopes is equal to the regression coefficient on the interaction term. The difference (“gap”) between the regression lines changes as a linear function of the continuous predictor, and this change is again equal to the regression coefficient on the interaction term. These last two points are equivalent ways of stating the central idea behind a (two-way) interaction: the relationship between two variables changes as a function of a third variable. When interpreting an interaction, the researcher chooses which pair of variables will be the “focal relationship” and which variable will be the moderator. Centering the continuous predictor can be helpful for ensuring that the regression coefficient on the binary variable remains interpretable in the presense of an interaction. "],["6.4-inference-for-interactions-6.html", "6.4 Inference for interactions", " 6.4 Inference for interactions Compared to the two simple regression models discussed in Section 6.1, the real power of the multiple regression model is that it facilitates statistical inference about the simple trends and “gaps”. For example, in the standard summary output for the regression model, we now have a test of whether the interaction terms is statistically significant. You should feel comfortable interpreting the regression coefficients in this output based on the work we have done so far in this chapter, but now is a good time double check. The interpretation of the standard errors, tests of statistical significance, R-squared, etc, are all the same as in Chapter 4. summary(mod5) ## ## Call: ## lm(formula = achmat12 ~ reading_dev + gender + genderXreading_dev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.058 -3.786 0.501 4.077 16.289 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.2944 0.3513 157.41 &lt; 2e-16 *** ## reading_dev 0.7282 0.0470 15.49 &lt; 2e-16 *** ## genderMale 3.4993 0.5214 6.71 5.3e-11 *** ## genderXreading_dev -0.1779 0.0651 -2.73 0.0065 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.8 on 496 degrees of freedom ## Multiple R-squared: 0.462, Adjusted R-squared: 0.459 ## F-statistic: 142 on 3 and 496 DF, p-value: &lt;2e-16 Note that the output above doesn’t answer all of the questions we might have about a interaction. For example, since we are interested in the gender gap in Math Achievement, we might want to know for which students the gap is statistically significant. In particular, does the gap disappear for students with higher levels of Reading Achievement? We can answer this type of question by testing marginal effects, which are a generalization of the marginal means discussed in section 6.2. In general, marginal effects are useful when the goal is to “follow up” a significant interaction in which the focal predictor is categorical (the moderator cab be categorical or continuous). Note that marginal effects are only of interest when the interaction is significant – if the interaction is not significant, then we know that the relationship between the focal variables doesn’t depend on the moderator. 6.4.1 Marginal effects There are three main types of marginal effects. To explain these three approaches, first let’s write the gender gap in Math Achievement using a slightly more compact notation. \\[ \\widehat Y (Male) - \\widehat Y (Female) = b_2 + b_3 X_1 = \\Delta(X_1) \\] Then we having the following marginal effects that can be computed: Marginal effects at the mean (MEM): Report the gap at the mean value of \\(X_1\\) \\[ MEM = \\Delta(\\bar X_1) \\] Average marginal effect (AVE): Report the average of the marginal effect: \\[ AVE = \\frac{\\sum_i \\Delta(X_{i1})}{N} \\] Marginal effects at represenative values (MERV): Report the marginal effect for a range of “interesting” values \\[ MERV = \\{\\Delta(X^*_1), \\Delta(X^\\dagger_1), \\dots \\} \\] MEM and AVE are equivalent for linear regression models (but we will visit the distinction again when we get to logistic regression). In this section we focus on MERV, which is the more widely used approach. One usual choice for the “interesting values” is the quartiles of \\(X_1\\), which is reported below. Another popular choice is the mean of \\(X_1\\) plus or minus 1 SD. # Install the package if you haven&#39;t already done so # install.packages(&quot;emmeans&quot;) # Load the package into memory library(emmeans) # Fit the model using R&#39;s formula syntax for interaction &#39;*&#39; mod6 &lt;- lm(achmat12 ~ gender*achrdg12, data = NELS) # Use the emmeans function to get the gender means on math, broken down by reading gap &lt;- emmeans(mod6, specs = &quot;gender&quot;, by = &quot;achrdg12&quot;, cov.reduce = quantile) # Test whether the differences are significant contrast(gap, method = &quot;pairwise&quot;) ## achrdg12 = 31.8: ## contrast estimate SE df t.ratio p.value ## Female - Male -7.74 1.637 496 -4.728 &lt;.0001 ## ## achrdg12 = 51.2: ## contrast estimate SE df t.ratio p.value ## Female - Male -4.27 0.593 496 -7.207 &lt;.0001 ## ## achrdg12 = 57.0: ## contrast estimate SE df t.ratio p.value ## Female - Male -3.25 0.529 496 -6.138 &lt;.0001 ## ## achrdg12 = 61.7: ## contrast estimate SE df t.ratio p.value ## Female - Male -2.41 0.658 496 -3.659 0.0003 ## ## achrdg12 = 68.1: ## contrast estimate SE df t.ratio p.value ## Female - Male -1.28 0.967 496 -1.321 0.1872 The output shows the gender gap in Math Achievement for 5 values of Reading Achievement. The values of Reading Achievement are its 5 quartiles. Please examine the statistical significance of the gender gap in Math Achievement at the 5 quartiles of Reading Achievement and make a conclusion whether the gap “dissapeared” for students with higher levels of Reading Achievement. Please be prepared to share your answer in class! Note that the computations going on “under the hood” for testing marginal means are pretty complicated. You can read more details here: https://cran.r-project.org/web/packages/emmeans/vignettes/basics.html 6.4.2 Simple trends Another way to follow-up a significant interaction is to examine the statistical significance of the simple trends. As discussed in Section 6.3, the simple trends aren’t very meaningful in the context of our example, so this section is just about illustrating the technique, not about adding anything to our discussion of the gender gap in STEM. In general, testing simple trends can be useful when following-up a significant interaction in which the focal predictor is continuous (the moderator can be categorical or continuous). We can understand simple trends in reference to what they add to the standard summary output for the lm function (reported at the beginning of Section 6.4): The regression coefficient on the continuous predictors tells us about simple trend for the group designated as zero on the binary predictor (e.g, simply for females). The regression coefficient on the interaction term tells whether the simple trends differ for the two groups (e.g., whether the simple trend for males differs from the simple trend for females). Note that what is missing, or implicit, in this output is a test of the simple trend for males (the group designated as one on the binary predictor). The standard regression output does not provide a statistical test of whether this trend is different from zero. To get this, we need to test the simple trends. The test of the simple trends for the example are reported below. Again, these aren’t super interesting in the context of our example, but you should check your understanding of simple trends by writing down an interpretation of the output below. # Use the emtrends function to get the regression coefficients on reading, broken down by gender simple_slopes &lt;- emtrends(mod6, var = &quot;achrdg12&quot;, specs = &quot;gender&quot;) test(simple_slopes) ## gender achrdg12.trend SE df t.ratio p.value ## Female 0.728 0.0470 496 15.487 &lt;.0001 ## Male 0.550 0.0451 496 12.208 &lt;.0001 6.4.3 A note on plotting Another nice advantage of having everything in one model is that we can level-up our plotting. Check out this plot from the visreg package (and its only line of code!). You should be able to draw a similar conclusion from the plot as you did from looking at the MERVs. # Install the package if you haven&#39;t already done so # install.packages(&quot;visreg&quot;) # Load the package into memory library(visreg) mod6 &lt;- lm(achmat12 ~ gender*achrdg12, data= NELS) visreg(mod6, xvar = &quot;achrdg12&quot;, by = &quot;gender&quot;, overlay = TRUE) Figure 6.4: Example of a plot using the visreg package. 6.4.4 Summary When making inferences about an interaction: Often we can get all of the information we need from the regression coefficient on the interaction term, and its associated test of significance. If the interaction isn’t significant, we stop there. But if the interaction is significant, we may want to report more information about how the focal relationship depends on the moderator. When the focal predictor is categorical it can be interesting to “follow-up” a significant interaction by taking a closer look at the statistical significance of the marginal means (e.g, how the gender gap in Math changes as a function of Reading) When the focal predictor is continuous, it can be interesting to “follow-up” a significant interaction by taking a closer look at the statistical significance of the simple trends / simple slopes. "],["6.5-two-continuous-predictors-6.html", "6.5 Two continuous predictors", " 6.5 Two continuous predictors In this section we address the situation in there is an interaction between two continuous predictors. The regression equation and overall interpretation is the same as the previous sections – e.g., the relationship between Y and X1 changes as a function of X2. However, there are also some special details that crop up when considering an interaction between two continuous predictors. In this section we will address: The importance of centering the two predictors. When there are two continuous predictors, centering helps interpret the coefficients on the predictors (just like in Section 6.3), and can additionally be helpful for reducing the correlation between the predictors and the interaction. How to follow-up a significant interaction using simple trends. Because the predictors are continuous, the focus is on simple trends rather than marginal means. First, we introduce an new example. 6.5.1 Another NELS example To illustrate an interaction between two continuous predictors, let’s replace Gender with SES in our previous analysis. Apologies that this new example is mainly for convenience and doesn’t represent a great research question about, e.g., why the relationships between Math and Reading might change as a function of SES. The example data and overall approach with SES as the moderator are illustrated below (note that the values 9, 19, and 28 are 10th, 50th, and 90th percentiles SES, respectively). #Interaction without centering mod7 &lt;- lm(achmat12 ~ achrdg12*ses, data = NELS) # Note that band = F removes the confidence intervals visreg(mod7, xvar = &quot;achrdg12&quot;, by = &quot;ses&quot;, overlay = TRUE, band = F) Figure 6.5: Math (achmat), Reading (achrdg), and SES 6.5.2 Centering the predictors Centering the predictors facilitates the interpretation of their regression coefficients in the presence of an interaction, just as it did Section 6.3. In particular, the coefficients \\(b_1\\) and \\(b_2\\) in the regression model \\[ \\widehat Y = b_0 + b_1X_1 + b_2X_2 + b_3 (X_1 \\times X_2) \\] can be interpreted in terms of the following simple trends: \\[\\begin{align} \\widehat Y(X_2 =0) &amp; = b_0 + b_1X_1 \\\\ \\widehat Y(X_1 =0)&amp; = b_0 + b_2X_2. \\tag{6.3} \\end{align}\\] For example, \\(b_1\\) is the relationship between \\(Y\\) and \\(X_1\\), when \\(X_2\\) is equal to zero. In general, setting a variable to the value of zero may not be meaningful. But, when setting a centered variable (i.e., a deviation score) to zero, this is equivalent to setting the original variable to its mean. So, if the variables in Equation (6.3) were centered, we could say that \\(b_1\\) is the relationship between \\(Y\\) and \\(X_1\\), when \\(X_2\\) is equal to its mean. Again, this is just the same trick as Section 6.3, but this time both predictors are continuous and both are centered. There is another reason for centering continuous predictors when there is an interaction in the model, and this has to do with reducing the correlation among the predictors and their interaction. In general, the interaction term will be positively correlated with both predictors if (a) the predictors themselves are positively correlated and (b) the predictors take on strictly positive (or strictly negative) values. Highly correlated predictors lead to redundant information the model, so we generally want to avoid this situation (this is technically called multicollinearity and we discuss it in more detail in a Chapter 7). Centering can “break” the correlation between the preditors and the interaction, thereby making the predictors less redundant with their interaction. To see how this works, let’s take a look at Figure 6.6. The left hand panel shows that SES and its interaction with reading are highly correlated. This is because (a) SES and Reading are themselves positively correlated, and (b) both SES and Reading take on strictly positive values. As mentioned, the interaction term will be positively correlated with both predictors whenever these two conditions hold. (The figure just shows the correlation between SES and the interaction, but the same situation holds for Reading.) attach(NELS) # Correlation without centering r &lt;- cor(ses, achrdg12*ses) # Plot par(mfrow = c(1, 2)) title &lt;- paste0(&quot;correlation = &quot;, round(r, 3)) plot(ses, achrdg12*ses, col = &quot;#4B9CD3&quot;, main = title, xlab = &quot;SES&quot;, ylab = &quot;SES X Reading&quot;) achrdg12_dev &lt;- achrdg12 - mean(achrdg12) ses_dev &lt;- ses - mean(ses) r &lt;- cor(ses_dev, achrdg12_dev*ses_dev) # Plot title &lt;- paste0(&quot;correlation = &quot;, round(r, 3)) plot(ses_dev, achrdg12_dev*ses_dev, col = &quot;#4B9CD3&quot;, main = title, xlab = &quot;SES Centered&quot;, ylab = &quot;SES Centered X Reading Centered&quot;) Figure 6.6: Correlation Between SES and SES X Reading, With and Without Centering detach(NELS) We can see in the right hand panel of Figure 6.6 how centering the two predictors “breaks” the linear relationship between SES and its interaction with Reading. After centering, the relationship between the SES and its interaction is now highly non-linear, and the correlation is approximately zero. Again, the same is true for the relationship between Reading and the interaction, but the figure only shows the situation for SES. The upshot of all this is that centering reduces multicollinearity between the “main effects” of the predictors and their interaction. The two sets of output below show the regression of Math on Reading, SES, and their interaction. The first ouptut does not center the predictors, but the second output does (the _dev notation denotes the centered predictors). We can see that SES is a significant predictor in the centered model but not in the “un-centered” model. This has to do with changing the interpretation of the coefficient (it now represents the relationship between Math and SES for students with average Reading), and also reflects the fact that SES is no longer so highly correlated with the interaction term after centering. attach(NELS) # Without centering mod7 &lt;- lm(achmat12 ~ achrdg12*ses) summary(mod7) ## ## Call: ## lm(formula = achmat12 ~ achrdg12 * ses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.134 -3.894 0.728 4.130 15.015 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.84920 5.38980 4.80 2.1e-06 *** ## achrdg12 0.51160 0.09943 5.15 3.9e-07 *** ## ses -0.10011 0.29121 -0.34 0.73 ## achrdg12:ses 0.00427 0.00520 0.82 0.41 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.03 on 496 degrees of freedom ## Multiple R-squared: 0.418, Adjusted R-squared: 0.415 ## F-statistic: 119 on 3 and 496 DF, p-value: &lt;2e-16 # With centering mod8 &lt;- lm(achmat12 ~ achrdg12_dev*ses_dev) summary(mod8) ## ## Call: ## lm(formula = achmat12 ~ achrdg12_dev * ses_dev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.134 -3.894 0.728 4.130 15.015 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.82622 0.28691 198.07 &lt;2e-16 *** ## achrdg12_dev 0.59031 0.03610 16.35 &lt;2e-16 *** ## ses_dev 0.13730 0.04149 3.31 0.001 ** ## achrdg12_dev:ses_dev 0.00427 0.00520 0.82 0.412 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.03 on 496 degrees of freedom ## Multiple R-squared: 0.418, Adjusted R-squared: 0.415 ## F-statistic: 119 on 3 and 496 DF, p-value: &lt;2e-16 detach(NELS) To check your understanding of the output above, please provide an interpretation of all four regression coefficients in the centered model. Your interpretations should make reference to the situation where one or both predictors are equal to zero (see Equation (6.3) above) and should also mentioned the interpretation of the value of zero for the centered variables Note that the interaction, the R-squared, and their associated tests do not change value. This is discussed in more detail in the extra material at the end of this section, but it is sufficient to note that centering only affects the interpretation of the main effects (and the intercept, of course). 6.5.3 Simple trends Centering helps us interpret the “main effects” of the individual predictors, but we haven’t yet discussed how to interpret the interaction term when both predictors are continuous. In the case with a binary predictor, we had two different regression lines. So what do we get when both predictors are continuous? The usual way to answer this question is an extension of the MERV approach to marginal effects, discussed in Section 6.4. The basic idea is to present the relationship between the two focal variables, for a selection of values of the moderator. This idea is shown in Figure 6.5 above. As with MERV, the choice of values of the moderator is up to the researcher, but some usual choices are The quartiles of the moderator (i.e., the five number summary) or subset thereof M \\(\\pm\\) 1 SD of the moderator A selection of percentiles (visreg uses the 10th, 50th, and 90th) These are all doing very similar things, so choosing among them isn’t very important. Although the interaction between Reading and SES was not significant in our example model, let’s break down the interaction using SES as the moderator, just to see how this approach works. The first part output below shows the simple slopes for the three values of SES shown in Figure 6.5 (i.e., the 10th, 50th, and 90th deciles). We can see that the simple slopes are all different from zero (and the non-significant interaction tells us that they are not different from one-another). # Break down interaction with SES as moderator simple_slopes &lt;-emtrends(mod7, var = &quot;achrdg12&quot;, specs = &quot;ses&quot;, at = list(ses = c(9, 19, 28))) test(simple_slopes) ## ses achrdg12.trend SE df t.ratio p.value ## 9 0.550 0.0583 496 9.429 &lt;.0001 ## 19 0.593 0.0365 496 16.251 &lt;.0001 ## 28 0.631 0.0639 496 9.878 &lt;.0001 6.5.4 Summary When regressing an outcome on two continuous predictors and their interaction, the overall interpretation of the model is same as discussed in Section 6.3, but: It is useful to center both predictors, to facilitate the interpretation of the main effects (i.e., regression coefficients on the individual predictors), and to reduce the correlation between the predictors and their interaction (i.e., reduce multicollinearity). When following up a significant interaction, the usual approach is to report the simple trends between the focal variables for a selection of values of the moderator (e.g., a selection of percentiles). The example illustrated how to do this even though the interaction was not significant, but you shouldn’t follow up a non-significant interaction. 6.5.5 Extra: How centering works* It might seem that centering both predictors to improve the chances of getting significant main effects is a dubious practice. However, using the centered or the un-centered variables doesn’t really make a difference in term of what predictors are in the model are. The follow algebra show why, using \\(D = X - \\bar X\\) for the centered variables: \\[\\begin{align} \\widehat Y &amp; = b_0 + b_1D_1 + b_1D_2 + b_3 (D_1 \\times D_2) \\\\ &amp; = b_0^* + (b_1 - b_3 \\bar X_1) X_1 + (b_2 - b_3 \\bar X_2) X_2 + b_3 (X_1 \\times X_2) \\\\ \\text{where} &amp; \\\\ \\\\ b_0^* &amp; = a - b_1\\bar X_1 - b_2\\bar X_2 - b_3\\bar X_1\\bar X_2. \\end{align}\\] The second line of the equation shows that we are not changing what we regress \\(Y\\) on – i.e., the predictors are still \\(X_1\\) and \\(X_2\\). We are re-packaging the intercept and main effects, which is exactly the purpose of this approach. But, centering does not change the regression coefficient for the interaction – it is still interpreted with respect to the un-centered variables. "],["6.6-two-categorical-predictors-6.html", "6.6 Two categorical predictors", " 6.6 Two categorical predictors This section addresses interactions between two categorical predictors. Up until now, we have looked at interactions only for categorical predictors that are dichotomous. In this section we address an example in which one of the categorical predictors has more than two levels. This requires combining what we learned about contrast coding (Chapter 5) with what we have learned about interactions. One nice aspect of interactions among categorical predictors is that we usually don’t need to use procedures like marginal effects to follow up significant interactions, so long as we make good use of contrast coding. In experimental (as opposed to observational) settings, interactions among categorical predictors fall under the much larger topic of ANOVA and experimental design. The analysis we look at in this section is a two-way between-subjects ANOVA, meaning that there are two categorical predictors considered, as well as their interaction, and both predictors are cross-sectional. ANOVA is a big topic and is not the focus of this course. However, we will discuss how to summarize the results of our analysis in an ANOVA table, and consider how this differs from the standard regression approach. 6.6.1 An example from ECLS For this topic we will switch over to the ECLS data and examine how SES and Pre-K attendance interact to predict Math Achievement at the beginning of Kindergarten. The variables we will examine are Math Achievement at the beginning of K (c1rmscal). This is the number of correct questions on a test with approximately 70 items. Whether the child attended Pre-K (p1center). This is a binary variable that indicates pre-K attendance. SES, coded as quintiles (wksesq5). We will denote this variable as SES, but keep in mind it is quintiles in this example (e.g., SES = 1 are the respondents with SES between the minimum and the first quintile). Coding SES as quintiles allows us to consider it as a categorical predictor with 5 levels. This is convenient for our illustration of interactions between categorical predictors. It is also a widely-used practice in policy research, because SES often has non-linear relationships with outcome variables of interest, and these relationships can be more easily captured by treating SES as a categorical variable. In this analysis, our focus will be whether the “effect” of Pre-K on Math Achievement depends on (i.e., is moderated by) the child’s SES. Please note that I will use the term “effect” in this section to simplify language, but we know that Pre-K attendance was not randomly assigned in ECLS, so please keep in mind that this terminology is not strictly correct. The relationship among the three variables is summarized in the visreg plot below. We can see that the effect of Pre-K on Math Achievement appears to differ as a function of SES – i.e., it appears that there is an interaction between Pre-K and SES. Our goal in this section is to produce an analysis corresponding to the figure. Before moving on, please take a moment to write down your interpretation of Figure 6.7, focussing on how it illustrates an interaction between SES and Pre-K. Additionally, please describe how the figure would be different if there was no interaction between Pre-K and SES. load(&quot;ECLS2577.Rdata&quot;) ecls$prek &lt;- factor(2 - ecls$p1center) ecls$wksesq5 &lt;- factor(ecls$wksesq5) mod &lt;- lm(c1rmscal ~ prek*wksesq5, data = ecls) visreg::visreg(mod, xvar = &quot;wksesq5&quot;, by = &quot;prek&quot;, partial = F, rug = F, overlay = T, strip.names = T, xlab = &quot;SES&quot;, ylab = &quot;Math Achievement in K&quot;) Figure 6.7: Math Achievement, Pre-K Attendence, and SES 6.6.2 The “no-interaction” model As in Section 6.2, we will start with a model that includes only the main effects of SES and Pre-K. Seeing where that model “goes wrong” is a good way of understanding the interaction between the two predictors. In order to represent a model with multiple categorical predictors, it is helpful to change our notation from the usual \\(Y\\) and \\(X\\) to the more informative “variable names” notation: \\[\\begin{align} \\widehat Y = b_0 + b_1 PREK + b_2SES_2 + b_3SES_3 + b_4 SES_4 + b_5 SES_5. \\tag{6.4} \\end{align}\\] In this notation, the predictor variables are indicators (binary dummies) that use the variable names rather than \\(X_1\\), \\(X_2\\), etc. The variable \\(PREK\\) is just the indicator for Pre-K attendance, as defined above. The variable \\(SES_j\\) is an indicator for the j-th quintile of SES. Note that both predictors use reference-group coding, as discussed in Chapter 5. For \\(PREK\\), reference-group coding is implied because it is a binary indicator. For \\(SES\\), reference-group coding is accomplished by omitting the dummy for the first quintile (i.e., the first quintile is the reference group). We can interpret the coefficients in this model using the same two-step procedure described in Chapter 5. Since there are many terms in the model, things are going to start getting messy quickly, so brace yourself for some long equations (but simple math!). The main points about the interpretation of this model are as follows. The intercept is the predicted value of Math Achievement for students in the first SES quintile who did not attend Pre-K. This corresponds to the blue line in the first column of Figure 6.7. \\[\\begin{align} \\widehat Y(PREK = 0, SES = 1) &amp; = b_0 + b_1 (0) + b_2(0)+ b_3(0) + b_4 (0) + b_5 (0) \\\\ &amp; = b_0 \\end{align}\\] The effect of Pre-K attendance for students in the first SES quintile is equal to \\(b_1\\). This corresponds to the difference between the red and blue lines in the first column of Figure 6.7. \\[\\begin{align} \\widehat Y(PREK = 1, SES = 1) &amp; = b_0 + b_1 (1) + b_2(0)+ b_3(0) + b_4 (0) + b_5 (0) \\\\ &amp; = b_0 + b_1 \\\\ \\implies &amp; \\end{align}\\] \\[\\begin{align} \\widehat Y(PREK = 1, SES = 1) - \\widehat Y(PREK = 0, SES = 1) &amp; = b_1 \\end{align}\\] Because the model in Equation (6.4) does not include an interaction, we know that it implies the effect of pre-K is constant over levels of SES. Below we consider SES = 2, but the same approach works for the other levels of SES. \\[\\begin{align} \\widehat Y(PREK = 0, SES = 2) &amp; = b_0 + b_1 (0) + b_2(1 )+ b_3(0) + b_4 (0) + b_5 (0)\\\\ &amp; = b_0 + b_2 \\\\ \\\\ \\widehat Y(PREK = 1, SES = 2)&amp; = b_0 + b_1 (1) + b_2(1)+ b_3(0) + b_4 (0) + b_5 (0)\\\\ &amp; = b_0 + b_1 + b_2 \\\\ \\\\ \\implies &amp; \\end{align}\\] \\[\\begin{align} \\widehat Y(PREK = 1, SES = 2) - \\widehat Y(PREK = 0, SES = 2) = b_1 \\end{align}\\] This equation says that the difference between the red and blue lines in the second column of Figure 6.7 is the same as the difference in the first column – i.e., they both equal \\(b_1\\). This is what it means for there to be no interaction between two categorical predictors. If you want more practice with this, you can show that Equation (6.4) implies the effect of Pre-K is constant over all levels of SES. Additionally, you can use the 2-step approach to show that the effect of SES is constant over levels of Pre-K attendance. 6.6.3 Adding the interaction(s) We have just seen that Equation (6.4) implies that the effect of Pre-K is constant over levels SES, and vise versa. In order to address our research question about whether the relationship between Pre-K attendance and Math Achievement depends on children’s SES, we will need to add something to the model – an interaction (surprise!). We know that interactions are just products (multiplication) of predictor variables. But, since SES is represented as 4 dummies, this means we need 4 products in order to represent the interaction of Pre-K with SES. The resulting model can be written: \\[\\begin{align} \\widehat Y = &amp; b_0 + b_1 PREK + b_2SES_2 + b_3SES_3 + b_4 SES_4 + b_5 SES_5 + \\\\ &amp; b_6 (PREK \\times SES_2) + b_7(PREK \\times SES_3) + \\\\ &amp; b_8 (PREK \\times SES_4) + b_9 (PREK \\times SES_5) \\tag{6.5} \\end{align}\\] As you can see, we have a lot of predictors in this model! Although we are only considering two distinct “conceptual” predictors, we have 9 coefficients in our regression model (+ the intercept). Again, there are a few main things to notice: The interpretation of the intercept has not changed. It still corresponds to the blue line in the first column of Figure 6.7. The regression coefficient on \\(PREK\\) is still the “effect” of Pre-K for students in the first SES quintile. It corresponds to the difference between the red and blue line in the first column of Figure 6.7. This is because all the \\(SES_j\\) variables are equal to zero for students in the first SES quintile, and so all of the interaction terms in Equation (6.5) are equal to zero for this case. The effect of Pre-K is no longer constant over levels of SES. Again we will focus on SES = 2, but the same approach works for the other levels of SES. \\[\\begin{align} \\widehat Y(PREK = 0, SES = 2) &amp; = b_0 + b_1 (0) + b_2(1 )+ b_3(0) + b_4 (0) + b_5 (0) + \\\\ &amp; b_6 (0 \\times 1) + b_7(0 \\times 0) + b_8 (0 \\times 0) + b_9 (0\\times 0) \\\\ &amp; = b_0 + b_2 \\\\ \\\\ \\widehat Y(PREK = 1, SES = 2) &amp; = b_0 + b_1 (1) + b_2(1)+ b_3(0) + b_4 (0) + b_5 (0) + \\\\ &amp; b_6 (1 \\times 1) + b_7(1 \\times 0) + b_8 (1 \\times 0) + b_9 (1\\times 0) \\\\ &amp; = b_0 + b_1 + b_2 + b_6 \\\\ \\\\ \\implies &amp; \\end{align}\\] \\[\\begin{align} \\widehat Y(PREK = 1, SES = 2) - \\widehat Y(PREK = 0, SES = 2) = b_1 + b_6 \\end{align}\\] The last line shows that the “effect” of Pre-K for students in the second SES quintile is \\(b_1 + b_6\\). This is not the same as for the effect for students in the first quintile, which was just \\(b_1\\). In other words, the difference between the red and blue lines in the first column of Figure 6.7 (i.e., \\(b_1\\)) is not equal to the difference in the second column (i.e., \\(b_1 + b_6\\)). Consequently, our new model with the interaction better reflects the example data. The same approach shows that the effect of Pre-K at each level of SES results in a similar equation: \\[\\begin{align} \\widehat Y(PREK = 1, SES = 3) - \\widehat Y(PREK = 0, SES = 3) &amp; = b_1 + b_7 \\\\ \\widehat Y(PREK = 1, SES = 4) - \\widehat Y(PREK = 0, SES = 4) &amp; = b_1 + b_8 \\\\ \\widehat Y(PREK = 1, SES = 5) - \\widehat Y(PREK = 0, SES = 5) &amp; = b_1 + b_9 \\\\ \\end{align}\\] This pattern makes it clear that, to isolate effect of each interaction (i.e., \\(b_6\\) through \\(b_9\\)), we need to subtract off \\(b_1\\) – i.e., we need to subtract off the effect of Pre-K for students in the first SES quintile. In this sense, the interpretation of \\(b_1\\) is quite similar the interpretation of the intercept in regular reference-group coding (see Section 5.7.4). It is the “reference effect” or baseline to which the interaction terms are compared. For example The interaction between Pre-K and the second SES quintile is the additional effect pre-K has on Math Achievement for students in the second SES quintile, as compared to the effect in the first SES quintile. The interaction between Pre-K and the third SES quintile is the additional effect pre-K has on Math Achievement for students in the 3rd SES quintile, as compared to the effect in the first SES quintile. etc etc. Mathematically, the interaction terms are represented as “differences-in-differences”. For example, \\[\\begin{align} b_6 &amp; = [\\widehat Y(PREK = 1, SES = 2) - \\widehat Y(PREK = 0, SES = 2)] - b_1 \\\\ &amp; = [\\widehat Y(PREK = 1, SES = 2) - \\widehat Y(PREK = 0, SES = 2)] \\\\ &amp; - [\\widehat Y(PREK = 1, SES = 1) - \\widehat Y(PREK = 0, SES = 1)] \\end{align}\\] This looks quite complicated but it is just an extension of reference-group coding. This equation is saying that the “reference effect” or “baseline” for interpreting the interaction (\\(b_6\\)) is the effect of Pre-K in the first SES quintile (i.e., \\(b_1\\)). As noted above, all of the interaction terms have the same reference effect. 6.6.4 Back to the example That last section was a lot to take in, so let’s put some numbers on the page to check our understanding. The output below shows the summary for a model that regresses Math Achievement on Pre-K, SES, and their interaction. Please write down an interpretation of magnitude, direction, and statistical significance of each regression coefficient in this output (including the intercept), and be prepared to share your answers in class. Remember that wksesq5 is the variable code for the SES quintiles – the digit that follows the variable code indicates the level of variable. It may be helpful to refer to Figure 6.7 in your interpretations. mod &lt;- lm(c1rmscal ~ prek*wksesq5, data = ecls) summary(mod) ## ## Call: ## lm(formula = c1rmscal ~ prek * wksesq5, data = ecls) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.768 -4.768 -0.975 3.955 31.232 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.045 0.735 21.82 &lt; 2e-16 *** ## prek1 -0.373 0.960 -0.39 0.6973 ## wksesq52 2.293 0.957 2.40 0.0166 * ## wksesq53 2.930 0.913 3.21 0.0013 ** ## wksesq54 4.631 0.944 4.91 9.9e-07 *** ## wksesq55 7.299 1.034 7.06 2.2e-12 *** ## prek1:wksesq52 1.064 1.212 0.88 0.3801 ## prek1:wksesq53 2.109 1.154 1.83 0.0679 . ## prek1:wksesq54 1.671 1.168 1.43 0.1527 ## prek1:wksesq55 2.797 1.234 2.27 0.0235 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.9 on 2567 degrees of freedom ## Multiple R-squared: 0.162, Adjusted R-squared: 0.159 ## F-statistic: 55.2 on 9 and 2567 DF, p-value: &lt;2e-16 6.6.5 The ANOVA approach The output in the previous section is detailed enough that it is not usually required to follow-up a significant interaction among categorical predictors using marginal effects. However, the summary output omits some information we might be interested in. For example, the Pre-K indicator in the above output tells us the effect of Pre-K, but only for children in the first SES quintile. We might also want to know, what is the main effect of Pre-K across levels of SES – i.e., is there a significant difference in Math Achievement for students who attended Pre-K or not, after controlling for their level of SES? Similarly, what is the main effect of SES? I’ll note that some people think it is bad practice to interpret main effects in the presence of an interaction. The main effects tell us what the effect of a predictor is “on average” or overall, after controlling for the other predictor(s). But the interaction tells us that this effect depends on the other predictor(s). Some people think that you shouldn’t report the overall effect when the “real message” of the interaction is that the effect changes as a function of the other predictors. I think that main effects and interactions aren’t really incompatible concepts, but you should be careful with them. Anyway, we can summarize the main effects of Pre-K and SES, well as their interaction, by asking how much variance they explain, after controlling for the other predictor(s). This is the ANOVA approach we discussed last semester, but now applied to two categorical predictors. The ANOVA table for our example is below, and it is followed by the R-squared coefficients for each predictor, which are called “eta-squared” (\\(\\eta^2\\)) in the context of ANOVA. These R-squared (eta-squared) coefficients tell us what proportion of the variance in Math Achievement is attributable to the main effects and the interaction. Note that the R-squared (eta-squared) values do not add up to the total R-squared reported in the lm output above. This is because any variance that is jointly explained by the predictors is not attributed to individual predictors (e.g., the area “B” in the Venn diagram in Figure ?? is not counted towards the effects for the individual predictors). In the next chapter, we will address how to build models so that we can systematically divide up the variance explained by the predictors. # ANOVA Table anova(mod) ## Analysis of Variance Table ## ## Response: c1rmscal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## prek 1 3434 3434 72.14 &lt;2e-16 *** ## wksesq5 4 19914 4978 104.58 &lt;2e-16 *** ## prek:wksesq5 4 299 75 1.57 0.18 ## Residuals 2567 122198 48 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # R-squared (eta-squared) #install.packages(&quot;effectsize&quot;) effectsize::eta_squared(mod, partial = F) ## # Effect Size for ANOVA (Type I) ## ## Parameter | Eta2 | 95% CI ## -------------------------------------- ## prek | 0.02 | [0.01, 1.00] ## wksesq5 | 0.14 | [0.12, 1.00] ## prek:wksesq5 | 2.05e-03 | [0.00, 1.00] ## ## - One-sided CIs: upper bound fixed at [1.00]. Please write down your interpretation of the ANOVA table and R-squared (eta-squared) coefficients and be prepared to share you thoughts in class. Note that the ANOVA output leads to different conclusions than the regression output above. We will discuss the discrepancies between the ANOVA and regression output in class. "],["6.7-workbook-3.html", "6.7 Workbook", " 6.7 Workbook This section collects the questions asked in this chapter. We will discuss these questions in class. If you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you! So, please engage with each question by writing down one or more answers, asking clarifying questions, posing follow up questions, etc. Section 6.1 attach(NELS) mod1 &lt;- lm(achmat12[females] ~ achrdg12[females]) mod2 &lt;- lm(achmat12[males] ~ achrdg12[males]) # Plot reading and math for females plot(achrdg12[females], achmat12[females], xlab = &quot;Reading&quot;, ylab = &quot;Math&quot;) abline(mod1, lwd = 2) # Add points and line for males points(achrdg12[males], achmat12[males], col = &quot;#4B9CD3&quot;, pch = 2) abline(mod2, col = &quot;#4B9CD3&quot;, lwd = 2) # Add a legend legend(x = &quot;topleft&quot;, legend = levels(gender), pch = c(1, 2), col = c(1, &quot;#4B9CD3&quot;)) Figure 6.8: Math Achievement, Reading Achievement, and Gender. detach(NELS) Take a minute to think about what this plot is telling us about the relationships among Math Achievement, Reading Achievement, and Gender. Is the gender gap in math constant? Is the relationship between math and reading the same for males and females? Can you provide a summary of the plot in terms of what it says about the gender gap in Math Achievement? Section 6.2 No interaction model: The regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the simple trends and the gender gap in Math Achievement for the NELS example. (Don’t worry about statistical significance, just focus on the meaning of the coefficients.) mod3 &lt;- lm(achmat12 ~ achrdg12 + gender, data = NELS) coef(mod3) ## (Intercept) achrdg12 genderMale ## 19.9812 0.6355 3.5017 Section 6.3 Interaction model: The regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the interaction between Gender and Reading. (Don’t worry about statistical significance, just focus on the meaning of the coefficients.) attach(NELS) genderXachrdg12 &lt;- (as.numeric(gender) - 1) * achrdg12 mod4 &lt;- lm(achmat12 ~ achrdg12 + gender + genderXachrdg12) coef(mod4) ## (Intercept) achrdg12 genderMale genderXachrdg12 ## 14.8031 0.7282 13.3933 -0.1779 detach(NELS) Interaction model with centered continuous predictor: Please write down your interpretation of the intercept and the regression coefficient for Gender in the regression output below. attach(NELS) # compute the deviation scores for reading reading_dev &lt;- achrdg12 - mean(achrdg12, na.rm = T) # Run the interaction model as above genderXreading_dev &lt;- (as.numeric(gender) - 1) * reading_dev mod5 &lt;- lm(achmat12 ~ reading_dev + gender + genderXreading_dev) coef(mod5) ## (Intercept) reading_dev genderMale genderXreading_dev ## 55.2944 0.7282 3.4993 -0.1779 detach(NELS) Section 6.4 The output shows the gender gap in Math Achievement for 5 values of Reading Achievement. The values of Reading Achievement are the 5 quartiles. Please examine the statistical significance of the gender gap in Math Achievement at the 5 quartiles of Reading Achievement and make a conclusion whether the gap “dissapeared” for students with higher levels of Reading Achievement. # Install the package if you haven&#39;t already done so # install.packages(&quot;emmeans&quot;) # Load the package into memory library(emmeans) # Fit the model using R&#39;s formula syntax for interaction &#39;*&#39; mod6 &lt;- lm(achmat12 ~ gender*achrdg12, data= NELS) # Use the emmeans function to get the gender means on math, broken down by reading gap &lt;- emmeans(mod6, specs = &quot;gender&quot;, by = &quot;achrdg12&quot;, cov.reduce = quantile) # Test whether the differences are significant contrast(gap, method = &quot;pairwise&quot;) ## achrdg12 = 31.8: ## contrast estimate SE df t.ratio p.value ## Female - Male -7.74 1.637 496 -4.728 &lt;.0001 ## ## achrdg12 = 51.2: ## contrast estimate SE df t.ratio p.value ## Female - Male -4.27 0.593 496 -7.207 &lt;.0001 ## ## achrdg12 = 57.0: ## contrast estimate SE df t.ratio p.value ## Female - Male -3.25 0.529 496 -6.138 &lt;.0001 ## ## achrdg12 = 61.7: ## contrast estimate SE df t.ratio p.value ## Female - Male -2.41 0.658 496 -3.659 0.0003 ## ## achrdg12 = 68.1: ## contrast estimate SE df t.ratio p.value ## Female - Male -1.28 0.967 496 -1.321 0.1872 Section 6.5 To check your understanding of centering with two continuous predictors, please provide an interpretation of all four regression coefficients in the centered model (below). Your interpretations should make reference to the situation where one or both predictors are equal to zero (see Equation (6.3)) and should also mentioned the interpretation of the value of zero for the centered variables. attach(NELS) mod8 &lt;- lm(achmat12 ~ achrdg12_dev*ses_dev) summary(mod8) ## ## Call: ## lm(formula = achmat12 ~ achrdg12_dev * ses_dev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.134 -3.894 0.728 4.130 15.015 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.82622 0.28691 198.07 &lt;2e-16 *** ## achrdg12_dev 0.59031 0.03610 16.35 &lt;2e-16 *** ## ses_dev 0.13730 0.04149 3.31 0.001 ** ## achrdg12_dev:ses_dev 0.00427 0.00520 0.82 0.412 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.03 on 496 degrees of freedom ## Multiple R-squared: 0.418, Adjusted R-squared: 0.415 ## F-statistic: 119 on 3 and 496 DF, p-value: &lt;2e-16 detach(NELS) Section 6.6 Please take a moment to write down your interpretation of the figure below, focussing on how it illustrates an interaction between SES and Pre-K. Additionally, please describe how the figure would be different if there was no interaction between Pre-K and SES. load(&quot;ECLS2577.Rdata&quot;) ecls$prek &lt;- factor(2 - ecls$p1center) ecls$wksesq5 &lt;- factor(ecls$wksesq5) mod &lt;- lm(c1rmscal ~ prek*wksesq5, data = ecls) visreg::visreg(mod, xvar = &quot;wksesq5&quot;, by = &quot;prek&quot;, partial = F, rug = F, overlay = T, strip.names = T, xlab = &quot;SES&quot;, ylab = &quot;Math Achievement in K&quot;) Please write down an interpretation of magnitude, direction, and statistical significance of each regression coefficient in this output (including the intercept), and be prepared to share your answers in class. Remember that wksesq5 is the variable code for the SES quintiles – the digit that follows the variable code indicates the level of variable. summary(mod) ## ## Call: ## lm(formula = c1rmscal ~ prek * wksesq5, data = ecls) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.768 -4.768 -0.975 3.955 31.232 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.045 0.735 21.82 &lt; 2e-16 *** ## prek1 -0.373 0.960 -0.39 0.6973 ## wksesq52 2.293 0.957 2.40 0.0166 * ## wksesq53 2.930 0.913 3.21 0.0013 ** ## wksesq54 4.631 0.944 4.91 9.9e-07 *** ## wksesq55 7.299 1.034 7.06 2.2e-12 *** ## prek1:wksesq52 1.064 1.212 0.88 0.3801 ## prek1:wksesq53 2.109 1.154 1.83 0.0679 . ## prek1:wksesq54 1.671 1.168 1.43 0.1527 ## prek1:wksesq55 2.797 1.234 2.27 0.0235 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.9 on 2567 degrees of freedom ## Multiple R-squared: 0.162, Adjusted R-squared: 0.159 ## F-statistic: 55.2 on 9 and 2567 DF, p-value: &lt;2e-16 Please write down your interpretation of the ANOVA table and R-squared (eta-squared) coefficients reported below, and be prepared to share you thoughts in class.** . Please write down your interpretation of the ANOVA table and R-squared (eta-squared) coefficients and be prepared to share you thoughts in class. anova(mod) ## Analysis of Variance Table ## ## Response: c1rmscal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## prek 1 3434 3434 72.14 &lt;2e-16 *** ## wksesq5 4 19914 4978 104.58 &lt;2e-16 *** ## prek:wksesq5 4 299 75 1.57 0.18 ## Residuals 2567 122198 48 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 effectsize::eta_squared(mod, partial = F) ## # Effect Size for ANOVA (Type I) ## ## Parameter | Eta2 | 95% CI ## -------------------------------------- ## prek | 0.02 | [0.01, 1.00] ## wksesq5 | 0.14 | [0.12, 1.00] ## prek:wksesq5 | 2.05e-03 | [0.00, 1.00] ## ## - One-sided CIs: upper bound fixed at [1.00]. "],["6.8-exercises-6.html", "6.8 Exercises", " 6.8 Exercises These exercises provide an overview of how to add interactions using the lm function, how to center continuous predictors, and how to follow-up significant interactions with the emmeans package. 6.8.1 Binary + continuous + interaction There are multiple ways of implementing interactions in R. We can “hard code” new variables into our data (e.g., the product of a binary gender variable and reading) We can use R’s formula notation for single term interactions (:) We can use R’s formula notation for factorial interactions (*) The following code illustrates the three approaches and shows that they all producing the same output. In general, the * syntax is the easiest to use, so we will stick with that one going forward. The variables used in the example are from the NELS data: achmat12 is Mat Achievement (percent correct on a mat test) in grade 12. achrdg12 is Reading Achievement (percent correct on a reading test) in grade 12. gender is dichotomous encoding of gender with values Male and Female (it is not a binary variable, but a factor, as discussed in Section 9.8). load(&quot;NELS.RData&quot;) attach(NELS) # Interaction via hard coding genderXreading &lt;- (as.numeric(gender) - 1) * achrdg12 mod1 &lt;- lm(achmat12 ~ achrdg12 + gender + genderXreading) summary(mod1) ## ## Call: ## lm(formula = achmat12 ~ achrdg12 + gender + genderXreading) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.058 -3.786 0.501 4.077 16.289 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.8031 2.6492 5.59 3.8e-08 *** ## achrdg12 0.7282 0.0470 15.49 &lt; 2e-16 *** ## genderMale 13.3933 3.6583 3.66 0.00028 *** ## genderXreading -0.1779 0.0651 -2.73 0.00652 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.8 on 496 degrees of freedom ## Multiple R-squared: 0.462, Adjusted R-squared: 0.459 ## F-statistic: 142 on 3 and 496 DF, p-value: &lt;2e-16 # Interaction via `:` operator mod2 &lt;- lm(achmat12 ~ achrdg12 + gender + achrdg12:gender) summary(mod2) ## ## Call: ## lm(formula = achmat12 ~ achrdg12 + gender + achrdg12:gender) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.058 -3.786 0.501 4.077 16.289 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.8031 2.6492 5.59 3.8e-08 *** ## achrdg12 0.7282 0.0470 15.49 &lt; 2e-16 *** ## genderMale 13.3933 3.6583 3.66 0.00028 *** ## achrdg12:genderMale -0.1779 0.0651 -2.73 0.00652 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.8 on 496 degrees of freedom ## Multiple R-squared: 0.462, Adjusted R-squared: 0.459 ## F-statistic: 142 on 3 and 496 DF, p-value: &lt;2e-16 # Interaction via `*` operator mod3 &lt;- lm(achmat12 ~ achrdg12*gender) summary(mod3) ## ## Call: ## lm(formula = achmat12 ~ achrdg12 * gender) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.058 -3.786 0.501 4.077 16.289 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.8031 2.6492 5.59 3.8e-08 *** ## achrdg12 0.7282 0.0470 15.49 &lt; 2e-16 *** ## genderMale 13.3933 3.6583 3.66 0.00028 *** ## achrdg12:genderMale -0.1779 0.0651 -2.73 0.00652 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.8 on 496 degrees of freedom ## Multiple R-squared: 0.462, Adjusted R-squared: 0.459 ## F-statistic: 142 on 3 and 496 DF, p-value: &lt;2e-16 Before moving on, check your interpretation of the coefficients in the models. In particular, what does the regression coefficient on the interaction term mean? 6.8.2 Centering continuous predictors As noted in Section 6.3, the regression coefficient on Gender is not very interpretable when there is an interaction in the model. In the above output, the coefficient on gender tells us the gender gap in Math Achievement when achrdg12 = 0. We can fix this issue by re-scaling achrdg12 so that zero has a meaningful value. One easy and widely used approach is to center achrdg12 at its mean. When a variable is centered at its mean it is called a deviation score. Let’s see what happens when we use deviation scores achrdg12 instead of the “raw” score # Re-run the model with reading centered at its mean achrdg12_dev &lt;- achrdg12 - mean(achrdg12) mod4 &lt;- lm(achmat12 ~ achrdg12_dev*gender) summary(mod4) ## ## Call: ## lm(formula = achmat12 ~ achrdg12_dev * gender) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.058 -3.786 0.501 4.077 16.289 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.2944 0.3513 157.41 &lt; 2e-16 *** ## achrdg12_dev 0.7282 0.0470 15.49 &lt; 2e-16 *** ## genderMale 3.4993 0.5214 6.71 5.3e-11 *** ## achrdg12_dev:genderMale -0.1779 0.0651 -2.73 0.0065 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.8 on 496 degrees of freedom ## Multiple R-squared: 0.462, Adjusted R-squared: 0.459 ## F-statistic: 142 on 3 and 496 DF, p-value: &lt;2e-16 Note that the intercept and the regression coefficient on gender have changed values compared to mod3. What is the interpretation of these coefficients in the new model? Also note the coefficient on reading and the interaction don’t change. We showed why in section 6.5.5. Next, let’s plot our model with the interaction term. One advantage of having everything in a single model is that we can level-up our plotting! The following code uses the visreg package. Note that the error bands in the plot are produced using the standard errors from emmeans, which is discussed in the following section. If you want to know more about how visreg works, type help(visreg). # Install the package if you haven&#39;t already done so # install.packages(&quot;visreg&quot;) # Load the package into memory library(visreg) visreg(mod3, xvar = &quot;achrdg12&quot;, by = &quot;gender&quot;, overlay = TRUE) 6.8.3 Breaking down a significant interaction If an interaction is significant, then we usually want to report a bit more information about how the focal relationship changes as a function of the moderators. There are two main ways to do this: Marginal effects (aka marginal means, least squares means, adjusted means): This approach is used when the focal predictor is categorical and we want to compare means across the categories, as a function of the moderator. Simple trends (aka simple slopes): This approach is used when the focal predictor is continuous and we want to examine the slopes of the simple trends as a function of the moderator. Usually, the researcher will chose one or the other approach, whichever is best suited to address the research questions of interest. Our example was motivated by consideration of the gender gap in STEM (i.e., the relationship between a STEM and a categorical predictor), so the marginal effects approach is better suited. We will also illustrate simple trends, just to show how that approach works. 6.8.4 Marginal effects Let’s breakdown the interaction by asking how the relationship between Math and Gender (i.e., the gender achievement gap in Math) changes as a function of Reading. This can be done using emmeans package, and the main function in that pacakge is also called emmeans. The three main arguments for the emmeans function: object – the output of lm. This is the first argument specs – which factors in the model we want the means of (i.e., the focal predictor) by – which predictor(s) we want to use to breakdown the means (i.e., the moderator(s)) We can use emmeans to compute the marginal effect at the mean (MEM) as follows: # Install the package if you haven&#39;t already done so # install.packages(&quot;emmeans&quot;) # Load the package into memory library(emmeans) # Use the emmeans function to get the gender means on math, broken down by reading gap &lt;- emmeans(mod3, specs = &quot;gender&quot;, by = &quot;achrdg12&quot;) summary(gap) ## achrdg12 = 55.6: ## gender emmean SE df lower.CL upper.CL ## Female 55.3 0.351 496 54.6 56.0 ## Male 58.8 0.385 496 58.0 59.5 ## ## Confidence level used: 0.95 # Test whether the difference is significant contrast(gap, method = &quot;pairwise&quot;) ## achrdg12 = 55.6: ## contrast estimate SE df t.ratio p.value ## Female - Male -3.5 0.521 496 -6.712 &lt;.0001 In the above output, we only get one Gender difference in Math, and that is computed for the value of achrdg12 = 55.6, which is the mean value of Reading. As noted, this is called the marginal effect at the mean (MEM). It is often more helpful to report Gender difference for multiple different values of achrdg12, which is called MERV (marginal effects at representative values). While there are many ways to chose the representative values, one convenient approach approach is to use the quartiles of achrdg12. This is accomplished using the cov.reduce argument of emmeans as follows. # Use the the covarate reduce option of emmeans with the quantile function gap_quartiles &lt;- emmeans(mod3, specs = &quot;gender&quot;, by = &quot;achrdg12&quot;, cov.reduce = quantile) summary(gap_quartiles) ## achrdg12 = 31.8: ## gender emmean SE df lower.CL upper.CL ## Female 37.9 1.186 496 35.6 40.3 ## Male 45.7 1.129 496 43.5 47.9 ## ## achrdg12 = 51.2: ## gender emmean SE df lower.CL upper.CL ## Female 52.1 0.412 496 51.3 52.9 ## Male 56.4 0.426 496 55.6 57.2 ## ## achrdg12 = 57.0: ## gender emmean SE df lower.CL upper.CL ## Female 56.3 0.355 496 55.6 57.0 ## Male 59.6 0.393 496 58.8 60.3 ## ## achrdg12 = 61.7: ## gender emmean SE df lower.CL upper.CL ## Female 59.8 0.447 496 58.9 60.6 ## Male 62.2 0.482 496 61.2 63.1 ## ## achrdg12 = 68.1: ## gender emmean SE df lower.CL upper.CL ## Female 64.4 0.674 496 63.1 65.7 ## Male 65.7 0.693 496 64.3 67.0 ## ## Confidence level used: 0.95 # Test whether the gender difference in math achievement is significant at each quartile of reading achievement contrast(gap_quartiles, method = &quot;pairwise&quot;) ## achrdg12 = 31.8: ## contrast estimate SE df t.ratio p.value ## Female - Male -7.74 1.637 496 -4.728 &lt;.0001 ## ## achrdg12 = 51.2: ## contrast estimate SE df t.ratio p.value ## Female - Male -4.27 0.593 496 -7.207 &lt;.0001 ## ## achrdg12 = 57.0: ## contrast estimate SE df t.ratio p.value ## Female - Male -3.25 0.529 496 -6.138 &lt;.0001 ## ## achrdg12 = 61.7: ## contrast estimate SE df t.ratio p.value ## Female - Male -2.41 0.658 496 -3.659 0.0003 ## ## achrdg12 = 68.1: ## contrast estimate SE df t.ratio p.value ## Female - Male -1.28 0.967 496 -1.321 0.1872 At this point, you should be able to summarize your conclusions about the gender gap in Math and how it depends on Reading. 6.8.5 Simple trends Next we will show how to use emtrends to test the conditional or “simple” slopes of Math on Reading, given Gender. As mentioned, this approach is not very well suited to the example, but we are going through it here just to illustrate how to do this type of analysis. The three main arguments for emtrends are object – the output of lm. This is the first argument var – which continuous predictor in the model we want the slopes of specs – which factor predictor(s) in the model to break the trend down by Let’s see how it works. # Use the emtrends function to get the regression coefficients on reading, broken down by gender simple_slopes &lt;- emtrends(mod3, var = &quot;achrdg12&quot;, specs = &quot;gender&quot;) summary(simple_slopes) ## gender achrdg12.trend SE df lower.CL upper.CL ## Female 0.728 0.0470 496 0.636 0.821 ## Male 0.550 0.0451 496 0.462 0.639 ## ## Confidence level used: 0.95 test(simple_slopes) ## gender achrdg12.trend SE df t.ratio p.value ## Female 0.728 0.0470 496 15.487 &lt;.0001 ## Male 0.550 0.0451 496 12.208 &lt;.0001 The foregoing analysis tells us how the relationship between reading and math changes as a function of gender, and, in particular, whether the simple slopes are significant for males and females. Recall that the simple slope for females (the group coded zero) is just the regression coefficient on reading in the original lm output. So, the only new thing this output gives us is the simple slope for males. 6.8.6 Two continuous predictors Interactions with continuous predictors are basically the same as for continuous and categorical. One main issue is that we should always center the predictors, not only to facilitate interpretation of the regression coefficients, but also to reduce the correlation between the main effects and the interaction. For an example, let’s replace gender with SES from our previous analysis. Apologies that this new example is mainly for convenience and doesn’t represent a great research question about, e.g., about why the relationships between math and reading might change as a function of SES! Here we will focus on how centering affects the results of a regression with interactions among continuous predictors. # Without centering mod5 &lt;- lm(achmat12 ~ achrdg12*ses) summary(mod1) ## ## Call: ## lm(formula = achmat12 ~ achrdg12 + gender + genderXreading) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.058 -3.786 0.501 4.077 16.289 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.8031 2.6492 5.59 3.8e-08 *** ## achrdg12 0.7282 0.0470 15.49 &lt; 2e-16 *** ## genderMale 13.3933 3.6583 3.66 0.00028 *** ## genderXreading -0.1779 0.0651 -2.73 0.00652 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.8 on 496 degrees of freedom ## Multiple R-squared: 0.462, Adjusted R-squared: 0.459 ## F-statistic: 142 on 3 and 496 DF, p-value: &lt;2e-16 # With centering achrdg12_dev &lt;- achrdg12 - mean(achrdg12) ses_dev &lt;- ses - mean(ses) mod6 &lt;- lm(achmat12 ~ achrdg12_dev*ses_dev) summary(mod2) ## ## Call: ## lm(formula = achmat12 ~ achrdg12 + gender + achrdg12:gender) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.058 -3.786 0.501 4.077 16.289 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.8031 2.6492 5.59 3.8e-08 *** ## achrdg12 0.7282 0.0470 15.49 &lt; 2e-16 *** ## genderMale 13.3933 3.6583 3.66 0.00028 *** ## achrdg12:genderMale -0.1779 0.0651 -2.73 0.00652 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.8 on 496 degrees of freedom ## Multiple R-squared: 0.462, Adjusted R-squared: 0.459 ## F-statistic: 142 on 3 and 496 DF, p-value: &lt;2e-16 We can see that, while both models account for the same overall variation in math, SES is significant in the centered model. This has to do both with changing the interpretation of the coefficient (it now represents the relationship between math and reading for students with average reading) and because it is no longer so highly redundant with the interaction term. Although the interaction with SES was not significant in either model, let’s break down the interaction with emtrends just to see how it works. This time we will use the at option rather than the ’cov.reduceoption to break down the interaction. The values 9, 19, and 28 are the 10th, 50th, and 90th percentile of SES, which is the same approachvisreguses (You can overwrite the defaults using thebreaksargument -- seehelp(visreg)`). # Break down interaction with SES as moderator simple_slopes &lt;-emtrends(mod5, var = &quot;achrdg12&quot;, specs = &quot;ses&quot;, at = list(ses = c(9, 19, 28))) summary(simple_slopes) ## ses achrdg12.trend SE df lower.CL upper.CL ## 9 0.550 0.0583 496 0.435 0.665 ## 19 0.593 0.0365 496 0.521 0.664 ## 28 0.631 0.0639 496 0.506 0.757 ## ## Confidence level used: 0.95 Finally let’s summarize our (non significant) interaction with a nice plot. # Note that band = F removes the confidence intervals visreg(mod5, xvar = &quot;achrdg12&quot;, by = &quot;ses&quot;, overlay = TRUE, band = F) 6.8.7 Two categorical predictors For this topic we will switch over to the ECLS data and examine how SES and Pre-K attendance interact to predict Math Achievement at the beginning of Kindergarten. The variables we will examine are Math Achievement at the beginning of K (c1rmscal). This is the number of correct questions on a test with approximately 70 items. Whether the child attended Pre-K (p1center). This is a binary variable that indicates pre-K attendance. SES, coded as quintiles (wksesq5). We will denote this variable as SES, but keep in mind it is quintiles in this example (e.g., SES = 1 are the respondents with SES between the minimum and the first quintile). The regression model is as follows. Note that both variables need to be converted to factors in R, so that R will treat them as categorical variables. Also recall that in R the default contrast coding for categorical predictors is reference-group coding. load(&quot;ECLS2577.Rdata&quot;) ecls$prek &lt;- factor(2 - ecls$p1center) ecls$wksesq5 &lt;- factor(ecls$wksesq5) mod &lt;- lm(c1rmscal ~ prek*wksesq5, data = ecls) summary(mod) ## ## Call: ## lm(formula = c1rmscal ~ prek * wksesq5, data = ecls) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.768 -4.768 -0.975 3.955 31.232 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.045 0.735 21.82 &lt; 2e-16 *** ## prek1 -0.373 0.960 -0.39 0.6973 ## wksesq52 2.293 0.957 2.40 0.0166 * ## wksesq53 2.930 0.913 3.21 0.0013 ** ## wksesq54 4.631 0.944 4.91 9.9e-07 *** ## wksesq55 7.299 1.034 7.06 2.2e-12 *** ## prek1:wksesq52 1.064 1.212 0.88 0.3801 ## prek1:wksesq53 2.109 1.154 1.83 0.0679 . ## prek1:wksesq54 1.671 1.168 1.43 0.1527 ## prek1:wksesq55 2.797 1.234 2.27 0.0235 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.9 on 2567 degrees of freedom ## Multiple R-squared: 0.162, Adjusted R-squared: 0.159 ## F-statistic: 55.2 on 9 and 2567 DF, p-value: &lt;2e-16 To facilitate interpretation of the ouput, you can refer to the plot below. Each regression coefficient in the output corresponds to a feature of this plot. visreg::visreg(mod, xvar = &quot;wksesq5&quot;, by = &quot;prek&quot;, partial = F, rug = F, overlay = T, strip.names = T, xlab = &quot;SES&quot;, ylab = &quot;Math Achievement in K&quot;) In order to summarize the model as an ANOVA table, we can use the following code. Note that the ANOVA output tests the variance explained (i.e., R-squared) of the original variables, and does not include dummy variables. anova(mod) ## Analysis of Variance Table ## ## Response: c1rmscal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## prek 1 3434 3434 72.14 &lt;2e-16 *** ## wksesq5 4 19914 4978 104.58 &lt;2e-16 *** ## prek:wksesq5 4 299 75 1.57 0.18 ## Residuals 2567 122198 48 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In an ANOVA context, the R-squared statistics are called eta-squared. They are reported below: effectsize::eta_squared(mod, partial = F) ## # Effect Size for ANOVA (Type I) ## ## Parameter | Eta2 | 95% CI ## -------------------------------------- ## prek | 0.02 | [0.01, 1.00] ## wksesq5 | 0.14 | [0.12, 1.00] ## prek:wksesq5 | 2.05e-03 | [0.00, 1.00] ## ## - One-sided CIs: upper bound fixed at [1.00]. "]]
