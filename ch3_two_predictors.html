<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>EDUC 784 - 3&nbsp; Two predictors</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ch2_simple_regression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch3_two_predictors.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Two predictors</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">EDUC 784</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch1_review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2_simple_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Simple Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3_two_predictors.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Two predictors</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-interpretations-3" id="toc-sec-interpretations-3" class="nav-link active" data-scroll-target="#sec-interpretations-3"><span class="header-section-number">3.1</span> Interpretations of regression</a>
  <ul class="collapse">
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction"><span class="header-section-number">3.1.1</span> Prediction</a></li>
  <li><a href="#causation" id="toc-causation" class="nav-link" data-scroll-target="#causation"><span class="header-section-number">3.1.2</span> Causation</a></li>
  <li><a href="#explanation" id="toc-explanation" class="nav-link" data-scroll-target="#explanation"><span class="header-section-number">3.1.3</span> Explanation</a></li>
  </ul></li>
  <li><a href="#sec-ecls-3" id="toc-sec-ecls-3" class="nav-link" data-scroll-target="#sec-ecls-3"><span class="header-section-number">3.2</span> An example from ECLS</a>
  <ul class="collapse">
  <li><a href="#correlation-matrices" id="toc-correlation-matrices" class="nav-link" data-scroll-target="#correlation-matrices"><span class="header-section-number">3.2.1</span> Correlation matrices</a></li>
  </ul></li>
  <li><a href="#sec-model-3" id="toc-sec-model-3" class="nav-link" data-scroll-target="#sec-model-3"><span class="header-section-number">3.3</span> The two-predictor model</a></li>
  <li><a href="#sec-ols-3" id="toc-sec-ols-3" class="nav-link" data-scroll-target="#sec-ols-3"><span class="header-section-number">3.4</span> OLS with two predictors</a></li>
  <li><a href="#sec-interpretation-3" id="toc-sec-interpretation-3" class="nav-link" data-scroll-target="#sec-interpretation-3"><span class="header-section-number">3.5</span> Interpreting the coefficients</a>
  <ul class="collapse">
  <li><a href="#holding-the-other-predictor-constant" id="toc-holding-the-other-predictor-constant" class="nav-link" data-scroll-target="#holding-the-other-predictor-constant"><span class="header-section-number">3.5.1</span> “Holding the other predictor constant”</a></li>
  <li><a href="#controlling-for-the-other-predictor" id="toc-controlling-for-the-other-predictor" class="nav-link" data-scroll-target="#controlling-for-the-other-predictor"><span class="header-section-number">3.5.2</span> “Controlling for the other predictor”</a></li>
  <li><a href="#the-ecls-example" id="toc-the-ecls-example" class="nav-link" data-scroll-target="#the-ecls-example"><span class="header-section-number">3.5.3</span> The ECLS example</a></li>
  <li><a href="#sec-beta-3" id="toc-sec-beta-3" class="nav-link" data-scroll-target="#sec-beta-3"><span class="header-section-number">3.5.4</span> Standardized coefficients</a></li>
  </ul></li>
  <li><a href="#sec-rsquared-3" id="toc-sec-rsquared-3" class="nav-link" data-scroll-target="#sec-rsquared-3"><span class="header-section-number">3.6</span> (Multiple) R-squared</a>
  <ul class="collapse">
  <li><a href="#relation-with-simple-regression" id="toc-relation-with-simple-regression" class="nav-link" data-scroll-target="#relation-with-simple-regression"><span class="header-section-number">3.6.1</span> Relation with simple regression</a></li>
  <li><a href="#adjusted-r-squared" id="toc-adjusted-r-squared" class="nav-link" data-scroll-target="#adjusted-r-squared"><span class="header-section-number">3.6.2</span> Adjusted R-squared</a></li>
  <li><a href="#the-ecls-example-1" id="toc-the-ecls-example-1" class="nav-link" data-scroll-target="#the-ecls-example-1"><span class="header-section-number">3.6.3</span> The ECLS example</a></li>
  </ul></li>
  <li><a href="#sec-inference-3" id="toc-sec-inference-3" class="nav-link" data-scroll-target="#sec-inference-3"><span class="header-section-number">3.7</span> Inference</a>
  <ul class="collapse">
  <li><a href="#sec-inference-for-coeffecients-3" id="toc-sec-inference-for-coeffecients-3" class="nav-link" data-scroll-target="#sec-inference-for-coeffecients-3"><span class="header-section-number">3.7.1</span> Inference for the coefficients</a></li>
  <li><a href="#precision-of-hat-b" id="toc-precision-of-hat-b" class="nav-link" data-scroll-target="#precision-of-hat-b"><span class="header-section-number">3.7.2</span> Precision of <span class="math inline">\(\hat b\)</span></a></li>
  <li><a href="#inference-for-rsquared-4" id="toc-inference-for-rsquared-4" class="nav-link" data-scroll-target="#inference-for-rsquared-4"><span class="header-section-number">3.7.3</span> Inference for R-squared</a></li>
  <li><a href="#the-ecls-example-2" id="toc-the-ecls-example-2" class="nav-link" data-scroll-target="#the-ecls-example-2"><span class="header-section-number">3.7.4</span> The ECLS example</a></li>
  </ul></li>
  <li><a href="#workbook" id="toc-workbook" class="nav-link" data-scroll-target="#workbook"><span class="header-section-number">3.8</span> Workbook</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">3.9</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#the-ecls250-data" id="toc-the-ecls250-data" class="nav-link" data-scroll-target="#the-ecls250-data"><span class="header-section-number">3.9.1</span> The ECLS250 data</a></li>
  <li><a href="#multiple-regression-with-lm" id="toc-multiple-regression-with-lm" class="nav-link" data-scroll-target="#multiple-regression-with-lm"><span class="header-section-number">3.9.2</span> Multiple regression with <code>lm</code></a></li>
  <li><a href="#relations-between-simple-and-multiple-regression" id="toc-relations-between-simple-and-multiple-regression" class="nav-link" data-scroll-target="#relations-between-simple-and-multiple-regression"><span class="header-section-number">3.9.3</span> Relations between simple and multiple regression</a></li>
  <li><a href="#inference-with-2-predictors" id="toc-inference-with-2-predictors" class="nav-link" data-scroll-target="#inference-with-2-predictors"><span class="header-section-number">3.9.4</span> Inference with 2 predictors</a></li>
  <li><a href="#apa-reporting-of-results" id="toc-apa-reporting-of-results" class="nav-link" data-scroll-target="#apa-reporting-of-results"><span class="header-section-number">3.9.5</span> APA reporting of results</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-chap-3" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Two predictors</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="sec-interpretations-3" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-interpretations-3"><span class="header-section-number">3.1</span> Interpretations of regression</h2>
<p>Regression has three main interpretations:</p>
<ul>
<li>Prediction (focus on <span class="math inline">\(\hat Y\)</span>)</li>
<li>Causation (focus on <span class="math inline">\(b\)</span>)</li>
<li>Explanation (focus on <span class="math inline">\(R^2\)</span>)</li>
</ul>
<p>By understanding these interpretations, we will have a better idea of how regression is used in research. Each interpretation also provides a different perspective on the importance of using multiple predictor variables, rather than only a single predictor.</p>
<section id="prediction" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="prediction"><span class="header-section-number">3.1.1</span> Prediction</h3>
<p>Prediction means “to make known beforehand”. This was the original use of regression (<a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean#History">https://en.wikipedia.org/wiki/Regression_toward_the_mean#History</a>). In a regression contex, prediction means using observations of <span class="math inline">\(X\)</span> to make a guess about yet unobserved values of <span class="math inline">\(Y\)</span>. Our guess is <span class="math inline">\(\hat Y\)</span>, and this is why <span class="math inline">\(\hat Y\)</span> is called the “predicted value” of <span class="math inline">\(Y\)</span>.</p>
<p>When making predictions, we usually want some additional information about how precise the predictions will be. In OLS regression, this information is provided by the standard error of prediction <span class="citation" data-cites="fox-2016">(<a href="#ref-fox-2016" role="doc-biblioref"><strong>fox-2016?</strong></a>)</span>:</p>
<p><span id="eq-se-pred"><span class="math display">\[\text{SE}({\hat Y_i}) = \sqrt{\frac{SS_{\text{res}}}{N - 2} \left(1 +  \frac{1}{N} + \frac{(X_i - \bar X)^2}{\sum_j(X_j - \bar X)^2} \right)} \tag{3.1}\]</span></span></p>
<p>This statistic quantifies our uncertainty when making predictions based on observations of <span class="math inline">\(X\)</span> that were not in our original sample. The prediction errors for the NELS example in <a href="ch2_simple_regression.html"><span>Chapter&nbsp;2</span></a> are represented in <a href="#fig-pred-error-3">Figure&nbsp;<span>3.1</span></a> as a gray band around the regression line.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting library</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"NELS.RData"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run regression </span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(achmat08 <span class="sc">~</span> ses, <span class="at">data =</span> NELS)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute SE(Y-hat)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(NELS)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>ms_res <span class="ot">&lt;-</span> <span class="fu">var</span>(mod<span class="sc">$</span>residuals) <span class="sc">*</span> (n<span class="dv">-1</span>) <span class="sc">/</span> (n<span class="dv">-2</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>d_ses <span class="ot">&lt;-</span> NELS<span class="sc">$</span>ses <span class="sc">-</span> <span class="fu">mean</span>(NELS<span class="sc">$</span>ses) </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>se_yhat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(ms_res <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>n <span class="sc">+</span> d_ses<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span>(d_ses<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>gg_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>             <span class="at">achmat08 =</span> NELS<span class="sc">$</span>achmat08,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>             <span class="at">ses =</span> NELS<span class="sc">$</span>ses,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>             <span class="at">y_hat =</span> mod<span class="sc">$</span>fitted.values,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>             <span class="at">lwr =</span> mod<span class="sc">$</span>fitted.values <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se_yhat,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>             <span class="at">upr =</span> mod<span class="sc">$</span>fitted.values <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se_yhat)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(gg_data, <span class="fu">aes</span>(<span class="at">x =</span> ses, <span class="at">y =</span> achmat08))<span class="sc">+</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">'#3B9CD3'</span>, <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> ses, <span class="at">y =</span> y_hat), <span class="at">color =</span> <span class="st">"grey35"</span>) <span class="sc">+</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin=</span>lwr,<span class="at">ymax=</span>upr),<span class="at">alpha=</span><span class="fl">0.3</span>) <span class="sc">+</span> </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ylab</span>(<span class="st">"Math Achievement (Grade 8)"</span>) <span class="sc">+</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">xlab</span>(<span class="st">"SES"</span>) <span class="sc">+</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-pred-error-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch3_two_predictors_files/figure-html/fig-pred-error-3-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;3.1: Prediction Error for NELS Example.</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see in the figure that the error band is quite wide. So, we might wonder how to make our predictions more precise. On way to do this is by including more predictors in the regression model – i.e., multiple regression.</p>
<p>To see why including more predictors improves the precision of predictions, note that the standard error of prediction shown in <a href="#eq-se-pred">Equation&nbsp;<span>3.1</span></a> increases with <span class="math inline">\(SS_{\text{res}}\)</span>, which is the variation in the outcome that is <em>not</em> explained by the predictor (see <a href="ch2_simple_regression.html#sec-rsquared-2"><span>Section&nbsp;2.4</span></a>). In most situations, <span class="math inline">\(SS_{\text{res}}\)</span> is the largest contributor the prediction error. As we will see below, one way to reduce <span class="math inline">\(SS_{\text{res}}\)</span> is by adding more predictors to the model.</p>
<section id="more-about-prediction" class="level4" data-number="3.1.1.1">
<h4 data-number="3.1.1.1" class="anchored" data-anchor-id="more-about-prediction"><span class="header-section-number">3.1.1.1</span> More about prediction</h4>
<p>There has been a resurgence of interest in prediction in recent years, especially in machine learning. Although the methods used in machine learning are often more complicated than OLS regression, the basic problem is the same. Rather than theoretically derived prediction error (<a href="#eq-se-pred">Equation&nbsp;<span>3.1</span></a>), machine learning often uses the accuracy and precision of out-of-sample predictions as the main criterion for judging the quality of a model, which is often called “cross validation.” Machine learning has also introduced some new techniques for choosing which predictors to include in a model (i.e., “variable selection”). We will touch on these topics later in the course, although our main focus is OLS regression.</p>
<p>Regression got its name from a statistical property of predicted scores called “regression toward the mean.” To explain this property, let’s assume <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are z-scores (i.e., both variables have <span class="math inline">\(M = 0\)</span> and <span class="math inline">\(SD = 1\)</span>). Recall that this implies that <span class="math inline">\(a = 0\)</span> and <span class="math inline">\(b = r_{XY}\)</span>, so the regression equation reduces to</p>
<p><span class="math display">\[\hat Y = r_{XY} X\]</span></p>
<p>Since <span class="math inline">\(|r_{XY} | ≤ 1\)</span>, the absolute value of the <span class="math inline">\(\hat Y\)</span> must be less than or equal to that of <span class="math inline">\(X\)</span>. And, since both variables have <span class="math inline">\(M = 0\)</span>, this implies that <span class="math inline">\(\hat Y\)</span> is closer to the mean of <span class="math inline">\(Y\)</span> than <span class="math inline">\(X\)</span> is to the mean of <span class="math inline">\(X\)</span>. This is sometimes called regression toward the mean.</p>
</section>
</section>
<section id="causation" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="causation"><span class="header-section-number">3.1.2</span> Causation</h3>
<p>A causal interpretation of regression means that that changing <span class="math inline">\(X\)</span> by one unit will change <span class="math inline">\(E(Y|X)\)</span> by <span class="math inline">\(b\)</span> units. This is interpreted as a claim about the expected value of <span class="math inline">\(Y\)</span> “in real life”, not simply a claim about the mechanics of the regression line. In terms of our example, a causal interpretation would state that improving students’ SES by one unit will, on average, cause Math Achievement to increase by about half a percentage point.</p>
<p>The gold standard for inferring causality is to randomly assign people to different treatment conditions. In a regression context, treatment is represented by the independent variable, or the <span class="math inline">\(X\)</span> variable. While randomized experiments are possible in some settings, there are many types of variables that we cannot feasibly randomly assign (e.g., SES).</p>
<p>The concept of an omitted variable is used to describe what happens when we can’t (or don’t) randomly assign people to treatment conditions. An omitted variable is any variable that is correlated with both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. In our example, this would be any variable correlated with both Math Achievement and SES (e.g., School Quality). When we use random assignment, we ensure that <span class="math inline">\(X\)</span> is uncorrelated with <em>all</em> pre-treatment variables – i.e., randomization ensure that there are no omitted variables. However, when we don’t use random assignment, our results may be subject to <em>omitted variable bias</em>.</p>
<p>The overall idea of omitted variable bias is the same as “correlation <span class="math inline">\(\neq\)</span> causation”. The take-home message is summarized in the following points, which are stated in terms of the our NELS example.</p>
<ul>
<li><p>Any variable that is correlated with Math Achievement and with SES is called an omitted variable. One example is School Quality. This is an omitted variable because we did not include it as a predictor in our simple regression model.</p></li>
<li><p>The problem is not just that we have an incomplete picture of how School Quality is related to Math Achievement.</p></li>
<li><p>Omitted variable bias means that the predictor variable that <em>was included in the model</em> ends up having the wrong regression coefficient. Otherwise stated, the regression coefficient of SES is biased because we did not consider School Quality.</p></li>
<li><p>In order to mitigate omitted variable bias, we want to include plausible omitted variables in our regression models – i.e., multiple regression.</p></li>
</ul>
<section id="omitted-variable-bias" class="level4" data-number="3.1.2.1">
<h4 data-number="3.1.2.1" class="anchored" data-anchor-id="omitted-variable-bias"><span class="header-section-number">3.1.2.1</span> Omitted variable bias*</h4>
<p>Omitted variable bias is nicely explained by Gelman and Hill <span class="citation" data-cites="gelman-2007">(<a href="#ref-gelman-2007" role="doc-biblioref"><strong>gelman-2007?</strong></a>)</span>, and a modified version of their discussion is provided below. We start by assuming a “true” regression model with two predictors. In the context of our example, this means that there is one other variable, in addition to SES, that is important for predicting Math Achievement. Of course, there are many predictors of Math Achievement (see Section <a href="ch2_simple_regression.html#sec-example-2"><span>Section&nbsp;2.1</span></a>), but we only need two to explain the problem of omitted variable bias.</p>
<p>Write the “true” model as:</p>
<p><span id="eq-2parm"><span class="math display">\[
Y = a + b_1 X_1 + b_2 X_2 + \epsilon
\tag{3.2}\]</span></span></p>
<p>where <span class="math inline">\(X_1\)</span> is SES and <span class="math inline">\(X_2\)</span> is any other variable that is correlated with both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> (e.g., School Quality).</p>
<p>Next, imagine that instead of using the model in <a href="#eq-2parm">Equation&nbsp;<span>3.2</span></a>, we analyze the data using the model with just SES, leading to the usual simple regression:</p>
<p><span id="eq-1parm"><span class="math display">\[
\hat Y = a^* + b^*_1 X_1 + \epsilon^*
\tag{3.3}\]</span></span></p>
<p>The problem of omitted variable bias is that <span class="math inline">\(b_1 \neq b^*_1\)</span> – i.e., the regression coefficient in the true model is not the same as the regression coefficient in the model with only one predictor. This is perhaps surprising – leaving out School Quality gives us the wrong regression coefficient for SES!</p>
<p>To see why, start by writing <span class="math inline">\(X_2\)</span> as a function of <span class="math inline">\(X_1\)</span>.</p>
<p><span id="eq-X2"><span class="math display">\[
X_2 = \alpha + \beta X_1 + \nu
\tag{3.4}\]</span></span></p>
<p>Next we use <a href="#eq-X2">Equation&nbsp;<span>3.4</span></a> to substitute for <span class="math inline">\(X_2\)</span> in <a href="#eq-2parm">Equation&nbsp;<span>3.2</span></a>,</p>
<p><span class="math display">\[\begin{align}
Y &amp; = a + b_1 X_1 + b_2 X_2 + \epsilon \\
  &amp; = a + b_1 X_1 + b_2 (\alpha + \beta X_1 + \nu)  + \epsilon \\
  &amp; = \color{orange}{(a + \alpha)} + \color{green}{(b_1 + b_2\beta)} X_1 + (e + \nu) \label{eq-3parm}
\end{align}\]</span></p>
<p>Notice that in the last line, <span class="math inline">\(Y\)</span> is predicted using only <span class="math inline">\(X_1\)</span>, so it is equivalent to <a href="#eq-1parm">Equation&nbsp;<span>3.3</span></a>. Based on this comparison, we can write</p>
<ul>
<li><span class="math inline">\(a^* = \color{orange}{a + \alpha}\)</span></li>
<li><span class="math inline">\(b^*_1 = \color{green}{b_1 + b_2\beta}\)</span></li>
<li><span class="math inline">\(\epsilon^* = \epsilon + \nu\)</span></li>
</ul>
<p>The equation for <span class="math inline">\(b^*_1\)</span> is what we are most interested in. It shows that the regression parameter in our one-parameter model (<span class="math inline">\(b^*_1\)</span>) is not equal to the “true” regression parameter using both predictors (<span class="math inline">\(b_1\)</span>).</p>
<p>This is what omitted variable bias means – leaving out <span class="math inline">\(X_2\)</span> in Equation <a href="#eq-1parm">Equation&nbsp;<span>3.3</span></a> gives us the wrong regression parameter for <span class="math inline">\(X_1\)</span>. This is one of the main motivations for including more than one predictor variable in a regression model – i.e., to avoid omitted variable bias.</p>
<p>Notice that there two special situations in which omitted variable bias is not a problem:</p>
<ul>
<li>When the two predictors are not related – i.e., <span class="math inline">\(\beta = 0\)</span>.</li>
<li>When the second predictor is not related to <span class="math inline">\(Y\)</span> – i.e., <span class="math inline">\(b_2 = 0\)</span>.</li>
</ul>
</section>
</section>
<section id="explanation" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="explanation"><span class="header-section-number">3.1.3</span> Explanation</h3>
<p>Many uses of regression fall somewhere between prediction and causation. We want to do more than just predict outcomes of interest, but we often don’t have a basis for making the strong assumptions required for a causal interpretation of regression coefficients. This grey area between prediction and causation can be referred to as explanation.</p>
<p>In terms of our example, we might want to explain why eighth graders differ in their Math Achievement. There are large number of potential reasons for individual difference in Math Achievement, such as</p>
<ul>
<li>Student factors
<ul>
<li>attendance</li>
<li>past academic performance in Math</li>
<li>past academic performance in other subjects (Question: why include this?)</li>
<li>…</li>
</ul></li>
<li>School factors
<ul>
<li>their ELA teacher</li>
<li>the school they attend</li>
<li>their peers</li>
<li>…</li>
</ul></li>
<li>Home factors
<ul>
<li>SES</li>
<li>maternal education</li>
<li>paternal education</li>
<li>parental expectations</li>
<li>…</li>
</ul></li>
</ul>
<p>When the goal of an analysis is explanation, it usual to focus on the proportion of variation in the outcome variable that is explained by the predictors, i.e., R-squared (see <a href="ch2_simple_regression.html#sec-rsquared-2"><span>Section&nbsp;2.4</span></a>). Later in the course we will see how to systematically study the variance explained by individual predictors, or blocks of several predictors (e.g., student factors).</p>
<p>Note that even a long list of predictors such as that above leaves out potential omitted variables. While the addition of more predictors can help us explain more of the variation in Math Achievement, it is rarely the case that we can claim that all relevant variables have been included in the model.</p>
</section>
</section>
<section id="sec-ecls-3" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-ecls-3"><span class="header-section-number">3.2</span> An example from ECLS</h2>
<p>In the remainder of this chapter we will consider a new example from the 1998 Early Childhood Longitudinal Study (ECLS; <a href="https://nces.ed.gov/ecls/">https://nces.ed.gov/ecls/</a>). Below is a description of the data from the official NCES codebook (page 1-1 of <a href="https://nces.ed.gov/ecls/data/ECLSK_K8_Manual_part1.pdf">https://nces.ed.gov/ecls/data/ECLSK_K8_Manual_part1.pdf</a>):</p>
<p><em>The ECLS-K focuses on children’s early school experiences beginning with kindergarten and ending with eighth grade. It is a multisource, multimethod study that includes interviews with parents, the collection of data from principals and teachers, and student records abstracts, as well as direct child assessments. In the eighth-grade data collection, a student paper-and-pencil questionnaire was added. The ECLS-K was developed under the sponsorship of the U.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics (NCES). Westat conducted this study with assistance provided by Educational Testing Service (ETS) in Princeton, New Jersey.</em></p>
<p><em>The ECLS-K followed a nationally representative cohort of children from kindergarten into middle school. The base-year data were collected in the fall and spring of the 1998–99 school year when the sampled children were in kindergarten. A total of 21,260 kindergartners throughout the nation participated</em>.</p>
<p>The subset of the ECLS-K data used in this class was obtained from the link below.</p>
<p><a href="http://routledgetextbooks.com/textbooks/_author/ware-9780415996006/data.php" class="uri">http://routledgetextbooks.com/textbooks/_author/ware-9780415996006/data.php</a></p>
<p>The codebook for this subset of data is available on our course website. We will be using a small subset of <span class="math inline">\(N = 250\)</span> cases from the full example data set (the <code>ECLS250.RData</code> data)</p>
<p>We focus on the following three variables.</p>
<ul>
<li><p>Math Achievement in the first semester of Kindergarten. This variable can be interpreted as the number of questions (out of 60) answered correctly on a math test. Don’t worry – the respondents in this study did not have to write a 60-question math test in the first semester of K! Students only answered a few of the questions and their scores were re-scaled to be out a total of 60 questions afterwards.</p></li>
<li><p>Socioecomonic Status (SES), which is a composite of household factors (e.g., parental education, household income) ranging from 30-72.</p></li>
<li><p>Approaches to Learning (ATL), which is a teacher-reported measure of behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), d so that higher values represent more positive responses, and the scale is an unweighted average the six items.</p></li>
</ul>
<section id="correlation-matrices" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="correlation-matrices"><span class="header-section-number">3.2.1</span> Correlation matrices</h3>
<p>As with simple regression, correlation is the building block of multiple regression. So, we will start by examining the correlations in our example. We also introduce a new way of presenting correlations, the correlation matrix. The notation developed in this section will appear throughout the rest of the chapter.</p>
<p>In the scatter plots below, the panels are arranged in matrix format. The variable named on in the diagonal panels appears on the vertical (<span class="math inline">\(Y\)</span>) axis in its row and the horizontal (<span class="math inline">\(X\)</span>) axis in its column. For example, Math Achievement is on the vertical axis in the first row and the horizontal axis in the first column. Notice that plots below the diagonal are just mirror image of the plots above the diagonal.</p>
<div class="cell" data-layout-align="center">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"ECLS250.RData"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(ecls)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>example_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(c1rmscal, wksesl, t1learn)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(example_data) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Math"</span>, <span class="st">"SES"</span>, <span class="st">"ATL"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(example_data , <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-pairs-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="ch3_two_predictors_files/figure-html/fig-pairs-3-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;3.2: ECLS Example Data.</figcaption>
</figure>
</div>
</div>
</div>
<p>The format of Figure <a href="#fig-pairs-3">Figure&nbsp;<span>3.2</span></a> is the same as that of the correlation matrix among the variables, which is shown below.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(example_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          Math       SES       ATL
Math 1.0000000 0.4384619 0.3977048
SES  0.4384619 1.0000000 0.2877015
ATL  0.3977048 0.2877015 1.0000000</code></pre>
</div>
</div>
<p>Again, notice that the entries below the diagonal are mirrored by the entries above the diagonal. We can see that SES and ATL have similar correlations with Math Achievement (.44 and .40, respectively), and are also moderately correlated with each other (.29).</p>
<p>In order to represent the correlation matrix among a single outcome variable (<span class="math inline">\(Y\)</span>) and two predictors (<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>) we will use the following notation.</p>
<p><span class="math display">\[
\begin{array}{c}
\text{var } Y \\ \text{var } X_1 \\ \text{var } X_2
\end{array}
\quad
\left[
\begin{array}{ccc}
1       &amp; r_{Y1}  &amp; r_{Y2}  \\
r_{1Y}  &amp; 1       &amp; r_{12}  \\
r_{2Y}  &amp; r_{21}  &amp; 1
\end{array}
\right]
\]</span></p>
<!-- \[ -->
<!-- \begin{array}{c} -->
<!-- \text{var 1} \\ \text{var 2} \\ \text{var 3}  -->
<!-- \end{array} -->
<!-- \quad -->
<!-- \left[  -->
<!-- \begin{array}{ccc} -->
<!--  1       & r_{12}  & r_{13}  \\ -->
<!--  r_{21}  & 1       & r_{23}  \\ -->
<!--  r_{31}  & r_{32}  & 1 -->
<!-- \end{array} -->
<!--  \right] -->
<!-- \] -->
<p>in this notation, we replace use the <span class="math inline">\(r_{Y1} = \text{cor}(Y,X_1)\)</span> is the correlation between two variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span>. Note that each correlation coefficient <span class="math inline">\(r\)</span> has two subscripts that tell us which two variables are being correlated. For the outcome variable we use the subscript <span class="math inline">\(Y\)</span>, and for the two predictors we use the subscripts <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. As with the numerical examples, the values below the diagonal mirror the values above the diagonal. So, we really just need the three correlations shown in the matrix below.</p>
<p><span class="math display">\[
\begin{array}{c}
\text{var } Y \\ \text{var } X_1 \\ \text{var } X_2
\end{array}
\quad
\left[
\begin{array}{ccc}
-       &amp; r_{Y1}  &amp; r_{Y2}  \\
-  &amp; -       &amp; r_{12}  \\
-  &amp; -  &amp; -
\end{array}
\right]
\]</span></p>
<p>The three correlations are interpreted as follows:</p>
<ul>
<li><p><span class="math inline">\(r_{Y1}\)</span> - the correlation between the outcome (<span class="math inline">\(Y\)</span>) and the first predictor (<span class="math inline">\(X_1\)</span>).</p></li>
<li><p><span class="math inline">\(r_{Y2}\)</span> - the correlation between the outcome (<span class="math inline">\(Y\)</span>) and the second predictor (<span class="math inline">\(X_2\)</span>).</p></li>
<li><p><span class="math inline">\(r_{12}\)</span> - the correlation between the two predictors.</p></li>
</ul>
<p><strong>If you have questions about how scatter plots and correlations can be presented in matrix format, please write them down now and share them class.</strong></p>
</section>
</section>
<section id="sec-model-3" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-model-3"><span class="header-section-number">3.3</span> The two-predictor model</h2>
<p>In the ECLS example, we can think of Kindergarteners’ Math Achievement as the outcome variable, with SES and Approaches to Learning as potential predictors / explanatory variables. The multiple regression model for this example can be written as</p>
<p><span id="eq-yhat-3"><span class="math display">\[
\widehat Y = b_0 + b_1 X_1 + b_2 X_2
\tag{3.5}\]</span></span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\widehat Y\)</span> denotes the predicted Math Achievement</li>
<li><span class="math inline">\(X_1 = \;\)</span> SES and <span class="math inline">\(X_2 = \;\)</span> ATL (it doesn’t matter which predictor we denote as <span class="math inline">\(1\)</span> or <span class="math inline">\(2\)</span>)</li>
<li><span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> are the regression slopes</li>
<li>The intercept is denoted by <span class="math inline">\(b_0\)</span> (rather than <span class="math inline">\(a\)</span>).</li>
</ul>
<p>Just like simple regression, the residual for <a href="#eq-yhat-3">Equation&nbsp;<span>3.5</span></a> is defined as <span class="math inline">\(e = Y - \widehat Y\)</span> and the model can be equivalently written as <span class="math inline">\(Y = \widehat Y + e\)</span>. Also, remember that you can write out the model using the variable names in place of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> if that helps keep track of all the notation. For example,</p>
<p><span class="math display">\[
MATH = b_0 + b_1 SES + b_2 ATL + e.
\]</span></p>
<p>As mentioned in <a href="ch2_simple_regression.html"><span>Chapter&nbsp;2</span></a>, feel free to use whatever notation works best for you.</p>
<p>You might be wondering, what is the added value of multiple regression compared to the correlation co-efficients reported in the previous section? Well, correlations only consider two-variables-at-a-time. Multiple regression let’s us further consider how the predictors work together to explain variation in the outcome, and to consider the relationship between each predictor and the outcome while holding the other predictors constant. In the context of our example, multiple regression let’s us address the following questions:</p>
<ul>
<li>How much of variation in Math Achievement do both predictors explain together?</li>
<li>What is the relationship between Math Achievement and ATL if we hold SES constant?</li>
<li>Similarly, what is the relationship between Math Achievement and SES if we hold ATL constant?</li>
</ul>
<p>Notice that this is different from simple regression – simple regression was just a repackaging of correlation, but multiple regression is something new.</p>
</section>
<section id="sec-ols-3" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-ols-3"><span class="header-section-number">3.4</span> OLS with two predictors</h2>
<p>We can estimate the parameters of the two-predictor regression model in <a href="#eq-yhat-3">Equation&nbsp;<span>3.5</span></a> model using same approach as for simple regression. We do this by choosing the values of <span class="math inline">\(b_0, b_1, b_2\)</span> that minimize</p>
<p><span class="math display">\[SS_\text{res} = \sum_i e_i^2.\]</span></p>
<p>Solving the minimization problem (setting derivatives to zero) leads to the following equations for the regression coefficients. Remember, the subscript <span class="math inline">\(1\)</span> denotes the first predictor and the subscript <span class="math inline">\(2\)</span> denotes the second predictor – see <a href="#sec-ecls-3"><span>Section&nbsp;3.2</span></a> for notation. Also note that <span class="math inline">\(s\)</span> represents standard deviations.</p>
<p><span class="math display">\[\begin{align}
b_0 &amp; = \bar Y - b_1 \bar X_1 - b_2 \bar X_2 \\ \\
b_1 &amp; = \frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}} \frac{s_Y}{s_1} \\ \\
b_2 &amp; = \frac{r_{Y2} - r_{Y1} r_{12}}{1 - r^2_{12}} \frac{s_Y}{s_2}
\end{align}\]</span></p>
<p>As promised, these equations are more complicated than for simple regression :) The next section addresses the interpretation of the regression coefficients.</p>
</section>
<section id="sec-interpretation-3" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="sec-interpretation-3"><span class="header-section-number">3.5</span> Interpreting the coefficients</h2>
<p>An important part of using multiple regression is getting the interpretation of the regression coefficients correct. The basic interpretation is that the slope for SES represents how much predicted Math Achievement changes for a one unit increase of SES, <em>while holding ATL constant.</em> (The same interpretation holds when switching the predictors.) The important difference with simple regression is the “holding the other predictor constant” part, so let’s dig into it.</p>
<section id="holding-the-other-predictor-constant" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="holding-the-other-predictor-constant"><span class="header-section-number">3.5.1</span> “Holding the other predictor constant”</h3>
<p>Let’s start with the regression model for the predicted values:</p>
<p><span class="math display">\[ \widehat {MATH} = b_0 + b_1 SES + b_2 ATL\]</span></p>
<p>If we increase <span class="math inline">\(SES\)</span> by one unit and hold <span class="math inline">\(ATL\)</span> constant, we get new predicted value (denoted with an asterisk):</p>
<p><span class="math display">\[\widehat {MATH^*} = b_0 + b_1 (SES + 1) + b_2 ATL\]</span></p>
<p>The difference between <span class="math inline">\(\widehat{MATH^*}\)</span> and <span class="math inline">\(\widehat{MATH}\)</span> is how much the predicted value changes for a one unit increase in SES, while holding ATL constant:</p>
<p><span class="math display">\[\widehat{MATH^*} - \widehat{MATH}  = b_1\]</span></p>
<p>This why we interpret the regression coefficients in multiple regression differently than simple regression. In multiple regression, we interpret the “effect” of each predictor while holding the other predictor(s) constant. This is sometimes referred to as “ceteris paribus,” which Latin for “with other conditions remaining the same.” So, we could say that multiple regression is a statistical way of making ceteris paribus arguments.</p>
<p>Also note that we can see in the equations for <span class="math inline">\(\widehat {MATH}\)</span> that the interpretation of the regression intercept is basically the same as for simple regression: it is the value of <span class="math inline">\(\widehat Y\)</span> when <span class="math inline">\(X_1 = 0\)</span> and <span class="math inline">\(X_2 = 0\)</span> (i.e., still not very interesting).</p>
</section>
<section id="controlling-for-the-other-predictor" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="controlling-for-the-other-predictor"><span class="header-section-number">3.5.2</span> “Controlling for the other predictor”</h3>
<p>Another interpretation of the regression coefficients is in terms of the equations in for <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> presented in <a href="#sec-ols-3"><span>Section&nbsp;3.4</span></a>. For example, the equation for <span class="math inline">\(b_1\)</span> is</p>
<p><span class="math display">\[\begin{equation}
b_1 = \frac{r_{Y1} - r_{Y2} \color{red}{r_{12}}} {1 - \color{red}{r^2_{12}}} \frac{s_1}{s_Y}.
\end{equation}\]</span></p>
<p>This is the same equation as from <a href="#sec-ols-3"><span>Section&nbsp;3.4</span></a>, but the correlation between the predictors is shown in red. Note that if the predictors are uncorrelated (i.e., <span class="math inline">\(\color{red}{r^2_{12}} = 0\)</span>) then</p>
<p><span class="math display">\[b_1 = r_{Y1} \frac{s_1}{s_Y},\]</span></p>
<p>which is just the regression coefficient from simple regression (<a href="ch2_simple_regression.html#sec-ols-2"><span>Section&nbsp;2.3</span></a>).</p>
<p>In general, the formulas for the regression coefficients in the two-predictor model are more complicated because they “control for” or “account for” the relationship between the predictors. In simple regression, we only had one predictor, so we didn’t need to account for how the predictors were related to each other.</p>
<p>The equations for the regression coefficients show that, if the predictors are uncorrelated, then doing a multiple regression is just the same thing as doing simple regression multiple times. However, most of the time our predictors will be correlated, which is why regression coefficients in multiple regression ends up being more complicated than the coefficient in simple regression – in multiple regression, we “control for” the relationship between the predictors.</p>
</section>
<section id="the-ecls-example" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="the-ecls-example"><span class="header-section-number">3.5.3</span> The ECLS example</h3>
<p>Below, the R output from the ECLS example is reported. <strong>Please provide a written explanation of the regression coefficients for SES and ATL. If you have questions about how to interpret the coefficients, please also note them now and be prepared to share them in class. Note that this question is not asking about whether the coefficients are significant – it is asking what the numbers in the first column of the Coefficients table mean.</strong></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the regression model and print output</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl <span class="sc">+</span> t1learn)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ wksesl + t1learn)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.101  -4.034  -1.075   3.289  29.543 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.05016    2.90027  -2.086    0.038 *  
wksesl       0.35125    0.05633   6.235 1.94e-09 ***
t1learn      3.52125    0.67390   5.225 3.70e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.164 on 247 degrees of freedom
Multiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 
F-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
<section id="sec-beta-3" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="sec-beta-3"><span class="header-section-number">3.5.4</span> Standardized coefficients</h3>
<p>One question that arises in the interpretation of the example is the relative contribution of the two predictors to Kindergartener’s Math Achievement. In particular, the regression coefficient for ATL is 10 times larger than the regression coefficient for SES – does this mean that ATL is 10 times more important than SES?</p>
<p>The short answer is, “no.” ATL is on a scale of 1-4 whereas SES ranges from 30-72. In order to make the regression coefficients more comparable, we can standardize the <span class="math inline">\(X\)</span> variables so that they have the same variance.</p>
<p>Many researchers go a step further and standardize all of the variables <span class="math inline">\(Y, X_1, X_2\)</span> to be z-scores with M = 0 and SD = 1. The resulting regression coefficients are often called <span class="math inline">\(\beta\)</span>-coefficients or <span class="math inline">\(\beta\)</span>-weights.</p>
<p>The <span class="math inline">\(\beta\)</span>-weights are related to the regular regression coefficients from <a href="#sec-ols-3"><span>Section&nbsp;3.4</span></a>:</p>
<p><span class="math display">\[\beta_1 = b_1 \frac{s_1}{s_Y} = \frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}}\]</span> A similar expression holds for <span class="math inline">\(\beta_1\)</span></p>
<p>Note that the <code>lm</code> function in R does not provide an option to report standardized output. So, if you want to get the <span class="math inline">\(\beta\)</span>-coefficients in R, it’s easiest to just standardized the variables first and then do the regression with the standardized variables.</p>
<p>Regardless of how you compute them, the interpretation of the <span class="math inline">\(\beta\)</span>-coefficients is in terms of the standard deviation units of both the <span class="math inline">\(Y\)</span> variable and the <span class="math inline">\(X\)</span> variable – e.g., increasing <span class="math inline">\(X_1\)</span> by one standard deviation changes <span class="math inline">\(\hat Y\)</span> by <span class="math inline">\(\beta_1\)</span> standard deviations (holding the other predictors constant).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Unlike other software, R doesn't have a convenience functions for beta coefficients. </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>z_example_data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">scale</span>(example_data))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>z_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(Math <span class="sc">~</span> SES  <span class="sc">+</span> ATL, <span class="at">data =</span> z_example_data)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(z_mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Math ~ SES + ATL, data = z_example_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9590 -0.5604 -0.1493  0.4569  4.1043 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 1.337e-15  5.416e-02   0.000        1    
SES         3.533e-01  5.666e-02   6.235 1.94e-09 ***
ATL         2.961e-01  5.666e-02   5.225 3.70e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.8563 on 247 degrees of freedom
Multiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 
F-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We should be careful when using beta-coefficients to “ease” the comparison of regression predictors. In the context of our example, we might wonder whether the overall cost of raising a child’s Approaches to Learning by 1 SD is comparable to the overall cost of raising their family’s SES by 1 SD. In general, putting variables on the same scale is only a superficial way of making comparisons among their regression coefficients.</p>
<p><strong>Please write down an interpretation of the of beta (standardized) regression coefficients in the above output. Your interpretation should include reference to the fact that the variables have been standardized. Based on this analysis, do you think predictor more important than the other? Please be prepared to share your interpretations / questions in class!</strong></p>
</section>
</section>
<section id="sec-rsquared-3" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="sec-rsquared-3"><span class="header-section-number">3.6</span> (Multiple) R-squared</h2>
<p>R-squared in multiple regression has the same general formula and interpretation as in simple regression. The formula is</p>
<p><span class="math display">\[R^2 = \frac{SS_{\text{reg}}} {SS_{\text{total}}} \]</span></p>
<p>and it is interpreted as the proportion of variance in the outcome variable that is “associated with” or “explained by” its linear relationship with the predictor variables.</p>
<p>As discussed below, we can also say a bit more about R-squared in multiple regression.</p>
<section id="relation-with-simple-regression" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="relation-with-simple-regression"><span class="header-section-number">3.6.1</span> Relation with simple regression</h3>
<p>Like the regression coefficients in <a href="#sec-ols-3"><span>Section&nbsp;3.4</span></a>, the equation for R-squared can also be written in terms of the correlations among the three variables:</p>
<p><span class="math display">\[R^2 = \frac{r^2_{Y1} + r^2_{Y2} - 2 r_{12}r_{Y1}r_{Y2}}{1 - r^2_{12}}.\]</span></p>
<p>If the correlation between the predictors is zero, then this equation simplifies to</p>
<p><span class="math display">\[R^2 = r^2_{Y1} + r^2_{Y2}.\]</span> In words: When the predictors are uncorrelated, their total contribution to variance explained is just the sum of their individual contributions.</p>
<p>However, when the predictors are correlated, either positively or negatively, it can be show that</p>
<p><span class="math display">\[ R^2 &lt; r^2_{Y1} + r^2_{Y2}.\]</span></p>
<p>In other words: correlated predictors jointly explain less variance than if we added the contributions of each predictor considered separately. Intuitively, this is because correlated predictors share some variation with each other. If we considered the predictors one at a time, we double-count their shared variation.</p>
<p>The conceptual relationships between R-squared for one versus two predictors can be represented in terms of the following Venn diagram.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-venn-diagram" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="files/images/venn_diagram.png" class="img-fluid figure-img" width="272"></p>
<figcaption class="figure-caption">Figure&nbsp;3.3: Shared Variance Among <span class="math inline">\(Y\)</span>, <span class="math inline">\(X_1\)</span>, and <span class="math inline">\(X_2\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>The circles represent the variance of each variable and the overlap between circles represents their shared variance. When we conduct a multiple regression, the variance in the outcome explained by both predictors is equal to the sum of the areas A + B + C. If we intead conduct two simple regressions and then add up the R-squared values, we would double count the area labelled “B”.</p>

<!--
\begin{align}
R^2_{Y.12} & = \frac{A + B + C}{A + B + C + D} \\ \\
R^2_{Y.1} & = \frac{A + B}{A + B + C + D} \\ \\
R^2_{Y.2} & = \frac{B + C}{A + B + C + D} \\ \\
\end{align}

These equations imply 

\[ R^2_{Y.1} + R^2_{Y.2} = \frac{A + 2B + C}{A + B + C + D} \geq R^2_{Y.12} \] 

This explains the relationship among the R-squared values in Table \@ref(tab:compare). The reason that the sum of the R-squared values in the simple models is greater than the R-squared value in the two-predictor model is because the sum double counts the shared variation among the predictors (area B in the diagram). 

### Relation with $\beta$-weights

The squared standardized coefficients in Section \@ref(standardized-coefficients) are closely related to a quantity called the squared semi-partial correlation: 

\[ r^2_{Y1 \mid 2} = \beta_1^2 * (1 - r^2_{12}) \]

In this notation, the subscript $Y1 \mid 2$ denotes the correlation between $Y$ and $X_1$ after removing the (linear) association between $X_1$ and $X_2$. Similarly for $Y2 \mid 1$. 

The squared semi-partial correspond to the areas in the Venn diagrams as follows

\begin{align}
r^2_{Y1 \mid 2} & = \frac{A}{A + B + C + D} \\ \\
r^2_{Y2 \mid 1} & = \frac{B}{A + B + C + D} 
\end{align}

Using this relationship, we can see that squared semi partials (and hence the \beta-weights) 
\end{align} \]

-->
</section>
<section id="adjusted-r-squared" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="adjusted-r-squared"><span class="header-section-number">3.6.2</span> Adjusted R-squared</h3>
<p>The sample R-squared is an upwardly biased estimate of the population R-squared. This bias is illustrated in the figure below. In the example, we are considering simple regression (one predictor), and we assume that the population correlation between the predictor and the outcome is zero (i.e., <span class="math inline">\(\rho = 0\)</span>).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-adjusted" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="files/images/adjusted_rsquared.png" class="img-fluid figure-img" width="581"></p>
<figcaption class="figure-caption">Figure&nbsp;3.4: Sampling Distribution of <span class="math inline">\(r\)</span> and <span class="math inline">\(r^2\)</span> when $ ho = 0$.</figcaption>
</figure>
</div>
</div>
</div>
<p>In the left panel, we can see that “un-squared” correlation, <span class="math inline">\(r\)</span>, has a sampling distribution that is centered at the true value <span class="math inline">\(\rho = 0\)</span>. This means that <span class="math inline">\(r\)</span> is an unbiased estimate of <span class="math inline">\(\rho\)</span>.</p>
<p>But in the right panel, we can see that the sampling distribution of the squared correlation, <span class="math inline">\(r^2\)</span>, must have a mean greater than zero. This is because all of the sample-to-sample deviations in left panel are now positive (because they have been squared). Since the average value of <span class="math inline">\(r^2\)</span> is greater than the population value (<span class="math inline">\(\rho = 0\)</span>), <span class="math inline">\(r^2\)</span> is an upwardly biased estimate of <span class="math inline">\(\rho^2\)</span>. (i.e., it’s too large).</p>
<p>The adjusted R-squared corrects this bias. The formula for the adjustment is:</p>
<p><span class="math display">\[\tilde R^2 = 1 - (1 - R^2) \frac{N-1}{N - K - 1}\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the number of predictors in the model.</p>
<p>The formula contains two main terms, the proportion of residual variance, <span class="math inline">\((1 - R^2)\)</span>, and the adjustment factor (the ratio of <span class="math inline">\(N-1\)</span> to <span class="math inline">\(N-K-1\)</span>). We can understand how the adjustment works by considering these two terms.</p>
<p>First, it can be seen that the adjustment factor is larger when the number of predictors, <span class="math inline">\(K\)</span>, is large relative to the sample size, <span class="math inline">\(N\)</span>. So, roughly speaking, the adjustment will be more severe when there are a lot of predictors in the model relative to the sample size.</p>
<p>Second, it can also be seen that the adjustment proportional to <span class="math inline">\((1 - R^2)\)</span>. This means that the adjustment is more severe if the model explains less variance in the outcome. For example, if <span class="math inline">\(R^2 = .9\)</span> and the adjustment factor is <span class="math inline">\(1.1\)</span>, then adjusted <span class="math inline">\(R^2 = .89\)</span>. In this case the adjustment is a decrease of 1% of variance explained. But if we start off explaining less variance, say <span class="math inline">\(R^2 = .1\)</span> and use the same adjustment factor, then adjusted <span class="math inline">\(R^2 = .01\)</span>. Now the adjustment is a decrease of 9% variance explained.</p>
<p>In summary, the overall interpretation of adusted R-squared is as follows: the adjustment will be larger when there are lots of predictors in the model but they don’t explain much variance in the outcome. This situation is sometimes called “overfitting” the data, so we can think of adjusted R-squared as a correction for overfitting.</p>
<p>There is no established standard for when you should reported R-squared or adjusted R-squared. I recommend that you report both whenever they would would lead to different substantive conclusions. We can discuss this more in class.</p>
</section>
<section id="the-ecls-example-1" class="level3" data-number="3.6.3">
<h3 data-number="3.6.3" class="anchored" data-anchor-id="the-ecls-example-1"><span class="header-section-number">3.6.3</span> The ECLS example</h3>
<p>As shown in <a href="#sec-beta-3"><span>Section&nbsp;3.5.4</span></a>, the R-squared for the ECLS example is equal to .2726 and the adjusted R-squared is equal to .2668. <strong>Please write down your interpretation of these value and be prepared to share your answer in class.</strong></p>
</section>
</section>
<section id="sec-inference-3" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="sec-inference-3"><span class="header-section-number">3.7</span> Inference</h2>
<p>There isn’t really any thing new that about inference with multiple regression, except the formula for the standard errors (see <span class="citation" data-cites="fox2016">(<a href="#ref-fox2016" role="doc-biblioref"><strong>fox2016?</strong></a>)</span> chap.6). We present the formulas for an abribrary number of predictors, denoted <span class="math inline">\(k = 1, \dots K\)</span>.</p>
<section id="sec-inference-for-coeffecients-3" class="level3" data-number="3.7.1">
<h3 data-number="3.7.1" class="anchored" data-anchor-id="sec-inference-for-coeffecients-3"><span class="header-section-number">3.7.1</span> Inference for the coefficients</h3>
<p>In multiple regression</p>
<p><span id="eq-se-3"><span class="math display">\[SE({\widehat b_k}) = \frac{\text{SD}(Y)}{\text{SD}(X)} \sqrt{\frac{1 - R^2}{N - K - 1}} \times \sqrt{\frac{1}{1 - R_k^2}}
\tag{3.6}\]</span></span></p>
<p>In this formula, <span class="math inline">\(K\)</span> denotes the number of predictors and <span class="math inline">\(R^2_k\)</span> is the R-squared that results from regressing predictor <span class="math inline">\(k\)</span> on the other <span class="math inline">\(K-1\)</span> predictors (without the <span class="math inline">\(Y\)</span> variable).</p>
<p>Notice that the first part of the standard error (before the “<span class="math inline">\(\times\)</span>”) is the same as simple regression (see <a href="ch2_simple_regression.html#sec-inference-2"><span>Section&nbsp;2.7</span></a>). The last part, which includes <span class="math inline">\(R^2_k\)</span>, is different and we talk about it more below.</p>
<p>The standard errors can be used to construct t-tests and confidence intervals using the same approach as for simple regression (see <a href="ch2_simple_regression.html#sec-inference-2"><span>Section&nbsp;2.7</span></a>). The degrees of freedom for the t-distribution is <span class="math inline">\(N - K -1\)</span> (this applies to simple regression too, where <span class="math inline">\(K = 1\)</span>).</p>
</section>
<section id="precision-of-hat-b" class="level3" data-number="3.7.2">
<h3 data-number="3.7.2" class="anchored" data-anchor-id="precision-of-hat-b"><span class="header-section-number">3.7.2</span> Precision of <span class="math inline">\(\hat b\)</span></h3>
<p>We can use <a href="#eq-se-3">Equation&nbsp;<span>3.6</span></a> to understand the factors that influence the size of the standard errors of the regression coefficients. Recall that standard errors describe the sample-to-sample variability of a statistic. If there is a lot sample-to-sample variability, the statistic is said to be imprecise. <a href="#eq-se-3">Equation&nbsp;<span>3.6</span></a> shows us what factors make <span class="math inline">\(\hat b\)</span> more or less precise.</p>
<ul>
<li>The standard errors <em>decrease</em> with
<ul>
<li>The sample size, <span class="math inline">\(N\)</span></li>
<li>The proportion of variance in the outcome explained by the predictors, <span class="math inline">\(R^2\)</span></li>
</ul></li>
<li>The standard errors <em>increase</em> with
<ul>
<li>The number of predictors, <span class="math inline">\(K\)</span></li>
<li>The proportion of variance in the predictor that is explained by the other predictors, <span class="math inline">\(R^2_k\)</span></li>
</ul></li>
</ul>
<p>So, large sample sizes and a large proportion of variance explained lead to precise estimates of the regression coefficients. On the other hand, including many predictors that are highly correlated with each other leads to less precision. In particular, the situation where <span class="math inline">\(R^2_k\)</span> approaches the value of <span class="math inline">\(1\)</span> is called <em>multicollinearity</em>. We will talk about multicollinearity in more detail in <span class="quarto-unresolved-ref">?sec-chap-5</span>.</p>
</section>
<section id="inference-for-rsquared-4" class="level3" data-number="3.7.3">
<h3 data-number="3.7.3" class="anchored" data-anchor-id="inference-for-rsquared-4"><span class="header-section-number">3.7.3</span> Inference for R-squared</h3>
<p>The R-squared statistic in multiple regression tells us how much variation in the outcome is explained by all of the predictors together. If the predictors do not explain any variation, then the population R-squared is equal to zero.</p>
<p>Notice that <span class="math inline">\(R^2 = 0\)</span> implies <span class="math inline">\(b_1 = b_2 = ... = b_k = 0\)</span> (in the population). So, testing the significance of R-squared is equivalent to testing whether any of the regression parameters are non-zero. When we addressed ANOVA last semester, we called this the omnibus hypothesis. But in regression analysis, it is usually just referred to as a test of R-squared.</p>
<p>The null hypothesis <span class="math inline">\(H_0 : R^2 = 0\)</span> can be tested using the statistic</p>
<p><span class="math display">\[F = \frac{\widehat R^2 / K}{(1 - \widehat R^2) / (N - K - 1)},\]</span></p>
<p>which has an F-distribution on <span class="math inline">\(J\)</span> and <span class="math inline">\(N - k -1\)</span> degrees of freedom when the null hypothesis is true.</p>
</section>
<section id="the-ecls-example-2" class="level3" data-number="3.7.4">
<h3 data-number="3.7.4" class="anchored" data-anchor-id="the-ecls-example-2"><span class="header-section-number">3.7.4</span> The ECLS example</h3>
<p>The R output for the ECLS example is presented (again) below. <strong>Please write down your conclusions about the statistical significance of the predictors and the R-squared statistic, and be prepared to share your answer in class. Please also write down the factors that affect precision the precision of the regression coefficients. This would be a good opportunity to practice APA formatting.</strong></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ wksesl + t1learn)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.101  -4.034  -1.075   3.289  29.543 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.05016    2.90027  -2.086    0.038 *  
wksesl       0.35125    0.05633   6.235 1.94e-09 ***
t1learn      3.52125    0.67390   5.225 3.70e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.164 on 247 degrees of freedom
Multiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 
F-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
</section>
<section id="workbook" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="workbook"><span class="header-section-number">3.8</span> Workbook</h2>
<p>This section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in <span class="quarto-unresolved-ref">?sec-exercises-3</span>. The lesson will <strong>not</strong> be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.</p>
<p><a href="#sec-ecls-3"><span>Section&nbsp;3.2</span></a></p>
<p>If you have questions about the interpretation of a correlation matrix (below) or pairwise plots (see <a href="#sec-ecls-3"><span>Section&nbsp;3.2</span></a>), please write them down now and share them class.</p>
<p>Numerical output for the ECLS example:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(example_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          Math       SES       ATL
Math 1.0000000 0.4384619 0.3977048
SES  0.4384619 1.0000000 0.2877015
ATL  0.3977048 0.2877015 1.0000000</code></pre>
</div>
</div>
<p>Mathematical notation for formulas</p>
<p><span class="math display">\[
\begin{array}{c}
\text{var } Y \\ \text{var } X_1 \\ \text{var } X_2
\end{array}
\quad
\left[
\begin{array}{ccc}
1       &amp; r_{Y1}  &amp; r_{Y2}  \\
r_{1Y}  &amp; 1       &amp; r_{12}  \\
r_{2Y}  &amp; r_{21}  &amp; 1
\end{array}
\right]
\]</span></p>
<p><a href="#sec-interpretation-3"><span>Section&nbsp;3.5</span></a></p>
<p>Below, the R output from the ECLS example is reported. Please provide a written explanation of the regression coefficients for SES and ATL. If you have questions about how to interpret the coefficients, please also note them now and be prepared to share them in class. Note that this question is not asking about whether the coefficients are significant – it is asking what the numbers in the first column of the Coefficients table mean.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the regression model and print output</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl <span class="sc">+</span> t1learn)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ wksesl + t1learn)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.101  -4.034  -1.075   3.289  29.543 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.05016    2.90027  -2.086    0.038 *  
wksesl       0.35125    0.05633   6.235 1.94e-09 ***
t1learn      3.52125    0.67390   5.225 3.70e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.164 on 247 degrees of freedom
Multiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 
F-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p><a href="#sec-beta-3"><span>Section&nbsp;3.5.4</span></a></p>
<p>Please write down an interpretation of the of beta (standardized) regression coefficients in the above output. Your interpretation should include reference to the fact that the variables have been standardized. Based on this analysis, do you think predictor more important than the other?</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Unlike other software, R doesn't have a convenience functions for beta coefficients. </span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>z_example_data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">scale</span>(example_data))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>z_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(Math <span class="sc">~</span> SES  <span class="sc">+</span> ATL, <span class="at">data =</span> z_example_data)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(z_mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Math ~ SES + ATL, data = z_example_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9590 -0.5604 -0.1493  0.4569  4.1043 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 1.337e-15  5.416e-02   0.000        1    
SES         3.533e-01  5.666e-02   6.235 1.94e-09 ***
ATL         2.961e-01  5.666e-02   5.225 3.70e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.8563 on 247 degrees of freedom
Multiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 
F-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p><a href="#sec-rsquared-3"><span>Section&nbsp;3.6</span></a></p>
<p>The R-squared for the ECLS example is equal to .2726 and the adjusted R-squared is equal to .2668. Please write down your interpretation of these value and be prepared to share your answer in class.</p>
<p><span class="citation" data-cites="ref-inference-3">(<a href="#ref-ref-inference-3" role="doc-biblioref"><strong>ref-inference-3?</strong></a>)</span></p>
<p>The R output for the ECLS example is presented (again) below. Please write down your conclusions about the statistical significance of the predictors and the R-squared statistic, and be prepared to share your answer in class. This would be a good opportunity to practice APA formatting. Please also write down the factors that negatively affect the precision of the regression coefficients and address whether you think they are problematic for the example.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ wksesl + t1learn)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.101  -4.034  -1.075   3.289  29.543 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.05016    2.90027  -2.086    0.038 *  
wksesl       0.35125    0.05633   6.235 1.94e-09 ***
t1learn      3.52125    0.67390   5.225 3.70e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.164 on 247 degrees of freedom
Multiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 
F-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
<section id="exercises" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="exercises"><span class="header-section-number">3.9</span> Exercises</h2>
<p>These exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)</p>
<p>Before staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”</p>
<section id="the-ecls250-data" class="level3" data-number="3.9.1">
<h3 data-number="3.9.1" class="anchored" data-anchor-id="the-ecls250-data"><span class="header-section-number">3.9.1</span> The ECLS250 data</h3>
<p>Let’s start by getting our example data loaded into R.</p>
<p>Make sure to download the file <code>ECLS250.RData</code> from Canvas and then double click the file to open it</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"ECLS250.RData"</span>) <span class="co"># load new example</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(ecls) <span class="co"># attach </span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># knitr and kable are just used to print nicely -- you can just use head(ecls[, 1:5]) </span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">head</span>(ecls[, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">caseid</th>
<th style="text-align: right;">gender</th>
<th style="text-align: right;">race</th>
<th style="text-align: right;">c1rrscal</th>
<th style="text-align: right;">c1rrttsco</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">960</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">58</td>
</tr>
<tr class="even">
<td style="text-align: right;">113</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">39</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1828</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">22</td>
<td style="text-align: right;">50</td>
</tr>
<tr class="even">
<td style="text-align: right;">1693</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">21</td>
<td style="text-align: right;">50</td>
</tr>
<tr class="odd">
<td style="text-align: right;">643</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">39</td>
</tr>
<tr class="even">
<td style="text-align: right;">772</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">21</td>
<td style="text-align: right;">49</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The naming conventions for these data are bit challenging.</p>
<ul>
<li><p>Variable names begin with <code>c</code>, <code>p</code>, or <code>t</code> depending on whether the respondent was the child, parent, or teacher. Variables that start with <code>wk</code> were created by the ECLS using other data sources available in during the kindergarten year of the study.</p></li>
<li><p>The time points (1-4 denoting fall and spring of K and Gr 1) appear as the second character.</p></li>
<li><p>The rest of the name describes the variable.</p></li>
</ul>
<p>The variables we will use for this illustration are:</p>
<ul>
<li><p><code>c1rmscal</code>: Child’s score on a math assessment, in first semester of Kindergarten . The scores can be interpreted as number of correct responses out of a total of approximately 60 math exam questions.</p></li>
<li><p><code>wksesl</code>: An SES composite of household factors (e.g., parental education, household income) ranging from 30-72.</p></li>
<li><p><code>t1learn</code>: Approaches to Learning Scale (ATLS), teacher reported in first semester of kindergarten. This scale measures behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), so that higher values represent more positive responses, and the scale is an unweighted average the six items.</p></li>
</ul>
<p>To get started lets produce the simple regression of Math with SES. This is another look at the relationship between Academic Achievement and SES that we discussed in Chapter <a href="ch2_simple_regression.html"><span>Chapter&nbsp;2</span></a>). If you do not feel comfortable running this analysis or interpreting the output, take another look at <a href="ch2_simple_regression.html#sec-exercises-2"><span>Section&nbsp;2.11</span></a>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> wksesl, </span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> c1rmscal, </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch3_two_predictors_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ wksesl)

Residuals:
     Min       1Q   Median       3Q      Max 
-16.1314  -4.3549  -0.8486   3.6775  31.5358 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.61595    2.73925   0.225    0.822    
wksesl       0.43594    0.05674   7.683 3.61e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.482 on 248 degrees of freedom
Multiple R-squared:  0.1922,    Adjusted R-squared:  0.189 
F-statistic: 59.03 on 1 and 248 DF,  p-value: 3.612e-13</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(wksesl, c1rmscal)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4384619</code></pre>
</div>
</div>
</section>
<section id="multiple-regression-with-lm" class="level3" data-number="3.9.2">
<h3 data-number="3.9.2" class="anchored" data-anchor-id="multiple-regression-with-lm"><span class="header-section-number">3.9.2</span> Multiple regression with <code>lm</code></h3>
<p>First, let’s tale a look at the “zero-order” relationship among the three variables. This type of descriptive, two-way analysis is a good way to get familiar with your data before getting into multiple regression. We can see that the variables are all moderately correlated and their relationships appear reasonably linear.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use cbind to create a data.frame with just the 3 variables we want to examine</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">cbind</span>(c1rmscal, wksesl, t1learn)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlations</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          c1rmscal    wksesl   t1learn
c1rmscal 1.0000000 0.4384619 0.3977048
wksesl   0.4384619 1.0000000 0.2877015
t1learn  0.3977048 0.2877015 1.0000000</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatterplots</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(data, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="ch3_two_predictors_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>In terms of input, multiple regression with <code>lm</code> is similar to simple regression. The only difference is the model formula. To include more predictors in a formula, just include them on the right hand side, separated by at <code>+</code> sign.</p>
<ul>
<li>e.g, <code>Y ~ Χ1 + Χ2</code></li>
</ul>
<p>For our example, let’s consider the regression of math achievement on SES and Approaches to Learning. We’ll save our result as <code>mod1</code> which is short for “model one”.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl <span class="sc">+</span> t1learn)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ wksesl + t1learn)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.101  -4.034  -1.075   3.289  29.543 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.05016    2.90027  -2.086    0.038 *  
wksesl       0.35125    0.05633   6.235 1.94e-09 ***
t1learn      3.52125    0.67390   5.225 3.70e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.164 on 247 degrees of freedom
Multiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 
F-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We can see from the output that regression coefficient for <code>t1learn</code> is about 3.5. This means that, as the predictor increases by a single unit, children’s predicted math scores increase by 3.5 points (out of 60), after controlling for the SES. You should be able to provide a similar interpretation of the regression coefficient for <code>wksesl</code>. Together, both predictors accounted for about 27% of the variation in students’ math scores. In education, this would be considered a pretty strong relationship.</p>
<p>We will talk about the statistical tests later on. For now let’s consider the relationship with simple regression.</p>
</section>
<section id="relations-between-simple-and-multiple-regression" class="level3" data-number="3.9.3">
<h3 data-number="3.9.3" class="anchored" data-anchor-id="relations-between-simple-and-multiple-regression"><span class="header-section-number">3.9.3</span> Relations between simple and multiple regression</h3>
<p>First let’s consider how the two simple regression compare to the multiple regression with two variables. Here is the relevant output:</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the multiple regression output to the simple regressions</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>mod2a <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ wksesl)

Residuals:
     Min       1Q   Median       3Q      Max 
-16.1314  -4.3549  -0.8486   3.6775  31.5358 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.61595    2.73925   0.225    0.822    
wksesl       0.43594    0.05674   7.683 3.61e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.482 on 248 degrees of freedom
Multiple R-squared:  0.1922,    Adjusted R-squared:  0.189 
F-statistic: 59.03 on 1 and 248 DF,  p-value: 3.612e-13</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>mod2b <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> t1learn)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ t1learn)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.399  -4.211  -0.997   3.770  31.844 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   7.0394     2.1485   3.276   0.0012 ** 
t1learn       4.7301     0.6929   6.826 6.66e-11 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.618 on 248 degrees of freedom
Multiple R-squared:  0.1582,    Adjusted R-squared:  0.1548 
F-statistic:  46.6 on 1 and 248 DF,  p-value: 6.665e-11</code></pre>
</div>
</div>
<p>The important things to note here are</p>
<ul>
<li><p>The regression coefficients from the simple models (<span class="math inline">\(b_{ses} = 4.38\)</span> and <span class="math inline">\(b_{t1learn} = 4.73\)</span>) are larger than the regression coefficients from the two-predictor model. Can you explain why? (Hint: see Section <a href="#sec-interpretation-3"><span>Section&nbsp;3.5</span></a>.</p></li>
<li><p>The R-squared values in the two simple models (.194 + .158 = .352) add up to more than the R-squared in the two-predictor model (.274). Again, take a moment to think about why before reading on. (Hint: see Section <a href="#sec-rsquared-3"><span>Section&nbsp;3.6</span></a>.)</p></li>
</ul>
</section>
<section id="inference-with-2-predictors" class="level3" data-number="3.9.4">
<h3 data-number="3.9.4" class="anchored" data-anchor-id="inference-with-2-predictors"><span class="header-section-number">3.9.4</span> Inference with 2 predictors</h3>
<p>Let’s move on now to consider the statistical tests and confidence intervals provided with the <code>lm</code> summary output.</p>
<p>For regression with more than one predictor, both the t-tests and F-tests have a very similar construction and interpretation as with simple regression. The main differences are the formulas, not so much the interpretations of the procedures. Some differences:</p>
<ul>
<li><p>The degrees of freedom for both tests now involve <span class="math inline">\(K\)</span>, the number of predictors.</p></li>
<li><p>The standard error of the b-weight is more complicated, because it involves the inter-correlation among the predictors.</p></li>
</ul>
<p>We can see for <code>mod1</code> that both b-weights are significant at the .05 level, and so is the R-square. As mentioned previously, it is not usual to interpret or report results on the regression intercept unless you have a special reason to do so (e.g., see the next chapter).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Revisting the output of mod1</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ wksesl + t1learn)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.101  -4.034  -1.075   3.289  29.543 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.05016    2.90027  -2.086    0.038 *  
wksesl       0.35125    0.05633   6.235 1.94e-09 ***
t1learn      3.52125    0.67390   5.225 3.70e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.164 on 247 degrees of freedom
Multiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 
F-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</section>
<section id="apa-reporting-of-results" class="level3" data-number="3.9.5">
<h3 data-number="3.9.5" class="anchored" data-anchor-id="apa-reporting-of-results"><span class="header-section-number">3.9.5</span> APA reporting of results</h3>
<p>Here is how we might write out the results of our regression using APA format. The numbers are taken from the output below. When we have a regression model with many predictors, or are comparing among different models, it is more usual to put all the relevant statistics in a table rather than writing them out one by one. We will see how to do that later on in the course.For more info on APA format, see the APA publications manual: <a href="https://www.apastyle.org/manual">(https://www.apastyle.org/manual)</a>.</p>
<ul>
<li><p>The regression of Math Achievement on SES was positive and statistically significant at the .05 level (<span class="math inline">\(b = 3.53, t(247) = 6.27, p &lt; .001\)</span>).</p></li>
<li><p>The regression of Math Achievement on Approaches to Learning was also positive and statistically significant at the .05 level (<span class="math inline">\(b = 3.50, t(247) = 5.20, p &lt; .001\)</span>).</p></li>
<li><p>Together both predictors accounted for about 27% of the variation in Math Achievement (<span class="math inline">\(R^2\)</span> = .274, adjusted <span class="math inline">\(R^2\)</span> = .268), which was also statistically significant at the .05 level (<span class="math inline">\(F(2, 247) = 45.54, p &lt; .001\)</span>).</p></li>
</ul>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = c1rmscal ~ wksesl + t1learn)

Residuals:
    Min      1Q  Median      3Q     Max 
-14.101  -4.034  -1.075   3.289  29.543 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.05016    2.90027  -2.086    0.038 *  
wksesl       0.35125    0.05633   6.235 1.94e-09 ***
t1learn      3.52125    0.67390   5.225 3.70e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.164 on 247 degrees of freedom
Multiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 
F-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch2_simple_regression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Simple Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb38" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="an">fold:</span><span class="co"> true</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="an">editor:</span><span class="co"> </span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co">  markdown: </span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co">    wrap: 72</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="fu"># Two predictors {#sec-chap-3}</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interpretations of regression {#sec-interpretations-3}</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>Regression has three main interpretations:</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Prediction (focus on $\hat Y$)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Causation (focus on $b$)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Explanation (focus on $R^2$)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>By understanding these interpretations, we will have a better idea of how regression is used in research. Each interpretation also provides a different perspective on the importance of using multiple predictor variables, rather than only a single predictor.</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prediction</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>Prediction means “to make known beforehand”. This was the original use of regression (<span class="co">[</span><span class="ot">https://en.wikipedia.org/wiki/Regression_toward_the_mean#History</span><span class="co">](https://en.wikipedia.org/wiki/Regression_toward_the_mean#History)</span>). In a regression contex, prediction means using observations of $X$ to make a guess about yet unobserved values of $Y$. Our guess is $\hat Y$, and this is why $\hat Y$ is called the "predicted value" of $Y$. </span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>When making predictions, we usually want some additional information about how precise the predictions will be. In OLS regression, this information is provided by the standard error of prediction @fox-2016:  </span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>$$\text{SE}({\hat Y_i}) = \sqrt{\frac{SS_{\text{res}}}{N - 2} \left(1 +  \frac{1}{N} + \frac{(X_i - \bar X)^2}{\sum_j(X_j - \bar X)^2} \right)}$$ {#eq-se-pred}</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>This statistic quantifies our uncertainty when making predictions based on observations of $X$ that were not in our original sample. The prediction errors for the NELS example in @sec-chap-2 are represented in @fig-pred-error-3 as a gray band around the regression line.</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-pred-error-3, message=F, fig.cap = 'Prediction Error for NELS Example.', fig.align = 'center'}</span></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting library</span></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"NELS.RData"</span>)</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Run regression </span></span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(achmat08 <span class="sc">~</span> ses, <span class="at">data =</span> NELS)</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute SE(Y-hat)</span></span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(NELS)</span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>ms_res <span class="ot">&lt;-</span> <span class="fu">var</span>(mod<span class="sc">$</span>residuals) <span class="sc">*</span> (n<span class="dv">-1</span>) <span class="sc">/</span> (n<span class="dv">-2</span>)</span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a>d_ses <span class="ot">&lt;-</span> NELS<span class="sc">$</span>ses <span class="sc">-</span> <span class="fu">mean</span>(NELS<span class="sc">$</span>ses) </span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>se_yhat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(ms_res <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>n <span class="sc">+</span> d_ses<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span>(d_ses<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a>gg_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a>             <span class="at">achmat08 =</span> NELS<span class="sc">$</span>achmat08,</span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a>             <span class="at">ses =</span> NELS<span class="sc">$</span>ses,</span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a>             <span class="at">y_hat =</span> mod<span class="sc">$</span>fitted.values,</span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a>             <span class="at">lwr =</span> mod<span class="sc">$</span>fitted.values <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> se_yhat,</span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a>             <span class="at">upr =</span> mod<span class="sc">$</span>fitted.values <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> se_yhat)</span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(gg_data, <span class="fu">aes</span>(<span class="at">x =</span> ses, <span class="at">y =</span> achmat08))<span class="sc">+</span></span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">'#3B9CD3'</span>, <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> ses, <span class="at">y =</span> y_hat), <span class="at">color =</span> <span class="st">"grey35"</span>) <span class="sc">+</span></span>
<span id="cb38-59"><a href="#cb38-59" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">ymin=</span>lwr,<span class="at">ymax=</span>upr),<span class="at">alpha=</span><span class="fl">0.3</span>) <span class="sc">+</span> </span>
<span id="cb38-60"><a href="#cb38-60" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ylab</span>(<span class="st">"Math Achievement (Grade 8)"</span>) <span class="sc">+</span></span>
<span id="cb38-61"><a href="#cb38-61" aria-hidden="true" tabindex="-1"></a>    <span class="fu">xlab</span>(<span class="st">"SES"</span>) <span class="sc">+</span></span>
<span id="cb38-62"><a href="#cb38-62" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb38-63"><a href="#cb38-63" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-64"><a href="#cb38-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-65"><a href="#cb38-65" aria-hidden="true" tabindex="-1"></a>We can see in the figure that the error band is quite wide. So, we might wonder how to make our predictions more precise. On way to do this is by including more predictors in the regression model -- i.e., multiple regression. </span>
<span id="cb38-66"><a href="#cb38-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-67"><a href="#cb38-67" aria-hidden="true" tabindex="-1"></a>To see why including more predictors improves the precision of predictions, note that the standard error of prediction shown in @eq-se-pred increases with $SS_{\text{res}}$, which is the variation in the outcome that is *not* explained by the predictor (see @sec-rsquared-2). In most situations, $SS_{\text{res}}$ is the largest contributor the prediction error. As we will see below, one way to reduce $SS_{\text{res}}$ is by adding more predictors to the model. </span>
<span id="cb38-68"><a href="#cb38-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-69"><a href="#cb38-69" aria-hidden="true" tabindex="-1"></a><span class="fu">#### More about prediction</span></span>
<span id="cb38-70"><a href="#cb38-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-71"><a href="#cb38-71" aria-hidden="true" tabindex="-1"></a>There has been a resurgence of interest in prediction in recent years, especially in machine learning. Although the methods used in machine learning are often more complicated than OLS regression, the basic problem is the same. Rather than theoretically derived prediction error (@eq-se-pred), machine learning often uses the accuracy and precision of out-of-sample predictions as the main criterion for judging the quality of a model, which is often called "cross validation." Machine learning has also introduced some new techniques for choosing which predictors to include in a model (i.e., "variable selection"). We will touch on these topics later in the course, although our main focus is OLS regression.</span>
<span id="cb38-72"><a href="#cb38-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-73"><a href="#cb38-73" aria-hidden="true" tabindex="-1"></a>Regression got its name from a statistical property of predicted scores called "regression toward the mean." To explain this property, let's assume $Y$ and $X$ are z-scores (i.e., both variables have $M = 0$ and $SD = 1$). Recall that this implies that $a = 0$ and $b = r_{XY}$, so the regression equation reduces to</span>
<span id="cb38-74"><a href="#cb38-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-75"><a href="#cb38-75" aria-hidden="true" tabindex="-1"></a>$$\hat Y = r_{XY} X$$</span>
<span id="cb38-76"><a href="#cb38-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-77"><a href="#cb38-77" aria-hidden="true" tabindex="-1"></a>Since $|r_{XY} | ≤ 1$, the absolute value of the $\hat Y$ must be less than or equal to that of $X$. And, since both variables have $M = 0$, this implies that $\hat Y$ is closer to the mean of $Y$ than $X$ is to the mean of $X$. This is sometimes called regression toward the mean.</span>
<span id="cb38-78"><a href="#cb38-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-79"><a href="#cb38-79" aria-hidden="true" tabindex="-1"></a><span class="fu">### Causation</span></span>
<span id="cb38-80"><a href="#cb38-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-81"><a href="#cb38-81" aria-hidden="true" tabindex="-1"></a>A causal interpretation of regression means that that changing $X$ by one unit will change $E(Y|X)$ by $b$ units. This is interpreted as a claim about the expected value of $Y$ "in real life", not simply a claim about the mechanics of the regression line. In terms of our example, a causal interpretation would state that improving students' SES by one unit will, on average, cause Math Achievement to increase by about half a percentage point. </span>
<span id="cb38-82"><a href="#cb38-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-83"><a href="#cb38-83" aria-hidden="true" tabindex="-1"></a>The gold standard for inferring causality is to randomly assign people to different treatment conditions. In a regression context, treatment is represented by the independent variable, or the $X$ variable. While randomized experiments are possible in some settings, there are many types of variables that we cannot feasibly randomly assign (e.g., SES). </span>
<span id="cb38-84"><a href="#cb38-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-85"><a href="#cb38-85" aria-hidden="true" tabindex="-1"></a>The concept of an omitted variable is used to describe what happens when we can't (or don't) randomly assign people to treatment conditions. An omitted variable is any variable that is correlated with both $Y$ and $X$. In our example, this would be any variable correlated with both Math Achievement and SES (e.g., School Quality). When we use random assignment, we ensure that $X$ is uncorrelated with *all* pre-treatment variables -- i.e., randomization ensure that there are no omitted variables. However, when we don't use random assignment, our results may be subject to *omitted variable bias*. </span>
<span id="cb38-86"><a href="#cb38-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-87"><a href="#cb38-87" aria-hidden="true" tabindex="-1"></a>The overall idea of omitted variable bias is the same as "correlation $\neq$ causation". The take-home message is summarized in the following points, which are stated in terms of the our NELS example. </span>
<span id="cb38-88"><a href="#cb38-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-89"><a href="#cb38-89" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Any variable that is correlated with Math Achievement and with SES is called an omitted variable. One example is School Quality. This is an omitted variable because we did not include it as a predictor in our simple regression model. </span>
<span id="cb38-90"><a href="#cb38-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-91"><a href="#cb38-91" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The problem is not just that we have an incomplete picture of how School Quality is related to Math Achievement. </span>
<span id="cb38-92"><a href="#cb38-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-93"><a href="#cb38-93" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Omitted variable bias means that the predictor variable that *was included in the model* ends up having the wrong regression coefficient. Otherwise stated, the regression coefficient of SES is biased because we did not consider School Quality.</span>
<span id="cb38-94"><a href="#cb38-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-95"><a href="#cb38-95" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In order to mitigate omitted variable bias, we want to include plausible omitted variables in our regression models -- i.e., multiple regression. </span>
<span id="cb38-96"><a href="#cb38-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-97"><a href="#cb38-97" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Omitted variable bias*</span></span>
<span id="cb38-98"><a href="#cb38-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-99"><a href="#cb38-99" aria-hidden="true" tabindex="-1"></a>Omitted variable bias is nicely explained by Gelman and Hill @gelman-2007, and a modified version of their discussion is provided below. We start by assuming a "true" regression model with two predictors. In the context of our example, this means that there is one other variable, in addition to SES, that is important for predicting Math Achievement. Of course, there are many predictors of Math Achievement (see Section @sec-example-2), but we only need two to explain the problem of omitted variable bias.</span>
<span id="cb38-100"><a href="#cb38-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-101"><a href="#cb38-101" aria-hidden="true" tabindex="-1"></a>Write the "true" model as:</span>
<span id="cb38-102"><a href="#cb38-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-103"><a href="#cb38-103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-104"><a href="#cb38-104" aria-hidden="true" tabindex="-1"></a>Y = a + b_1 X_1 + b_2 X_2 + \epsilon</span>
<span id="cb38-105"><a href="#cb38-105" aria-hidden="true" tabindex="-1"></a>$$ {#eq-2parm}</span>
<span id="cb38-106"><a href="#cb38-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-107"><a href="#cb38-107" aria-hidden="true" tabindex="-1"></a>where $X_1$ is SES and $X_2$ is any other variable that is correlated with both $Y$ and $X_1$ (e.g., School Quality).</span>
<span id="cb38-108"><a href="#cb38-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-109"><a href="#cb38-109" aria-hidden="true" tabindex="-1"></a>Next, imagine that instead of using the model in @eq-2parm, we analyze the data using the model with just SES, leading to the usual simple regression:</span>
<span id="cb38-110"><a href="#cb38-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-111"><a href="#cb38-111" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-112"><a href="#cb38-112" aria-hidden="true" tabindex="-1"></a>\hat Y = a^* + b^*_1 X_1 + \epsilon^*</span>
<span id="cb38-113"><a href="#cb38-113" aria-hidden="true" tabindex="-1"></a>$$ {#eq-1parm}</span>
<span id="cb38-114"><a href="#cb38-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-115"><a href="#cb38-115" aria-hidden="true" tabindex="-1"></a>The problem of omitted variable bias is that $b_1 \neq b^*_1$ -- i.e., the regression coefficient in the true model is not the same as the regression coefficient in the model with only one predictor. This is perhaps surprising -- leaving out School Quality gives us the wrong regression coefficient for SES!</span>
<span id="cb38-116"><a href="#cb38-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-117"><a href="#cb38-117" aria-hidden="true" tabindex="-1"></a>To see why, start by writing $X_2$ as a function of $X_1$.</span>
<span id="cb38-118"><a href="#cb38-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-119"><a href="#cb38-119" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-120"><a href="#cb38-120" aria-hidden="true" tabindex="-1"></a>X_2 = \alpha + \beta X_1 + \nu</span>
<span id="cb38-121"><a href="#cb38-121" aria-hidden="true" tabindex="-1"></a>$$ {#eq-X2}</span>
<span id="cb38-122"><a href="#cb38-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-123"><a href="#cb38-123" aria-hidden="true" tabindex="-1"></a>Next we use @eq-X2 to substitute for $X_2$ in @eq-2parm,</span>
<span id="cb38-124"><a href="#cb38-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-125"><a href="#cb38-125" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb38-126"><a href="#cb38-126" aria-hidden="true" tabindex="-1"></a> Y &amp; = a + b_1 X_1 + b_2 X_2 + \epsilon <span class="sc">\\</span></span>
<span id="cb38-127"><a href="#cb38-127" aria-hidden="true" tabindex="-1"></a>  &amp; = a + b_1 X_1 + b_2 (\alpha + \beta X_1 + \nu)  + \epsilon <span class="sc">\\</span></span>
<span id="cb38-128"><a href="#cb38-128" aria-hidden="true" tabindex="-1"></a>  &amp; = \color{orange}{(a + \alpha)} + \color{green}{(b_1 + b_2\beta)} X_1 + (e + \nu) \label{eq-3parm}</span>
<span id="cb38-129"><a href="#cb38-129" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb38-130"><a href="#cb38-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-131"><a href="#cb38-131" aria-hidden="true" tabindex="-1"></a>Notice that in the last line, $Y$ is predicted using only $X_1$, so it is equivalent to @eq-1parm. Based on this comparison, we can write</span>
<span id="cb38-132"><a href="#cb38-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-133"><a href="#cb38-133" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$a^* = \color{orange}{a + \alpha}$</span>
<span id="cb38-134"><a href="#cb38-134" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$b^*_1 = \color{green}{b_1 + b_2\beta}$</span>
<span id="cb38-135"><a href="#cb38-135" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\epsilon^* = \epsilon + \nu$</span>
<span id="cb38-136"><a href="#cb38-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-137"><a href="#cb38-137" aria-hidden="true" tabindex="-1"></a>The equation for $b^*_1$ is what we are most interested in. It shows that the regression parameter in our one-parameter model ($b^*_1$) is not equal to the "true" regression parameter using both predictors ($b_1$).</span>
<span id="cb38-138"><a href="#cb38-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-139"><a href="#cb38-139" aria-hidden="true" tabindex="-1"></a>This is what omitted variable bias means -- leaving out $X_2$ in Equation @eq-1parm gives us the wrong regression parameter for $X_1$. This is one of the main motivations for including more than one predictor variable in a regression model -- i.e., to avoid omitted variable bias.</span>
<span id="cb38-140"><a href="#cb38-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-141"><a href="#cb38-141" aria-hidden="true" tabindex="-1"></a>Notice that there two special situations in which omitted variable bias is not a problem:</span>
<span id="cb38-142"><a href="#cb38-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-143"><a href="#cb38-143" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>When the two predictors are not related -- i.e., $\beta = 0$.</span>
<span id="cb38-144"><a href="#cb38-144" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>When the second predictor is not related to $Y$ -- i.e., $b_2 = 0$.</span>
<span id="cb38-145"><a href="#cb38-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-146"><a href="#cb38-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### Explanation</span></span>
<span id="cb38-147"><a href="#cb38-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-148"><a href="#cb38-148" aria-hidden="true" tabindex="-1"></a>Many uses of regression fall somewhere between prediction and causation. We want to do more than just predict outcomes of interest, but we often don't have a basis for making the strong assumptions required for a causal interpretation of regression coefficients. This grey area between prediction and causation can be referred to as explanation.</span>
<span id="cb38-149"><a href="#cb38-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-150"><a href="#cb38-150" aria-hidden="true" tabindex="-1"></a>In terms of our example, we might want to explain why eighth graders differ in their Math Achievement. There are large number of potential reasons for individual difference in Math Achievement, such as</span>
<span id="cb38-151"><a href="#cb38-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-152"><a href="#cb38-152" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Student factors</span>
<span id="cb38-153"><a href="#cb38-153" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>attendance</span>
<span id="cb38-154"><a href="#cb38-154" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>past academic performance in Math</span>
<span id="cb38-155"><a href="#cb38-155" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>past academic performance in other subjects (Question: why include this?)</span>
<span id="cb38-156"><a href="#cb38-156" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>...</span>
<span id="cb38-157"><a href="#cb38-157" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>School factors</span>
<span id="cb38-158"><a href="#cb38-158" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>their ELA teacher</span>
<span id="cb38-159"><a href="#cb38-159" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>the school they attend</span>
<span id="cb38-160"><a href="#cb38-160" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>their peers </span>
<span id="cb38-161"><a href="#cb38-161" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>...</span>
<span id="cb38-162"><a href="#cb38-162" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Home factors</span>
<span id="cb38-163"><a href="#cb38-163" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>SES</span>
<span id="cb38-164"><a href="#cb38-164" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>maternal education</span>
<span id="cb38-165"><a href="#cb38-165" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>paternal education</span>
<span id="cb38-166"><a href="#cb38-166" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>parental expectations</span>
<span id="cb38-167"><a href="#cb38-167" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>...</span>
<span id="cb38-168"><a href="#cb38-168" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb38-169"><a href="#cb38-169" aria-hidden="true" tabindex="-1"></a>When the goal of an analysis is explanation, it usual to focus on the proportion of variation in the outcome variable that is explained by the predictors, i.e., R-squared (see @sec-rsquared-2). Later in the course we will see how to systematically study the variance explained by individual predictors, or blocks of several predictors (e.g., student factors). </span>
<span id="cb38-170"><a href="#cb38-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-171"><a href="#cb38-171" aria-hidden="true" tabindex="-1"></a>Note that even a long list of predictors such as that above leaves out potential omitted variables. While the addition of more predictors can help us explain more of the variation in Math Achievement, it is rarely the case that we can claim that all relevant variables have been included in the model. </span>
<span id="cb38-172"><a href="#cb38-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-173"><a href="#cb38-173" aria-hidden="true" tabindex="-1"></a><span class="fu">## An example from ECLS {#sec-ecls-3}</span></span>
<span id="cb38-174"><a href="#cb38-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-175"><a href="#cb38-175" aria-hidden="true" tabindex="-1"></a>In the remainder of this chapter we will consider a new example from the 1998 Early Childhood Longitudinal Study (ECLS; <span class="co">[</span><span class="ot">https://nces.ed.gov/ecls/</span><span class="co">](https://nces.ed.gov/ecls/)</span>). Below is a description of the data from the official NCES codebook (page 1-1 of  <span class="co">[</span><span class="ot">https://nces.ed.gov/ecls/data/ECLSK_K8_Manual_part1.pdf</span><span class="co">](https://nces.ed.gov/ecls/data/ECLSK_K8_Manual_part1.pdf)</span>): </span>
<span id="cb38-176"><a href="#cb38-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-177"><a href="#cb38-177" aria-hidden="true" tabindex="-1"></a>*The ECLS-K focuses on children’s early school experiences beginning with kindergarten and ending with eighth grade. It is a multisource, multimethod study that includes interviews with parents, the collection of data from principals and teachers, and student records abstracts, as well as direct child assessments. In the eighth-grade data collection, a student paper-and-pencil questionnaire was added. The ECLS-K was developed under the sponsorship of the U.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics (NCES). Westat conducted this study with assistance provided by Educational Testing Service (ETS) in Princeton, New Jersey.*</span>
<span id="cb38-178"><a href="#cb38-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-179"><a href="#cb38-179" aria-hidden="true" tabindex="-1"></a>*The ECLS-K followed a nationally representative cohort of children from kindergarten into middle school. The base-year data were collected in the fall and spring of the 1998–99 school year when the sampled children were in kindergarten. A total of 21,260 kindergartners throughout the nation participated*.</span>
<span id="cb38-180"><a href="#cb38-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-181"><a href="#cb38-181" aria-hidden="true" tabindex="-1"></a>The subset of the ECLS-K data used in this class was obtained from the link below. </span>
<span id="cb38-182"><a href="#cb38-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-183"><a href="#cb38-183" aria-hidden="true" tabindex="-1"></a><span class="ot">&lt;http://routledgetextbooks.com/textbooks/_author/ware-9780415996006/data.php&gt;</span></span>
<span id="cb38-184"><a href="#cb38-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-185"><a href="#cb38-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-186"><a href="#cb38-186" aria-hidden="true" tabindex="-1"></a>The codebook for this subset of data is available on our course website. We will be using a small subset of $N = 250$ cases from the full example data set (the  <span class="in">`ECLS250.RData`</span> data) </span>
<span id="cb38-187"><a href="#cb38-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-188"><a href="#cb38-188" aria-hidden="true" tabindex="-1"></a>We focus on the following three variables. </span>
<span id="cb38-189"><a href="#cb38-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-190"><a href="#cb38-190" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Math Achievement in the first semester of Kindergarten. This variable can be interpreted as the number of questions (out of 60) answered correctly on a math test. Don't worry -- the respondents in this study did not have to write a 60-question math test in the first semester of K! Students only answered a few of the questions and their scores were re-scaled to be out a total of 60 questions afterwards. </span>
<span id="cb38-191"><a href="#cb38-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-192"><a href="#cb38-192" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Socioecomonic Status (SES), which is a composite of household factors (e.g., parental education, household income) ranging from 30-72. </span>
<span id="cb38-193"><a href="#cb38-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-194"><a href="#cb38-194" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Approaches to Learning (ATL), which is a teacher-reported measure of behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), </span>
<span id="cb38-195"><a href="#cb38-195" aria-hidden="true" tabindex="-1"></a>d</span>
<span id="cb38-196"><a href="#cb38-196" aria-hidden="true" tabindex="-1"></a>so that higher values represent more positive responses, and the scale is an unweighted average the six items.</span>
<span id="cb38-197"><a href="#cb38-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-198"><a href="#cb38-198" aria-hidden="true" tabindex="-1"></a><span class="fu">### Correlation matrices</span></span>
<span id="cb38-199"><a href="#cb38-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-200"><a href="#cb38-200" aria-hidden="true" tabindex="-1"></a>As with simple regression, correlation is the building block of multiple regression. So, we will start by examining the correlations in our example. We also introduce a new way of presenting correlations, the correlation matrix. The notation developed in this section will appear throughout the rest of the chapter. </span>
<span id="cb38-201"><a href="#cb38-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-202"><a href="#cb38-202" aria-hidden="true" tabindex="-1"></a>In the scatter plots below, the panels are arranged in matrix format. The variable named on in the diagonal panels appears on the vertical ($Y$) axis in its row and the horizontal ($X$) axis in its column. For example, Math Achievement is on the vertical axis in the first row and the horizontal axis in the first column. Notice that plots below the diagonal are just mirror image of the plots above the diagonal. </span>
<span id="cb38-203"><a href="#cb38-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-204"><a href="#cb38-204" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-pairs-3, fig.cap = 'ECLS Example Data.', fig.align = 'center'}</span></span>
<span id="cb38-205"><a href="#cb38-205" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"ECLS250.RData"</span>)</span>
<span id="cb38-206"><a href="#cb38-206" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(ecls)</span>
<span id="cb38-207"><a href="#cb38-207" aria-hidden="true" tabindex="-1"></a>example_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(c1rmscal, wksesl, t1learn)</span>
<span id="cb38-208"><a href="#cb38-208" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(example_data) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Math"</span>, <span class="st">"SES"</span>, <span class="st">"ATL"</span>)</span>
<span id="cb38-209"><a href="#cb38-209" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(example_data , <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb38-210"><a href="#cb38-210" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-211"><a href="#cb38-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-212"><a href="#cb38-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-213"><a href="#cb38-213" aria-hidden="true" tabindex="-1"></a>The format of Figure @fig-pairs-3 is the same as that of the correlation matrix among the variables, which is shown below. </span>
<span id="cb38-214"><a href="#cb38-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-217"><a href="#cb38-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-218"><a href="#cb38-218" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(example_data)</span>
<span id="cb38-219"><a href="#cb38-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-220"><a href="#cb38-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-221"><a href="#cb38-221" aria-hidden="true" tabindex="-1"></a>Again, notice that the entries below the diagonal are mirrored by the entries above the diagonal. We can see that SES and ATL have similar correlations with Math Achievement (.44 and .40, respectively), and are also moderately correlated with each other (.29). </span>
<span id="cb38-222"><a href="#cb38-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-223"><a href="#cb38-223" aria-hidden="true" tabindex="-1"></a>In order to represent the correlation matrix among a single outcome variable ($Y$) and two predictors ($X_1$ and $X_2$) we will use the following notation. </span>
<span id="cb38-224"><a href="#cb38-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-225"><a href="#cb38-225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-226"><a href="#cb38-226" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb38-227"><a href="#cb38-227" aria-hidden="true" tabindex="-1"></a>\text{var } Y <span class="sc">\\</span> \text{var } X_1 <span class="sc">\\</span> \text{var } X_2</span>
<span id="cb38-228"><a href="#cb38-228" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb38-229"><a href="#cb38-229" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb38-230"><a href="#cb38-230" aria-hidden="true" tabindex="-1"></a>\left[ </span>
<span id="cb38-231"><a href="#cb38-231" aria-hidden="true" tabindex="-1"></a>\begin{array}{ccc}</span>
<span id="cb38-232"><a href="#cb38-232" aria-hidden="true" tabindex="-1"></a> 1       &amp; r_{Y1}  &amp; r_{Y2}  <span class="sc">\\</span></span>
<span id="cb38-233"><a href="#cb38-233" aria-hidden="true" tabindex="-1"></a> r_{1Y}  &amp; 1       &amp; r_{12}  <span class="sc">\\</span></span>
<span id="cb38-234"><a href="#cb38-234" aria-hidden="true" tabindex="-1"></a> r_{2Y}  &amp; r_{21}  &amp; 1</span>
<span id="cb38-235"><a href="#cb38-235" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb38-236"><a href="#cb38-236" aria-hidden="true" tabindex="-1"></a> \right]</span>
<span id="cb38-237"><a href="#cb38-237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-238"><a href="#cb38-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-239"><a href="#cb38-239" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \[ --&gt;</span></span>
<span id="cb38-240"><a href="#cb38-240" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \begin{array}{c} --&gt;</span></span>
<span id="cb38-241"><a href="#cb38-241" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \text{var 1} \\ \text{var 2} \\ \text{var 3}  --&gt;</span></span>
<span id="cb38-242"><a href="#cb38-242" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \end{array} --&gt;</span></span>
<span id="cb38-243"><a href="#cb38-243" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \quad --&gt;</span></span>
<span id="cb38-244"><a href="#cb38-244" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \left[  --&gt;</span></span>
<span id="cb38-245"><a href="#cb38-245" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \begin{array}{ccc} --&gt;</span></span>
<span id="cb38-246"><a href="#cb38-246" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--  1       &amp; r_{12}  &amp; r_{13}  \\ --&gt;</span></span>
<span id="cb38-247"><a href="#cb38-247" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--  r_{21}  &amp; 1       &amp; r_{23}  \\ --&gt;</span></span>
<span id="cb38-248"><a href="#cb38-248" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--  r_{31}  &amp; r_{32}  &amp; 1 --&gt;</span></span>
<span id="cb38-249"><a href="#cb38-249" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \end{array} --&gt;</span></span>
<span id="cb38-250"><a href="#cb38-250" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--  \right] --&gt;</span></span>
<span id="cb38-251"><a href="#cb38-251" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \] --&gt;</span></span>
<span id="cb38-252"><a href="#cb38-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-253"><a href="#cb38-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-254"><a href="#cb38-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-255"><a href="#cb38-255" aria-hidden="true" tabindex="-1"></a>in this notation, we replace use the $r_{Y1} = \text{cor}(Y,X_1)$ is the correlation between two variables $Y$ and $X_1$. Note that each correlation coefficient $r$ has two subscripts that tell us which two variables are being correlated. For the outcome variable we use the subscript $Y$, and for the two predictors we use the subscripts $1$ and $2$. As with the numerical examples, the values below the diagonal mirror the values above the diagonal. So, we really just need the three correlations shown in the matrix below. </span>
<span id="cb38-256"><a href="#cb38-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-257"><a href="#cb38-257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-258"><a href="#cb38-258" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb38-259"><a href="#cb38-259" aria-hidden="true" tabindex="-1"></a>\text{var } Y <span class="sc">\\</span> \text{var } X_1 <span class="sc">\\</span> \text{var } X_2</span>
<span id="cb38-260"><a href="#cb38-260" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb38-261"><a href="#cb38-261" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb38-262"><a href="#cb38-262" aria-hidden="true" tabindex="-1"></a>\left[ </span>
<span id="cb38-263"><a href="#cb38-263" aria-hidden="true" tabindex="-1"></a>\begin{array}{ccc}</span>
<span id="cb38-264"><a href="#cb38-264" aria-hidden="true" tabindex="-1"></a><span class="ss"> -       </span>&amp; r_{Y1}  &amp; r_{Y2}  <span class="sc">\\</span></span>
<span id="cb38-265"><a href="#cb38-265" aria-hidden="true" tabindex="-1"></a><span class="ss"> -  </span>&amp; -       &amp; r_{12}  <span class="sc">\\</span></span>
<span id="cb38-266"><a href="#cb38-266" aria-hidden="true" tabindex="-1"></a><span class="ss"> -  </span>&amp; -  &amp; -</span>
<span id="cb38-267"><a href="#cb38-267" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb38-268"><a href="#cb38-268" aria-hidden="true" tabindex="-1"></a> \right]</span>
<span id="cb38-269"><a href="#cb38-269" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-270"><a href="#cb38-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-271"><a href="#cb38-271" aria-hidden="true" tabindex="-1"></a>The three correlations are interpreted as follows: </span>
<span id="cb38-272"><a href="#cb38-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-273"><a href="#cb38-273" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$r_{Y1}$ - the correlation between the outcome ($Y$) and the first predictor ($X_1$). </span>
<span id="cb38-274"><a href="#cb38-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-275"><a href="#cb38-275" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$r_{Y2}$ - the correlation between the outcome ($Y$) and the second predictor  ($X_2$). </span>
<span id="cb38-276"><a href="#cb38-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-277"><a href="#cb38-277" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$r_{12}$ - the correlation between the two predictors.</span>
<span id="cb38-278"><a href="#cb38-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-279"><a href="#cb38-279" aria-hidden="true" tabindex="-1"></a>**If you have questions about how scatter plots and correlations can be presented in matrix format, please write them down now and share them class.**</span>
<span id="cb38-280"><a href="#cb38-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-281"><a href="#cb38-281" aria-hidden="true" tabindex="-1"></a><span class="fu">## The two-predictor model {#sec-model-3}</span></span>
<span id="cb38-282"><a href="#cb38-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-283"><a href="#cb38-283" aria-hidden="true" tabindex="-1"></a>In the ECLS example, we can think of Kindergarteners' Math Achievement as the outcome variable, with SES and Approaches to Learning as potential predictors / explanatory variables. The multiple regression model for this example can be written as </span>
<span id="cb38-284"><a href="#cb38-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-285"><a href="#cb38-285" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb38-286"><a href="#cb38-286" aria-hidden="true" tabindex="-1"></a>\widehat Y = b_0 + b_1 X_1 + b_2 X_2 </span>
<span id="cb38-287"><a href="#cb38-287" aria-hidden="true" tabindex="-1"></a>$${#eq-yhat-3}</span>
<span id="cb38-288"><a href="#cb38-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-289"><a href="#cb38-289" aria-hidden="true" tabindex="-1"></a>where </span>
<span id="cb38-290"><a href="#cb38-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-291"><a href="#cb38-291" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\widehat Y$ denotes the predicted Math Achievement </span>
<span id="cb38-292"><a href="#cb38-292" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$X_1 = \;$ SES and $X_2 = \;$ ATL (it doesn't matter which predictor we denote as $1$ or $2$)</span>
<span id="cb38-293"><a href="#cb38-293" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$b_1$ and $b_2$ are the regression slopes</span>
<span id="cb38-294"><a href="#cb38-294" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The intercept is denoted by $b_0$ (rather than $a$).</span>
<span id="cb38-295"><a href="#cb38-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-296"><a href="#cb38-296" aria-hidden="true" tabindex="-1"></a>Just like simple regression, the residual for @eq-yhat-3 is defined as $e = Y - \widehat Y$ and the model can be equivalently written as $Y = \widehat Y + e$. Also, remember that you can write out the model using the variable names in place of $Y$ and $X$ if that helps keep track of all the notation. For example,  </span>
<span id="cb38-297"><a href="#cb38-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-298"><a href="#cb38-298" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb38-299"><a href="#cb38-299" aria-hidden="true" tabindex="-1"></a>MATH = b_0 + b_1 SES + b_2 ATL + e. </span>
<span id="cb38-300"><a href="#cb38-300" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb38-301"><a href="#cb38-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-302"><a href="#cb38-302" aria-hidden="true" tabindex="-1"></a>As mentioned in @sec-chap-2, feel free to use whatever notation works best for you. </span>
<span id="cb38-303"><a href="#cb38-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-304"><a href="#cb38-304" aria-hidden="true" tabindex="-1"></a>You might be wondering, what is the added value of multiple regression compared to the correlation co-efficients reported in the previous section? Well, correlations only consider two-variables-at-a-time. Multiple regression let's us further consider how the predictors work together to explain variation in the outcome, and to consider the relationship between each predictor and the outcome while holding the other predictors constant. In the context of our example, multiple regression let's us address the following questions:</span>
<span id="cb38-305"><a href="#cb38-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-306"><a href="#cb38-306" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>How much of variation in Math Achievement do both predictors explain together? </span>
<span id="cb38-307"><a href="#cb38-307" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>What is the relationship between Math Achievement and ATL if we hold SES constant? </span>
<span id="cb38-308"><a href="#cb38-308" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Similarly, what is the relationship between Math Achievement and SES if we hold ATL constant?</span>
<span id="cb38-309"><a href="#cb38-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-310"><a href="#cb38-310" aria-hidden="true" tabindex="-1"></a>Notice that this is different from simple regression -- simple regression was just a repackaging of correlation, but multiple regression is something new. </span>
<span id="cb38-311"><a href="#cb38-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-312"><a href="#cb38-312" aria-hidden="true" tabindex="-1"></a><span class="fu">## OLS with two predictors {#sec-ols-3}</span></span>
<span id="cb38-313"><a href="#cb38-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-314"><a href="#cb38-314" aria-hidden="true" tabindex="-1"></a>We can estimate the parameters of the two-predictor regression model in @eq-yhat-3 model using same approach as for simple regression. We do this by choosing the values of $b_0, b_1, b_2$ that minimize </span>
<span id="cb38-315"><a href="#cb38-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-316"><a href="#cb38-316" aria-hidden="true" tabindex="-1"></a>$$SS_\text{res} = \sum_i e_i^2.$$</span>
<span id="cb38-317"><a href="#cb38-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-318"><a href="#cb38-318" aria-hidden="true" tabindex="-1"></a>Solving the minimization problem (setting derivatives to zero) leads to the following equations for the regression coefficients. Remember, the subscript $1$ denotes the first predictor and the subscript $2$ denotes the second predictor -- see @sec-ecls-3 for notation. Also note that $s$ represents standard deviations.</span>
<span id="cb38-319"><a href="#cb38-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-320"><a href="#cb38-320" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb38-321"><a href="#cb38-321" aria-hidden="true" tabindex="-1"></a>b_0 &amp; = \bar Y - b_1 \bar X_1 - b_2 \bar X_2 <span class="sc">\\</span> <span class="sc">\\</span> </span>
<span id="cb38-322"><a href="#cb38-322" aria-hidden="true" tabindex="-1"></a>b_1 &amp; = \frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}} \frac{s_Y}{s_1} <span class="sc">\\</span> <span class="sc">\\</span></span>
<span id="cb38-323"><a href="#cb38-323" aria-hidden="true" tabindex="-1"></a>b_2 &amp; = \frac{r_{Y2} - r_{Y1} r_{12}}{1 - r^2_{12}} \frac{s_Y}{s_2}</span>
<span id="cb38-324"><a href="#cb38-324" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb38-325"><a href="#cb38-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-326"><a href="#cb38-326" aria-hidden="true" tabindex="-1"></a>As promised, these equations are more complicated than for simple regression :) The next section addresses the interpretation of the regression coefficients. </span>
<span id="cb38-327"><a href="#cb38-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-328"><a href="#cb38-328" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interpreting the coefficients {#sec-interpretation-3}</span></span>
<span id="cb38-329"><a href="#cb38-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-330"><a href="#cb38-330" aria-hidden="true" tabindex="-1"></a>An important part of using multiple regression is getting the interpretation of the regression coefficients correct. The basic interpretation is that the slope for SES represents how much predicted Math Achievement changes for a one unit increase of SES, *while holding ATL constant.* (The same interpretation holds when switching the predictors.) The important difference with simple regression is the "holding the other predictor constant" part, so let's dig into it. </span>
<span id="cb38-331"><a href="#cb38-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-332"><a href="#cb38-332" aria-hidden="true" tabindex="-1"></a><span class="fu">### "Holding the other predictor constant"</span></span>
<span id="cb38-333"><a href="#cb38-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-334"><a href="#cb38-334" aria-hidden="true" tabindex="-1"></a>Let's start with the regression model for the predicted values: </span>
<span id="cb38-335"><a href="#cb38-335" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb38-336"><a href="#cb38-336" aria-hidden="true" tabindex="-1"></a>$$ \widehat {MATH} = b_0 + b_1 SES + b_2 ATL$$ </span>
<span id="cb38-337"><a href="#cb38-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-338"><a href="#cb38-338" aria-hidden="true" tabindex="-1"></a>If we increase $SES$ by one unit and hold $ATL$  constant, we get new predicted value (denoted with an asterisk):</span>
<span id="cb38-339"><a href="#cb38-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-340"><a href="#cb38-340" aria-hidden="true" tabindex="-1"></a>$$\widehat {MATH^*} = b_0 + b_1 (SES + 1) + b_2 ATL$$</span>
<span id="cb38-341"><a href="#cb38-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-342"><a href="#cb38-342" aria-hidden="true" tabindex="-1"></a>The difference between $\widehat{MATH^*}$ and $\widehat{MATH}$ is how much the predicted value changes for a one unit increase in SES, while holding ATL constant: </span>
<span id="cb38-343"><a href="#cb38-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-344"><a href="#cb38-344" aria-hidden="true" tabindex="-1"></a>$$\widehat{MATH^*} - \widehat{MATH}  = b_1$$</span>
<span id="cb38-345"><a href="#cb38-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-346"><a href="#cb38-346" aria-hidden="true" tabindex="-1"></a>This why we interpret the regression coefficients in multiple regression differently than simple regression. In multiple regression, we interpret the "effect" of each predictor while holding the other predictor(s) constant. This is sometimes referred to as "ceteris paribus," which Latin for "with other conditions remaining the same." So, we could say that multiple regression is a statistical way of making ceteris paribus arguments. </span>
<span id="cb38-347"><a href="#cb38-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-348"><a href="#cb38-348" aria-hidden="true" tabindex="-1"></a>Also note that we can see in the equations for $\widehat {MATH}$ that the interpretation of the regression intercept is basically the same as for simple regression: it is the value of $\widehat Y$ when $X_1 = 0$ and $X_2 = 0$ (i.e., still not very interesting). </span>
<span id="cb38-349"><a href="#cb38-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-350"><a href="#cb38-350" aria-hidden="true" tabindex="-1"></a><span class="fu">### "Controlling for the other predictor"</span></span>
<span id="cb38-351"><a href="#cb38-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-352"><a href="#cb38-352" aria-hidden="true" tabindex="-1"></a>Another interpretation of the regression coefficients is in terms of the equations in for $b_1$ and $b_2$ presented in @sec-ols-3. For example, the equation for $b_1$ is</span>
<span id="cb38-353"><a href="#cb38-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-354"><a href="#cb38-354" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb38-355"><a href="#cb38-355" aria-hidden="true" tabindex="-1"></a>b_1 = \frac{r_{Y1} - r_{Y2} \color{red}{r_{12}}} {1 - \color{red}{r^2_{12}}} \frac{s_1}{s_Y}. </span>
<span id="cb38-356"><a href="#cb38-356" aria-hidden="true" tabindex="-1"></a>\end{equation} </span>
<span id="cb38-357"><a href="#cb38-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-358"><a href="#cb38-358" aria-hidden="true" tabindex="-1"></a>This is the same equation as from @sec-ols-3, but the correlation between the predictors is shown in red. Note that if the predictors are uncorrelated (i.e., $\color{red}{r^2_{12}} = 0$) then</span>
<span id="cb38-359"><a href="#cb38-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-360"><a href="#cb38-360" aria-hidden="true" tabindex="-1"></a>$$b_1 = r_{Y1} \frac{s_1}{s_Y},$$</span>
<span id="cb38-361"><a href="#cb38-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-362"><a href="#cb38-362" aria-hidden="true" tabindex="-1"></a>which is just the regression coefficient from simple regression (@sec-ols-2). </span>
<span id="cb38-363"><a href="#cb38-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-364"><a href="#cb38-364" aria-hidden="true" tabindex="-1"></a>In general, the formulas for the regression coefficients in the two-predictor model are more complicated because they "control for" or "account for" the relationship between the predictors. In simple regression, we only had one predictor, so we didn't need to account for how the predictors were related to each other. </span>
<span id="cb38-365"><a href="#cb38-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-366"><a href="#cb38-366" aria-hidden="true" tabindex="-1"></a>The equations for the regression coefficients show that, if the predictors are uncorrelated, then doing a multiple regression is just the same thing as doing simple regression multiple times. However, most of the time our predictors will be correlated, which is why regression coefficients in multiple regression ends up being more complicated than the coefficient in simple regression -- in multiple regression, we "control for" the relationship between the predictors. </span>
<span id="cb38-367"><a href="#cb38-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-368"><a href="#cb38-368" aria-hidden="true" tabindex="-1"></a><span class="fu">### The ECLS example</span></span>
<span id="cb38-369"><a href="#cb38-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-370"><a href="#cb38-370" aria-hidden="true" tabindex="-1"></a>Below, the R output from the ECLS example is reported. **Please provide a written explanation of the regression coefficients for SES and ATL. If you have questions about how to interpret the coefficients, please also note them now and be prepared to share them in class. Note that this question is not asking about whether the coefficients are significant -- it is asking what the numbers in the first column of the Coefficients table mean.**</span>
<span id="cb38-371"><a href="#cb38-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-372"><a href="#cb38-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-375"><a href="#cb38-375" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-376"><a href="#cb38-376" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the regression model and print output</span></span>
<span id="cb38-377"><a href="#cb38-377" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl <span class="sc">+</span> t1learn)</span>
<span id="cb38-378"><a href="#cb38-378" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb38-379"><a href="#cb38-379" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-380"><a href="#cb38-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-381"><a href="#cb38-381" aria-hidden="true" tabindex="-1"></a><span class="fu">### Standardized coefficients {#sec-beta-3}</span></span>
<span id="cb38-382"><a href="#cb38-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-383"><a href="#cb38-383" aria-hidden="true" tabindex="-1"></a>One question that arises in the interpretation of the example is the relative contribution of the two predictors to Kindergartener's Math Achievement. In particular, the regression coefficient for ATL is 10 times larger than the regression coefficient for SES -- does this mean that ATL is 10 times more important than SES? </span>
<span id="cb38-384"><a href="#cb38-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-385"><a href="#cb38-385" aria-hidden="true" tabindex="-1"></a>The short answer is, "no." ATL is on a scale of 1-4 whereas SES ranges from 30-72. In order to make the regression coefficients more comparable, we can standardize the $X$ variables so that they have the same variance. </span>
<span id="cb38-386"><a href="#cb38-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-387"><a href="#cb38-387" aria-hidden="true" tabindex="-1"></a>Many researchers go a step further and standardize all of the variables $Y, X_1, X_2$ to be z-scores with M = 0 and SD = 1. The resulting regression coefficients are often called $\beta$-coefficients or $\beta$-weights.</span>
<span id="cb38-388"><a href="#cb38-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-389"><a href="#cb38-389" aria-hidden="true" tabindex="-1"></a>The $\beta$-weights are related to the regular regression coefficients from @sec-ols-3:</span>
<span id="cb38-390"><a href="#cb38-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-391"><a href="#cb38-391" aria-hidden="true" tabindex="-1"></a>$$\beta_1 = b_1 \frac{s_1}{s_Y} = \frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}}$$ </span>
<span id="cb38-392"><a href="#cb38-392" aria-hidden="true" tabindex="-1"></a>A similar expression holds for $\beta_1$</span>
<span id="cb38-393"><a href="#cb38-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-394"><a href="#cb38-394" aria-hidden="true" tabindex="-1"></a>Note that the <span class="in">`lm`</span> function in R does not provide an option to report standardized output. So, if you want to get the $\beta$-coefficients in R, it's easiest to just standardized the variables first and then do the regression with the standardized variables. </span>
<span id="cb38-395"><a href="#cb38-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-396"><a href="#cb38-396" aria-hidden="true" tabindex="-1"></a>Regardless of how you compute them, the interpretation of the $\beta$-coefficients is in terms of the standard deviation units of both the $Y$ variable and the $X$ variable -- e.g., increasing $X_1$ by one standard deviation changes $\hat Y$ by $\beta_1$ standard deviations (holding the other predictors constant). </span>
<span id="cb38-397"><a href="#cb38-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-398"><a href="#cb38-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-401"><a href="#cb38-401" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-402"><a href="#cb38-402" aria-hidden="true" tabindex="-1"></a><span class="co"># Unlike other software, R doesn't have a convenience functions for beta coefficients. </span></span>
<span id="cb38-403"><a href="#cb38-403" aria-hidden="true" tabindex="-1"></a>z_example_data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">scale</span>(example_data))</span>
<span id="cb38-404"><a href="#cb38-404" aria-hidden="true" tabindex="-1"></a>z_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(Math <span class="sc">~</span> SES  <span class="sc">+</span> ATL, <span class="at">data =</span> z_example_data)</span>
<span id="cb38-405"><a href="#cb38-405" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(z_mod)</span>
<span id="cb38-406"><a href="#cb38-406" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-407"><a href="#cb38-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-408"><a href="#cb38-408" aria-hidden="true" tabindex="-1"></a>We should be careful when using beta-coefficients to "ease" the comparison of regression predictors. In the context of our example, we might wonder whether the overall cost of raising a child's Approaches to Learning by 1 SD is comparable to the overall cost of raising their family's SES by 1 SD. In general, putting variables on the same scale is only a superficial way of making comparisons among their regression coefficients. </span>
<span id="cb38-409"><a href="#cb38-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-410"><a href="#cb38-410" aria-hidden="true" tabindex="-1"></a>**Please write down an interpretation of the of beta (standardized) regression coefficients in the above output. Your interpretation should include reference to the fact that the variables have been standardized. Based on this analysis, do you think predictor more important than the other? Please be prepared to share your interpretations / questions in class!** </span>
<span id="cb38-411"><a href="#cb38-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-412"><a href="#cb38-412" aria-hidden="true" tabindex="-1"></a><span class="fu">## (Multiple) R-squared {#sec-rsquared-3}</span></span>
<span id="cb38-413"><a href="#cb38-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-414"><a href="#cb38-414" aria-hidden="true" tabindex="-1"></a>R-squared in multiple regression has the same general formula and interpretation as in simple regression. The formula is</span>
<span id="cb38-415"><a href="#cb38-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-416"><a href="#cb38-416" aria-hidden="true" tabindex="-1"></a>$$R^2 = \frac{SS_{\text{reg}}} {SS_{\text{total}}} $$</span>
<span id="cb38-417"><a href="#cb38-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-418"><a href="#cb38-418" aria-hidden="true" tabindex="-1"></a>and it is interpreted as the proportion of variance in the outcome variable that is "associated with" or "explained by" its linear relationship with the predictor variables.</span>
<span id="cb38-419"><a href="#cb38-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-420"><a href="#cb38-420" aria-hidden="true" tabindex="-1"></a>As discussed below, we can also say a bit more about R-squared in multiple regression. </span>
<span id="cb38-421"><a href="#cb38-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-422"><a href="#cb38-422" aria-hidden="true" tabindex="-1"></a><span class="fu">### Relation with simple regression</span></span>
<span id="cb38-423"><a href="#cb38-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-424"><a href="#cb38-424" aria-hidden="true" tabindex="-1"></a>Like the regression coefficients in @sec-ols-3, the equation for R-squared can also be written in terms of the correlations among the three variables: </span>
<span id="cb38-425"><a href="#cb38-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-426"><a href="#cb38-426" aria-hidden="true" tabindex="-1"></a>$$R^2 = \frac{r^2_{Y1} + r^2_{Y2} - 2 r_{12}r_{Y1}r_{Y2}}{1 - r^2_{12}}.$$</span>
<span id="cb38-427"><a href="#cb38-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-428"><a href="#cb38-428" aria-hidden="true" tabindex="-1"></a>If the correlation between the predictors is zero, then this equation simplifies to</span>
<span id="cb38-429"><a href="#cb38-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-430"><a href="#cb38-430" aria-hidden="true" tabindex="-1"></a>$$R^2 = r^2_{Y1} + r^2_{Y2}.$$</span>
<span id="cb38-431"><a href="#cb38-431" aria-hidden="true" tabindex="-1"></a>In words: When the predictors are uncorrelated, their total contribution to variance explained is just the sum of their individual contributions. </span>
<span id="cb38-432"><a href="#cb38-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-433"><a href="#cb38-433" aria-hidden="true" tabindex="-1"></a>However, when the predictors are correlated, either positively or negatively, it can be show that </span>
<span id="cb38-434"><a href="#cb38-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-435"><a href="#cb38-435" aria-hidden="true" tabindex="-1"></a>$$ R^2 &lt; r^2_{Y1} + r^2_{Y2}.$$</span>
<span id="cb38-436"><a href="#cb38-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-437"><a href="#cb38-437" aria-hidden="true" tabindex="-1"></a>In other words: correlated predictors jointly explain less variance than if we added the contributions of each predictor considered separately. Intuitively, this is because correlated predictors share some variation with each other. If we considered the predictors one at a time, we double-count their shared variation. </span>
<span id="cb38-438"><a href="#cb38-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-439"><a href="#cb38-439" aria-hidden="true" tabindex="-1"></a>The conceptual relationships between R-squared for one versus two predictors can be represented in terms of the following Venn diagram. </span>
<span id="cb38-440"><a href="#cb38-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-441"><a href="#cb38-441" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-venn-diagram, echo = F, fig.cap = "Shared Variance Among $Y$, $X_1$, and $X_2$.", fig.align = 'center'}</span></span>
<span id="cb38-442"><a href="#cb38-442" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/venn_diagram.png"</span>)</span>
<span id="cb38-443"><a href="#cb38-443" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-444"><a href="#cb38-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-445"><a href="#cb38-445" aria-hidden="true" tabindex="-1"></a>The circles represent the variance of each variable and the overlap between circles represents their shared variance. When we conduct a multiple regression, the variance in the outcome explained by both predictors is equal to the sum of the areas A + B + C. If we intead conduct two simple regressions and then add up the R-squared values, we would double count the area labelled "B". </span>
<span id="cb38-446"><a href="#cb38-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-447"><a href="#cb38-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-448"><a href="#cb38-448" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--</span></span>
<span id="cb38-449"><a href="#cb38-449" aria-hidden="true" tabindex="-1"></a><span class="co">\begin{align}</span></span>
<span id="cb38-450"><a href="#cb38-450" aria-hidden="true" tabindex="-1"></a><span class="co">R^2_{Y.12} &amp; = \frac{A + B + C}{A + B + C + D} \\ \\</span></span>
<span id="cb38-451"><a href="#cb38-451" aria-hidden="true" tabindex="-1"></a><span class="co">R^2_{Y.1} &amp; = \frac{A + B}{A + B + C + D} \\ \\</span></span>
<span id="cb38-452"><a href="#cb38-452" aria-hidden="true" tabindex="-1"></a><span class="co">R^2_{Y.2} &amp; = \frac{B + C}{A + B + C + D} \\ \\</span></span>
<span id="cb38-453"><a href="#cb38-453" aria-hidden="true" tabindex="-1"></a><span class="co">\end{align}</span></span>
<span id="cb38-454"><a href="#cb38-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-455"><a href="#cb38-455" aria-hidden="true" tabindex="-1"></a><span class="co">These equations imply </span></span>
<span id="cb38-456"><a href="#cb38-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-457"><a href="#cb38-457" aria-hidden="true" tabindex="-1"></a><span class="co">\[ R^2_{Y.1} + R^2_{Y.2} = \frac{A + 2B + C}{A + B + C + D} \geq R^2_{Y.12} \] </span></span>
<span id="cb38-458"><a href="#cb38-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-459"><a href="#cb38-459" aria-hidden="true" tabindex="-1"></a><span class="co">This explains the relationship among the R-squared values in Table \@ref(tab:compare). The reason that the sum of the R-squared values in the simple models is greater than the R-squared value in the two-predictor model is because the sum double counts the shared variation among the predictors (area B in the diagram). </span></span>
<span id="cb38-460"><a href="#cb38-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-461"><a href="#cb38-461" aria-hidden="true" tabindex="-1"></a><span class="al">###</span><span class="co"> Relation with $\beta$-weights</span></span>
<span id="cb38-462"><a href="#cb38-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-463"><a href="#cb38-463" aria-hidden="true" tabindex="-1"></a><span class="co">The squared standardized coefficients in Section \@ref(standardized-coefficients) are closely related to a quantity called the squared semi-partial correlation: </span></span>
<span id="cb38-464"><a href="#cb38-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-465"><a href="#cb38-465" aria-hidden="true" tabindex="-1"></a><span class="co">\[ r^2_{Y1 \mid 2} = \beta_1^2 * (1 - r^2_{12}) \]</span></span>
<span id="cb38-466"><a href="#cb38-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-467"><a href="#cb38-467" aria-hidden="true" tabindex="-1"></a><span class="co">In this notation, the subscript $Y1 \mid 2$ denotes the correlation between $Y$ and $X_1$ after removing the (linear) association between $X_1$ and $X_2$. Similarly for $Y2 \mid 1$. </span></span>
<span id="cb38-468"><a href="#cb38-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-469"><a href="#cb38-469" aria-hidden="true" tabindex="-1"></a><span class="co">The squared semi-partial correspond to the areas in the Venn diagrams as follows</span></span>
<span id="cb38-470"><a href="#cb38-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-471"><a href="#cb38-471" aria-hidden="true" tabindex="-1"></a><span class="co">\begin{align}</span></span>
<span id="cb38-472"><a href="#cb38-472" aria-hidden="true" tabindex="-1"></a><span class="co">r^2_{Y1 \mid 2} &amp; = \frac{A}{A + B + C + D} \\ \\</span></span>
<span id="cb38-473"><a href="#cb38-473" aria-hidden="true" tabindex="-1"></a><span class="co">r^2_{Y2 \mid 1} &amp; = \frac{B}{A + B + C + D} </span></span>
<span id="cb38-474"><a href="#cb38-474" aria-hidden="true" tabindex="-1"></a><span class="co">\end{align}</span></span>
<span id="cb38-475"><a href="#cb38-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-476"><a href="#cb38-476" aria-hidden="true" tabindex="-1"></a><span class="co">Using this relationship, we can see that squared semi partials (and hence the \beta-weights) </span></span>
<span id="cb38-477"><a href="#cb38-477" aria-hidden="true" tabindex="-1"></a><span class="co">\end{align} \]</span></span>
<span id="cb38-478"><a href="#cb38-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-479"><a href="#cb38-479" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb38-480"><a href="#cb38-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-481"><a href="#cb38-481" aria-hidden="true" tabindex="-1"></a><span class="fu">### Adjusted R-squared</span></span>
<span id="cb38-482"><a href="#cb38-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-483"><a href="#cb38-483" aria-hidden="true" tabindex="-1"></a>The sample R-squared is an upwardly biased estimate of the population R-squared.</span>
<span id="cb38-484"><a href="#cb38-484" aria-hidden="true" tabindex="-1"></a>This bias is illustrated in the figure below. In the example, we are considering simple regression (one predictor), and we assume that the population correlation between the predictor and the outcome is zero (i.e., $\rho = 0$). </span>
<span id="cb38-485"><a href="#cb38-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-486"><a href="#cb38-486" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-adjusted, echo = F, fig.cap = "Sampling Distribution of $r$ and $r^2$ when $\rho = 0$.", fig.align = 'center'}</span></span>
<span id="cb38-487"><a href="#cb38-487" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"files/images/adjusted_rsquared.png"</span>)</span>
<span id="cb38-488"><a href="#cb38-488" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-489"><a href="#cb38-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-490"><a href="#cb38-490" aria-hidden="true" tabindex="-1"></a>In the left panel, we can see that "un-squared" correlation, $r$, has a sampling distribution that is centered at the true value $\rho = 0$. This means that $r$ is an unbiased estimate of $\rho$. </span>
<span id="cb38-491"><a href="#cb38-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-492"><a href="#cb38-492" aria-hidden="true" tabindex="-1"></a>But in the right panel, we can see that the sampling distribution of the squared correlation,  $r^2$, must have a mean greater than zero. This is because all of the sample-to-sample deviations in left panel are now positive (because they have been squared). Since the average value of $r^2$ is greater than the population value ($\rho = 0$), $r^2$ is an upwardly biased estimate of $\rho^2$.  (i.e., it's too large). </span>
<span id="cb38-493"><a href="#cb38-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-494"><a href="#cb38-494" aria-hidden="true" tabindex="-1"></a>The adjusted R-squared corrects this bias. The formula for the adjustment is: </span>
<span id="cb38-495"><a href="#cb38-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-496"><a href="#cb38-496" aria-hidden="true" tabindex="-1"></a>$$\tilde R^2 = 1 - (1 - R^2) \frac{N-1}{N - K - 1}$$</span>
<span id="cb38-497"><a href="#cb38-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-498"><a href="#cb38-498" aria-hidden="true" tabindex="-1"></a>where $K$ is the number of predictors in the model. </span>
<span id="cb38-499"><a href="#cb38-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-500"><a href="#cb38-500" aria-hidden="true" tabindex="-1"></a>The formula contains two main terms, the proportion of residual variance, $(1 - R^2)$, and the adjustment factor (the ratio of $N-1$ to $N-K-1$). We can understand how the adjustment works by considering these two terms.  </span>
<span id="cb38-501"><a href="#cb38-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-502"><a href="#cb38-502" aria-hidden="true" tabindex="-1"></a>First, it can be seen that the adjustment factor is larger when the number of predictors, $K$, is large relative to the sample size, $N$. So, roughly speaking, the adjustment will be more severe when there are a lot of predictors in the model relative to the sample size. </span>
<span id="cb38-503"><a href="#cb38-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-504"><a href="#cb38-504" aria-hidden="true" tabindex="-1"></a>Second, it can also be seen that the adjustment proportional to $(1 - R^2)$. This means that the adjustment is more severe if the model explains less variance in the outcome. For example, if $R^2 = .9$ and the adjustment factor is $1.1$, then adjusted $R^2 = .89$. In this case the adjustment is a decrease of 1% of variance explained. But if we start off explaining less variance, say $R^2 = .1$ and use the same adjustment factor, then adjusted $R^2 = .01$. Now the adjustment is a decrease of 9% variance explained. </span>
<span id="cb38-505"><a href="#cb38-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-506"><a href="#cb38-506" aria-hidden="true" tabindex="-1"></a>In summary, the overall interpretation of adusted R-squared is as follows: the adjustment will be larger when there are lots of predictors in the model but they don't explain much variance in the outcome. This situation is sometimes called "overfitting" the data, so we can think of adjusted R-squared as a correction for overfitting.  </span>
<span id="cb38-507"><a href="#cb38-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-508"><a href="#cb38-508" aria-hidden="true" tabindex="-1"></a>There is no established standard for when you should reported R-squared or adjusted R-squared. I recommend that you report both whenever they would would lead to different substantive conclusions. We can discuss this more in class. </span>
<span id="cb38-509"><a href="#cb38-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-510"><a href="#cb38-510" aria-hidden="true" tabindex="-1"></a><span class="fu">### The ECLS example</span></span>
<span id="cb38-511"><a href="#cb38-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-512"><a href="#cb38-512" aria-hidden="true" tabindex="-1"></a>As shown in @sec-beta-3, the R-squared for the ECLS example is equal to .2726  and the adjusted R-squared is equal to .2668. **Please write down your interpretation of these value and be prepared to share your answer in class.** </span>
<span id="cb38-513"><a href="#cb38-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-514"><a href="#cb38-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-515"><a href="#cb38-515" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference{#sec-inference-3}</span></span>
<span id="cb38-516"><a href="#cb38-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-517"><a href="#cb38-517" aria-hidden="true" tabindex="-1"></a>There isn't really any thing new that about inference with multiple regression, except the formula for the standard errors (see @fox2016 chap.6). We present the formulas for an abribrary number of predictors, denoted $k = 1, \dots K$. </span>
<span id="cb38-518"><a href="#cb38-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-519"><a href="#cb38-519" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference for the coefficients {#sec-inference-for-coeffecients-3}</span></span>
<span id="cb38-520"><a href="#cb38-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-521"><a href="#cb38-521" aria-hidden="true" tabindex="-1"></a>In multiple regression</span>
<span id="cb38-522"><a href="#cb38-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-523"><a href="#cb38-523" aria-hidden="true" tabindex="-1"></a>$$SE({\widehat b_k}) = \frac{\text{SD}(Y)}{\text{SD}(X)} \sqrt{\frac{1 - R^2}{N - K - 1}} \times \sqrt{\frac{1}{1 - R_k^2}} </span>
<span id="cb38-524"><a href="#cb38-524" aria-hidden="true" tabindex="-1"></a>$${#eq-se-3}</span>
<span id="cb38-525"><a href="#cb38-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-526"><a href="#cb38-526" aria-hidden="true" tabindex="-1"></a>In this formula, $K$ denotes the number of predictors and $R^2_k$ is the R-squared that results from regressing predictor $k$ on the other $K-1$ predictors (without the $Y$ variable). </span>
<span id="cb38-527"><a href="#cb38-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-528"><a href="#cb38-528" aria-hidden="true" tabindex="-1"></a>Notice that the first part of the standard error (before the "$\times$") is the same as simple regression (see @sec-inference-2). The last part, which includes $R^2_k$, is different and we talk about it more below.</span>
<span id="cb38-529"><a href="#cb38-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-530"><a href="#cb38-530" aria-hidden="true" tabindex="-1"></a>The standard errors can be used to construct t-tests and confidence intervals using the same approach as for simple regression (see @sec-inference-2). The degrees of freedom for the t-distribution is $N - K -1$ (this applies to simple regression too, where $K = 1$). </span>
<span id="cb38-531"><a href="#cb38-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-532"><a href="#cb38-532" aria-hidden="true" tabindex="-1"></a><span class="fu">### Precision of $\hat b$</span></span>
<span id="cb38-533"><a href="#cb38-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-534"><a href="#cb38-534" aria-hidden="true" tabindex="-1"></a>We can use @eq-se-3 to understand the factors that influence the size of the standard errors of the regression coefficients. Recall that standard errors describe the sample-to-sample variability of a statistic. If there is a lot sample-to-sample variability, the statistic is said to be imprecise.  @eq-se-3 shows us what factors make $\hat b$ more or less precise. </span>
<span id="cb38-535"><a href="#cb38-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-536"><a href="#cb38-536" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The standard errors *decrease* with </span>
<span id="cb38-537"><a href="#cb38-537" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The sample size, $N$ </span>
<span id="cb38-538"><a href="#cb38-538" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The proportion of variance in the outcome explained by the predictors, $R^2$</span>
<span id="cb38-539"><a href="#cb38-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-540"><a href="#cb38-540" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The standard errors *increase* with</span>
<span id="cb38-541"><a href="#cb38-541" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The number of predictors, $K$</span>
<span id="cb38-542"><a href="#cb38-542" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The proportion of variance in the predictor that is explained by the other predictors, $R^2_k$</span>
<span id="cb38-543"><a href="#cb38-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-544"><a href="#cb38-544" aria-hidden="true" tabindex="-1"></a>So, large sample sizes and a large proportion of variance explained lead to precise estimates of the regression coefficients.  On the other hand, including many predictors that are highly correlated with each other leads to less precision. In particular, the situation where $R^2_k$ approaches the value of $1$ is called *multicollinearity*. We will talk about multicollinearity in more detail in @sec-chap-5. </span>
<span id="cb38-545"><a href="#cb38-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-546"><a href="#cb38-546" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference for R-squared{#inference-for-rsquared-4}</span></span>
<span id="cb38-547"><a href="#cb38-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-548"><a href="#cb38-548" aria-hidden="true" tabindex="-1"></a>The R-squared statistic in multiple regression tells us how much variation in the outcome is explained by all of the predictors together. If the predictors do not explain any variation, then the population R-squared is equal to zero. </span>
<span id="cb38-549"><a href="#cb38-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-550"><a href="#cb38-550" aria-hidden="true" tabindex="-1"></a>Notice that $R^2 = 0$ implies $b_1 = b_2 = ... = b_k = 0$ (in the population). So, testing the significance of R-squared is equivalent to testing whether any of the regression parameters are non-zero. When we addressed ANOVA last semester, we called this the omnibus hypothesis. But in regression analysis, it is usually just referred to as a test of R-squared. </span>
<span id="cb38-551"><a href="#cb38-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-552"><a href="#cb38-552" aria-hidden="true" tabindex="-1"></a>The null hypothesis $H_0 : R^2 = 0$ can be tested using the statistic</span>
<span id="cb38-553"><a href="#cb38-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-554"><a href="#cb38-554" aria-hidden="true" tabindex="-1"></a>$$F = \frac{\widehat R^2 / K}{(1 - \widehat R^2) / (N - K - 1)},$$</span>
<span id="cb38-555"><a href="#cb38-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-556"><a href="#cb38-556" aria-hidden="true" tabindex="-1"></a>which has an F-distribution on $J$ and $N - k -1$ degrees of freedom when the null hypothesis is true. </span>
<span id="cb38-557"><a href="#cb38-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-558"><a href="#cb38-558" aria-hidden="true" tabindex="-1"></a><span class="fu">### The ECLS example</span></span>
<span id="cb38-559"><a href="#cb38-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-560"><a href="#cb38-560" aria-hidden="true" tabindex="-1"></a>The R output for the ECLS example is presented (again) below. **Please write down your conclusions about the statistical significance of the predictors and the  R-squared statistic, and be prepared to share your answer in class. Please also write down the factors that affect precision the precision of the regression coefficients. This would be a good opportunity to practice APA formatting.**</span>
<span id="cb38-561"><a href="#cb38-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-564"><a href="#cb38-564" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-565"><a href="#cb38-565" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb38-566"><a href="#cb38-566" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-567"><a href="#cb38-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-568"><a href="#cb38-568" aria-hidden="true" tabindex="-1"></a><span class="fu">## Workbook</span></span>
<span id="cb38-569"><a href="#cb38-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-570"><a href="#cb38-570" aria-hidden="true" tabindex="-1"></a>This section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in @sec-exercises-3. The lesson will **not** be a lecture that reviews all of the material in the chapter! So, if you haven't written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.  </span>
<span id="cb38-571"><a href="#cb38-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-572"><a href="#cb38-572" aria-hidden="true" tabindex="-1"></a>@sec-ecls-3</span>
<span id="cb38-573"><a href="#cb38-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-574"><a href="#cb38-574" aria-hidden="true" tabindex="-1"></a>If you have questions about the interpretation of a correlation matrix (below) or pairwise plots (see @sec-ecls-3), please write them down now and share them class.</span>
<span id="cb38-575"><a href="#cb38-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-576"><a href="#cb38-576" aria-hidden="true" tabindex="-1"></a>Numerical output for the ECLS example: </span>
<span id="cb38-577"><a href="#cb38-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-580"><a href="#cb38-580" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-581"><a href="#cb38-581" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(example_data)</span>
<span id="cb38-582"><a href="#cb38-582" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-583"><a href="#cb38-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-584"><a href="#cb38-584" aria-hidden="true" tabindex="-1"></a>Mathematical notation for formulas </span>
<span id="cb38-585"><a href="#cb38-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-586"><a href="#cb38-586" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-587"><a href="#cb38-587" aria-hidden="true" tabindex="-1"></a>\begin{array}{c}</span>
<span id="cb38-588"><a href="#cb38-588" aria-hidden="true" tabindex="-1"></a>\text{var } Y <span class="sc">\\</span> \text{var } X_1 <span class="sc">\\</span> \text{var } X_2</span>
<span id="cb38-589"><a href="#cb38-589" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb38-590"><a href="#cb38-590" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb38-591"><a href="#cb38-591" aria-hidden="true" tabindex="-1"></a>\left[ </span>
<span id="cb38-592"><a href="#cb38-592" aria-hidden="true" tabindex="-1"></a>\begin{array}{ccc}</span>
<span id="cb38-593"><a href="#cb38-593" aria-hidden="true" tabindex="-1"></a> 1       &amp; r_{Y1}  &amp; r_{Y2}  <span class="sc">\\</span></span>
<span id="cb38-594"><a href="#cb38-594" aria-hidden="true" tabindex="-1"></a> r_{1Y}  &amp; 1       &amp; r_{12}  <span class="sc">\\</span></span>
<span id="cb38-595"><a href="#cb38-595" aria-hidden="true" tabindex="-1"></a> r_{2Y}  &amp; r_{21}  &amp; 1</span>
<span id="cb38-596"><a href="#cb38-596" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb38-597"><a href="#cb38-597" aria-hidden="true" tabindex="-1"></a> \right]</span>
<span id="cb38-598"><a href="#cb38-598" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-599"><a href="#cb38-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-600"><a href="#cb38-600" aria-hidden="true" tabindex="-1"></a>@sec-interpretation-3</span>
<span id="cb38-601"><a href="#cb38-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-602"><a href="#cb38-602" aria-hidden="true" tabindex="-1"></a>Below, the R output from the ECLS example is reported. Please provide a written explanation of the regression coefficients for SES and ATL. If you have questions about how to interpret the coefficients, please also note them now and be prepared to share them in class. Note that this question is not asking about whether the coefficients are significant -- it is asking what the numbers in the first column of the Coefficients table mean. </span>
<span id="cb38-603"><a href="#cb38-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-604"><a href="#cb38-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-607"><a href="#cb38-607" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-608"><a href="#cb38-608" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the regression model and print output</span></span>
<span id="cb38-609"><a href="#cb38-609" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl <span class="sc">+</span> t1learn)</span>
<span id="cb38-610"><a href="#cb38-610" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb38-611"><a href="#cb38-611" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-612"><a href="#cb38-612" aria-hidden="true" tabindex="-1"></a>@sec-beta-3</span>
<span id="cb38-613"><a href="#cb38-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-614"><a href="#cb38-614" aria-hidden="true" tabindex="-1"></a>Please write down an interpretation of the of beta (standardized) regression coefficients in the above output. Your interpretation should include reference to the fact that the variables have been standardized. Based on this analysis, do you think predictor more important than the other? </span>
<span id="cb38-615"><a href="#cb38-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-618"><a href="#cb38-618" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-619"><a href="#cb38-619" aria-hidden="true" tabindex="-1"></a><span class="co"># Unlike other software, R doesn't have a convenience functions for beta coefficients. </span></span>
<span id="cb38-620"><a href="#cb38-620" aria-hidden="true" tabindex="-1"></a>z_example_data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">scale</span>(example_data))</span>
<span id="cb38-621"><a href="#cb38-621" aria-hidden="true" tabindex="-1"></a>z_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(Math <span class="sc">~</span> SES  <span class="sc">+</span> ATL, <span class="at">data =</span> z_example_data)</span>
<span id="cb38-622"><a href="#cb38-622" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(z_mod)</span>
<span id="cb38-623"><a href="#cb38-623" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-624"><a href="#cb38-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-625"><a href="#cb38-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-626"><a href="#cb38-626" aria-hidden="true" tabindex="-1"></a>@sec-rsquared-3</span>
<span id="cb38-627"><a href="#cb38-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-628"><a href="#cb38-628" aria-hidden="true" tabindex="-1"></a>The R-squared for the ECLS example is equal to .2726  and the adjusted R-squared is equal to .2668. Please write down your interpretation of these value and be prepared to share your answer in class.</span>
<span id="cb38-629"><a href="#cb38-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-630"><a href="#cb38-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-631"><a href="#cb38-631" aria-hidden="true" tabindex="-1"></a>@ref-inference-3</span>
<span id="cb38-632"><a href="#cb38-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-633"><a href="#cb38-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-634"><a href="#cb38-634" aria-hidden="true" tabindex="-1"></a>The R output for the ECLS example is presented (again) below. Please write down your conclusions about the statistical significance of the predictors and the  R-squared statistic, and be prepared to share your answer in class. This would be a good opportunity to practice APA formatting. Please also write down the factors that negatively affect the precision of the regression coefficients and address whether you think they are problematic for the example. </span>
<span id="cb38-635"><a href="#cb38-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-638"><a href="#cb38-638" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-639"><a href="#cb38-639" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb38-640"><a href="#cb38-640" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-641"><a href="#cb38-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-642"><a href="#cb38-642" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb38-643"><a href="#cb38-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-644"><a href="#cb38-644" aria-hidden="true" tabindex="-1"></a>These exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don't need to work on it before class (but you can if you want.) </span>
<span id="cb38-645"><a href="#cb38-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-646"><a href="#cb38-646" aria-hidden="true" tabindex="-1"></a>Before staring this section, you may find it useful to scroll to the top of the page, click on the "&lt;/&gt; Code" menu, and select "Show All Code."</span>
<span id="cb38-647"><a href="#cb38-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-648"><a href="#cb38-648" aria-hidden="true" tabindex="-1"></a><span class="fu">### The ECLS250 data  </span></span>
<span id="cb38-649"><a href="#cb38-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-650"><a href="#cb38-650" aria-hidden="true" tabindex="-1"></a>Let's start by getting our example data loaded into R. </span>
<span id="cb38-651"><a href="#cb38-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-652"><a href="#cb38-652" aria-hidden="true" tabindex="-1"></a>Make sure to download the file <span class="in">`ECLS250.RData`</span> from Canvas and then double click the file to open it</span>
<span id="cb38-653"><a href="#cb38-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-654"><a href="#cb38-654" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo = F}</span></span>
<span id="cb38-655"><a href="#cb38-655" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(ecls)</span>
<span id="cb38-656"><a href="#cb38-656" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-657"><a href="#cb38-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-660"><a href="#cb38-660" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-661"><a href="#cb38-661" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"ECLS250.RData"</span>) <span class="co"># load new example</span></span>
<span id="cb38-662"><a href="#cb38-662" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(ecls) <span class="co"># attach </span></span>
<span id="cb38-663"><a href="#cb38-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-664"><a href="#cb38-664" aria-hidden="true" tabindex="-1"></a><span class="co"># knitr and kable are just used to print nicely -- you can just use head(ecls[, 1:5]) </span></span>
<span id="cb38-665"><a href="#cb38-665" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">head</span>(ecls[, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]))</span>
<span id="cb38-666"><a href="#cb38-666" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-667"><a href="#cb38-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-668"><a href="#cb38-668" aria-hidden="true" tabindex="-1"></a>The naming conventions for these data are bit challenging. </span>
<span id="cb38-669"><a href="#cb38-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-670"><a href="#cb38-670" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Variable names begin with <span class="in">`c`</span>, <span class="in">`p`</span>, or <span class="in">`t`</span> depending on whether the respondent was the child, parent, or teacher. Variables that start with <span class="in">`wk`</span> were created by the ECLS using other data sources available in during the kindergarten year of the study. </span>
<span id="cb38-671"><a href="#cb38-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-672"><a href="#cb38-672" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The time points (1-4 denoting fall and spring of K and Gr 1) appear as the second character.</span>
<span id="cb38-673"><a href="#cb38-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-674"><a href="#cb38-674" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The rest of the name describes the variable.</span>
<span id="cb38-675"><a href="#cb38-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-676"><a href="#cb38-676" aria-hidden="true" tabindex="-1"></a>The variables we will use for this illustration are:</span>
<span id="cb38-677"><a href="#cb38-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-678"><a href="#cb38-678" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="in">`c1rmscal`</span>: Child's score on a math assessment, in first semester of Kindergarten . The scores can be interpreted as  number of correct responses out of a total of approximately 60 math exam questions.  </span>
<span id="cb38-679"><a href="#cb38-679" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb38-680"><a href="#cb38-680" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="in">`wksesl`</span>: An SES composite of household factors (e.g., parental education, household income) ranging from 30-72. </span>
<span id="cb38-681"><a href="#cb38-681" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb38-682"><a href="#cb38-682" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="in">`t1learn`</span>: Approaches to Learning Scale (ATLS), teacher reported in first semester of kindergarten. This scale measures behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), so that higher values represent more positive responses, and the scale is an unweighted average the six items. </span>
<span id="cb38-683"><a href="#cb38-683" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb38-684"><a href="#cb38-684" aria-hidden="true" tabindex="-1"></a>To get started lets produce the simple regression of Math with SES. This is another look at the relationship between Academic Achievement and SES that we discussed in Chapter @sec-chap-2). If you do not feel comfortable running this analysis or interpreting the output, take another look at @sec-exercises-2. </span>
<span id="cb38-685"><a href="#cb38-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-688"><a href="#cb38-688" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-689"><a href="#cb38-689" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> wksesl, </span>
<span id="cb38-690"><a href="#cb38-690" aria-hidden="true" tabindex="-1"></a>     <span class="at">y =</span> c1rmscal, </span>
<span id="cb38-691"><a href="#cb38-691" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"#4B9CD3"</span>)</span>
<span id="cb38-692"><a href="#cb38-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-693"><a href="#cb38-693" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl)</span>
<span id="cb38-694"><a href="#cb38-694" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(mod)</span>
<span id="cb38-695"><a href="#cb38-695" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span>
<span id="cb38-696"><a href="#cb38-696" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(wksesl, c1rmscal)</span>
<span id="cb38-697"><a href="#cb38-697" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-698"><a href="#cb38-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-699"><a href="#cb38-699" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multiple regression with `lm`</span></span>
<span id="cb38-700"><a href="#cb38-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-701"><a href="#cb38-701" aria-hidden="true" tabindex="-1"></a>First, let's tale a look at the "zero-order" relationship among the three variables. This type of descriptive, two-way analysis is a good way to get familiar with your data before getting into multiple regression. We can see that the variables are all moderately correlated and their relationships appear reasonably linear.</span>
<span id="cb38-702"><a href="#cb38-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-703"><a href="#cb38-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-706"><a href="#cb38-706" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-707"><a href="#cb38-707" aria-hidden="true" tabindex="-1"></a><span class="co"># Use cbind to create a data.frame with just the 3 variables we want to examine</span></span>
<span id="cb38-708"><a href="#cb38-708" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">cbind</span>(c1rmscal, wksesl, t1learn)</span>
<span id="cb38-709"><a href="#cb38-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-710"><a href="#cb38-710" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlations</span></span>
<span id="cb38-711"><a href="#cb38-711" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(data)</span>
<span id="cb38-712"><a href="#cb38-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-713"><a href="#cb38-713" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatterplots</span></span>
<span id="cb38-714"><a href="#cb38-714" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(data, <span class="at">col =</span> <span class="st">"#4B9CD3"</span>) </span>
<span id="cb38-715"><a href="#cb38-715" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-716"><a href="#cb38-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-717"><a href="#cb38-717" aria-hidden="true" tabindex="-1"></a>In terms of input, multiple regression with <span class="in">`lm`</span> is similar to simple regression. The only difference is the model formula. To include more predictors in a formula, just include them on the right hand side, separated by at <span class="in">`+`</span> sign. </span>
<span id="cb38-718"><a href="#cb38-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-719"><a href="#cb38-719" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>e.g, <span class="in">`Y ~ Χ1 + Χ2`</span> </span>
<span id="cb38-720"><a href="#cb38-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-721"><a href="#cb38-721" aria-hidden="true" tabindex="-1"></a>For our example, let's consider the regression of math achievement on SES and Approaches to Learning. We'll save our result as <span class="in">`mod1`</span> which is short for "model one".</span>
<span id="cb38-722"><a href="#cb38-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-725"><a href="#cb38-725" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-726"><a href="#cb38-726" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl <span class="sc">+</span> t1learn)</span>
<span id="cb38-727"><a href="#cb38-727" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb38-728"><a href="#cb38-728" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-729"><a href="#cb38-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-730"><a href="#cb38-730" aria-hidden="true" tabindex="-1"></a>We can see from the output that regression coefficient for <span class="in">`t1learn`</span> is about 3.5. This means that, as the predictor increases by a single unit, children's predicted math scores increase by 3.5 points (out of 60), after controlling for the SES. You should be able to provide a similar interpretation of the regression coefficient for <span class="in">`wksesl`</span>. Together, both predictors accounted for about 27% of the variation in students' math scores. In education, this would be considered a pretty strong relationship.  </span>
<span id="cb38-731"><a href="#cb38-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-732"><a href="#cb38-732" aria-hidden="true" tabindex="-1"></a>We will talk about the statistical tests later on. For now let's consider the relationship with simple regression. </span>
<span id="cb38-733"><a href="#cb38-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-734"><a href="#cb38-734" aria-hidden="true" tabindex="-1"></a><span class="fu">### Relations between simple and multiple regression</span></span>
<span id="cb38-735"><a href="#cb38-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-736"><a href="#cb38-736" aria-hidden="true" tabindex="-1"></a>First let's consider how the two simple regression compare to the multiple regression with two variables. Here is the relevant output:  </span>
<span id="cb38-737"><a href="#cb38-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-740"><a href="#cb38-740" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-741"><a href="#cb38-741" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the multiple regression output to the simple regressions</span></span>
<span id="cb38-742"><a href="#cb38-742" aria-hidden="true" tabindex="-1"></a>mod2a <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> wksesl)</span>
<span id="cb38-743"><a href="#cb38-743" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2a)</span>
<span id="cb38-744"><a href="#cb38-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-745"><a href="#cb38-745" aria-hidden="true" tabindex="-1"></a>mod2b <span class="ot">&lt;-</span> <span class="fu">lm</span>(c1rmscal <span class="sc">~</span> t1learn)</span>
<span id="cb38-746"><a href="#cb38-746" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2b)</span>
<span id="cb38-747"><a href="#cb38-747" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-748"><a href="#cb38-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-749"><a href="#cb38-749" aria-hidden="true" tabindex="-1"></a>The important things to note here are </span>
<span id="cb38-750"><a href="#cb38-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-751"><a href="#cb38-751" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The regression coefficients from the simple models ($b_{ses} = 4.38$ and $b_{t1learn} = 4.73$) are larger than the regression coefficients from the two-predictor model. Can you explain why? (Hint: see Section @sec-interpretation-3.</span>
<span id="cb38-752"><a href="#cb38-752" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb38-753"><a href="#cb38-753" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The R-squared values in the two simple models (.194 + .158 = .352) add up to more than the R-squared in the two-predictor model (.274). Again, take a moment to think about why before reading on. (Hint: see Section @sec-rsquared-3.) </span>
<span id="cb38-754"><a href="#cb38-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-755"><a href="#cb38-755" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference with 2 predictors</span></span>
<span id="cb38-756"><a href="#cb38-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-757"><a href="#cb38-757" aria-hidden="true" tabindex="-1"></a>Let's move on now to consider the statistical tests and confidence intervals provided with the <span class="in">`lm`</span> summary output. </span>
<span id="cb38-758"><a href="#cb38-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-759"><a href="#cb38-759" aria-hidden="true" tabindex="-1"></a>For regression with more than one predictor, both the t-tests and F-tests have a very similar construction and interpretation as with simple regression. The main differences are the formulas, not so much the interpretations of the procedures. Some differences: </span>
<span id="cb38-760"><a href="#cb38-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-761"><a href="#cb38-761" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The degrees of freedom for both tests now involve $K$, the number of predictors. </span>
<span id="cb38-762"><a href="#cb38-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-763"><a href="#cb38-763" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The standard error of the b-weight is more complicated, because it involves the inter-correlation among the predictors. </span>
<span id="cb38-764"><a href="#cb38-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-765"><a href="#cb38-765" aria-hidden="true" tabindex="-1"></a>We can see for <span class="in">`mod1`</span> that both b-weights are significant at the .05 level, and so is the R-square. As mentioned previously, it is not usual to interpret or report results on the regression intercept unless you have a special reason to do so (e.g., see the next chapter). </span>
<span id="cb38-766"><a href="#cb38-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-769"><a href="#cb38-769" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-770"><a href="#cb38-770" aria-hidden="true" tabindex="-1"></a><span class="co"># Revisting the output of mod1</span></span>
<span id="cb38-771"><a href="#cb38-771" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb38-772"><a href="#cb38-772" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-773"><a href="#cb38-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-774"><a href="#cb38-774" aria-hidden="true" tabindex="-1"></a><span class="fu">### APA reporting of results</span></span>
<span id="cb38-775"><a href="#cb38-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-776"><a href="#cb38-776" aria-hidden="true" tabindex="-1"></a>Here is how we might write out the results of our regression using APA format. The numbers are taken from the output below. When we have a regression model with many predictors, or are comparing among different models, it is more usual to put all the relevant statistics in a table rather than writing them out one by one. We will see how to do that later on in the course.For more info on APA format, see the APA publications manual: <span class="co">[</span><span class="ot">(https://www.apastyle.org/manual)</span><span class="co">](https://www.apastyle.org/manual)</span>. </span>
<span id="cb38-777"><a href="#cb38-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-778"><a href="#cb38-778" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The regression of Math Achievement on SES was positive and statistically significant at the .05 level ($b = 3.53, t(247) = 6.27, p &lt; .001$).</span>
<span id="cb38-779"><a href="#cb38-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-780"><a href="#cb38-780" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The regression of Math Achievement on Approaches to Learning was also positive and statistically significant at the .05 level ($b = 3.50, t(247) = 5.20, p &lt; .001$).</span>
<span id="cb38-781"><a href="#cb38-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-782"><a href="#cb38-782" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Together both predictors accounted for about 27\% of the variation in Math Achievement ($R^2$ = .274, adjusted $R^2$ = .268), which was also statistically significant at the .05 level ($F(2, 247) = 45.54, p &lt; .001$). </span>
<span id="cb38-783"><a href="#cb38-783" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb38-786"><a href="#cb38-786" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb38-787"><a href="#cb38-787" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb38-788"><a href="#cb38-788" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-789"><a href="#cb38-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-790"><a href="#cb38-790" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb38-791"><a href="#cb38-791" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo = F}</span></span>
<span id="cb38-792"><a href="#cb38-792" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(ecls)</span>
<span id="cb38-793"><a href="#cb38-793" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>