[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDUC 784",
    "section": "",
    "text": "Preface\nThese are the course notes for EDUC 784. Readings are assigned before class. Sections denoted with an asterisk (*) are optional.\nThe notes contain questions that are written in bold font. The questions are also collected in a section called “Workbook” that appears towards the end of each chapter. During class time, we will discuss the Workbook questions, your answers, any additional question you have, etc. It is really important for you to do the readings, and write down your responses to the questions, before class. You won’t get much out of the lessons if you haven’t done this preparation.\nSome chapters contain a section called “Exercises” that collects all of the R code from that chapter into a single overall workflow. You don’t need to do the Exercises before class, but you can if you want to. If a chapter doesn’t have an Exercises section, that means we will be working on an assignment together instead."
  },
  {
    "objectID": "review.html#sec-summation-1",
    "href": "review.html#sec-summation-1",
    "title": "1  Review",
    "section": "1.1 Summation notation",
    "text": "1.1 Summation notation\nSummation notation uses the symbol \\(\\Sigma\\) to stand-in for summation. For example, instead of writing\n\\[ X_1 + X_2 + X_3 + .... + X_N\\]\nto represent the sum of the values of the variable \\(X\\) in a sample of size \\(N\\), we can instead write:\n\\[ \\sum_{i=1}^{N} X_i. \\]\nThe symbol \\(\\Sigma\\) means “add.” The symbol is called “Sigma” – it’s the capital Greek letter corresponding to the Latin letter “S”. The value \\(i\\) is called the index, and \\(1\\) is the starting value of the index and \\(N\\) is the end value of the index. You can choose whatever start and end values you want to sum over. For example, if we just want to add the second and third values of \\(X\\), we write\n\\[ \\sum_{i=2}^{3} X_i = X_2 + X_3. \\]\nWhen the start and end evalues are clear from context, we can use a shorthand notation that omits them. In the following, it is implicit that the sum is over the all available values of \\(X\\) (i.e., from \\(1\\) to \\(N\\)):\n\\[ \\sum_i X_i. \\]"
  },
  {
    "objectID": "review.html#sec-rules-1",
    "href": "review.html#sec-rules-1",
    "title": "1  Review",
    "section": "1.2 Rules of summation",
    "text": "1.2 Rules of summation\nThere are rules for manipulating summation notation that are useful for deriving results in statistics. These rules are things you learned about addition in grade school, but they are presented using summation notation. You don’t need to do mathematical proofs or derivations in this class, but you will occasionally see some derivations in these notes (mainly in the optional sections).\nHere are the rules:\nRule 1: Sum of a constant (multiplication). Summing the values of a constant \\(c\\) is the same as multiplication. Specifically, if you add a constant \\(c\\) to itself \\(N\\) times, this just \\(N\\) times the constant:\n\\[\\begin{align}\n\\sum_{i = 1}^{N} c &= c + c +  .... \\\\\n& = Nc\n\\end{align}\\]\nRule 2: Distributive property. The sum of a variable \\(X_i\\) times a constant \\(c\\) is equal to the constant times the sum.\n\\[\\begin{align}\n\\sum_{i = 1}^{N} c X_i &= cX_1 + cX_2 + .... \\\\\n& = c(X_1 + X_2 + ....) \\\\\n& = c \\sum_{i = 1}^{N} X_i\n\\end{align}\\]\nRule 3: Associative property. It doesn’t matter what order we do addition in:\n\\[\\begin{align}\n\\sum_{i = 1}^{N} (X_i + Y_i) &= (X_1 + Y_1) + (X_2 + Y_2) + .... \\\\\n& = (X_1 + X_2 + ....) + (Y_1 + Y_2 + ....) \\\\\n& = \\sum_{i = 1}^{N} X_i  + \\sum_{i = 1}^{N} Y_i\n\\end{align}\\]"
  },
  {
    "objectID": "review.html#sec-stats-1",
    "href": "review.html#sec-stats-1",
    "title": "1  Review",
    "section": "1.3 Sample statistics",
    "text": "1.3 Sample statistics\nSummation notation is useful for writing the formulas of statistics. The main statistics we use in the class are the mean, standard deviation, variance, covariance, and correlation. These are the building blocks for regression. Their symbols and formulas are presented below (using the shorthand summation notation). If you don’t remember their interpretation, you will need to go back to your Stat 1 notes.\n\nThe mean\n\n\\[\\bar X = \\frac{\\sum_i X_i}{N}\\]\n\nThe variance can be written as \\(\\text{var}(X)\\) or sometimes using the symbol \\(s^2\\)\n\n\\[ \\text{var}(X) = \\frac{\\sum_i (X_i - \\bar X)^2}{N - 1} \\]\n\nThe standard deviation can be written \\(\\text{SD}(X)\\) or using the letter \\(s\\)\n\n\\[ \\text{SD}(X) = \\sqrt{\\text{var}(X)} \\]\n\nThe covariance is a generalization of the variance to two variables, it describes how they co-vary:\n\n\\[\\text{cov}(X, Y) = \\frac{\\sum_i (X_i - \\bar X) (Y_i - \\bar Y)}{N - 1} \\]\n\nThe correlation is the covariance divided by the product of the standard deviations of the variables. It takes on values between -1 and 1 and describes the strength and direction of the linear relationship between two variables.\n\n\\[\\text{cor}(X, Y) = \\frac{\\text{cov}(X, Y)}{\\sqrt{\\text{var}(X)} \\sqrt{\\text{var}(Y)}} \\]\nFor numerical examples see Section 1.10.8."
  },
  {
    "objectID": "review.html#sec-properties-1",
    "href": "review.html#sec-properties-1",
    "title": "1  Review",
    "section": "1.4 Some properties of sample statistics",
    "text": "1.4 Some properties of sample statistics\nThe following are some useful properties of the sample statistics reviewed above. You can derive these properties using the rules of summation. For each property, the beginning of the derivation is shown. You should know the properties but completing the derivations is optional.\nSum of deviations from the mean. If we subtract the mean from each data point, we have what is called a deviation (or deviation score): \\(d_i = X_i - \\bar X\\). It is always the case that \\(\\sum_i d_i = 0\\)\n\nDerivation\n\n\\[ \\sum_i d_i = \\sum_i(X_i - \\bar X) = \\dots \\]\nMean of a linear transformation. If \\(Y_i = A + B X_i\\) with known constants \\(A\\) and \\(B\\), then \\(\\bar{Y} = A + B \\bar{X}\\)\n\nDerivation:\n\n\\[ \\bar{Y} =  \\frac{\\sum_i Y_i}{N} =  \\frac{\\sum_i( A + B X_i)}{N} = \\dots \\] Variance of a linear transformation. If \\(Y_i = A + B X_i\\) with known constants \\(A\\) and \\(B\\), then \\(\\text{var}(Y) = B^2\\text{var}(X)\\)\n\nDerivation\n\n\\[ \\text{var}(Y) =  \\frac{\\sum_i (Y_i - \\bar{Y})^2}{N-1} =  \\frac{\\sum_i ((A + B X_i) - (A + B \\bar{X}))^2}{N-1} = \\dots\\] Mean and variance of a z-score. The z-score (or standardized score) is defined as \\(Z_i = (X_i - \\bar{X}) / \\text{SD}(X)\\). Standardized scores are useful because they have \\(\\bar{Z} = 0\\) and \\(\\text{var}(Z) = 1\\).\n\nDerivation: use the rules for linear transformation with \\(A = -\\bar{X} / \\text{SD}(X)\\) and \\(B = 1/ \\text{SD}(X)\\).\n\nCovariance of linear transformation. If \\(Y_i = A + B X_i\\) and \\(W_i = C + D U_i\\) with known constants \\(A\\), \\(B\\), \\(C\\), \\(D\\), then \\(\\text{cov}(Y, W) = BD\\,\\text{cov}(X, U)\\)\n\nDerivation\n\n\\[ \\text{cov}(Y, W) =  \\frac{\\sum_i ((A + B X_i) - (A + B \\bar{X}))((C + D U_i) - (C + D \\bar{U}))}{N-1} = \\dots\\]\nCorrelation of linear transformation. If \\(Y_i = A + B X_i\\) and \\(W_i = C + D U_i\\) with known constants \\(A\\), \\(B\\), \\(C\\), \\(D\\), then \\(\\text{cor}(Y, W) = \\text{cor}(X, U)\\) – i.e., the correlation is not affected by linear transformations.\n\nDerivation: use the rules for variances and covariances of linear transformation and the formula for correlation."
  },
  {
    "objectID": "review.html#bias-and-precision",
    "href": "review.html#bias-and-precision",
    "title": "1  Review",
    "section": "1.5 Bias and precision",
    "text": "1.5 Bias and precision\nIn this section we consider two more important properties of a statistic. These properties are defined in terms of the sampling distribution of a statistic. Recall that a sampling distribution arise from the following thought experiment: 1. Take a random sample of size \\(N\\) from a population of interest. 2. Compute a statistic using the sample data. It can be any statistic, but let’s say the mean, \\(\\bar X\\), for concreteness. 3. Write down the value of the mean, and then return the sample to the population.\nAfter doing these 3 steps many times, you will have many values the sample mean,\n\\[ \\bar{X}_1, \\bar{X}_2, \\bar{X}_3, \\bar{X}_4, \\bar{X}_5 ... \\]\nThe distribution of these sample means is called the sampling distribution (of the mean).\nA sampling distribution is just like any other distribution – so it has its own mean, and its own variance, etc. These statistics, when computed for a sampling distribution, have special names. We are espeically interested in the following two statistics.\n\nThe expected value of the mean, denoted \\(E(\\bar X)\\), is the mean of the sampling distribution of the mean. That is a mouthful! That is why we say the expected value of a statistic rather than the mean of a statistic. It’s called the expected value because it’s the average value over many samples.\nThe standard error of the mean, denoted \\(SE(\\bar X)\\), is the standard deviation of the sampling distribution of the mean. It describes the sample-to-sample variation of the mean around its expected value.\n\nNow for the two additional properties of a statistic:\n\nBias: If the expected value of a statistic is equal to a population parameter, we say that the statistic is an unbiased estimate of that parameter. For example, the expected value of the sample mean is equal to the population mean (in symbols: \\(E(\\bar{X}) = \\mu)\\), so we say that the sample mean is an unbiased estimate of the population mean.\nPrecision: The inverse of the squared standard error (i.e., \\(1 / \\text{SE}(\\bar{X})^2\\)) is called the precision of a statistic. So, the less a statistic varies from sample to sample, the more precise it is. That should hopefully make intuitive sense. The main thing to know about precision is that it is usually increasing in the sample size – i.e., we get more precise estimates by using larger samples. Again, this should feel intuitive.\n\nBelow is a figure that is often used to illustrate the ideas of bias and precision. The middle of the concentric circles represent the target parameter (like a bull’s eye) and the dots represent the sampling distribution of a statistic. You should be able to describe each panel in terms of the bias and precision of the statistic.\n\n\n\nFigure 1.1: Bias and Precision"
  },
  {
    "objectID": "review.html#t-tests",
    "href": "review.html#t-tests",
    "title": "1  Review",
    "section": "1.6 t-tests",
    "text": "1.6 t-tests\nThe \\(t\\)-test is used to make an inference about the value of an unknown population parameter. The test compares the value of an unbiased estimate of the parameter to a hypothesized value of the parameter. The conceptual formula for a \\(t\\)-test is\n\\[ t = \\frac{\\text{unbiased estimate} - \\text{hypothesized value }}{\\text{standard error}} \\]\nWhen we conduct a \\(t\\)-test, the basic rationale is as follows: “if the estimate statistic is close the hypothesized value of the parameter, then the numerator should be small relative to the standard error, and so \\(t\\) should be close to zero.”\nTypically, the hypothesized value of the population parameter is equal to zero, in which case it is called a null hypothesis. The null hypothesis usually translates into a research hypothesis of “no effect” or “no relationship.” So, if \\(t\\) is close to zero, it means there was no effect.\nIn order to determine what values of \\(t\\) are “close to zero”, we refer to its sampling distribution, which is called the \\(t\\)-distribution. The \\(t\\)-distribution tells what values of \\(t\\) are typical if the null hypothesis is true. (More specifically: if the sample statistic is normally distributed and its expected value equal to the hypothesized value, then \\(t\\) has a “central” \\(t\\)-distribution.)\nSome examples of the \\(t\\)-distribution are shown below. The x-axis denotes values of the statistic \\(t\\) shown above, and \\(\\nu\\) is a parameter called the “degrees of freedom” (more on this below). You can see that the \\(t\\)-distribution looks like a normal distribution centered a zero. So, when the null hypothesis is true, the expected value of \\(t\\) is zero. Informally, we could say that values greater than \\(\\pm 2\\) are pretty unlikely, and values greater than \\(\\pm 4\\) are very unlikely. Keep in the mind that these are the values of \\(t\\) we are expecting if the null hypothesis is true.\n\n\n\nFigure 1.2: t-distribution (source: https://en.wikipedia.org/wiki/Student%27s_t-distribution)\n\n\nMore formally, we can compare the value of \\(t\\) computed in a sample, denoted as \\(t_{\\text{obs}}\\), to a “critical value”, denoted as \\(t^*\\). The critical value is chosen so that the “significance level”, defined as \\(\\alpha = \\text{Prob}(|t| \\geq t^*)\\), is sufficiently small.\nThis significance level is chosen by the researcher. It represents our tolerance for false positives or Type I Errors – i.e., incorrectly rejecting the null hypothesis, or incorrectly concluding there is an effect when there isn’t one. When we set \\(\\alpha\\) to a small number, we are saying that we want the probability of a false positive to be small. This means we are going to need strong evidence before we reject the null hypothesis – i.e., the value of \\(t\\) would need to be very unlikely under the null hypothesis.\nThere are two equivalent ways of “formally” conducting a \\(t\\)-test.\n\nCompare the observed value of \\(t\\) to the critical value. Specifically: if \\(t_{\\text{obs}} &gt; t^*\\), reject the null hypothesis.\nCompare the significance level chosen by the research, \\(\\alpha\\), to the “p-value” of the test, computed as \\(p = \\text{Prob}(|t| \\geq |t_{\\text{obs}}|)\\). Specifically: if \\(p &lt; \\alpha\\), reject the null hypothesis.\n\nInformally, both of these just mean that the absolute value of \\(t\\) should be pretty big (i.e., greater than \\(t^*\\)) before we reject the null hypothesis.\nOne last thing before moving on: the t-distribution has a single parameter called its “degrees of freedom”, which is denoted as \\(\\nu\\) in Figure 1.2. The degrees of freedom are always an increasing function of the sample size, with larger samples leading to more degrees of freedom. When the degrees of freedom approach \\(\\infty\\), the \\(t\\)-distribution approaches a normal distribution. This means that that the differences between a \\(t\\)-test and a \\(z\\)-test is pretty minor in large samples (say \\(N \\geq 100\\)).\nHowever, when the degrees of freedom are small, the \\(t\\)-distribution has wider tails than the normal distribution. This is also shown in Figure 1.2. The tails of the distribution are important when doing statistical tests, because we are interested knowing about large / unlikely values of \\(t\\). So, in small samples (say \\(N &lt; 100\\)), it is important to use the \\(t\\)-distribution."
  },
  {
    "objectID": "review.html#confidence-intervals",
    "href": "review.html#confidence-intervals",
    "title": "1  Review",
    "section": "1.7 Confidence intervals",
    "text": "1.7 Confidence intervals\nA confidence interval uses the same equation as a \\(t\\)-test, except we solve for the population parameter rather than the value of \\(t\\). Whereas a \\(t\\)-test lets us make a guess about specific value of the parameter of interest (i.e., the null-hypothesized value), a confidence interval gives us a range of values that include the parameter of interest, with some degree of “confidence.”\nConfidence intervals have the general formula:\n\\[\\text{Interval} = \\text{sample value} \\pm t \\times {\\text{standard error}}. \\] We get the value of \\(t\\) from the \\(t\\)-distribution. In particular, if we want the interval to include the true population parameter \\((1-\\alpha) \\times 100\\%\\) of the time, then we choose \\(t\\) to be the \\(\\alpha/2 \\times 100\\) percentile of the \\(t\\)-distribution. For example, if we set \\(\\alpha = .05\\), we will have a \\((1-\\alpha) \\times 100 = 95\\%\\) confidence interval by choosing \\(t\\) to be the \\(\\alpha/2 \\times 100 = 2.5\\)-th percentile of the \\(t\\)-distribution.\nAs mentioned, \\(t\\)-tests and confidence intervals are closely related. In particular, if the confidence interval includes the value \\(0\\), this is the same as retaining the null hypothesis that the parameter is equal to \\(0\\). This should make intuitive sense. If the confidence interval includes \\(0\\), we are saying that it is a reasonable value of the population parameter, so we should not reject that value. This relationship assumes we use the same level of \\(\\alpha\\) for both the test and confidence interval.\nIn summary, if the confidence interval includes zero, we retain the null hypothesis at the stated level of \\(\\alpha\\). If the confidence interval does not include zero, we reject the null hypothesis at the stated level of \\(\\alpha\\)."
  },
  {
    "objectID": "review.html#f-tests",
    "href": "review.html#f-tests",
    "title": "1  Review",
    "section": "1.8 F-tests",
    "text": "1.8 F-tests\nThe \\(F\\)-test is used to infer if two independent variances have the same expected value. This turns out to be useful when we analyze the variance of a variable into different sources (i.e., Analysis of Variance or ANOVA).\nA variance can be defined as a sum-of-squares divided by its degrees of freedom. For example, the sample variance is just a sum-of-squared deviations from the sample mean (a sum of squares) divided by \\(N - 1\\) (its degrees of freedom).\nThe generic formula for an F-test is the ratio of two variances:\n\\[F = \\frac{SS_A / df_A}{SS_B / df_B}, \\]\nwhere \\(SS\\) denotes sums-of-squares and \\(df\\) denotes degrees of freedom.\nJust the like t-test, the \\(F\\)-test is called by the letter “F” because it has an \\(F\\)-distribution when the null hypothesis is true (i.e., when the variances have the same expected value). The plot below shows some examples of \\(F\\)- distributions. These distributions tell us the values of \\(F\\) that are likely, if the null hypothesis is true\n\n\n\nFigure 1.3: F-distribution (source: https://en.wikipedia.org/wiki/F-distribution)\n\n\nThe F distribution has two parameters, which are referred to as the “degrees of freedom in the numerator” and the “degrees of freedom in the denominator” (in the figure, d1 and d2, respectively). We always write the numerator \\(df\\) first and then the denominator \\(df\\). So, the green line in the figure is “an \\(F\\)-distribution on 10 and 1 degrees of freedom”, which means the \\(df\\) in the numerator is 10 and the \\(df\\) in the denominator is 1.\nWe use an \\(F\\)-test the same way we use a \\(t\\)-test – we set a significance level and use this level to determine how large the value of \\(F\\) needs to be for us to reject the null hypothesis. The main difference is that \\(F\\) is non-negative, because it is the ratio of squared numbers. We don’t usually compute confidence intervals for statistics with an \\(F\\) distribution."
  },
  {
    "objectID": "review.html#apa-reporting",
    "href": "review.html#apa-reporting",
    "title": "1  Review",
    "section": "1.9 APA reporting",
    "text": "1.9 APA reporting\nIt is important to be able to write up the results of statistical analyses in a way that other people will understand. For this reason, there are conventions about how to report statistical results. In this class, we will mainly use Table and Figures (formatted in R) rather than inline text. But sometimes reporting statistics using inline text unavoidable, in which case this course will use APA formatting. You don’t need to use APA in this class, but you should be familiar with some kind of conventions for reporting statistical results in your academic writing.\nThe examples below illustrate APA conventions. We haven’t covered the examples, they are just illustrative of the formatting (spacing, italics, number of decimal places, whether or not to use a leading zero before a decimal, etc). More details are available online (for example, here).\n\nJointly, the two predictors explained about 22% of the variation in Academic Achievement, which was statistically significant at the .05 level (\\(R^2 = .22, F(2, 247) = 29.63, p &lt; .001\\)).\nAfter controlling for SES, a one unit of increase in Maternal Education was associated with \\(b = 1.33\\) units of increase in Academic Achievement (\\(t(247) = 5.26, p &lt; .001\\)).\nAfter controlling for Maternal Education, a one unit of increase in SES was associated with \\(b = 0.32\\) units of increase in Academic Achievement. This was a statistically significant relationship (\\(t(247) = 2.91, p &lt; .01\\))."
  },
  {
    "objectID": "review.html#sec-exercizes-1",
    "href": "review.html#sec-exercizes-1",
    "title": "1  Review",
    "section": "1.10 Exercises",
    "text": "1.10 Exercises\nThis section will walk through some basics of programming with R. We will get started with this part of the review in the first class. You don’t need to do it before class.\nIf you are already familiar with R, please skim through the content and work on getting the NELS data loaded. If you are not familiar with R, or would like to brush up your R skills, you should work through this section.\n\n1.10.1 General info about R\nSome things to know about R before getting started:\n\nR is case sensitive. It matters if you useCAPS or lowercase in your code.\nEach new R command should begin on its own line.\nUnlike many other programming languages, R commands do not need to end with punctuation (e.g., ; or .).\nR uses the hashtag symbol (#) for comments. Comments are ignored by R but can be helpful for yourself and others to understand what your code does. An example is below.\n\n\n# This is a comment. R doesn't read it.\n# Below is a code snippet. R will read it and return the result. \n2 + 2\n\n[1] 4\n\n\n\nR’s working memory is cumulative. This means that you have to run code in order, one line after the next. It also means that any code you run is still hanging around in R’s memory until you clear it away using rm or the brush icon in R Studio - make sure to ask about how to do this in class if you aren’t sure.\n\n\n\n1.10.2 The basics\nAs we have just seen, R can do basic math like a calculator. Some more examples are presented in the code snippets below. R’s main math symbols are\n\n+ addition\n- subtraction or negative numbers\n* multiplication\n/ division (don’t use \\)\n^ or ** exponentiation\n\n\n2 * 2\n\n[1] 4\n\n\n\n# Remember pedmas? Make sure to use parentheses \"()\", \n# not brackets \"[]\" or braces \"{}\"\n(2 - 3) * 4 / 5 \n\n[1] -0.8\n\n\n\n# Exponentiation can be done two ways\n2^3\n\n[1] 8\n\n2**3\n\n[1] 8\n\n\n\n# Square roots are \"squirt\". Again, make sure to use \"()\", \n# not brackets \"[]\" or braces \"{}\"\nsqrt(25)\n\n[1] 5\n\n\n\n# Logs and exponents, base e (2.718282....) by default \nlog(100)\n\n[1] 4.60517\n\nexp(1)\n\n[1] 2.718282\n\n\n\n# We can override the default log by using the \"base\" option\nlog(100, base = 2)\n\n[1] 6.643856\n\n\n\n# Special numbers...\npi \n\n[1] 3.141593\n\n\n\n\n1.10.3 The help function\nThe help function is your best friend when using R. If we want more info on how to use an R function (like log), type:\n\nhelp(log)\n\nIf you don’t exactly remember the name of the function, using ??log will open a more complete menu of options.\n\n\n1.10.4 Logicals and strings\nR can also work with logical symbols that evaluate to TRUE or FALSE. R’s main logical symbols are\n\n== is equal to\n!= is not equal to\n&gt; greater than\n&lt; less than\n&gt;= greater than or equal to\n&lt;= less than or equal to\n\nHere are some examples:\n\n2 + 2 == 4\n\n[1] TRUE\n\n\n\n2 + 2 == 5\n\n[1] FALSE\n\n\n\n2 + 3 &gt; 5\n\n[1] FALSE\n\n\n\n2 + 3 &gt;= 5\n\n[1] TRUE\n\n\nThe main thing to note is that the logical operators return TRUE or FALSE as their output, not numbers. There are also symbols for logical conjunction (&) and disjunction (|), but we won’t get to those until later.\nIn addition to numbers and logicals, R can work with text (also called “strings”). We wont use strings a lot but they are worth knowing about.\n\n\"This is a string in R. The quotation marks tell R the input is text.\"\n\n[1] \"This is a string in R. The quotation marks tell R the input is text.\"\n\n\n\n\n1.10.5 Assignment (naming)\nOften we want to save the result of a calculation so that we can use it later on. In R, this means we need to assign the result a name. Once we assign the result a name, we can use that name to refer to the result, without having to re-do the calculation that produced the result. For example:\n\nx &lt;- 2 + 2\n\nNow we have given the result of 2 + 2 the name “x” using the assignment operator, &lt;-.\nNote that R no longer prints the result of the calculation to the console. If we want to see the result, we can type x\n\n# To see the result a name refers to, just type the name\nx \n\n[1] 4\n\n\nWe can also do assignment with the = operator.\n\ny = 2 + 2\ny\n\n[1] 4\n\n\nIt’s important to note that the = operator also gets used in other ways (e.g., to override default values in functions like log). Also, the math interpretation of “=” doesn’t really capture what is happening with assignment in computer code. In the above code, we are not saying that “2 + 2 equals y.” Instead, we are saying, “2 + 2 equals 4 and I want to refer to 4 later with the name ‘y’.”\nAlmost anything in R can be given a name and thereby saved in memory for later use. Assignment will become a lot more important when we name things like datasets, so that we can use the data for other things later on.\nA few other side notes:\n\nNames cannot include spaces or start with numbers. If you want separate words in a name, consider using a period ., an underscore _, or CamelCaseNotation.\nYou can’t use the same name twice. If you use a name, and then later on re-assign that same name to a different result, the name will now only represent the new result. The old result will no longer have a name, it will be lost in the computer’s memory and will be cleaned up by R’s garbage collector. Because R’s memory is cumulative, it’s important to keep track of names to make sure you know what’s what.\nR has some names that are reserved for built-in stuff, like log and exp and pi. You can override those names, but R will give a warning. If you override the name, this means you can’t use the built-in until you delete that name (e.g., rm(x)).\n\n\n\n1.10.6 Pop-quiz\n\nIn words, describe what the following R commands do.\n\nx &lt;- 7\nx = 7\nx == 7\n7 -&gt; x\n7 &gt; x\n\nAnswers: Check the commands in R.\n\n\n\n1.10.7 Vectors\nOften we want to work with multiple numbers or other objects at once. R has many data types or “objects” for doing this, for example, vectors, matrices, arrays, data.frames, and lists. We will start by looking at vectors.\nHere is an example vector, containing the sequence of integers from 15 to 25.\n\n# A vector containing the sequence of integers from 15 to 25\ny &lt;- 15:25\ny\n\n [1] 15 16 17 18 19 20 21 22 23 24 25\n\n\nWhen we work with a vector of numbers, sometimes we only want to access a subset of them. To access elements of a vector we use the square bracket notation []. Here are some examples of how to index a vector with R:\n\n# Print the first element of the vector y\n# Note: use brackets \"[]\" not parens\"()\"\ny[1]\n\n[1] 15\n\n\n\n# The first 3 elements\ny[1:3]\n\n[1] 15 16 17\n\n\n\n# The last 5\ny[6:11]\n\n[1] 20 21 22 23 24 25\n\n\nWe can also access elements of a vector that satisfy a given logical condition.\n\n# Print the elements of the vector y that are greater than the value 22\ny[y &gt; 22]\n\n[1] 23 24 25\n\n\nThis trick often comes in handy so its worth understanding how it works. First let’s look again at what y is, and what the logical statement y &gt; 22 evaluates to:\n\n# This is the vector y\ny\n\n [1] 15 16 17 18 19 20 21 22 23 24 25\n\n# This is the logical expression y &gt; 22\ny &gt; 22\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nWe can see that y &gt; 22 evaluates to TRUE or FALSE depending on whether the correspond number in the vector y is greater than 22. When we use the logical vector as an index – R will then return all the values for which y &gt; 22 is TRUE.\nIn general, we can index a vector y with any logical vector of the same length as y. The result will return only the values for which the logical vector is TRUE.\n\n\n1.10.8 Computing sample stats\nThe following are examples of statistical operations you can do with vectors of numbers. These examples follow closely to Section 1.1 to Section 1.4\n\n# Making a vector with the \"c\" command (combine) \nx &lt;- c(10, 9, 15, 15, 20, 17)\n\n# Find out how long a vector is (i.e.. the sample size)\nlength(x)\n\n[1] 6\n\n# Add up the elements of a vector\nsum(x)\n\n[1] 86\n\n# Add up the elements of a subset of a vector\nsum(x[2:3])\n\n[1] 24\n\n# Check the distributive rule\nsum(x*2) == sum(x) * 2 \n\n[1] TRUE\n\n# Check the associative rule\ny &lt;- c(5, 11, 11, 19, 13, 15)\nsum(x) + sum(y) == sum(x + y) \n\n[1] TRUE\n\n# Compute the mean\nmean(x)\n\n[1] 14.33333\n\n# Compute the variance and sd\nvar(x)\n\n[1] 17.46667\n\nsd(x)\n\n[1] 4.179314\n\n# Compute the covariance and correlation\ncov(x, y)\n\n[1] 10.66667\n\ncor(x, y)\n\n[1] 0.5457986\n\n\n\n\n1.10.9 Working with datasets\nMost of the time, we will be reading-in data from an external source. The easiest way to do this is if the data is in the .RData file format. Then we can just double click the .Rdata file and Rstudio will open the file, or we can use the load command in the console – both do the same thing.\nTo get started, lets load the NELS data. The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see https://nces.ed.gov/surveys/nels88/).\nThis data and codebook are available on Canvas site of the course under “Files/Data/NELS” and are linked in the “Module” for Week 1.” You need to download the data onto your machine and then open the data file (e.g., by clicking it, or double-clicking, or whatever you do to open files on your computer). That will do the same thing as the following line of code\n\n#This is what happens when you double-click NELS.RData\nload(\"NELS.RData\")\n\nThe function dim reports the number of rows (500 persons) and columns (48 variables) for the data set.\n\ndim(NELS)\n\n[1] 500  48\n\n\nIf you want to look at the data in a spreadsheet, use the following command. It won’t render anything in this book, but you can see what it does in R. (You may need to install XQuartz from www.xquartz.org if you are using a Mac.)\n\nView(NELS)\n\nIf you want to edit the data set using the spreadsheet, use edit(NELS). However, R’s spreadsheet editor is pretty wimpy, so if you want to edit data in spreadsheet format, use Excel or something.\nWorking with data is often made easier by “attaching”” the dataset. When a dataset it attached, this means that we can refer to the columns of the datasetby their names directly\n\n# Attach the data set\nattach(NELS)\n\n# Print the first 10 values of the gender variable\ngender[1:10]\n\n [1] Male   Female Male   Female Male   Female Female Female Female Male  \nLevels: Female Male\n\n\nWarning about attaching datasets. Once you attach a dataset, all of the column names in that dataset enter R’s working memory. If the column names in your dataset were already used, the old names are overwritten. If you attach the same dataset more than once in the same session, R will print a warning telling you that the previously named objects have been “masked” – this won’t affect your analyses, but it can be irritating.\nThe basic point: we should only attach each dataset once per R session. Once you are done using a data set it is good practice to detach it:\n\ndetach(NELS)\n\n\n\n1.10.10 Preview of next week\nFigure 1.4 shows the relationship between Grade 8 Math Achievement (percent correct on a math test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). Once you have reproduced this figure, you are ready to start the next chapter.\n\n# Load and attach the NELS data\nload(\"NELS.RData\")\nattach(NELS)\n\n# Scatter plot\nplot(x = ses, \n     y = achmat08, \n     col = \"#4B9CD3\", \n     ylab = \"Math Achievement (Grade 8)\", \n     xlab = \"SES\")\n\n# Run a simple linear regression \nmod &lt;- lm(achmat08 ~ ses)\n\n# Add the regression line to the plot\nabline(mod) \n\n\n\n\nFigure 1.4: Math Achievement and SES (NELS88)."
  }
]