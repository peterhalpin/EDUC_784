[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDUC 784",
    "section": "",
    "text": "Preface\nThese are the course notes for EDUC 784. Readings are assigned before class. Sections denoted with an asterisk (*) are optional.\nThe notes contain questions that are written in bold font. The questions are also collected in a section called “Workbook” that appears towards the end of each chapter. During class time, we will discuss the Workbook questions, your answers, any additional question you have, etc. It is really important for you to do the readings, and write down your responses to the questions, before class. You won’t get much out of the lessons if you haven’t done this preparation.\nSome chapters contain a section called “Exercises” that collects all of the R code from that chapter into a single overall workflow. You don’t need to do the Exercises before class, but you can if you want to. If a chapter doesn’t have an Exercises section, that means we will be working on an assignment together instead.\nImage credit: Daniela Rodriguez-Mincey, Spring 2023"
  },
  {
    "objectID": "ch1_review.html#sec-summation-1",
    "href": "ch1_review.html#sec-summation-1",
    "title": "1  Review",
    "section": "1.1 Summation notation",
    "text": "1.1 Summation notation\nSummation notation uses the symbol \\(\\Sigma\\) to stand-in for summation. For example, instead of writing\n\\[ X_1 + X_2 + X_3 + .... + X_N\\]\nto represent the sum of the values of the variable \\(X\\) in a sample of size \\(N\\), we can instead write:\n\\[ \\sum_{i=1}^{N} X_i. \\]\nThe symbol \\(\\Sigma\\) means “add.” The symbol is called “Sigma” – it’s the capital Greek letter corresponding to the Latin letter “S”. The value \\(i\\) is called the index, and \\(1\\) is the starting value of the index and \\(N\\) is the end value of the index. You can choose whatever start and end values you want to sum over. For example, if we just want to add the second and third values of \\(X\\), we write\n\\[ \\sum_{i=2}^{3} X_i = X_2 + X_3. \\]\nWhen the start and end values are clear from context, we can use a shorthand notation that omits them. In the following, it is implicit that the sum is over all available values of \\(X\\) (i.e., from \\(1\\) to \\(N\\)):\n\\[ \\sum_i X_i. \\]"
  },
  {
    "objectID": "ch1_review.html#sec-rules-1",
    "href": "ch1_review.html#sec-rules-1",
    "title": "1  Review",
    "section": "1.2 Rules of summation",
    "text": "1.2 Rules of summation\nThere are rules for manipulating summation notation that are useful for deriving results in statistics. You don’t need to do mathematical proofs or derivations in this class, but you will occasionally see some derivations in these notes (mainly in the optional sections).\nHere are the rules:\nRule 1: Sum of a constant (multiplication). Adding a constant to itself is equivalent to multiplication.\n\\[\\begin{align}\n\\sum_{i = 1}^{N} c &= c + c +  .... \\\\\n& = Nc\n\\end{align}\\]\nRule 2: Distributive property. Multiplying each term of a sum by a constant is the same as multiplying the sum by a constant.\n\\[\\begin{align}\n\\sum_{i = 1}^{N} c X_i &= cX_1 + cX_2 + .... \\\\\n& = c(X_1 + X_2 + ....) \\\\\n& = c \\sum_{i = 1}^{N} X_i\n\\end{align}\\]\nRule 3: Associative property. It doesn’t matter what order we do addition in.\n\\[\\begin{align}\n\\sum_{i = 1}^{N} (X_i + Y_i) &= (X_1 + Y_1) + (X_2 + Y_2) + .... \\\\\n& = (X_1 + X_2 + ....) + (Y_1 + Y_2 + ....) \\\\\n& = \\sum_{i = 1}^{N} X_i  + \\sum_{i = 1}^{N} Y_i\n\\end{align}\\]"
  },
  {
    "objectID": "ch1_review.html#sec-stats-1",
    "href": "ch1_review.html#sec-stats-1",
    "title": "1  Review",
    "section": "1.3 Sample statistics",
    "text": "1.3 Sample statistics\nSummation notation is useful for writing the formulas of statistics. The main statistics we use in the class are the mean, standard deviation, variance, covariance, and correlation. These are the building blocks for regression. Their symbols and formulas are presented below (using the shorthand summation notation). If you don’t remember their interpretation, you will need to go back to your Stat 1 notes.\n\nThe mean\n\n\\[\\bar X = \\frac{\\sum_i X_i}{N}\\]\n\nThe variance can be written as \\(\\text{var}(X)\\) or sometimes using the symbol \\(s^2\\)\n\n\\[ \\text{var}(X) = \\frac{\\sum_i (X_i - \\bar X)^2}{N - 1} \\]\n\nThe standard deviation can be written \\(\\text{SD}(X)\\) or using the letter \\(s\\)\n\n\\[ \\text{SD}(X) = \\sqrt{\\text{var}(X)} \\]\n\nThe covariance is a generalization of the variance to two variables, it describes how they co-vary:\n\n\\[\\text{cov}(X, Y) = \\frac{\\sum_i (X_i - \\bar X) (Y_i - \\bar Y)}{N - 1} \\]\n\nThe correlation is the covariance divided by the product of the standard deviations of the variables. It takes on values between -1 and 1 and describes the strength and direction of the linear relationship between two variables.\n\n\\[\\text{cor}(X, Y) = \\frac{\\text{cov}(X, Y)}{\\sqrt{\\text{var}(X)} \\sqrt{\\text{var}(Y)}} \\]\nFor numerical examples see Section 1.10.8."
  },
  {
    "objectID": "ch1_review.html#sec-properties-1",
    "href": "ch1_review.html#sec-properties-1",
    "title": "1  Review",
    "section": "1.4 Properties of sample statistics",
    "text": "1.4 Properties of sample statistics\nThe following are some useful properties of the sample statistics reviewed above. The properties tell us what happens to means, variances, covariances, and correlations when a variable is linearly transformed. We often linearly transform data (e.g., to compute percentages, proportions, z-scores, and in linear regression), so these properties turn out to be quite handy.\nYou can derive the properties using the rules of summation. For each property, the beginning of the derivation is shown. You should know the properties but completing the derivations is optional.\nSum of deviations from the mean. If we subtract the mean from each data point, we have a deviation (or deviation score): \\(d_i = X_i - \\bar X\\). Deviation scores sum to zero: \\(\\sum_i d_i = 0\\).\n\nDerivation\n\n\\[ \\sum_i d_i = \\sum_i(X_i - \\bar X) = \\dots \\]\nMean of a linear transformation. If \\(Y_i = A + B X_i\\) with known constants \\(A\\) and \\(B\\), then \\(\\bar{Y} = A + B \\bar{X}\\)\n\nDerivation:\n\n\\[ \\bar{Y} =  \\frac{\\sum_i Y_i}{N} =  \\frac{\\sum_i( A + B X_i)}{N} = \\dots \\] Variance of a linear transformation. If \\(Y_i = A + B X_i\\) with known constants \\(A\\) and \\(B\\), then \\(\\text{var}(Y) = B^2\\text{var}(X)\\)\n\nDerivation\n\n\\[ \\text{var}(Y) =  \\frac{\\sum_i (Y_i - \\bar{Y})^2}{N-1} =  \\frac{\\sum_i ((A + B X_i) - (A + B \\bar{X}))^2}{N-1} = \\dots\\] Mean and variance of a z-score. The z-score (or standardized score) is defined as \\(Z_i = (X_i - \\bar{X}) / \\text{SD}(X)\\). Standardized scores are useful because \\(\\bar{Z} = 0\\) and \\(\\text{var}(Z) = 1\\).\n\nDerivation: use the rules for linear transformation with \\(A = -\\bar{X} / \\text{SD}(X)\\) and \\(B = 1/ \\text{SD}(X)\\).\n\nCovariance of linear transformations. If \\(Y_i = A + B X_i\\) and \\(W_i = C + D U_i\\) with known constants \\(A\\), \\(B\\), \\(C\\), \\(D\\), then \\(\\text{cov}(Y, W) = BD\\,\\text{cov}(X, U)\\)\n\nDerivation\n\n\\[ \\text{cov}(Y, W) =  \\frac{\\sum_i ((A + B X_i) - (A + B \\bar{X}))((C + D U_i) - (C + D \\bar{U}))}{N-1} = \\dots\\]\nCorrelation of linear transformations. If \\(Y_i = A + B X_i\\) and \\(W_i = C + D U_i\\) with known constants \\(A\\), \\(B\\), \\(C\\), \\(D\\), then \\(\\text{cor}(Y, W) = \\text{cor}(X, U)\\) – i.e., the correlation is not affected by linear transformations.\n\nDerivation: use the rules for variances and covariances of linear transformation and the formula for correlation."
  },
  {
    "objectID": "ch1_review.html#bias-and-precision",
    "href": "ch1_review.html#bias-and-precision",
    "title": "1  Review",
    "section": "1.5 Bias and precision",
    "text": "1.5 Bias and precision\nIn this section we consider two more important properties of sample statistics. These properties are defined in terms of sampling distributions. Recall that a sampling distribution arises from the following thought experiment:\n\nTake a random sample of size \\(N\\) from a population of interest.\nCompute a statistic using the sample data. It can be any statistic, but let’s say the mean, \\(\\bar X\\), for concreteness.\nWrite down the value of the mean, and then return the sample to the population.\n\nAfter doing these 3 steps many times, you will have many values the sample mean,\n\\[ \\bar{X}_1, \\bar{X}_2, \\bar{X}_3, \\bar{X}_4, \\bar{X}_5, ... \\]\nThe distribution of these sample means is called a sampling distribution (i.e., the sampling distribution of the mean). A sampling distribution is just like any other distribution – so it has its own mean, and its own variance, etc. These quantities, when computed for a sampling distribution, have special names.\n\nThe expected value of the mean, denoted \\(E(\\bar X)\\), is the mean of the sampling distribution of the mean. That is a mouthful! That is why we say the “expected value” or “expectation” of a statistic rather than the mean of a statistic. It’s called the expected value because it’s the average value over many samples.\nThe standard error of the mean, denoted \\(SE(\\bar X)\\), is the standard deviation of the sampling distribution of the mean. It describes the sample-to-sample variation of the mean around its expected value.\n\nNow for the two additional properties of sample statistics:\n\nBias: If the expected value of a statistic is equal to a population parameter, we say that the statistic is an unbiased estimate of that parameter. For example, the expected value of the sample mean is equal to the population mean (in symbols: \\(E(\\bar{X}) = \\mu)\\), so we say that the sample mean is an unbiased estimate of the population mean.\nPrecision: The inverse of the squared standard error (i.e., \\(1 / \\text{SE}(\\bar{X})^2\\)) is called the precision of a statistic. So, the less a statistic varies from sample to sample, the more precise it is. That should hopefully make intuitive sense. The main thing to know about precision is that it is usually increasing in the sample size – i.e., we get more precise estimates by using larger samples. Again, this should feel intuitive.\n\nBelow is a figure that is often used to illustrate the ideas of bias and precision. The middle of the concentric circles represent the target parameter (like a bull’s eye) and the dots represent the sampling distribution of a statistic. You should be able to describe each panel in terms of the bias and precision of the statistic.\n\n\n\nFigure 1.1: Bias and Precision"
  },
  {
    "objectID": "ch1_review.html#t-tests",
    "href": "ch1_review.html#t-tests",
    "title": "1  Review",
    "section": "1.6 t-tests",
    "text": "1.6 t-tests\nThe \\(t\\)-test is used to make an inference about the value of an unknown population parameter. The test compares the value of an unbiased estimate of the parameter to a hypothesized value of the parameter. It is assumed that the sampling distribution of the estimate is a normal distribution, so the \\(t\\)-test applies to statistics like means and regression coefficients.\nThe conceptual formula for a \\(t\\)-test is\n\\[ t = \\frac{\\text{unbiased estimate} - \\text{hypothesized value }}{\\text{standard error}} \\] When we conduct a \\(t\\)-test, the basic rationale is as follows: If the “true” population parameter is equal to the hypothesized value, then the estimate should be close to the hypothesized value, and so \\(t\\) should be close to zero.\nIn order to determine what values of \\(t\\) are “close to zero”, we refer to its sampling distribution, which is called the \\(t\\)-distribution. The \\(t\\)-distribution tells what values of \\(t\\) are typical if the hypothesis is true. Some examples of the \\(t\\)-distribution are shown in the figure below. The x-axis denotes values of the statistic \\(t\\) shown above, and \\(\\nu\\) is parameter called the “degrees of freedom” (more on this soon).\n\n\n\nFigure 1.2: t-distribution (source: https://en.wikipedia.org/wiki/Student%27s_t-distribution)\n\n\nYou can see that the \\(t\\)-distribution looks like a normal distribution centered a zero. So, when the hypothesis is true, the expected value of \\(t\\) is zero. Informally, we could say that, if the hypothesis is true, values of \\(t\\) greater than \\(\\pm 2\\) are pretty unlikely, and values greater than \\(\\pm 4\\) are very unlikely.\nMore formally, we can make an inference about whether the population parameter is equal to the hypothesized value by comparing the value of \\(t\\) computed in our sample, denoted as \\(t_{\\text{obs}}\\), to a “critical value”, denoted as \\(t^*\\). The critical value is chosen so that the “significance level”, defined as \\(\\alpha = \\text{Prob}(|t| \\geq t^*)\\), is sufficiently small. The significance level is chosen by the researcher. Often it is set to \\(.05\\) or \\(.01.\\)\nThere are two equivalent ways of “formally” conducting a \\(t\\)-test.\n\nCompare the observed value of \\(t\\) to the critical value. Specifically: if \\(|t_{\\text{obs}}| &gt; t^*\\), reject the hypothesis.\nCompare the significance level chosen by the researcher, \\(\\alpha\\), to the “p-value” of the test, computed as \\(p = \\text{Prob}(|t| \\geq |t_{\\text{obs}}|)\\). Specifically: if \\(p &lt; \\alpha\\), reject the hypothesis.\n\nInformally, both of these just mean that the absolute value of \\(t\\) should be pretty big (i.e., greater than \\(t^*\\)) before we reject the hypothesis.\nA couple more things before moving on.\nFirst, the hypothesized value of the population parameter is often zero, in which case it is called a null hypothesis. The null hypothesis usually translates into a research hypothesis of “no effect” or “no relationship.” So, if we reject the null hypothesis, we conclude that there was an effect.\nSecond, the t-distribution has a single parameter called its “degrees of freedom”, which is denoted as \\(\\nu\\) in Figure 1.2. The degrees of freedom are an increasing function of the sample size, with larger samples leading to more degrees of freedom. When the degrees of freedom approach \\(\\infty\\), the \\(t\\)-distribution approaches a normal distribution. This means that the difference between a \\(t\\)-test and a \\(z\\)-test is pretty minor in large samples (say \\(N \\geq 100\\))."
  },
  {
    "objectID": "ch1_review.html#confidence-intervals",
    "href": "ch1_review.html#confidence-intervals",
    "title": "1  Review",
    "section": "1.7 Confidence intervals",
    "text": "1.7 Confidence intervals\nA confidence interval uses the same equation as a \\(t\\)-test, except we solve for the population parameter rather than the value of \\(t\\). Whereas a \\(t\\)-test lets us make a guess about specific value of the parameter of interest (i.e., the null-hypothesized value), a confidence interval gives us a range of values that include the parameter of interest, with some degree of “confidence.”\nConfidence intervals have the general formula:\n\\[\\text{Interval} = \\text{unbiased estimate} \\pm t \\times {\\text{standard error}}. \\] We get the value of \\(t\\) from the \\(t\\)-distribution. In particular, if we want the interval to include the true population parameter \\((1-\\alpha) \\times 100\\%\\) of the time, then we choose \\(t\\) to be the \\(\\alpha/2 \\times 100\\) percentile of the \\(t\\)-distribution. For example, if we set \\(\\alpha = .05\\), we will have a \\((1-\\alpha) \\times 100 = 95\\%\\) confidence interval by choosing \\(t\\) to be the \\(\\alpha/2 \\times 100 = 2.5\\)-th percentile of the \\(t\\)-distribution.\nAs mentioned, \\(t\\)-tests and confidence intervals are closely related. In particular, if the confidence interval includes the value \\(0\\), this is the same as retaining the null hypothesis that the parameter is equal to \\(0\\). This should make intuitive sense. If the confidence interval includes \\(0\\), we are saying that it is a reasonable value of the population parameter, so we should not reject that value. This equivalence between tests and confidence intervals assumes we use the same level of \\(\\alpha\\) for both of them.\nIn summary, if the confidence interval includes zero, we retain the null hypothesis at the stated level of \\(\\alpha\\). If the confidence interval does not include zero, we reject the null hypothesis at the stated level of \\(\\alpha\\)."
  },
  {
    "objectID": "ch1_review.html#f-tests",
    "href": "ch1_review.html#f-tests",
    "title": "1  Review",
    "section": "1.8 F-tests",
    "text": "1.8 F-tests\nThe \\(F\\)-test is used to infer if two independent variances have the same expected value. This turns out to be useful when we analyze the variance of a variable into different sources (i.e., Analysis of Variance or ANOVA).\nA variance can be defined as a sum-of-squares divided by its degrees of freedom. For example, the sample variance is just a sum-of-squared deviations from the sample mean (a sum of squares) divided by \\(N - 1\\) (its degrees of freedom).\nThe generic formula for an F-test is the ratio of two variances:\n\\[F = \\frac{SS_A / df_A}{SS_B / df_B}, \\]\nwhere \\(SS\\) denotes sums-of-squares and \\(df\\) denotes degrees of freedom.\nJust the like \\(t\\)-test, the \\(F\\)-test is called by the letter “F” because it has an \\(F\\)-distribution when the null hypothesis is true (i.e., when the variances have the same expected value). The plot below shows some examples of \\(F\\)- distributions. These distributions tell us the values of \\(F\\) that are likely, if the null hypothesis is true.\n\n\n\nFigure 1.3: F-distribution (source: https://en.wikipedia.org/wiki/F-distribution)\n\n\nThe F distribution has two parameters, which are referred to as the “degrees of freedom in the numerator” and the “degrees of freedom in the denominator” (in the figure, d1 and d2, respectively). We always write the numerator \\(df\\) first and then the denominator \\(df\\). So, the green line in the figure is “an \\(F\\)-distribution on 10 and 1 degrees of freedom”, which means the \\(df\\) in the numerator is 10 and the \\(df\\) in the denominator is 1.\nWe use an \\(F\\)-test the same way we use a \\(t\\)-test – we set a significance level and use this level to determine how large the value of \\(F\\) needs to be for us to reject the null hypothesis. The main difference is that \\(F\\) is non-negative, because it is the ratio of squared numbers. We don’t usually compute confidence intervals for statistics with an \\(F\\)-distribution."
  },
  {
    "objectID": "ch1_review.html#apa-reporting",
    "href": "ch1_review.html#apa-reporting",
    "title": "1  Review",
    "section": "1.9 APA reporting",
    "text": "1.9 APA reporting\nIt is important to write up the results of statistical analyses in a way that other people will understand. For this reason, there are conventions about how to report statistical results. In this class, we will mainly use tables and figures (formatted in R) rather than inline text. But sometimes reporting statistics using inline text unavoidable, in which case this course will use APA formatting. You don’t need to use APA in this class, but you should be familiar with some kind of conventions for reporting statistical results in your academic writing.\nThe examples below illustrate APA conventions. We haven’t covered the examples, they are just illustrative of the formatting (spacing, italics, number of decimal places, whether or not to use a leading zero before a decimal, etc). More details are available online (for example, here).\n\nJointly, the two predictors explained about 22% of the variation in Academic Achievement, which was statistically significant at the .05 level (\\(R^2 = .22, F(2, 247) = 29.63, p &lt; .001\\)).\nAfter controlling for SES, a one unit of increase in Maternal Education was associated with \\(b = 1.33\\) units of increase in Academic Achievement (\\(t(247) = 5.26, p &lt; .001\\)).\nAfter controlling for Maternal Education, a one unit of increase in SES was associated with \\(b = 0.32\\) units of increase in Academic Achievement. This was a statistically significant relationship (\\(t(247) = 2.91, p &lt; .01\\))."
  },
  {
    "objectID": "ch1_review.html#sec-exercizes-1",
    "href": "ch1_review.html#sec-exercizes-1",
    "title": "1  Review",
    "section": "1.10 Exercises",
    "text": "1.10 Exercises\nThis section will walk through some basics of programming with R. We will get started with this part of the review in the first class. You don’t need to do it before class.\nIf you are already familiar with R, please skim through the content and work on getting the NELS data loaded. If you are not familiar with R, or would like to brush up your R skills, you should work through this section.\n\n1.10.1 General info about R\nSome things to know about R before getting started:\n\nR is case sensitive. It matters if you useCAPS or lowercase in your code.\nEach new R command should begin on its own line.\nUnlike many other programming languages, R commands do not need to end with punctuation (e.g., ; or .).\nR uses the hash tag symbol (#) for comments. Comments are ignored by R but can be helpful for yourself and others to understand what your code does. An example is below.\n\n\n# This is a comment. R doesn't read it.\n# Below is a code snippet. R will read it and return the result. \n2 + 2\n\n[1] 4\n\n\n\nR’s working memory is cumulative. This means that you have to run code in order, one line after the next. It also means that any code you run is still hanging around in R’s memory until you clear it away using rm or the brush icon in R Studio - make sure to ask about how to do this in class if you aren’t sure.\n\n\n\n1.10.2 The basics\nAs we have just seen, R can do basic math like a calculator. Some more examples are presented in the code snippets below. R’s main math symbols are\n\n+ addition\n- subtraction or negative numbers\n* multiplication\n/ division (don’t use \\)\n^ or ** exponentiation\n\n\n2 * 2\n\n[1] 4\n\n\n\n# Remember pedmas? Make sure to use parentheses \"()\", \n# not brackets \"[]\" or braces \"{}\"\n(2 - 3) * 4 / 5 \n\n[1] -0.8\n\n\n\n# Exponentiation can be done two ways\n2^3\n\n[1] 8\n\n2**3\n\n[1] 8\n\n\n\n# Square roots are \"squirt\". Again, make sure to use \"()\", \n# not brackets \"[]\" or braces \"{}\"\nsqrt(25)\n\n[1] 5\n\n\n\n# Logs and exponents, base e (2.718282....) by default \nlog(100)\n\n[1] 4.60517\n\nexp(1)\n\n[1] 2.718282\n\n\n\n# We can override the default log by using the \"base\" option\nlog(100, base = 2)\n\n[1] 6.643856\n\n\n\n# Special numbers...\npi \n\n[1] 3.141593\n\n\n\n\n1.10.3 The help function\nThe help function is your best friend when using R. If we want more info on how to use an R function (like log), type:\n\nhelp(log)\n\nIf you don’t exactly remember the name of the function, using ??log will open a more complete menu of options.\n\n\n1.10.4 Logicals and strings\nR can also work with logical symbols that evaluate to TRUE or FALSE. R’s main logical symbols are\n\n== is equal to\n!= is not equal to\n&gt; greater than\n&lt; less than\n&gt;= greater than or equal to\n&lt;= less than or equal to\n\nHere are some examples:\n\n2 + 2 == 4\n\n[1] TRUE\n\n\n\n2 + 2 == 5\n\n[1] FALSE\n\n\n\n2 + 3 &gt; 5\n\n[1] FALSE\n\n\n\n2 + 3 &gt;= 5\n\n[1] TRUE\n\n\nThe main thing to note is that the logical operators return TRUE or FALSE as their output, not numbers. There are also symbols for logical conjunction (&) and disjunction (|), but we won’t get to those until later.\nIn addition to numbers and logicals, R can work with text (also called “strings”). We wont use strings a lot but they are worth knowing about.\n\n\"This is a string in R. The quotation marks tell R the input is text.\"\n\n[1] \"This is a string in R. The quotation marks tell R the input is text.\"\n\n\n\n\n1.10.5 Assignment (naming)\nOften we want to save the result of a calculation so that we can use it later on. In R, this means we need to assign the result a name. Once we assign the result a name, we can use that name to refer to the result, without having to re-do the calculation that produced the result. For example:\n\nx &lt;- 2 + 2\n\nNow we have given the result of 2 + 2 the name “x” using the assignment operator, &lt;-.\nNote that R no longer prints the result of the calculation to the console. If we want to see the result, we can type x\n\n# To see the result a name refers to, just type the name\nx \n\n[1] 4\n\n\nWe can also do assignment with the = operator.\n\ny = 2 + 2\ny\n\n[1] 4\n\n\nIt’s important to note that the = operator also gets used in other ways (e.g., to override default values in functions like log). Also, the math interpretation of “=” doesn’t really capture what is happening with assignment in computer code. In the above code, we are not saying that “2 + 2 equals y.” Instead, we are saying, “2 + 2 equals 4 and I want to refer to 4 later with the name ‘y’.”\nAlmost anything in R can be given a name and thereby saved in memory for later use. Assignment will become a lot more important when we name things like datasets, so that we can use the data for other things later on.\nA few other side notes:\n\nNames cannot include spaces or start with numbers. If you want separate words in a name, consider using a period ., an underscore _, or CamelCaseNotation.\nYou can’t use the same name twice. If you use a name, and then later on re-assign that same name to a different result, the name will now only represent the new result. The old result will no longer have a name, it will be lost in the computer’s memory and will be cleaned up by R’s garbage collector. Because R’s memory is cumulative, it’s important to keep track of names to make sure you know what’s what.\nR has some names that are reserved for built-in stuff, like log and exp and pi. You can override those names, but R will give a warning. If you override the name, this means you can’t use the built-in functions until you delete that name (e.g., rm(x)).\n\n\n\n1.10.6 Pop-quiz\n\nIn words, describe what the following R commands do.\n\nx &lt;- 7\nx = 7\nx == 7\n7 -&gt; x\n7 &gt; x\n\nAnswers: Check the commands in R.\n\n\n\n1.10.7 Vectors\nIn research settings, we often want to work with multiple numbers at once (surprise!). R has many data types or “objects” for doing this, for example, vectors, matrices, arrays, data.frames, and lists. We will start by looking at vectors.\nHere is an example vector, containing the sequence of integers from 15 to 25.\n\n# A vector containing the sequence of integers from 15 to 25\ny &lt;- 15:25\ny\n\n [1] 15 16 17 18 19 20 21 22 23 24 25\n\n\nWhen we work with a vector of numbers, sometimes we only want to access a subset of them. To access elements of a vector we use the square bracket notation []. Here are some examples of how to index a vector with R:\n\n# Print the first element of the vector y\n# Note: use brackets \"[]\" not parens\"()\"\ny[1]\n\n[1] 15\n\n\n\n# The first 3 elements\ny[1:3]\n\n[1] 15 16 17\n\n\n\n# The last 5\ny[6:11]\n\n[1] 20 21 22 23 24 25\n\n\nWe can also access elements of a vector that satisfy a given logical condition.\n\n# Print the elements of the vector y that are greater than the value 22\ny[y &gt; 22]\n\n[1] 23 24 25\n\n\nThis trick often comes in handy so its worth understanding how it works. First let’s look again at what y is, and what the logical statement y &gt; 22 evaluates to:\n\n# This is the vector y\ny\n\n [1] 15 16 17 18 19 20 21 22 23 24 25\n\n# This is the logical expression y &gt; 22\ny &gt; 22\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nWe can see that y &gt; 22 evaluates to TRUE or FALSE depending on whether the correspond number in the vector y is greater than 22. When we use the logical vector as an index – R will then return all the values for which y &gt; 22 is TRUE.\nIn general, we can index a vector y with any logical vector of the same length as y. The result will return only the values for which the logical vector is TRUE.\n\n\n1.10.8 Computing sample stats\nThe following are examples of statistical operations you can do with vectors of numbers. These examples follow closely to Section 1.1 to Section 1.4\n\n# Making a vector with the \"c\" command (combine) \nx &lt;- c(10, 9, 15, 15, 20, 17)\n\n# Find out how long a vector is (i.e., the sample size)\nlength(x)\n\n[1] 6\n\n# Add up the elements of a vector\nsum(x)\n\n[1] 86\n\n# Add up the elements of a subset of a vector\nsum(x[2:3])\n\n[1] 24\n\n# Check the distributive rule\nsum(x*2) == sum(x) * 2 \n\n[1] TRUE\n\n# Check the associative rule\ny &lt;- c(5, 11, 11, 19, 13, 15)\nsum(x) + sum(y) == sum(x + y) \n\n[1] TRUE\n\n# Compute the mean\nmean(x)\n\n[1] 14.33333\n\n# Compute the variance\nvar(x)\n\n[1] 17.46667\n\n# Compute the standard deviation\nsd(x)\n\n[1] 4.179314\n\n# Compute the covariance\ncov(x, y)\n\n[1] 10.66667\n\n# Compute the correlation\ncor(x, y)\n\n[1] 0.5457986\n\n\n\n\n1.10.9 Working with datasets\nMost of the time, we will be reading-in data from an external source. The easiest way to do this is if the data is in the .RData file format. Then we can just double-click the .Rdata file and Rstudio will open the file, or we can use the load command in the console – both do the same thing.\nTo get started, let’s load the NELS data. The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see https://nces.ed.gov/surveys/nels88/).\nThis data and codebook are available on the Canvas site of the course under “Files/Data/NELS” and are linked in the “Module” for Week 1. You need to download the data onto your machine and then open the data file (e.g., by clicking it, or double-clicking, or whatever you do to open files on your computer). That will do the same thing as the following line of code\n\n#This is what happens when you double-click NELS.RData\nload(\"NELS.RData\")\n\nThe function dim reports the number of rows (500 persons) and columns (48 variables) for the data set.\n\ndim(NELS)\n\n[1] 500  48\n\n\nIf you want to look at the data in a spreadsheet, use the following command. It won’t render anything in this book, but you can see what it does in R. (You may need to install XQuartz from https://www.xquartz.org if you are using a Mac.)\n\nView(NELS)\n\nIf you want to edit the data set using the spreadsheet, use edit(NELS). However, R’s spreadsheet editor is pretty wimpy, so if you want to edit data in spreadsheet format, use Excel or something.\nWorking with data is often made easier by “attaching”” the dataset. When a dataset it attached, this means that we can refer to the columns of the dataset by their names.\n\n# Attach the data set\nattach(NELS)\n\n# Print the first 10 values of the NELS gender variable\ngender[1:10]\n\n [1] Male   Female Male   Female Male   Female Female Female Female Male  \nLevels: Female Male\n\n\nWarning about attaching datasets. Once you attach a dataset, all of the column names in that dataset enter R’s working memory. If the column names in your dataset were already used, the old names are overwritten. If you attach the same dataset more than once in the same session, R will print a warning telling you that the previously named objects have been “masked” – this won’t affect your analyses, but it can be irritating.\nThe basic point: we should only attach each dataset once per R session. Once you are done using a data set it is good practice to detach it:\n\ndetach(NELS)\n\n\n\n1.10.10 Preview of next week\nFigure 1.4 shows the relationship between Grade 8 Math Achievement (percent correct on a math test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). Once you have reproduced this figure, you are ready to start the next chapter.\n\n# Load and attach the NELS data\nload(\"NELS.RData\")\nattach(NELS)\n\n# Scatter plot\nplot(x = ses, \n     y = achmat08, \n     col = \"#4B9CD3\", \n     ylab = \"Math Achievement (Grade 8)\", \n     xlab = \"SES\")\n\n# Run a simple linear regression \nmod &lt;- lm(achmat08 ~ ses)\n\n# Add the regression line to the plot\nabline(mod)\n\n# Detach the data set\ndetach(NELS)\n\n\n\n\nFigure 1.4: Math Achievement and SES (NELS88)."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-example-2",
    "href": "ch2_simple_regression.html#sec-example-2",
    "title": "2  Simple Regression",
    "section": "2.1 An example from NELS",
    "text": "2.1 An example from NELS\nLet’s begin by considering an example. Figure 2.1 shows the relationship between Grade 8 Math Achievement (percent correct on a math test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see https://nces.ed.gov/surveys/nels88/).\n\n\nCode\n# Load and attach the NELS88 data\nload(\"NELS.RData\")\nattach(NELS)\n\n# Scatter plot\nplot(x = ses, \n     y = achmat08, \n     col = \"#4B9CD3\", \n     ylab = \"Math Achievement (Grade 8)\", \n     xlab = \"SES\")\n\n# Run the regression model\nmod &lt;- lm(achmat08 ~ ses)\n\n# Add the regression line to the plot\nabline(mod) \n\n\n\n\n\nFigure 2.1: Math Achievement and SES (NELS88).\n\n\n\n\nThe strength and direction of the linear relationship between the two variables is summarized by their correlation. In this sample, the value of the correlation is:\n\n\nCode\ncor(achmat08, ses)\n\n\n[1] 0.3182484\n\n\nThis is a moderate, positive correlation between Math Achievement and SES. This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in math (higher Math Achievement).\nThe relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., https://www.apa.org/pi/ses/resources/publications/education). Please look over this web page and be prepared to share your thoughts/questions about the relationship between SES and academic achievement and its relevance for education research."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-regression-line-2",
    "href": "ch2_simple_regression.html#sec-regression-line-2",
    "title": "2  Simple Regression",
    "section": "2.2 The regression line",
    "text": "2.2 The regression line\nThe section presents three interchangeable ways of writing the regression line in Figure 2.1. You should be familiar with all 3 ways of presenting regression equations and you are welcome to use whichever approach you like best in your writing for this class.\nThe regression line in Figure 2.1 can be represented mathematically as\n\\[\n\\widehat Y = a + b X\n\\tag{2.1}\\]\nwhere\n\n\\(Y\\) denotes Math Achievement\n\\(X\\) denotes SES\n\\(a\\) represents the regression intercept (the value of \\(\\widehat Y\\) when \\(X = 0\\))\n\\(b\\) represents the regression slope (how much \\(\\widehat Y\\) changes for each unit of increase in \\(X\\))\n\nIn this equation, the symbol \\(\\widehat Y\\) represents the “predicted value” of Math Achievement for a given value of SES. In Figure 2.1, the predicted values are represented by the regression line. The “observed values” of Math Achievement are denoted as \\(Y\\). In Figure 2.1, these values are represented by the points in the scatter plot.\nSome more terminology: the \\(Y\\) variable is often referred to as the “outcome” or the “dependent variable.” The \\(X\\) variable is often referred to as the “predictor”, “independent variable”, “explanatory variable”, or “covariate.” Different areas of research have different conventions about terminology for regression. We talk more about “big picture” interpretations of regression in Chapter 3.\nThe difference between an observed value \\(Y\\) and its predicted value \\(\\widehat Y\\) is called a residual. Residuals are denoted as \\(e = Y - \\widehat Y\\). The residuals for a subset of the data points in Figure 2.1 are shown in pink in Figure 2.2\n\n\nCode\n# Get predicted values from regression model\nyhat &lt;- mod$fitted.values\n\n# select a subset of the data\nset.seed(10)\nindex &lt;- sample.int(500, 30)\n\n# plot again\nplot(x = ses[index], \n     y = achmat08[index], \n     ylab = \"Math Achievement (Grade 8)\", \n     xlab = \"SES\")\n\nabline(mod)\n\n# Add pink lines\nsegments(x0 = ses[index], \n         y0 = yhat[index], \n         x1 = ses[index], \n         y1 = achmat08[index], \n         col = 6, lty = 3)\n\n# Overwrite dots to make it look at bit better\npoints(x = ses[index], \n       y = achmat08[index], \n       col = \"#4B9CD3\", \n       pch = 16)\n\n\n\n\n\nFigure 2.2: Residuals for a Subsample of the Example.\n\n\n\n\nNotice that \\(Y = \\widehat Y + e\\) by definition:\n\\[\nY = \\widehat Y + e = \\widehat Y + (Y - \\widehat Y ) = Y.\n\\]\nThis leads to a second way of writing out a regression model:\n\\[\nY = a + bX + e.  \n\\tag{2.2}\\]\nThe difference between Equation 2.1 and Equation 2.2 is that the former lets us talk about the predicted values (\\(\\hat Y\\)), whereas the latter lets us talk about the observed data points (\\(Y\\)).\nA third way to write out the model is using the variable names (or abbreviations) in place of the more generic “X, Y” notation. For example,\n\\[MATH = a + b(SES) + e \\tag{2.3}\\]\nThis notation is useful when talking about a specific example, because we don’t have to remember what \\(Y\\) and \\(X\\) stand for. But this notation is more clunky and doesn’t lend itself talking about regression in general or writing other mathematical expressions related to regression.\nYou should be familiar with all 3 ways of presenting regression equations (Equation 2.1, Equation 2.2, and Equation 2.3) and you are welcome to use whichever approach you like best in this class."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-ols-2",
    "href": "ch2_simple_regression.html#sec-ols-2",
    "title": "2  Simple Regression",
    "section": "2.3 OLS",
    "text": "2.3 OLS\nThis section talks about how to estimate the regression intercept (denoted as \\(a\\) in Equation 2.1) and the regression slope (denoted as \\(b\\) in Equation 2.1). The intercept and slope are collectively referred to as the “parameters” of the regression line. They are also referred to as “regression coefficients.”\nOur overall goal in this section is to “fit a line to the data” – i.e., we want to select the values of the regression coefficients that best represent our data. An intuitive way to approach this problem is by minimizing the residuals – i.e., minimizing the total amount of “pink” in Figure 2.2. We can operationalize this intuitive idea by minimizing the sum of squared residuals:\n\\[\nSS_{\\text{res}} = \\sum_{i=1}^{N} e_i^2 = \\sum_{i=1}^{N} (Y_i - a - b X_i)^2\n\\]\nwhere \\(i = 1 \\dots N\\) indexes the respondents in the sample. When we estimate the regression coefficients by minimizing \\(SS_{\\text{res}}\\), this is called ordinary least squares (OLS) regression. OLS is very widely used and is the main focus of this course, although we will visit some other approaches in the second half of the course.\nThe values of the regression coefficients that minimize \\(SS_{\\text{res}}\\) can be found using calculus (i.e., compute the derivatives of \\(SS_{\\text{res}}\\) and set them to zero). This approach leads to the following equations for the regression coefficients:\n\\[\na = \\bar Y - b \\bar X \\quad \\quad \\quad \\quad b = \\frac{\\text{cov}(X, Y)}{s^2_X} = \\text{cor}(X, Y) \\frac{s_Y}{s_X}\n\\tag{2.4}\\]\n(If you aren’t familiar with the symbols in these equations, check out the review materials in Chapter 1 for a refresher.)\nThe formulas in Equation 2.4 tell us how to compute the regression coefficients using our sample data. However, on face value, these formulas don’t tell us much about how to interpret the regression coefficients. For interpreting the regression coefficients, it is more straightforward to refer to Equation 2.1.\nTo clarify:\n\nTo interpret the regression intercept, use Equation 2.1: It is the value of \\(\\hat Y\\) when \\(X = 0\\). Similarly, the regression slope is how much \\(\\hat Y\\) changes for a one-unit increase in \\(X\\).\nTo compute the regression coefficients, use Equation 2.4. These formulas are not very intuitive – they are just what we get when we fit a line to the data using OLS.\n\nIt is important to emphasize that the formulas in Equation 2.4 do lead to some useful mathematical results about regression. Section 2.9, which is optional, derives some of the main results. If you want a deeper mathematical understanding of regression, make sure to check out this section. If you prefer to just learn about the results as they become relevant and skip the math, that is OK too.\n\n2.3.1 Correlation and regression\nBefore moving on, it is worth noting something that we can learn from Equation 2.4 without too much math: the regression slope is just a re-packaging of the correlation coefficient. In particular, if we assume that \\(X\\) and \\(Y\\) are z-scores (i.e., they are standardized to have mean of zero and variance of one), then Equation 2.4 reduces to:\n\n\\(a = 0\\)\n\\(b = \\text{cov}(X, Y) = \\text{cor}(X, Y)\\)\n\nThere are two important things to note here.\nFirst, the difference between correlation and simple regression depends on the scale of the variables. Otherwise stated, if we standardize both \\(Y\\) and \\(X\\), then regression is just correlation. In particular, if the correlation is equal to zero, then the regression slope is also equal to zero – these are just two equivalent ways of saying that the variables are not (linearly) related.\nSecond, this relationship between correlation and regression holds only for simple regression (i.e., one predictor). When we get to multiple regression, we will see that relationship between regression and correlation (and covariance) gets more complicated.\nFor the NELS example in Figure 2.1, the regression intercept and slope are, respectively:\n\n\nCode\ncoef(mod)\n\n\n(Intercept)         ses \n 48.6780338   0.4292604 \n\n\nPlease write down an interpretation of these numbers and be prepared to share your answers in class. How would your interpretation change if, rather than the value of the slope shown above, we had \\(b = 0\\)?"
  },
  {
    "objectID": "ch2_simple_regression.html#sec-rsquared-2",
    "href": "ch2_simple_regression.html#sec-rsquared-2",
    "title": "2  Simple Regression",
    "section": "2.4 R-squared",
    "text": "2.4 R-squared\nIn this section we introduce another statistic that is commonly used in regression, called “R-squared” (in symbols: \\(R^2\\)). First we will talk about its interpretation, then we will show how it is computed.\nR-squared is the proportion of variance in the outcome variable that is associated with, or “explained by”, the predictor variable. In terms of the NELS example, the variance of the outcome can be interpreted in terms of individual differences in Math Achievement – i.e., how students “deviate” from, or vary around, the mean level of Math Achievement. R-squared tells us the extent to which these individual differences in Math Achievement are associated with, or explained by, individual differences in SES.\nAs mentioned, R-squared is a proportion. Because R-squared is a proportion, it takes on values between \\(0\\) and \\(1\\). If \\(R^2 = 0\\) then a student’s SES doesn’t tell us anything about their Math Achievement – this is the same as saying the two variables aren’t correlated, or that there is no (linear) relationship between Math Achievement and SES. If \\(R^2 = 1\\), then all of the data points fall exactly on the regression line, and we can perfectly predict each student’s Math Achievement from their SES.\nYou might be asking – why do we need R-squared? We already have the regression coefficient (which is just a repackaging of the correlation), so why do we need yet another way of describing the relationship between Math Achievement and SES? This is very true for simple regression! However, when we move on to multiple regression, we will see that R-squared lets us talk about the relationship between the outcome and all of the predictors, or any subset of the predictors, whereas the regression coefficient only lets us talk about the relationship with one predictor at a time.\nTo see how R-squared is computed for the NELS example, let’s consider Figure 2.3. The horizontal grey line denotes the mean of Math Achievement. Recall that the variance of \\(Y\\) is computed using the sum-of-squared deviations from the mean. For each student, these deviations from the mean can be divided into two parts. The Figure shows these two parts for a single student, using black and pink dashed lines:\n\nThe black dashed line represents the extent to which the student’s deviation from the mean level of Math Achievement is explained by the linear relationship between Math Achievement and SES.\nThe pink dashed line is the regression residual, which was introduced in Section 2.2. This is the variation in Math Achievement that is “left over” after considering the linear relationship with SES.\n\n\n\n\n\n\nFigure 2.3: The Idea Behind R-squared.\n\n\n\n\nThe R-squared statistic averages the variation in Math Achievement associated with SES (i.e., the black dashed line) for all students in the sample, and then divides by the total variation in Math Achievement (i.e., black + pink).\nThe derivation of the R-squared statistic is not very complicated and provides some useful notation. To simplify the derivation, we can work the numerator of the variance, which is called the “total sum of squares:”\n\\[SS_{\\text{total}} = \\sum_{i = 1}^N (Y_i - \\bar Y)^2. \\]\nNext we add and subtract the predicted values (that old trick!):\n\\[SS_{\\text{total}} = \\sum_{i = 1}^N [(Y_i - \\widehat Y_i) + (\\widehat Y_i - \\bar Y)]^2. \\]\nThe right-hand-side can be reduced to two other sums of squares using the rules of summation algebra (see Section 1.2 – the derivation is long but not complicated).\n\\[\nSS_{\\text{total}} = \\sum_{i = 1}^N (Y_i - \\widehat Y_i)^2 + \\sum_{i = 1}^N (\\widehat Y_i - \\bar Y)^2.\n\\]\nThe first term on the right-hand-side is just the sum of squared residuals (\\(SS_\\text{res}\\)) from Section 2.3. The second term is called the sum of squared regression and denoted \\(SS_\\text{reg}\\). Using this notation we can re-write the previous equation as\n\\[ SS_{\\text{total}} = SS_\\text{res} + SS_\\text{reg} \\]\nand the R-squared statistic is computed as\n\\[R^2 = SS_{\\text{reg}} / SS_{\\text{total}}. \\]\nAs discussed above, this quantity can be interpreted as the proportion of variance in \\(Y\\) that is explained by its linear relationship with \\(X\\).\nFor the NELS example, the R-squared statistic is:\n\n\nCode\nsummary(mod)$r.squared\n\n\n[1] 0.1012821\n\n\nPlease write down an interpretation of this number and be prepared to share your answer in class. Hint: Instead of talking about proportions, it is often helpful to multiply by 100 and talk about percentages instead."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-population-model-2",
    "href": "ch2_simple_regression.html#sec-population-model-2",
    "title": "2  Simple Regression",
    "section": "2.5 The population model",
    "text": "2.5 The population model\nUp to this point we have discussed simple linear regression as a way of describing the relationship between two variables in a sample. The next step is to discuss statistical inference. Recall that statistical inference involves generalizing from a sample to the population from which the sample was drawn.\nIn the NELS example, the population of interest is U.S. eighth graders in 1988. We want to be able to draw conclusions about that population based on the sample of eighth graders that participated in NELS. In order to do that, we make some statistical assumptions about the population, which are collectively referred to as the population model.\nThe population model for simple linear regression is summarized in Figure 2.4. The three assumptions associated with this model are written below. We talk about how to check the plausibility of these assumptions in ?sec-chapter-7.\n\n\n\n\n\nFigure 2.4: The Regression Population Model.\n\n\n\n\nThe three assumptions:\n\nNormality: The values of \\(Y\\) conditional on \\(X\\), denoted \\(Y|X\\), are normally distributed. The figure shows these distributions for three values of \\(X\\). We can write this assumption formally as\n\n\\[Y | X \\sim  N(\\mu_{Y | X} , \\sigma^2_{Y | X}) \\]\n\n(This notation should be familiar from EDUC 710. In general, we write \\(Y \\sim N(\\mu, \\sigma^2)\\) to denote that the variable \\(Y\\) has a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\).)\n\n\nHomoskedasticity: The conditional distributions have equal variances (also called “homogeneity of variance”, or simply “equal variances”).\n\n\\[ \\sigma^2_{Y| X} = \\sigma^2 \\]\n\nLinearity: The means of the conditional distributions are a linear function of \\(X\\).\n\n\\[ \\mu_{Y| Χ} = a + bX \\]\nThese three assumptions are summarized by writing\n\\[ Y|X \\sim N(a + bX, \\sigma^2). \\] Sometimes it will be easier to state the assumptions in terms of the population residuals, \\(\\epsilon = Y - \\mu_{Y|X}\\). The residuals have distribution \\(\\epsilon \\sim N(0, \\sigma^2)\\).\nSometimes it will also be easier to write the population regression line using expected values, \\(E(Y|X)\\), rather than \\(\\mu_{Y|X}\\). Both of these are interpreted the same way – they denote the mean of \\(Y\\) for a given value of \\(X\\).\nAn additional assumption is usually made about the data in the sample – that they were obtained as a simple random sample from the population. We will see some ways of dealing with other types of samples later on this course, but for now we can consider this a background assumption that applies to OLS regression.\nFrom a mathematical perspective, these assumptions are important because they can be used to prove (a) that OLS regression provides unbiased estimates of the population regression coefficients and (b) that the OLS estimates are more precise than any other unbiased estimates of the population regression coefficients. There are other variations on these assumptions, which are sometimes called the Gauss-Markov assumptions see https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem.\nFrom a practical perspective, these assumptions are important conditions that we should check when conducting data analyses. If the assumptions are violated – particularly the linearity assumption – then our statistical model may not be a good representation of the population. If the model is not a good representation of the population, then inferences based on the model may provide misleading conclusions about the population."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-notation-2",
    "href": "ch2_simple_regression.html#sec-notation-2",
    "title": "2  Simple Regression",
    "section": "2.6 Clarifying notation",
    "text": "2.6 Clarifying notation\nAt this point we have used the mathematical symbols for regression (e.g., \\(a\\), \\(b\\)) in two different ways:\n\nIn Section 2.2 they denoted sample statistics.\nIn Section 2.5 they denoted population parameters.\n\nThe population versus sample notation for regression is a bit of a hot mess, but the following conventions are used.\n\n\n\n\n\n\n\n\nConcept\nSample statistic\nPopulation parameter\n\n\n\n\nregression line\n\\(\\widehat Y\\)\n\\(\\mu_{Y|X}\\) or \\(E(Y|X)\\)\n\n\nslope\n\\(\\widehat b\\)\n\\(b\\)\n\n\nintercept\n\\(\\widehat a\\)\n\\(a\\)\n\n\nresidual\n\\(e\\)\n\\(\\epsilon\\)\n\n\nvariance explained\n\\(\\widehat R^2\\)\n\\(R^2\\)\n\n\n\nThe “hats” always denote sample quantities, and the Greek letters always denote population quantities, but there is some lack of consistency. For example, why not use \\(\\beta\\) instead of \\(b\\) for the population slope? Well, \\(\\beta\\) is conventionally used to denote standardized regression coefficients in the sample, so its already taken (more on this in Chapter 4).\nIf it is clear from context that we are talking about the sample rather than the population, then the hats are usually omitted from the statistics \\(\\widehat a\\), \\(\\widehat b\\), and \\(\\widehat R^2\\). This doesn’t apply to \\(\\widehat Y\\), because the hat is required to distinguish the predicted values from the data points.\nAnother thing to note is that while \\(\\widehat Y\\) is often called a predicted value, \\(E(Y|X)\\) is not usually referred to this way. It is called the conditional mean function or the conditional expectation function. Using this language, we can say that regression is about estimating the conditional mean function.\nPlease be prepared for a pop quiz on notation during class!"
  },
  {
    "objectID": "ch2_simple_regression.html#sec-inference-2",
    "href": "ch2_simple_regression.html#sec-inference-2",
    "title": "2  Simple Regression",
    "section": "2.7 Inference",
    "text": "2.7 Inference\nThis section reviews the main inferential procedures for regression. The formulas presented in this section are used to produce standard errors, t-tests, p-values, and confidence intervals for the regression coefficients, as well as an F-test for R-squared. It is very unlikely that you will ever need to compute these formulas by hand, so don’t worry about memorizing them.\nHowever, it is important that you can interpret the numerical results in research settings. The interpretations of these procedures were reviewed in Chapter 1 and should be familiar from EDUC 710. This sections documents the formulas for simple regression and then asks you to interpret the results in the context of the NELS example.\nIt is worth noting that the regression intercept is often not of interest in simple regression. Recall that the intercept is the value of \\(\\widehat Y\\) when \\(X = 0\\). So, unless we have a hypothesis or research question about this particular value of \\(X\\) (e.g., eighth graders with \\(SES = 0\\)), we won’t be interested in a test of the regression intercept. When we get into to multiple regression, we will see some situations where the intercept is of interest.\n\n2.7.1 Inference for coefficients\nWhen the population model is true, \\(\\widehat b\\) is an unbiased estimate of \\(b\\) (in symbols: \\(E(\\hat b) = b\\)). The standard error of \\(\\widehat b\\) is equal to (see (fox-2016?), Section 6.1):\n\\[ SE(\\widehat b) = \\frac{s_Y}{s_X} \\sqrt{\\frac{1-R^2}{N-2}} . \\] Using these two results, we can compute t-tests and confidence intervals for the regression slope in the usual way.\nt-tests\nThe null hypothesis \\(H_0: \\widehat b = b_0\\) can be tested against the alternative \\(H_A: \\widehat b \\neq b_0\\) using the test statistic:\n\\[ t = \\frac{\\widehat b - b_0}{SE(\\widehat b)}, \\]\nwhich has a t-distribution on \\(N-2\\) degrees of freedom when the null hypothesis is true.\nThe test assumes that the population model is correct. The null hypothesized value of the parameter is usually chosen to be \\(b_0 = 0\\), in which case the test is interpreted in terms of the “statistical significance” of the regression slope.\nConfidence intervals\nFor a given Type I Error rate, \\(\\alpha\\), the corresponding \\((1-\\alpha) \\times 100\\%\\) confidence interval is\n\\[ b_0 = \\widehat b \\pm t_{\\alpha/2} \\times SE(\\widehat b), \\]\nwhere \\(t_{\\alpha/2}\\) denotes the \\(\\alpha/2\\) quantile of the \\(t\\)-distribution with \\(N-2\\) degrees of freedom. For example, if \\(\\alpha\\) is chosen to be \\(.05\\), the corresponding \\(95\\%\\) confidence interval uses \\(t_{.025}\\), or the 2.5-th percentile of the t-distribution.\nThe standard error for the regression intercept, presented below, can be used to compute t-tests and confidence intervals for \\(\\hat a\\):\n\\[\nSE(\\widehat a) = \\sqrt{\\frac{SS_{\\text{res}}}{N-2} \\left(\\frac{1}{N} + \\frac{\\bar X^2}{(N-1)s^2_X}\\right)}.\n\\]\n\n\n2.7.2 Inference for R-squared\nThe null hypothesis \\(H_0: R^2 = 0\\) can be tested against the alternative \\(H_A: R^2 \\neq 0\\) using the F-test:\n\\[ F = (N-2) \\frac{\\widehat R^2}{1-\\widehat R^2}, \\]\nwhich has a F-distribution on \\(1\\) and \\(N – 2\\) degrees of freedom when the null is true. The test assumes that the population model is true. Confidence intervals for R-squared are generally not reported.\n\n\n2.7.3 The NELS example\nFor the NELS example, the standard errors, t-test, and p-values of the regression coefficients are shown in the table below (along with the OLS estimates). The F-test appears in the text below the table. Note that the output uses the terminology “multiple R-squared” to refer to R-squared.\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = achmat08 ~ ses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5995  -6.5519  -0.1475   6.0226  27.6634 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  48.6780     1.1282  43.147  &lt; 2e-16 ***\nses           0.4293     0.0573   7.492 3.13e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.863 on 498 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09948 \nF-statistic: 56.12 on 1 and 498 DF,  p-value: 3.127e-13\n\n\nThe \\(95\\%\\) confidence intervals for the regression coefficients are:\n\n\nCode\nconfint(mod)\n\n\n                 2.5 %     97.5 %\n(Intercept) 46.4614556 50.8946120\nses          0.3166816  0.5418392\n\n\nPlease write down your interpretation of the t-tests, confidence intervals, and F-test, and be prepared to share your answers in class. Hint: state whether the tests are statistically significant and the corresponding conclusions are about the population parameters. For confidence intervals, report the range of values we are “confident” about. For practice, you might want to try using APA notation in your answer (or whatever style conventions are used in your field)."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-power-2",
    "href": "ch2_simple_regression.html#sec-power-2",
    "title": "2  Simple Regression",
    "section": "2.8 Power analysis*",
    "text": "2.8 Power analysis*\nStatistical power is the probability of rejecting the null hypothesis, when it is indeed false. Rejecting the null hypothesis when it is false is sometimes called a “true positive”, meaning we have correctly inferred that a parameter of interest is not zero. Power analysis is useful for designing studies so that the statistical power / true positive rate is satisfactory.\nIn practice, statistical power comes down to having a large enough sample size. Consequently, power analysis is important when planning studies (e.g., in research grants proposals). In this class, we will not be planning any studies – rather, we will be working with secondary data analyses. In this context, power analysis is not very interesting, and so we do not mention it much. Nonetheless, power analysis is important for research and so we review the basics here.\nPower analysis in regression is very similar to power analysis for the tests we studied last semester. There are five ingredients that go into a power analysis:\n\nThe desired Type I Error rate, \\(\\alpha\\).\nThe desired level of statistical power.\nThe sample size, \\(N\\).\nThe number of predictors.\nThe effect size, which is Cohen’s f-squared statistic (AKA the signal to noise ratio):\n\n\\[ f^2 = {\\frac{R^2}{1-R^2}}. \\]\nIn principal, we can plug-in values for any four of these ingredients and then solve for the fifth. But, as mentioned, power analysis is most useful when we solve for \\(N\\) while planning a study. When solving for \\(N\\) “prospectively,” the effect size \\(f^2\\) should be based on reports of R-squared in past research. Power and \\(\\alpha\\) are usually chosen to be .8 and .05, respectively.\nThe example below shows the sample size required to detect an effect size of \\(R^2 = .1\\). This effect size was based on the NELS example discussed above. Note that the values \\(u\\) and \\(v\\) denote the degrees of freedom in the numerator and denominator of the F-test of R-squared, respectively. The former provides information about the number of predictors in the model, the latter about sample size.\n\n\nCode\n# Install package\n# install.packages(\"pwr\")\n\n# Load library\nlibrary(pwr)\n\n# Run power analysis\npwr.f2.test(u = 1, f2 = .1 / (1 - .1), sig.level = .05, power = .8)\n\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 70.61137\n             f2 = 0.1111111\n      sig.level = 0.05\n          power = 0.8\n\n\nRounding up, we would require 72 persons in the sample in order to have an 80% chance of detecting an effect size of \\(R^2 = .1\\) with simple regression.\nAnother use of power analysis is to solve for the effect size. This can be useful when the sample size is constrained by external factors (e.g., budget). In this situation, we can use power analysis to address whether the sample size is sufficient to detect an effect that is “reasonable” (again, based on past research). In the NELS example, we have \\(N=500\\) observations. The output below reports the smallest effect size we can detect with a power of \\(.8\\) and \\(\\alpha = .05\\). This is sometimes called the “minimum detectable effect size” (MDES).\n\n\nCode\npwr.f2.test(u = 1, v = 498, sig.level = .05, power = .8)\n\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 498\n             f2 = 0.01575443\n      sig.level = 0.05\n          power = 0.8\n\n\nWith a sample size of 500, and power of 80%, the MDES for simple regression is \\(R^2 = f^2 / (1 + f^2) \\approx .03\\). Based on this calculation, we can conclude that this sample size is sufficient for applications of simple linear regression in which we expect to explain at least 3% of the variance in the outcome."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-properties-2",
    "href": "ch2_simple_regression.html#sec-properties-2",
    "title": "2  Simple Regression",
    "section": "2.9 Properties of OLS*",
    "text": "2.9 Properties of OLS*\nThis section summarizes some properties of OLS regression that will be used later on. You can skip this section if you aren’t interested in the math behind regression – the results will be mentioned again when needed.\nThe derivations in this section follow from the three rules of summation reviewed Section 1.2 and make use of the properties of means, variances, and covariances already derived in Section 1.4. If you have any questions about the derivations, I would be happy to address them in class during open lab time.\n\n2.9.1 Residuals\nWe start with two important implications of Equation 2.4 for the OLS residuals. In particular, OLS residuals always have mean zero and are uncorrelated with the predictor variable. These properties generalize to multiple regression.\nFirst we show that\n\\[\\text{mean} (e) = 0. \\tag{2.5}\\]\nFrom Equation 2.4 we have\n\\[a = \\bar Y - b \\bar X \\] Solving for \\(\\bar Y\\) gives\n\\[\\bar Y = a + b \\bar X. \\] Since \\(\\hat Y\\) is a linear transformation of \\(X\\), we know from Section 1.4 that\n\\[ \\text{mean} (\\hat Y) = a + b \\bar X. \\]\nThe previous two equations imply that \\(\\text{mean} (\\hat Y) = \\bar Y\\). Consequently,\n\\[\\text{mean}(e) = \\text{mean}(Y - \\hat Y) = \\text{mean}(Y) - \\text{mean}(\\hat Y) = \\bar Y - \\bar Y = 0\\]\nNext we show that\n\\[\\text{cov}(X, e) = 0.  \\tag{2.6}\\]\nThe derivation is:\n\\[\\begin{align}\n\\text{cov}(X, e) & = \\text{cov}(X, Y - \\hat Y) \\\\\n& = \\text{cov}(X, Y)  - \\text{cov}(X, \\hat Y) \\\\\n& = \\text{cov}(X, Y)  - \\text{cov}(X, a + b X) \\\\\n& = \\text{cov}(X, Y)  - b \\, \\text{var}(X) \\\\\n& = \\text{cov}(X, Y)  - \\left(\\frac{\\text{cov}(X, Y)} {\\text{var}(X)} \\right) \\text{var}(X) \\\\\n& = 0\n\\end{align}\\]\nThe second last line uses the expression for the slope in Equation 2.4.\n\n\n2.9.2 Multiple correlation (\\(R\\))\nAbove we defined \\(R^2\\) as a proportion of variance. This was a bit lazy. Instead, we can start with the definition of the multiple correlation\n\\[R = \\text{cor}(Y, \\hat Y)\\]\nand from this definition derive the result, shown above, that \\(R^2\\) is the proportion of variance in \\(Y\\) associated with \\(\\hat Y\\).\nLet’s start by showing that\n\\[\\text{cov}(Y, \\hat Y) = \\text{var}(\\hat Y) \\tag{2.7}\\]\nBefore deriving this result, note that Equation 2.4 implies\n\\[\\text{cov}(X, Y) = b \\, \\text{var}(X),\\]\nand, using the the variance of a linear transformation (Section 1.4), we have\n\\[ \\text{var} (\\hat Y) = \\text{var}(a + b X) = b^2 \\text{var}(X). \\]\nThese two results are used on the third and fourth lines of the following derivation, respectively.\n\\[\\begin{align}\n\\text{cov}(Y, \\hat Y)  & = \\text{cov}(Y, a + b X) \\\\\n& = b \\,\\text{cov}(Y, X) \\\\\n& = b^2 \\,\\text{var}(X) \\\\\n& = \\text{var}(\\hat Y) \\\\\n\\end{align}\\]\nNext we show that \\(R^2 = \\text{var}(\\hat Y) / \\text{var}(Y)\\):\n\\[\\begin{align}\nR^2 & = [\\text{cor}(Y, \\hat Y)]^2 \\\\\n& = \\frac{[\\text{cov}(Y, \\hat Y)]^2}{\\text{var}(Y) \\; \\text{var}(\\hat Y)} \\\\\n& = \\frac{[\\text{var}(\\hat Y)]^2}{\\text{var}(Y) \\; \\text{var}(\\hat Y)} \\\\\n& = \\frac{\\text{var}(\\hat Y)}{\\text{var}(Y)}. \\\\\n\\end{align}\\]\nThis derivation is nicer than the one in Section 2.4 because it obtains a result about \\(R^2\\) using the definition of \\(R\\). However, this derivation does not show that the resulting ratio is a proportion, which requires a second step (which also uses Equation 2.7):\n\\[\\begin{align}\n\\text{var}(e) & = \\text{var}(Y - \\hat Y) \\\\\n& = \\text{var}(Y) + \\text{var}(\\hat Y) - 2 \\text{cov}(Y, \\hat Y) \\\\\n& = \\text{var}(Y) + \\text{var}(\\hat Y) - 2 \\text{var}(\\hat Y) \\\\\n& = \\text{var}(Y) - \\text{var}(\\hat Y).\n\\end{align}\\]\nRe-arranging gives\n\\[ \\text{var}(Y) =  \\text{var}(\\hat Y) + \\text{var}(e),\\] which shows that \\(0 \\leq \\text{var}(\\hat Y) \\leq \\text{var}(Y).\\)\nOne last detail concerns the relation between the multiple correlation \\(R\\) and the regular correlation coefficient \\(r = \\text{cor}(Y, X)\\). Using the invariance of the correlation under linear transformation (Section 1.4), we have\n\\[ R = \\text{cor}(Y, \\widehat Y) = \\text{cor}(Y, a + bX) = \\text{cor}(Y, X) = r\\]\nConsequently, in simple regression, \\(R^2 = r^2\\) – i.e., the proportion of variance explained by the predictor is just the squared Pearson product-moment correlation. When we add multiple predictors, this relationship between \\(R^2\\) and \\(r^2\\) no longer holds."
  },
  {
    "objectID": "ch2_simple_regression.html#workbook-2",
    "href": "ch2_simple_regression.html#workbook-2",
    "title": "2  Simple Regression",
    "section": "2.10 Workbook",
    "text": "2.10 Workbook\nThis section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in Section 2.11. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 2.1\n\n\nCode\n# Scatter plot\nplot(x = ses, \n     y = achmat08, \n     col = \"#4B9CD3\", \n     ylab = \"Math Achievement (Grade 8)\", \n     xlab = \"SES\")\n\n# Run the regression model\nmod &lt;- lm(achmat08 ~ ses)\n\n# Add the regression line to the plot\nabline(mod) \n\n\n\n\n\nMath Achievement and SES (NELS88).\n\n\n\n\nThe strength and direction of the linear relationship between the two variables is summarized by their correlation. In this sample, the value of the correlation is:\n\n\nCode\ncor(achmat08, ses)\n\n\n[1] 0.3182484\n\n\nThis correlation means that eighth graders from more well-off families (higher SES) also tended to do better in Math (higher Math Achievement). This relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., https://www.apa.org/pi/ses/resources/publications/education). Please look over this web page and be prepared to share your thoughts about this relationship.\nSection 2.3\nFor the NELS example, the regression intercept and slope are, respectively:\n\n\nCode\ncoef(mod)\n\n\n(Intercept)         ses \n 48.6780338   0.4292604 \n\n\nPlease write down an interpretation of these numbers and be prepared to share your answers in class. How would your interpretation change if, rather than the value of the slope shown above, we had \\(b = 0\\)?\nSection 2.4\nFor the NELS example, the R-squared statistic is:\n\n\nCode\nsummary(mod)$r.squared\n\n\n[1] 0.1012821\n\n\nPlease write down an interpretation of this number and be prepared to share your answer in class. Hint: Instead of talking about proportions, it is often helpful to multiply by 100 and talk about percentages instead.\nSection 2.6\nPlease be prepared for a pop quiz on notation during class!\n\n\n\nConcept\nSample statistic\nPopulation parameter\n\n\n\n\nregression line\n\n\n\n\nslope\n\n\n\n\nintercept\n\n\n\n\nresidual\n\n\n\n\nvariance explained\n\n\n\n\n\nSection 2.7\nFor the NELS example, the standard errors, t-test, and p-values of the regression coefficients are shown in the table below (along with the OLS estimates). The F-test appears in the text below the table. Note that the output uses the terminology “multiple R-squared” to refer to R-squared.\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = achmat08 ~ ses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5995  -6.5519  -0.1475   6.0226  27.6634 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  48.6780     1.1282  43.147  &lt; 2e-16 ***\nses           0.4293     0.0573   7.492 3.13e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.863 on 498 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09948 \nF-statistic: 56.12 on 1 and 498 DF,  p-value: 3.127e-13\n\n\nThe \\(95\\%\\) confidence intervals for the regression coefficients are:\n\n\nCode\nconfint(mod)\n\n\n                 2.5 %     97.5 %\n(Intercept) 46.4614556 50.8946120\nses          0.3166816  0.5418392\n\n\nPlease write down your interpretation of the t-tests, confidence intervals, and F-test, and be prepared to share your answers in class. Hint: state whether the tests are statistically significant and the corresponding conclusions are about the population parameters. For confidence intervals, report the range of values we are “confident” about. For practice, you might want to try using APA notation in your answer (or whatever style conventions are used in your field)."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-exercises-2",
    "href": "ch2_simple_regression.html#sec-exercises-2",
    "title": "2  Simple Regression",
    "section": "2.11 Exercises",
    "text": "2.11 Exercises\nThese exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)\nBefore staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\n\n2.11.1 The lm function\nThe functionlm, short for “linear model”, is used to estimate linear regressions using OLS. It also provides a lot of useful output.\nThe main argument that the we provides to the lm function is a formula. For the simple regression of Y on X, a formula has the syntax:\nY ~ X\nHere Y denotes the outcome variable and X is the predictor variable. The tilde ~ just means “equals”, but the equals sign = is already used to for other stuff in R, so ~ is used instead. We will see more complicated formulas as we go through the course. For more information on R’s formula syntax, see help(formula).\nLet’s take a closer look using the following two variables from the NELS data.\n\nachmat08: eighth grade math achievement (percent correct on a math test)\nses: a composite measure of socio-economic status, on a scale from 0-35\n\n\n\nCode\n# Load the data. Note that you can click on the .RData file and RStudio will load it\n# load(\"NELS.RData\") #Un-comment this line to run\n\n# Attach the data: will discuss this in class\n# attach(NELS) #Un-comment this line to run!\n\n# Scatter plot of math achievement against SES\nplot(x = ses, y = achmat08, col = \"#4B9CD3\")\n\n# Regress math achievement on SES; save output as \"mod\"\nmod &lt;- lm(achmat08 ~ ses)\n\n# Add the regression line to the plot\nabline(mod)\n\n\n\n\n\nCode\n# Print the regression coefficients\ncoef(mod)\n\n\n(Intercept)         ses \n 48.6780338   0.4292604 \n\n\nLet’s do some quick calculations to check that the lm output corresponds the formulas for the slope and intercept in Section 2.3:\n\\[ a = \\bar Y - b \\bar X \\quad \\text{and} \\quad b = \\frac{\\text{cov}(X, Y)}{\\text{var}(X)}. \\]\nWe won’t usually do this kind of “manual” calculation, but it is a good way consolidate knowledge presented in the readings with the output presented by R. It is also useful to refresh our memory about some useful R functions and how the R language works.\n\n\nCode\n# Compute the slope as the covariance divided by the variance of X\ncov_xy &lt;- cov(achmat08, ses)\nvar_x &lt;- var(ses)\nb &lt;- cov_xy / var_x\n\n# Compare the \"manual\" calculation to the output from lm. \nb\n\n\n[1] 0.4292604\n\n\nCode\n# Compute the y-intercept using from the two means and the slope\nxbar &lt;- mean(ses)\nybar &lt;- mean(achmat08)\n\na &lt;- ybar - b * xbar\n\n# Compare the \"manual\" calculation to the output from lm. \na\n\n\n[1] 48.67803\n\n\nLet’s also check our interpretation of the parameters. If the answers to these questions are not clear, please make sure to ask in class!\n\nWhat is the predicted value of achmat08 when ses is equal to zero?\nHow much does the predicted value of achmat08 increase for each unit of increase in ses?\n\n\n\n2.11.2 Variance explained\nAnother way to describe the relationship between the two variables is by considering the amount of variation in \\(Y\\) that is associated with (or explained by) its relationship with \\(X\\). Recall that one way to do this is via the “variance” decomposition\n\\[ SS_{\\text{total}} = SS_{\\text{res}} + SS_{\\text{reg}}\\]\nfrom which we can compute the proportion of variation in Y that is associated with the regression model:\n\\[R^2 = \\frac{SS_{\\text{reg}}}{SS_{\\text{total}}}.\\]\nThe R-squared for the example is presented in the output below. You should be able to provide an interpretation of this number, so if it’s not clear make sure to ask in class!\n\n\nCode\n# R-squared from the example\nsummary(mod)$r.squared\n\n\n[1] 0.1012821\n\n\nAs above, let’s compute \\(R^2\\) “by hand” for our example.\n\n\nCode\n# Compute the sums of squares\nybar &lt;- mean(achmat08)\nss_total &lt;- sum((achmat08 - ybar)^2)\nss_reg &lt;- sum((yhat - ybar)^2)\nss_res &lt;-  sum((achmat08 - yhat)^2)\n\n# Check that SS_total = SS_reg + SS_res\nss_total\n\n\n[1] 43526.91\n\n\nCode\nss_reg + ss_res\n\n\n[1] 43526.91\n\n\nCode\n# Compute R-squared (compare to value from lm)\nss_reg/ss_total\n\n\n[1] 0.1012821\n\n\nCode\n# Also check that R-squared is really equal to the square of the PPMC\ncor(achmat08, ses)^2\n\n\n[1] 0.1012821\n\n\n\n\n2.11.3 Predicted values and residuals\nThe lm function returns the predicted values \\(\\widehat{Y_i}\\) and residuals \\(e_i\\) and which we can access using the $ operator. These are useful for various reasons, especially model diagnostics, which we discuss later in the course. For now, lets just take a look at the residual vs fitted plot to illustrate the code.\n\n\nCode\nyhat &lt;- mod$fitted.values\nres &lt;- mod$resid\n\nplot(yhat, res, col = \"#4B9CD3\")\n\n\n\n\n\nAlso note that the residuals values have mean zero and are uncorrelated with the predictor – this is always the case in OLS (See Section 2.9})\nmean(res)\ncor(yhat, res)\n\n\n2.11.4 Inference\nNext let’s address statistical inference, or how we can make conclusions about a population based on a sample from that population.\nWe can use the summary function to test the coefficients in our model.\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = achmat08 ~ ses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5995  -6.5519  -0.1475   6.0226  27.6634 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  48.6780     1.1282  43.147  &lt; 2e-16 ***\nses           0.4293     0.0573   7.492 3.13e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.863 on 498 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09948 \nF-statistic: 56.12 on 1 and 498 DF,  p-value: 3.127e-13\n\n\nIn the table, the t-test and p-values are for the null hypothesis that the corresponding coefficient is zero in the population. We can see that the intercept and slope are both significantly different from zero at the .05 level. However, the test of the intercept is not very meaningful (why?).\nThe text below the table summarizes the output for R-squared, including its F-test, it’s degrees of freedom, and the p-value. (We will talk about adjusted R-square in Chapter 4)\nWe can also use the confint function to obtain confidence intervals for the regression coefficients. Use help to find out more about the confint function.\n\n\nCode\nconfint(mod)\n\n\n                 2.5 %     97.5 %\n(Intercept) 46.4614556 50.8946120\nses          0.3166816  0.5418392\n\n\nBe sure to remember the correct interpretation of confidence intervals: there is a 95% chance that the interval includes the true parameter value (not: there is a 95% chance that the parameter falls in the interval). For example, there is a 95% chance that the interval [.32, .54] includes the true regression coefficient for SES.\n\n\n2.11.5 Writing up results\nWe could write up the results from this analysis in APA format as follows. You should practice doing this kind of thing, because it is important to be able to write up the results of your analyses in a way that people in your area of research will understand.\nIn this analysis, we considered the relationship between Math Achievement in Grade 8 (percent correct on a math test) and SES (a composite on a scale from \\(0-35\\)). Regressing Math Achievement on SES, the relationship was positive and statistically significant at the \\(.05\\) level (\\(b = 0.43\\), \\(t(498) = 7.49\\), \\(p &lt; .001\\), \\(95\\% \\text{ CI: } [0.32, 0.54]\\)). SES explained about \\(10\\%\\) of the variation in Math Achievement (\\(R^2 = .10\\), \\(F(1, 498) = 56.12\\), \\(p &lt; .001\\)).\n\n\n2.11.6 Additional exercises\nIf time permits, we will address these additional exercises in class.\nThese exercises replace achmat08 with\n\nachrdg08: eighth grade Reading Achievement (percent correct on a reading test)\n\nPlease answer the following questions using R.\n\nPlot achrdg08 against ses.\nWhat is the correlation between achrdg08 and ses? How does it compare to the correlation with Math and SES?\nHow much variation in Reading is explained by SES? Is the proportion of variance explained significant at the .05 level?\nHow much do predicted Reading scores increase for a one unit of increase in SES? Is this a statistically significant at the .05 level?\nWhat are your overall conclusions about the relationship between Academic Achievement and SES in the NELS data? Write up your results using APA formatting or whatever conventions are used in your area of research."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-interpretations-3",
    "href": "ch3_two_predictors.html#sec-interpretations-3",
    "title": "3  Two predictors",
    "section": "3.1 Interpretations",
    "text": "3.1 Interpretations\nMultiple regression has three main interpretations:\n\nPrediction (focus on \\(\\hat Y\\))\nCausation (focus on \\(b\\))\nExplanation (focus on \\(R^2\\))\n\nBy understanding these interpretations, we will have a better idea of how multiple regression is used in research. Each interpretation also provides a different perspective on the importance of using multiple predictor variables, rather than only a single predictor.\n\n3.1.1 Prediction\nPrediction was the original use of regression (https://en.wikipedia.org/wiki/Regression_toward_the_mean#History). In the context of simple regression, prediction means using observations of \\(X\\) to make a guess about yet unobserved values of \\(Y\\). Our guess is \\(\\hat Y\\), and this is why \\(\\hat Y\\) is called the “predicted value” of \\(Y\\).\nWhen making predictions, we usually want some additional information about how precise the predictions are. In OLS regression, this information is provided by the standard error of prediction (fox-2016?):\n\\[\\text{SE}({\\hat Y_i}) = \\sqrt{\\frac{SS_{\\text{res}}}{N - 2} \\left(1 +  \\frac{1}{N} + \\frac{(X_i - \\bar X)^2}{\\sum_j(X_j - \\bar X)^2} \\right)} \\tag{3.1}\\]\nThis statistic quantifies our uncertainty when making predictions based on observations of \\(X\\) that were not in our original sample. The prediction errors for the NELS example in Chapter 2 are represented in Figure 3.1 as a gray band around the regression line.\n\n\nCode\n# Plotting library\nlibrary(ggplot2)\n\n# Load data\nload(\"NELS.RData\")\n\n# Run regression \nmod &lt;- lm(achmat08 ~ ses, data = NELS)\n\n# Compute SE(Y-hat)\nn &lt;- nrow(NELS)\nms_res &lt;- var(mod$residuals) * (n-1) / (n-2)\nd_ses &lt;- NELS$ses - mean(NELS$ses) \nse_yhat &lt;- sqrt(ms_res * (1 + 1/n + d_ses^2 / sum(d_ses^2)))\n\n# Plotting\ngg_data &lt;- data.frame(\n             achmat08 = NELS$achmat08,\n             ses = NELS$ses,\n             y_hat = mod$fitted.values,\n             lwr = mod$fitted.values - 1.96 * se_yhat,\n             upr = mod$fitted.values + 1.96 * se_yhat)\n\nggplot(gg_data, aes(x = ses, y = achmat08))+\n    geom_point(color='#3B9CD3', size = 2) +\n    geom_line(aes(x = ses, y = y_hat), color = \"grey35\") +\n    geom_ribbon(aes(ymin=lwr,ymax=upr),alpha=0.3) + \n    ylab(\"Math Achievement (Grade 8)\") +\n    xlab(\"SES\") +\n    theme_bw()\n\n\n\n\n\nFigure 3.1: Prediction Error for NELS Example.\n\n\n\n\nWe can see in the figure that the error band is quite wide. So, we might wonder how to make our predictions more precise. On way to do this is by including more predictors in the regression model – i.e., multiple regression.\nTo see why including more predictors improves the precision of predictions, note that the standard error of prediction shown in Equation 3.1 increases with \\(SS_{\\text{res}}\\), which is the variation in the outcome that is not explained by the predictor (see Section 2.4). In most situations, \\(SS_{\\text{res}}\\) is the largest contributor the prediction error. As we will see below, one way to reduce \\(SS_{\\text{res}}\\) is by adding more predictors to the model.\n\n3.1.1.1 More about prediction\nRegression got its name from a statistical property of predicted scores called “regression toward the mean.” To explain this property, let’s assume \\(Y\\) and \\(X\\) are z-scores (i.e., both variables have \\(M = 0\\) and \\(SD = 1\\)). Recall that this implies that \\(a = 0\\) and \\(b = r_{XY}\\), so the regression equation reduces to\n\\[\\hat Y = r_{XY} X\\]\nSince \\(|r_{XY} | ≤ 1\\), the absolute value of the \\(\\hat Y\\) must be less than or equal to that of \\(X\\). And, since both variables have \\(M = 0\\), this implies that \\(\\hat Y\\) is closer to the mean of \\(Y\\) than \\(X\\) is to the mean of \\(X\\). This is sometimes called regression toward the mean.\nAlthough prediction was the original use of regression, many research problems do not involve prediction. For instance, there are no students in the NELS data for whom we need to predict Math Achievement – all of the test scores are already in the data! However, there has been a resurgence of interest in prediction in recent years, especially in machine learning. Although the methods used in machine learning are often more complicated than OLS regression, the basic problem is the same. Because the models are more complicated, theoretical results like Equation 3.1 are more difficult to obtain. Consequently, machine learning uses data-driven procedures like cross-validation to evaluate model predictions. As one example, we could evaluate the accuracy and precision of out-of-sample predictions by splitting our data into two samples, fitting the model in one sample (the “training data”), and then making predictions in the other sample (the “test data”). Equation 3.1 is a theoretical result saves us the trouble of doing this with OLS. Machine learning has also introduced some new techniques for choosing which predictors to include in a model (“variable selection” methods like the lasso). We will touch on these topics later in the course when we get to model building.\n\n\n\n3.1.2 Causation\nA causal interpretation of regression means that that changing \\(X\\) by one unit will change \\(E(Y|X)\\) by \\(b\\) units. This is interpreted as a claim about the expected value of \\(Y\\) “in real life”, not simply a claim about the mechanics of the regression line. In terms of our example, a causal interpretation would state that improving students’ SES by one unit will, on average, cause Math Achievement to increase by about half a percentage point.\nThe gold standard for inferring causality is to randomly assign people to different treatment conditions. In a regression context, treatment is represented by the independent variable, or the \\(X\\) variable. While randomized experiments are possible in some settings, there are many types of variables that we cannot feasibly randomly assign (e.g., SES).\nThe concept of an omitted variable is used to describe what happens when we can’t (or don’t) randomly assign people to treatment conditions. An omitted variable is any variable that is correlated with both \\(Y\\) and \\(X\\). In our example, this would be any variable correlated with both Math Achievement and SES (e.g., School Quality). When we use random assignment, we ensure that \\(X\\) is uncorrelated with all pre-treatment variables – i.e., randomization ensure that there are no omitted variables. However, when we don’t use random assignment, our results may be subject to omitted variable bias.\nThe overall idea of omitted variable bias is the same as “correlation \\(\\neq\\) causation”. The take-home message is summarized in the following points, which are stated in terms of the our NELS example.\n\nAny variable that is correlated with Math Achievement and with SES is called an omitted variable. One example is School Quality. This is an omitted variable because we did not include it as a predictor in our simple regression model.\nThe problem is not just that we have an incomplete picture of how School Quality is related to Math Achievement.\nOmitted variable bias means that the predictor variable that was included in the model ends up having the wrong regression coefficient. Otherwise stated, the regression coefficient of SES is biased because we did not consider School Quality.\nIn order to mitigate omitted variable bias, we want to include plausible omitted variables in our regression models – i.e., multiple regression.\n\n\n3.1.2.1 Omitted variable bias*\nOmitted variable bias is nicely explained by Gelman and Hill (gelman-2007?), and a modified version of their discussion is provided below. We start by assuming a “true” regression model with two predictors. In the context of our example, this means that there is one other variable, in addition to SES, that is important for predicting Math Achievement. Of course, there are many predictors of Math Achievement (see Section Section 2.1), but we only need two to explain the problem of omitted variable bias.\nWrite the “true” model as:\n\\[\nY = a + b_1 X_1 + b_2 X_2 + \\epsilon\n\\tag{3.2}\\]\nwhere \\(X_1\\) is SES and \\(X_2\\) is any other variable that is correlated with both \\(Y\\) and \\(X_1\\) (e.g., School Quality).\nNext, imagine that instead of using the model in Equation 3.2, we analyze the data using the model with just SES, leading to the usual simple regression:\n\\[\n\\hat Y = a^* + b^*_1 X_1 + \\epsilon^*\n\\tag{3.3}\\]\nThe problem of omitted variable bias is that \\(b_1 \\neq b^*_1\\) – i.e., the regression coefficient in the true model is not the same as the regression coefficient in the model with only one predictor. This is perhaps surprising – leaving out School Quality gives us the wrong regression coefficient for SES!\nTo see why, start by writing \\(X_2\\) as a function of \\(X_1\\).\n\\[\nX_2 = \\alpha + \\beta X_1 + \\nu\n\\tag{3.4}\\]\nNext we use Equation 3.4 to substitute for \\(X_2\\) in Equation 3.2,\n\\[\\begin{align}\nY & = a + b_1 X_1 + b_2 X_2 + \\epsilon \\\\\n  & = a + b_1 X_1 + b_2 (\\alpha + \\beta X_1 + \\nu)  + \\epsilon \\\\\n  & = \\color{orange}{(a + \\alpha)} + \\color{green}{(b_1 + b_2\\beta)} X_1 + (e + \\nu) \\label{eq-3parm}\n\\end{align}\\]\nNotice that in the last line, \\(Y\\) is predicted using only \\(X_1\\), so it is equivalent to Equation 3.3. Based on this comparison, we can write\n\n\\(a^* = \\color{orange}{a + \\alpha}\\)\n\\(b^*_1 = \\color{green}{b_1 + b_2\\beta}\\)\n\\(\\epsilon^* = \\epsilon + \\nu\\)\n\nThe equation for \\(b^*_1\\) is what we are most interested in. It shows that the regression parameter in our one-parameter model (\\(b^*_1\\)) is not equal to the “true” regression parameter using both predictors (\\(b_1\\)).\nThis is what omitted variable bias means – leaving out \\(X_2\\) in Equation Equation 3.3 gives us the wrong regression parameter for \\(X_1\\). This is one of the main motivations for including more than one predictor variable in a regression model – i.e., to avoid omitted variable bias.\nNotice that there two special situations in which omitted variable bias is not a problem:\n\nWhen the two predictors are not related – i.e., \\(\\beta = 0\\).\nWhen the second predictor is not related to \\(Y\\) – i.e., \\(b_2 = 0\\).\n\n\n\n\n3.1.3 Explanation\nMany uses of regression fall somewhere between prediction and causation. We want to do more than just predict outcomes of interest, but we often don’t have a basis for making the strong assumptions required for a causal interpretation of regression coefficients. This grey area between prediction and causation can be referred to as explanation.\nIn terms of our example, we might want to explain why eighth graders differ in their Math Achievement. There are large number of potential reasons for individual difference in Math Achievement, such as\n\nStudent factors\n\nattendance\npast academic performance in Math\npast academic performance in other subjects (Question: why include this?)\n…\n\nSchool factors\n\ntheir ELA teacher\nthe school they attend\ntheir peers\n…\n\nHome factors\n\nSES\nmaternal education\npaternal education\nparental expectations\n…\n\n\nWhen the goal of an analysis is explanation, it usual to focus on the proportion of variation in the outcome variable that is explained by the predictors, i.e., R-squared (see Section 2.4). Later in the course we will see how to systematically study the variance explained by individual predictors, or blocks of several predictors (e.g., student factors).\nNote that even a long list of predictors such as that above leaves out potential omitted variables. While the addition of more predictors can help us explain more of the variation in Math Achievement, it is rarely the case that we can claim that all relevant variables have been included in the model."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-ecls-3",
    "href": "ch3_two_predictors.html#sec-ecls-3",
    "title": "3  Two predictors",
    "section": "3.2 An example from ECLS",
    "text": "3.2 An example from ECLS\nIn the remainder of this chapter we will consider a new example from the 1998 Early Childhood Longitudinal Study (ECLS; https://nces.ed.gov/ecls/). Below is a description of the data from the official NCES codebook (page 1-1 of https://nces.ed.gov/ecls/data/ECLSK_K8_Manual_part1.pdf):\nThe ECLS-K focuses on children’s early school experiences beginning with kindergarten and ending with eighth grade. It is a multisource, multimethod study that includes interviews with parents, the collection of data from principals and teachers, and student records abstracts, as well as direct child assessments. In the eighth-grade data collection, a student paper-and-pencil questionnaire was added. The ECLS-K was developed under the sponsorship of the U.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics (NCES). Westat conducted this study with assistance provided by Educational Testing Service (ETS) in Princeton, New Jersey.\nThe ECLS-K followed a nationally representative cohort of children from kindergarten into middle school. The base-year data were collected in the fall and spring of the 1998–99 school year when the sampled children were in kindergarten. A total of 21,260 kindergartners throughout the nation participated.\nThe subset of the ECLS-K data used in this class was obtained from the link below.\nhttp://routledgetextbooks.com/textbooks/_author/ware-9780415996006/data.php\nThe codebook for this subset of data is available on our course website. In this chapter, we will be using a even smaller subset of \\(N = 250\\) cases from the example data set (the ECLS250.RData data)\nWe focus on the following three variables.\n\nMath Achievement in the first semester of Kindergarten. This variable can be interpreted as the number of questions (out of 60) answered correctly on a math test. Don’t worry – the respondents in this study did not have to write a 60-question math test in the first semester of K! Students only answered a few of the questions and their scores were re-scaled to be out a total of 60 questions afterwards.\nSocioecomonic Status (SES), which is a composite of household factors (e.g., parental education, household income) ranging from 30-72.\nApproaches to Learning (ATL), which is a teacher-reported measure of behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), with higher values representing more positive responses, and ATL is scored as an unweighted average the six items.\n\n\n3.2.1 Correlation matrices\nAs was the case in simple regression, the correlation coefficient is a building block of multiple regression. So, we will start by examining the correlations in our example. We also introduce a new way of presenting correlations, the correlation matrix. The notation developed in this section will appear throughout the rest of the chapter.\nIn the scatter plots below, the panels are arranged in matrix format. The variables named in the diagonal panels correspond to the vertical (\\(Y\\)) axis in that row and the horizontal (\\(X\\)) axis in that column. For example, Math is in the first diagonal, so it is the variable on vertical axis in the first row and the horizontal axis in the first column. This can be a bit confusing at first, so take a moment to make sure you know which variable is on which axis in each plot. Also notice that plots below the diagonal are just mirror image of the plots above the diagonal.\n\n\nCode\nload(\"ECLS250.RData\")\nattach(ecls)\nexample_data &lt;- data.frame(c1rmscal, wksesl, t1learn)\nnames(example_data) &lt;- c(\"Math\", \"SES\", \"ATL\")\npairs(example_data , col = \"#4B9CD3\")\n\n\n\n\n\nFigure 3.2: ECLS Example Data.\n\n\n\n\nThe format of Figure 3.2 is the same as that of the correlation matrix among the variables, which is shown below.\n\n\nCode\ncor(example_data)\n\n\n          Math       SES       ATL\nMath 1.0000000 0.4384619 0.3977048\nSES  0.4384619 1.0000000 0.2877015\nATL  0.3977048 0.2877015 1.0000000\n\n\nAgain, notice that the entries below the diagonal are mirrored by the entries above the diagonal. We can see that SES and ATL have similar correlations with Math Achievement (0.4385 and 0.3977, respectively), and are also moderately correlated with each other (0.2877).\nIn order to represent the correlation matrix among a single outcome variable (\\(Y\\)) and two predictors (\\(X_1\\) and \\(X_2\\)) we will use the following notation:\n\\[\n\\begin{array}{c}\n\\text{var } Y \\\\ \\text{var } X_1 \\\\ \\text{var } X_2\n\\end{array}\n\\quad\n\\left[\n\\begin{array}{ccc}\n1       & r_{Y1}  & r_{Y2}  \\\\\nr_{1Y}  & 1       & r_{12}  \\\\\nr_{2Y}  & r_{21}  & 1\n\\end{array}\n\\right]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this notation, \\(r_{Y1} = \\text{cor}(Y,X_1)\\) is the correlation between \\(Y\\) and \\(X_1\\). Note that each correlation coefficient (“\\(r\\)”) has two subscripts that tell us which two variables are being correlated. For the outcome variable we use the subscript \\(Y\\), and for the two predictors we use the subscripts \\(1\\) and \\(2\\). The order of the predictors doesn’t matter but we use the subscripts to keep track of which is which. In our example, \\(X_1\\) is SES and \\(X_2\\) is ATL.\nAs with the numerical examples, the values below the diagonal mirror the values above the diagonal. So, we really just need the three correlations shown in the matrix below.\n\\[\n\\begin{array}{c}\n\\text{var } Y \\\\ \\text{var } X_1 \\\\ \\text{var } X_2\n\\end{array}\n\\quad\n\\left[\n\\begin{array}{ccc}\n-       & r_{Y1}  & r_{Y2}  \\\\\n-  & -       & r_{12}  \\\\\n-  & -  & -\n\\end{array}\n\\right]\n\\]\nThe three correlations are interpreted as follows:\n\n\\(r_{Y1}\\) - the correlation between the outcome (\\(Y\\)) and the first predictor (\\(X_1\\)).\n\\(r_{Y2}\\) - the correlation between the outcome (\\(Y\\)) and the second predictor (\\(X_2\\)).\n\\(r_{12}\\) - the correlation between the two predictors.\n\nIf you have questions about how scatter plots and correlations can be presented in matrix format, please write them down now and share them class."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-model-3",
    "href": "ch3_two_predictors.html#sec-model-3",
    "title": "3  Two predictors",
    "section": "3.3 The two-predictor model",
    "text": "3.3 The two-predictor model\nIn the ECLS example, we can think of Kindergarteners’ Math Achievement as the outcome variable, with SES and Approaches to Learning as potential predictors / explanatory variables. The multiple regression model for this example can be written as\n\\[\n\\widehat Y = b_0 + b_1 X_1 + b_2 X_2\n\\tag{3.5}\\]\nwhere\n\n\\(\\widehat Y\\) denotes the predicted Math Achievement\n\\(X_1 = \\;\\) SES and \\(X_2 = \\;\\) ATL (it doesn’t matter which predictor we denote as \\(1\\) or \\(2\\))\n\\(b_1\\) and \\(b_2\\) are the regression slopes\nThe intercept is denoted by \\(b_0\\) (rather than \\(a\\)).\n\nJust like simple regression, the residual for Equation 3.5 is defined as \\(e = Y - \\widehat Y\\) and the model can be equivalently written as \\(Y = \\widehat Y + e\\). Also, remember that you can write out the model using the variable names in place of \\(Y\\) and \\(X\\) if that helps keep track of all the notation. For example,\n\\[\nMATH = b_0 + b_1 SES + b_2 ATL + e.\n\\]\nAs mentioned in Chapter 2, feel free to use whatever notation works best for you.\nYou might be wondering, what is the added value of multiple regression compared to the correlation co-efficients reported in the previous section? Well, correlations only consider two-variables-at-a-time. Multiple regression let’s us further consider how the predictors work together to explain variation in the outcome, and to consider the relationship between each predictor and the outcome while holding the other predictors constant. In the context of our example, multiple regression let’s us address the following questions:\n\nHow much of variation in Math Achievement do both predictors explain together?\nWhat is the relationship between Math Achievement and ATL if we hold SES constant?\nSimilarly, what is the relationship between Math Achievement and SES if we hold ATL constant?\n\nNotice that this is different from simple regression – simple regression was just a repackaging of correlation, but multiple regression is something new."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-ols-3",
    "href": "ch3_two_predictors.html#sec-ols-3",
    "title": "3  Two predictors",
    "section": "3.4 OLS with two predictors",
    "text": "3.4 OLS with two predictors\nWe can estimate the parameters of the two-predictor regression model in Equation 3.5 model using same approach as for simple regression. We do this by choosing the values of \\(b_0, b_1, b_2\\) that minimize\n\\[SS_\\text{res} = \\sum_i e_i^2.\\]\nSolving the minimization problem (setting derivatives to zero) leads to the following equations for the regression coefficients. Remember, the subscript \\(1\\) denotes the first predictor and the subscript \\(2\\) denotes the second predictor – see Section 3.2 for notation. Also note that \\(s\\) represents standard deviations.\n\\[\\begin{align}\nb_0 & = \\bar Y - b_1 \\bar X_1 - b_2 \\bar X_2 \\\\ \\\\\nb_1 & = \\frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}} \\frac{s_Y}{s_1} \\\\ \\\\\nb_2 & = \\frac{r_{Y2} - r_{Y1} r_{12}}{1 - r^2_{12}} \\frac{s_Y}{s_2}\n\\end{align}\\]\nAs promised, these equations are more complicated than for simple regression :) The next section addresses the interpretation of the regression coefficients."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-interpretation-3",
    "href": "ch3_two_predictors.html#sec-interpretation-3",
    "title": "3  Two predictors",
    "section": "3.5 Interpreting the coefficients",
    "text": "3.5 Interpreting the coefficients\nAn important part of using multiple regression is getting the interpretation of the regression coefficients correct. The basic interpretation is that the slope for SES represents how much predicted Math Achievement changes for a one unit increase of SES, while holding ATL constant. (The same interpretation holds when switching the predictors.) The important difference with simple regression is the “holding the other predictor constant” part, so let’s dig into it.\n\n3.5.1 “Holding the other predictor constant”\nLet’s start with the regression model for the predicted values:\n\\[ \\widehat {MATH} = b_0 + b_1 SES + b_2 ATL\\]\nIf we increase \\(SES\\) by one unit and hold \\(ATL\\) constant, we get new predicted value (denoted with an asterisk):\n\\[\\widehat {MATH^*} = b_0 + b_1 (SES + 1) + b_2 ATL\\]\nThe difference between \\(\\widehat{MATH^*}\\) and \\(\\widehat{MATH}\\) is how much the predicted value changes for a one unit increase in SES, while holding ATL constant:\n\\[\\widehat{MATH^*} - \\widehat{MATH}  = b_1\\]\nThis why we interpret the regression coefficients in multiple regression differently than simple regression. In simple regression, the slope is just a re-scaled version of the correlation. In multiple regression, the slope of each predictor is interpreted in terms of the “effect” of that predictor, while holding the other predictor(s) constant. This is sometimes referred to as “ceteris paribus,” which is Latin for “with other conditions remaining the same.” So, we could say that multiple regression is a statistical way of making ceteris paribus arguments.\nAlso note that we can see in the equations for \\(\\widehat {MATH}\\) that the interpretation of the regression intercept is basically the same as for simple regression: it is the value of \\(\\widehat Y\\) when \\(X_1 = 0\\) and \\(X_2 = 0\\) (i.e., still not very interesting).\n\n\n3.5.2 “Controlling for the other predictor”\nAnother interpretation of the regression coefficients is in terms of the equations for \\(b_1\\) and \\(b_2\\) presented in Section 3.4. For example, the equation for \\(b_1\\) is\n\\[\\begin{equation}\nb_1 = \\frac{r_{Y1} - r_{Y2} \\color{red}{r_{12}}} {1 - \\color{red}{r^2_{12}}} \\frac{s_1}{s_Y}.\n\\end{equation}\\]\nThis is the same equation as from Section 3.4, but the correlation between the predictors is shown in red. Note that if the predictors are uncorrelated (i.e., \\(\\color{red}{r^2_{12}} = 0\\)) then\n\\[b_1 = r_{Y1} \\frac{s_1}{s_Y},\\]\nwhich is just the regression coefficient from simple regression (Section 2.3).\nIn general, the formulas for the regression coefficients in the two-predictor model are more complicated because they “control for” or “account for” the relationship between the predictors. In simple regression, we only had one predictor, so we didn’t need to account for how the predictors were related to each other.\nThe equations for the regression coefficients show that, if the predictors are uncorrelated, then doing a multiple regression is just the same thing as doing simple regression multiple times. However, most of the time our predictors will be correlated, and multiple regression “controls for” the relationship between the predictors when examining the relationship between each predictor and the outcome.\n\n\n3.5.3 The ECLS example\nBelow, the R output from the ECLS example is reported. Please provide a written explanation of the regression coefficients for SES and ATL. If you have questions about how to interpret the coefficients, please also note them now and be prepared to share them in class. Note that this question is not asking about whether the coefficients are significant – it is asking what the numbers in the first column of the Coefficients table mean.\n\n\nCode\n# Run the regression model and print output\nmod1 &lt;- lm(c1rmscal ~ wksesl + t1learn)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.5.4 Standardized coefficients\nOne question that arises in the interpretation of the example is the relative contribution of the two predictors to Kindergartener’s Math Achievement. In particular, the regression coefficient for ATL is 10 times larger than the regression coefficient for SES – does this mean that ATL is 10 times more important than SES?\nThe short answer is, “no.” ATL is on a scale of 1-4 whereas SES ranges from 30-72. In order to make the regression coefficients more comparable, we can standardize the \\(X\\) variables so that they have the same variance. Many researchers go a step further and standardize all of the variables \\(Y, X_1, X_2\\) to be z-scores with M = 0 and SD = 1. The resulting regression coefficients are often called \\(\\beta\\)-coefficients or \\(\\beta\\)-weights (\\(\\beta\\) is pronounced “beta”).\nThe \\(\\beta\\)-weights are related to the regular regression coefficients from Section 3.4:\n\\[\\beta_1 = b_1 \\frac{s_1}{s_Y} = \\frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}}\\] A similar expression holds for \\(\\beta_2\\).\nNote that the lm function in R does not provide an option to report standardized output. So, if you want to get the \\(\\beta\\)-coefficients in R, it’s easiest to just standardized the variables first and then do the regression with the standardized variables.\nRegardless of how you compute them, the interpretation of the \\(\\beta\\)-coefficients is in terms of the standard deviation units of both the \\(Y\\) variable and the \\(X\\) variable – e.g., increasing \\(X_1\\) by one standard deviation changes \\(\\hat Y\\) by \\(\\beta_1\\) standard deviations (holding the other predictors constant).\n\n\nCode\n# Unlike other software, R doesn't have a convenience functions for beta coefficients. \nz_example_data &lt;- as.data.frame(scale(example_data))\nz_mod &lt;- lm(Math ~ SES  + ATL, data = z_example_data)\nsummary(z_mod)\n\n\n\nCall:\nlm(formula = Math ~ SES + ATL, data = z_example_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9590 -0.5604 -0.1493  0.4569  4.1043 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.337e-15  5.416e-02   0.000        1    \nSES         3.533e-01  5.666e-02   6.235 1.94e-09 ***\nATL         2.961e-01  5.666e-02   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8563 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\nWe should be careful when using beta-coefficients to “ease” the comparison of predictors. In the context of our example, we might wonder whether the overall cost of raising a child’s Approaches to Learning by 1 SD is comparable to the overall cost of raising their family’s SES by 1 SD. In general, putting variables on the same scale is only a superficial way of making comparisons among their regression coefficients.\nPlease write down an interpretation of the of beta (standardized) regression coefficients in the above output. Your interpretation should include reference to the fact that the variables have been standardized. Based on this analysis, do you think one predictor is more important than the other? Why or why not? Please be prepared to share your interpretations / questions in class!"
  },
  {
    "objectID": "ch3_two_predictors.html#sec-rsquared-3",
    "href": "ch3_two_predictors.html#sec-rsquared-3",
    "title": "3  Two predictors",
    "section": "3.6 (Multiple) R-squared",
    "text": "3.6 (Multiple) R-squared\nR-squared in multiple regression has the same general formula and interpretation as in simple regression. The formula is\n\\[R^2 = \\frac{SS_{\\text{reg}}} {SS_{\\text{total}}} \\]\nand it is interpreted as the proportion of variance in the outcome variable that is “associated with” or “explained by” its linear relationship with the predictor variables.\nAs discussed below, we can also say a bit more about R-squared in multiple regression.\n\n3.6.1 Relation with simple regression\nLike the regression coefficients in Section 3.4, the equation for R-squared can also be written in terms of the correlations among the three variables:\n\\[R^2 = \\frac{r^2_{Y1} + r^2_{Y2} - 2 r_{12}r_{Y1}r_{Y2}}{1 - r^2_{12}}.\\]\nIf the correlation between the predictors is zero, then this equation simplifies to\n\\[R^2 = r^2_{Y1} + r^2_{Y2}.\\] In words: When the predictors are uncorrelated, their total contribution to variance explained is just the sum of their individual contributions.\nHowever, when the predictors are correlated, either positively or negatively, it can be show that\n\\[ R^2 &lt; r^2_{Y1} + r^2_{Y2}.\\]\nIn other words: correlated predictors jointly explain less variance than if we added the contributions of each predictor considered separately. Intuitively, this is because correlated predictors share some variation with each other. If we considered the predictors one at a time, we double-count their shared variation.\nThe interpretation of R-squared for one versus two predictors can be explained in terms of the following Venn diagram.\n\n\n\n\n\nFigure 3.3: Shared Variance Among \\(Y\\), \\(X_1\\), and \\(X_2\\).\n\n\n\n\nIn the diagram, the circles represent the variance of each variable and the overlap between circles represents their shared variance (i.e., the R-squared for each pair of variables). When we conduct a multiple regression, the variance in the outcome explained by both predictors is equal to the sum of the areas A + B + C. If we instead conduct two simple regressions and then add up the R-squared values, we would double count the area labelled “B”.\nThe Venn diagram in Figure 3.3 is also useful for understanding other aspects of multiple regression. In the lesson we will discuss the following questions. Please write down your answers now so you are prepared to contribute to the discussion:\n\nWhich area represents the correlation between the predictors? \nWhich areas represent the regression coefficients from multiple regression?\nWhich areas represent the regression coefficients from simple regression?\n\n\n\n\n\n3.6.2 Adjusted R-squared\nThe sample R-squared is an upwardly biased estimate of the population R-squared. The adjusted R-squared corrects this bias. This section explains the main ideas.\nThe bias of R-squared is illustrated in the figure below. In the example, we are considering simple regression (one predictor), and we assume that the population correlation between the predictor and the outcome is zero (i.e., \\(\\rho = 0\\)).\n\n\n\n\n\nFigure 3.4: Sampling Distribution of \\(r\\) and \\(r^2\\) when $ ho = 0$.\n\n\n\n\nIn the left panel, we can see that “un-squared” correlation, \\(r\\), has a sampling distribution that is centered at the true value \\(\\rho = 0\\). This means that \\(r\\) is an unbiased estimate of \\(\\rho\\).\nBut in the right panel, we can see that the sampling distribution of the squared correlation, \\(r^2\\), must have a mean greater than zero. This is because all of the sample-to-sample deviations in left panel are now positive (because they have been squared). Since the average value of \\(r^2\\) is greater than 0, \\(r^2\\) is an upwardly biased estimate of \\(\\rho^2\\).\nThe adjusted R-squared corrects this bias. The formula for the adjustment is:\n\\[\\tilde R^2 = 1 - (1 - R^2) \\frac{N-1}{N - K - 1}\\]\nwhere \\(K\\) is the number of predictors in the model.\nThe formula contains two main terms, the proportion of residual variance, \\((1 - R^2)\\), and the adjustment factor (the ratio of \\(N-1\\) to \\(N-K-1\\)). We can understand how the adjustment works by considering these two terms.\nFirst, it can be seen that the adjustment factor is larger when the number of predictors, \\(K\\), is large relative to the sample size, \\(N\\). So, roughly speaking, the adjustment will be more severe when there are a lot of predictors in the model relative to the sample size.\nSecond, it can also be seen that the adjustment proportional to \\((1 - R^2)\\). This means that the adjustment is more severe if the model explains less variance in the outcome. For example, if \\(R^2 = .9\\) and the adjustment factor is \\(1.1\\), then adjusted \\(R^2 = .89\\). In this case the adjustment is a decrease of 1% of variance explained. But if we start off explaining less variance, say \\(R^2 = .1\\) and use the same adjustment factor, then adjusted \\(R^2 = .01\\). Now the adjustment is a decrease of 9% variance explained, even though we didn’t change the adjustment factor.\nIn summary, the overall interpretation of adusted R-squared is as follows: the adjustment will be larger when there are lots of predictors in the model but they don’t explain much variance in the outcome. This situation is sometimes called “overfitting” the data, so we can think of adjusted R-squared as a correction for overfitting.\nThere is no established standard for when you should reported R-squared or adjusted R-squared. I recommend that you report both whenever they would would lead to different substantive conclusions. We can discuss this more in class.\n\n\n3.6.3 The ECLS example\nAs shown in Section 3.5.4, the R-squared for the ECLS example is equal to .2726 and the adjusted R-squared is equal to .2668. Please write down your interpretation of these value and be prepared to share your answer in class."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-inference-3",
    "href": "ch3_two_predictors.html#sec-inference-3",
    "title": "3  Two predictors",
    "section": "3.7 Inference",
    "text": "3.7 Inference\nThere isn’t really any thing new that about inference with multiple regression, except the formula for the standard errors (see (fox2016?) chap.6). We present the formulas for an abribrary number of predictors, denoted \\(k = 1, \\dots K\\).\n\n3.7.1 Inference for the coefficients\nIn multiple regression\n\\[SE({\\widehat b_k}) = \\frac{\\text{SD}(Y)}{\\text{SD}(X)} \\sqrt{\\frac{1 - R^2}{N - K - 1}} \\times \\sqrt{\\frac{1}{1 - R_k^2}}\n\\tag{3.6}\\]\nIn this formula, \\(K\\) denotes the number of predictors and \\(R^2_k\\) is the R-squared that results from regressing predictor \\(k\\) on the other \\(K-1\\) predictors (without the \\(Y\\) variable).\nNotice that the first part of the standard error (before the “\\(\\times\\)”) is the same as simple regression (see Section 2.7). The last part, which includes \\(R^2_k\\), is different and we talk about it more below.\nThe standard errors can be used to construct t-tests and confidence intervals using the same approach as for simple regression (see Section 2.7). The degrees of freedom for the t-distribution is \\(N - K -1\\). This formula for the degrees of freedom applies to simple regression too, where \\(K = 1\\).\n\n\n3.7.2 Precision of \\(\\hat b\\)\nWe can use Equation 3.6 to understand the factors that influence the size of the standard errors of the regression coefficients. Recall that standard errors describe the sample-to-sample variability of a statistic. If there is a lot sample-to-sample variability, the statistic is said to be imprecise. Equation 3.6 shows us what factors make \\(\\hat b\\) more or less precise.\n\nThe standard errors decrease with\n\nThe sample size, \\(N\\)\nThe proportion of variance in the outcome explained by the predictors, \\(R^2\\)\n\nThe standard errors increase with\n\nThe number of predictors, \\(K\\)\nThe proportion of variance in the predictor that is explained by the other predictors, \\(R^2_k\\)\n\n\nSo, large sample sizes and a large proportion of variance explained lead to precise estimates of the regression coefficients. On the other hand, including many predictors that are highly correlated with each other leads to less precision. In particular, the situation where \\(R^2_k\\) approaches the value of \\(1\\) is called multicollinearity. We will talk about multicollinearity in more detail in ?sec-chap-5.\n\n\n3.7.3 Inference for R-squared\nThe R-squared statistic in multiple regression tells us how much variation in the outcome is explained by all of the predictors together. If the predictors do not explain any variation, then the population R-squared is equal to zero.\nNotice that \\(R^2 = 0\\) implies \\(b_1 = b_2 = ... = b_k = 0\\) (in the population). So, testing the significance of R-squared is equivalent to testing whether any of the regression parameters are non-zero. When we addressed ANOVA last semester, we called this the omnibus hypothesis. But in regression analysis, it is usually just referred to as a test of R-squared.\nThe null hypothesis \\(H_0 : R^2 = 0\\) can be tested using the statistic\n\\[F = \\frac{\\widehat R^2 / K}{(1 - \\widehat R^2) / (N - K - 1)},\\]\nwhich has an F-distribution on \\(K\\) and \\(N - K -1\\) degrees of freedom when the null hypothesis is true.\n\n\n3.7.4 The ECLS example\nThe R output for the ECLS example is presented (again) below. Please write down your conclusions about the statistical significance of the predictors and the R-squared statistic, and be prepared to share your answer in class. Please also write down the factors that affect the precision of the regression coefficients. This would be a good opportunity to practice APA formatting.\n\n\nCode\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "ch3_two_predictors.html#workbook",
    "href": "ch3_two_predictors.html#workbook",
    "title": "3  Two predictors",
    "section": "3.8 Workbook",
    "text": "3.8 Workbook\nThis section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in ?sec-exercises-3. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 3.2\nIf you have questions about the interpretation of a correlation matrix (below) or pairwise plots (see Section 3.2), please write them down now and share them class.\nNumerical output for the ECLS example:\n\n\nCode\ncor(example_data)\n\n\n          Math       SES       ATL\nMath 1.0000000 0.4384619 0.3977048\nSES  0.4384619 1.0000000 0.2877015\nATL  0.3977048 0.2877015 1.0000000\n\n\nMathematical notation for formulas\n\\[\n\\begin{array}{c}\n\\text{var } Y \\\\ \\text{var } X_1 \\\\ \\text{var } X_2\n\\end{array}\n\\quad\n\\left[\n\\begin{array}{ccc}\n1       & r_{Y1}  & r_{Y2}  \\\\\nr_{1Y}  & 1       & r_{12}  \\\\\nr_{2Y}  & r_{21}  & 1\n\\end{array}\n\\right]\n\\]\nSection 3.5\nBelow, the R output from the ECLS example is reported. Please provide a written explanation of the regression coefficients for SES and ATL. If you have questions about how to interpret the coefficients, please also note them now and be prepared to share them in class. Note that this question is not asking about whether the coefficients are significant – it is asking what the numbers in the first column of the Coefficients table mean.\n\n\nCode\n# Run the regression model and print output\nmod1 &lt;- lm(c1rmscal ~ wksesl + t1learn)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\nSection 3.5.4\nPlease write down an interpretation of the of beta (standardized) regression coefficients in the output below. Your interpretation should include reference to the fact that the variables have been standardized. Based on this analysis, do you think one predictor is more important than the other? Why or why not?\n\n\nCode\n# Unlike other software, R doesn't have a convenience functions for beta coefficients. \nz_example_data &lt;- as.data.frame(scale(example_data))\nz_mod &lt;- lm(Math ~ SES  + ATL, data = z_example_data)\nsummary(z_mod)\n\n\n\nCall:\nlm(formula = Math ~ SES + ATL, data = z_example_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9590 -0.5604 -0.1493  0.4569  4.1043 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.337e-15  5.416e-02   0.000        1    \nSES         3.533e-01  5.666e-02   6.235 1.94e-09 ***\nATL         2.961e-01  5.666e-02   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8563 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\nSection 3.6\nThe Venn diagram below is useful for understanding multiple regression. In the lesson we will discuss the following questions. Please write down your answers now so you are prepared to contribute to the discussion:\n\nWhich area represents the correlation between the predictors?\nWhich areas represent the R-squared from multiple regression?\nWhich areas represent the R-squared from simple regression?\nWhich areas represent the regression coefficients from multiple regression?\nWhich areas represent the regression coefficients from simple regression?\n\n\n\n\n\n\nShared Variance Among \\(Y\\), \\(X_1\\), and \\(X_2\\).\n\n\n\n\n\nLast question: The R-squared for the ECLS example is equal to .2726 and the adjusted R-squared is equal to .2668. Please write down your interpretation of these value and be prepared to share your answer in class.\n\nSection 3.7\nThe R output for the ECLS example is presented (again) below. Please write down your conclusions about the statistical significance of the predictors and the R-squared statistic, and be prepared to share your answer in class. This would be a good opportunity to practice APA formatting. Please also write down the factors that negatively affect the precision of the regression coefficients and address whether you think they are problematic for the example.\n\n\nCode\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "ch3_two_predictors.html#exercises",
    "href": "ch3_two_predictors.html#exercises",
    "title": "3  Two predictors",
    "section": "3.9 Exercises",
    "text": "3.9 Exercises\nThese exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)\nBefore staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\n\n3.9.1 The ECLS250 data\nLet’s start by getting our example data loaded into R.\nMake sure to download the file ECLS250.RData from Canvas and then double click the file to open it\n\n\nCode\nload(\"ECLS250.RData\") # load new example\nattach(ecls) # attach \n\n# knitr and kable are just used to print nicely -- you can just use head(ecls[, 1:5]) \nknitr::kable(head(ecls[, 1:5]))\n\n\n\n\n\ncaseid\ngender\nrace\nc1rrscal\nc1rrttsco\n\n\n\n\n960\n2\n1\n28\n58\n\n\n113\n1\n8\n14\n39\n\n\n1828\n1\n1\n22\n50\n\n\n1693\n1\n1\n21\n50\n\n\n643\n2\n1\n14\n39\n\n\n772\n1\n1\n21\n49\n\n\n\n\n\nThe naming conventions for these data are bit challenging.\n\nVariable names begin with c, p, or t depending on whether the respondent was the child, parent, or teacher. Variables that start with wk were created by the ECLS using other data sources available in during the kindergarten year of the study.\nThe time points (1-4 denoting fall and spring of K and Gr 1) appear as the second character.\nThe rest of the name describes the variable.\n\nThe variables we will use for this illustration are:\n\nc1rmscal: Child’s score on a math assessment, in first semester of Kindergarten . The scores can be interpreted as number of correct responses out of a total of approximately 60 math exam questions.\nwksesl: An SES composite of household factors (e.g., parental education, household income) ranging from 30-72.\nt1learn: Approaches to Learning Scale (ATLS), teacher reported in first semester of kindergarten. This scale measures behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), so that higher values represent more positive responses, and the scale is an unweighted average the six items.\n\nTo get started lets produce the simple regression of Math with SES. This is another look at the relationship between Academic Achievement and SES that we discussed in Chapter Chapter 2). If you do not feel comfortable running this analysis or interpreting the output, take another look at Section 2.11.\n\n\nCode\nplot(x = wksesl, \n     y = c1rmscal, \n     col = \"#4B9CD3\")\n\nmod &lt;- lm(c1rmscal ~ wksesl)\nabline(mod)\n\n\n\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.1314  -4.3549  -0.8486   3.6775  31.5358 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.61595    2.73925   0.225    0.822    \nwksesl       0.43594    0.05674   7.683 3.61e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.482 on 248 degrees of freedom\nMultiple R-squared:  0.1922,    Adjusted R-squared:  0.189 \nF-statistic: 59.03 on 1 and 248 DF,  p-value: 3.612e-13\n\n\nCode\ncor(wksesl, c1rmscal)\n\n\n[1] 0.4384619\n\n\n\n\n3.9.2 Multiple regression with lm\nFirst, let’s tale a look at the “zero-order” relationship among the three variables. This type of descriptive, two-way analysis is a good way to get familiar with your data before getting into multiple regression. We can see that the variables are all moderately correlated and their relationships appear reasonably linear.\n\n\nCode\n# Use cbind to create a data.frame with just the 3 variables we want to examine\ndata &lt;- cbind(c1rmscal, wksesl, t1learn)\n\n# Correlations\ncor(data)\n\n\n          c1rmscal    wksesl   t1learn\nc1rmscal 1.0000000 0.4384619 0.3977048\nwksesl   0.4384619 1.0000000 0.2877015\nt1learn  0.3977048 0.2877015 1.0000000\n\n\nCode\n# Scatterplots\npairs(data, col = \"#4B9CD3\") \n\n\n\n\n\nIn terms of input, multiple regression with lm is similar to simple regression. The only difference is the model formula. To include more predictors in a formula, just include them on the right hand side, separated by at + sign.\n\ne.g, Y ~ Χ1 + Χ2\n\nFor our example, let’s consider the regression of math achievement on SES and Approaches to Learning. We’ll save our result as mod1 which is short for “model one”.\n\n\nCode\nmod1 &lt;- lm(c1rmscal ~ wksesl + t1learn)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see from the output that regression coefficient for t1learn is about 3.5. This means that, as the predictor increases by a single unit, children’s predicted math scores increase by 3.5 points (out of 60), after controlling for the SES. You should be able to provide a similar interpretation of the regression coefficient for wksesl. Together, both predictors accounted for about 27% of the variation in students’ math scores. In education, this would be considered a pretty strong relationship.\nWe will talk about the statistical tests later on. For now let’s consider the relationship with simple regression.\n\n\n3.9.3 Relations between simple and multiple regression\nFirst let’s consider how the two simple regression compare to the multiple regression with two variables. Here is the relevant output:\n\n\nCode\n# Compare the multiple regression output to the simple regressions\nmod2a &lt;- lm(c1rmscal ~ wksesl)\nsummary(mod2a)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.1314  -4.3549  -0.8486   3.6775  31.5358 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.61595    2.73925   0.225    0.822    \nwksesl       0.43594    0.05674   7.683 3.61e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.482 on 248 degrees of freedom\nMultiple R-squared:  0.1922,    Adjusted R-squared:  0.189 \nF-statistic: 59.03 on 1 and 248 DF,  p-value: 3.612e-13\n\n\nCode\nmod2b &lt;- lm(c1rmscal ~ t1learn)\nsummary(mod2b)\n\n\n\nCall:\nlm(formula = c1rmscal ~ t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.399  -4.211  -0.997   3.770  31.844 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.0394     2.1485   3.276   0.0012 ** \nt1learn       4.7301     0.6929   6.826 6.66e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.618 on 248 degrees of freedom\nMultiple R-squared:  0.1582,    Adjusted R-squared:  0.1548 \nF-statistic:  46.6 on 1 and 248 DF,  p-value: 6.665e-11\n\n\nThe important things to note here are\n\nThe regression coefficients from the simple models (\\(b_{ses} = 0.44\\) and \\(b_{t1learn} = 4.73\\)) are larger than the regression coefficients from the two-predictor model. Can you explain why? (Hint: see Section Section 3.5.\nThe R-squared values in the two simple models (.192 + .158 = .350) add up to more than the R-squared in the two-predictor model (.273). Again, take a moment to think about why before reading on. (Hint: see Section Section 3.6.)\n\n\n\n3.9.4 Inference with 2 predictors\nLet’s move on now to consider the statistical tests and confidence intervals provided with the lm summary output.\nFor regression with more than one predictor, both the t-tests and F-tests have a very similar construction and interpretation as with simple regression. The main differences are the formulas, not so much the interpretations of the procedures. Some differences:\n\nThe degrees of freedom for both tests now involve \\(K\\), the number of predictors.\nThe standard error of the b-weight is more complicated, because it involves the inter-correlation among the predictors.\n\nWe can see for mod1 that both b-weights are significant at the .05 level, and so is the R-square. As mentioned previously, it is not usual to interpret or report results on the regression intercept unless you have a special reason to do so (e.g., see the next chapter).\n\n\nCode\n# Revisting the output of mod1\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.9.5 APA reporting of results\nThis section shows how we might write out the results of our regression using APA format. When we have a regression model with many predictors, or are comparing among different models, it is more usual to put all the relevant statistics in a table rather than writing them out one by one. We will see how to do that later on in the course. For more info on APA format, see the APA publications manual: (https://www.apastyle.org/manual).\n\nThe regression of Math Achievement on SES was positive and statistically significant at the .05 level (\\(b = 0.35, t(247) = 6.24, p &lt; .001\\)).\nThe regression of Math Achievement on Approaches to Learning was also positive and statistically significant at the .05 level (\\(b = 3.52, t(247) = 5.22, p &lt; .001\\)).\nTogether both predictors accounted for about 27% of the variation in Math Achievement (\\(R^2 = .273\\), \\(\\text{adjusted} R^2 = .267\\)), which was also statistically significant at the .05 level (\\(F(2, 247) = 46.29, p &lt; .001\\))."
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-interpretations-4",
    "href": "ch4_categorical_predictors.html#sec-interpretations-4",
    "title": "4  Categorical predictors",
    "section": "4.1 Focus on interpretation",
    "text": "4.1 Focus on interpretation\nCategorical predictors can challenging to understand because, depending on the contrast coding used, the model results can appear quite different.\nFor example, the two models below uses the same data and the same variables (Math Achievement regressed on Urbanicity), but their regression coefficients have different values. Why? Because the models used different contrast coding for Urbanicity. In the first model, Urban and Suburban students are compared to Rural students. In the second model, Rural and Suburban students are compared to the unweighted average across all three groups.\n\n\nCode\nload(\"NELS.RData\")\nattach(NELS)\n\n# run model with default contrast (treatment / dummy coding)\negA &lt;- lm(achrdg08 ~ urban)\n\n# change to sum / deviation contrasts and run again\ncontrasts(urban) &lt;- contr.sum(n = 3)\ncolnames(contrasts(urban)) &lt;- c(\"Rural\", \"Suburban\")\negB &lt;- lm(achrdg08 ~ urban)\n\n# print\nsummary(egA)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    54.9927     0.6885  79.876  &lt; 2e-16 ***\nurbanSuburban   0.6675     0.9117   0.732  0.46439    \nurbanUrban      3.1275     1.0480   2.984  0.00298 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nCode\nsummary(egB)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.2577     0.4021 139.897   &lt;2e-16 ***\nurbanRural     -1.2650     0.5654  -2.237   0.0257 *  \nurbanSuburban  -0.5975     0.5299  -1.128   0.2600    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nNote that the lm output doesn’t explicitly tell us what kind of coding was used. So, we need to know what is going on “under the hood” in order to interpret the output correctly. Also note that the while the Coefficients tables of the two models are different, they both explain the same amount of variance in the outcome. So we could say that both models “fit” the data equally well, but they provide different interpretations of the relationship between Math Achievement and Urbanicity.\nThe choice of which contrast to use is up to the researcher. This choice should reflect the research questions you want to address. For example, if we wanted to know how Urban and Suburban students differ from Rural students, that is what the first output shows us. If we wanted to know how Suburban and Rural students differ from the overall average, that is what the second output shows us. As we will discuss below, we can’t make all possible comparisons among groups in a single model, so we have to make some compromises when choosing contrasts. Understanding how to align our research questions with the choice of contrasts is a big part of what this chapter is about!"
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-social-constructs-4",
    "href": "ch4_categorical_predictors.html#sec-social-constructs-4",
    "title": "4  Categorical predictors",
    "section": "4.2 Data and social constructs",
    "text": "4.2 Data and social constructs\nBefore getting into the math, let’s consider some conceptual points.\nFirst, some terminology. Binary means the a variable can take on only two values: 1 and 0. If a variable takes on two values but these are represented with other numbers (e.g., 1 and 2) or with non-numeric values (“male”, “female”), it is called dichotomous rather than binary. Otherwise stated, a binary variable is a dichotomous variable whose values are 1 and 0. The term categorical is used to describe variables with two or more categories – it includes binary and dichotomous variables, as well as variables with more categories.\nNote that encoding a variable as dichotomous does not imply that the underlying social construct is dichotomous. For example, we can encode educational attainment as a dichotomous variable indicating whether or not a person has graduated high school. This does not imply that educational attainment has only two values “irl”, or even that educational attainment is best conceptualized in terms of years of formal education. Nonetheless, for many outcomes of interest it can be meaningful to consider whether individuals have completed high school (e.g., https://www.ssa.gov/policy/docs/research-summaries/education-earnings.html).\nIn general, the way that a variable is encoded in a dataset is not a statement about reality – it reflects a choice made by researchers about how to represent social constructs. When conducting quantitative research, we are often faced with less-than-ideal encodings of so-called demographic variables. For example, both NELS and ECLS conceptualize gender as dichotomous and use a limited set of mutually exclusive categories for race. These representations are not well aligned with current literature on gender and racial identity. Some recent perspectives on how these issues play into quantitative research are available here: https://www.sree.org/critical-perspectives.\nIn general, I would argue that categorical variables often have utility, especially in the study of social inequality. Here is an example of why I think gender qua “male/female” is a flawed but important consideration in global education: https://www.unicef.org/education/girls-education.\nPlease take a moment to write down your thoughts on the tensions that arise when conceptualizing social constructs such as gender or race as categorical, and I will invite you to share you thoughts in class."
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-binary-predictors-4",
    "href": "ch4_categorical_predictors.html#sec-binary-predictors-4",
    "title": "4  Categorical predictors",
    "section": "4.3 Binary predictors",
    "text": "4.3 Binary predictors\nLet’s start our interpretation of categorical predictors with the simplest case: a single binary predictor.\nFigure Figure 4.1 uses the NELS data to illustrate the regression of Reading Achievement in Grade 8 (achrdg08) on a binary encoding of Gender (female = 0, male = 1). There isn’t a lot going on the plot! However, we can see the conditional distributions of Reading Achievement for each value of Gender, and the means of the two groups are indicated.\n\n\nCode\n# Don't read this unless you really like working on graphics :) \nbinary_gender &lt;- (gender == \"Male\") * 1 \nplot(binary_gender, achrdg08, \n     col = \"#4B9CD3\", \n     xlab = \"binary gender (Female = 0, Male = 1)\",\n     ylab = \"Reading (Grade 8)\")\n\nmeans &lt;- tapply(achrdg08, binary_gender, mean)\nlabels &lt;- c(expression(bar(Y)[0]), expression(bar(Y)[1]))\n\ntext(x = c(.1, .9), y = means, labels = labels, cex = 1.5) \ntext(x = c(0, 1), y = means, labels = c(\"_\", \"_\"), cex = 2) \n\n\n\n\n\nFigure 4.1: Reading Achievement on Binary Gender.\n\n\n\n\nIn this situation, the simple regression equation from Section @ref(regression-line-2) still holds\n\\[\\widehat Y = b_0 + b_1 X, \\]\nand we can still interpret the regression slope in terms of a one-unit increase in \\(X\\). However, since \\(X\\) can only take on two values (0 or 1), there are also interpretations of the regression coefficients that apply in this situation. In this section we are interested in the this “more specific” interpretation of regression cofficients when \\(X\\) is dichotomous.\nThe general strategy for approaching the interpretation of regression coefficients with categorical predictors has two steps:\n\nStep 1. Plug the values for \\(X\\) into the regression equation to get the predicted values for each group \\[\n\\begin{align}\n\\widehat Y (Female) & = b_0 + b_1 (0) = b_0 \\\\\n\\widehat Y (Male) & = b_0 + b_1 (1) = b_0 + b_1\n\\end{align}\n\\]\nStep 2. Solve for the coefficients in terms the predicted values.\n\n\\[ b_0 = \\widehat Y (Female) \\tag{4.1}\\] \\[ b_1  = \\widehat Y (Male) - b_0 = \\widehat Y (Male) - \\widehat Y (Female)\n\\tag{4.2}\\]\nLooking at Equation 4.1 we can see that intercept (\\(b_0\\)) is equal to the predicted value of Reading Achievement for Females, and Equation 4.2 shows that the regression slope (\\(b_1\\)) is equal to the difference between predicted Reading Achievement for Males and Females.\nThere is one last detail that is important for interpreting these equations: for a single categorical predictor, the predicted values for each category are just the group means on the \\(Y\\) variable. This details, as we as some other interesting results on categorical variables, are shown in Section 4.7\nSo, using the notation of Figure 4.1, we can re-write Equation 4.1 and Equation 4.2 as\n\\[b_0 = \\bar Y_0   \\tag{4.3}\\]\n\\[ b_1 = \\bar Y_1 - \\bar Y_0  \\tag{4.4}\\]\nWe will use the equivalence between the predicted values and group means throughout this chapter. However, it is important to note this equivalence holds only when there is single categorical predictor in the model, and no other predictors. Additional predictors are discussed in the next chapter.\nFor the example data, regression coefficients are:\n\n\nCode\n# convert \"Female / Male\" coding to binary\ngender &lt;- NELS$gender\nbinary_gender &lt;- (gender == \"Male\")*1\nmod_binary &lt;- lm(achrdg08 ~ binary_gender)\ncoef(mod_binary)\n\n\n  (Intercept) binary_gender \n   56.4678022    -0.9223396 \n\n\nPlease take a moment and write down how these two numbers are related to Figure 4.1. In particular, what is \\(\\bar Y_0\\) equal to, what is \\(\\bar Y_1\\) equal to, and what is their difference equal to?\n\n4.3.1 Relation with t-tests of means\nSimple regression with a binary predictor is equivalent to conducting an independent samples t-test in which the \\(X\\) variable (Gender) is the grouping variable and the \\(Y\\) variable (Reading Achievement) is the outcome. The following output illustrates this.\nFor the regression model (same as above):\n\n\nCode\nsummary(mod_binary)\n\n\n\nCall:\nlm(formula = achrdg08 ~ binary_gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.7278  -6.1472   0.3784   6.9765  15.0045 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.4678     0.5342 105.703   &lt;2e-16 ***\nbinary_gender  -0.9223     0.7928  -1.163    0.245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.827 on 498 degrees of freedom\nMultiple R-squared:  0.00271,   Adjusted R-squared:  0.0007076 \nF-statistic: 1.353 on 1 and 498 DF,  p-value: 0.2452\n\n\nFor the independent samples t-test (with homogeneity of variance assumed):\n\n\nCode\nt.test(achrdg08 ~ binary_gender, var.equal = T)\n\n\n\n    Two Sample t-test\n\ndata:  achrdg08 by binary_gender\nt = 1.1633, df = 498, p-value = 0.2452\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.6353793  2.4800586\nsample estimates:\nmean in group 0 mean in group 1 \n       56.46780        55.54546 \n\n\nWe will go through the comparison between these two outputs in class together. If you have any questions about the relation between these two sets of output, please note them now and be prepared ask them in class.\n\n\n4.3.2 Summary\nWhen doing regression with a binary predictor:\n\nThe regression intercept is equal to the mean of the group coded “0”.\nThe regression slope is equal to the mean of the group coded “0” subtracted from the mean of the groups coded “1”.\nTesting \\(H_0: b_1 = 0\\) is equivalent to testing the mean difference \\(H_0: \\mu_1 – \\mu_0 = 0\\)\n\ni.e., regression with a binary variable is the same as a t-test of means for independent groups."
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-reference-group-coding-4",
    "href": "ch4_categorical_predictors.html#sec-reference-group-coding-4",
    "title": "4  Categorical predictors",
    "section": "4.4 Reference-group coding",
    "text": "4.4 Reference-group coding\nNow that we know how to do regression with a binary predictor works, let’s consider how to extend this approach to categorical predictors with \\(C ≥ 2\\) categories. There are many ways to do this, and the general topic is variously called “contrast coding”, “effect coding”, or “dummy coding”.\nThe basic idea is to represent the \\(C\\) categories of a predictor in terms of \\(C – 1\\) dummy variables. Binary coding of a dichotomous predictor is one example of this: We represented a categorical variable with \\(C = 2\\) categories using \\(1\\) binary predictor.\nThe most common approach to contrast coding is called reference-group coding. In R, the approach is called treatment contrasts and is the default coding for categorical predictors.\nIt is called reference-group coding because:\n\nThe researcher chooses a reference-group.\nThe intercept is interpreted as the mean of the reference-group.\nThe \\(C – 1\\) regression coefficients are interpreted as the mean differences between the \\(C – 1\\) other groups and the reference-group.\n\nNote that reference-group coding is a generalization of binary coding. In the example from Section Section 4.3:\n\nFemales were the reference-group.\nThe intercept was equal to the mean Reading Achievement for females.\nThe regression coefficient was equal to the mean difference between males and females.\n\nThe rest of this section considers how to generalize this approach to greater than 2 groups.\n\n4.4.1 A hypothetical example\nFigure Figure 4.2 presents a toy example. The data show the Age and marital status (Mstatus) of 16 hypothetical individuals. Marital status is encoded as\n\nSingle (never married)\nMarried\nDivorced\n\n\n\nCode\nknitr::include_graphics(\"files/images/marital_status1.png\")\n\n\n\n\n\nFigure 4.2: Toy Martital Status Example.\n\n\n\n\nThese 3 categories are represented by two binary variables, denoted \\(X_1\\) and \\(X_2\\).\n\n\\(X_1\\) is a binary variable that is equal to 1 when Mstatus is “married”, and equal to 0 otherwise.\n\\(X_2\\) is a binary variable that is equal to 1 when Mstatus is “divorced”, and equal to 0 otherwise.\n\nThe binary variables are often called dummies or indicators. For example, \\(X_1\\) is a dummy or indicator for married respondents.\nIn reference-group coding, the group that does not have a dummy variable is the reference group. It is also the group that is coded zero on all of the included dummies. In this example, “Single” is the reference group.\nWe can more compactly write down the contrast coding in the example using a contrast matrix .\n\\[\n\\begin{matrix} & X_1 & X_2\\\\\n                 \\text{Married} & 1 & 0  \\\\\n                 \\text{Divorced} & 0 & 1 \\\\\n                \\text{Single}  & 0 & 0 \\\\\n\\end{matrix}\n\\]\nThis notation is an abbreviated version of the data matrix in Figure 4.2. It summarizes how the two indicator variables correspond to the different levels of the categorical variable. As we will see in the Section 4.9, contrast matrices are useful for programming in R.\n\n\n4.4.2 Interpreting the regression coefficients\nRegressing Age on the dummies we have:\n\\[ \\widehat Y = b_0 + b_1 X_1 + b_2 X_2 \\]\nIn order to interpret the regression coefficients we can use the same two steps as in Section 4.3\n\nStep 1. Plug the values for the \\(X\\) variables into the regression equation to get the predicted values for each group\n\n\\[\\begin{align}\n\\widehat Y (Single) & = b_0 + b_1 (0) + b_2 (0) = b_0 \\\\\n\\widehat Y (Married) & = b_0 + b_1 (1) + b_2 (0) = b_0 + b_1 \\\\\n\\widehat Y (Divorced) & = b_0 + b_1 (0) + b_2 (1) = b_0 + b_2\n\\end{align}\\]\n\nStep 2. Solve for the model parameters in terms the predicted values.\n\n\\[\\begin{align}\nb_0 & = \\widehat Y (Single) \\\\\nb_1 & = \\widehat Y (Married) - b_0 = \\widehat Y (Married) - \\widehat Y (Single) \\\\\nb_2 & = \\widehat Y (Divorced) - b_0 = \\widehat Y (Divorced) - \\widehat Y (Single)\n\\end{align}\\]\nUsing the above equations, please write down an interpretation of the regression parameters for the hypothetical example. (Note: this question is not asking for a numerical answer, it is just asking you to put the above equations into words.)\n\n\n4.4.3 More than 3 categories\nFigure 4.3 extends the example by adding another category for Mstatus (“widowed”).\n\n\nCode\nknitr::include_graphics(\"files/images/marital_status2.png\")\n\n\n\n\n\nFigure 4.3: Toy Martital Status Example, Part 2.\n\n\n\n\nHere we are interested in the model\n\\[\\widehat Y = b_0 + b_1 X_1 + b_2 X_2 + b_3 X_3\\]\nPlease work through the following questions and be prepared to share your answers in class\n\nHow should \\(X_3\\) be coded so that “single” is the reference-group?\nWrite down the contrast matrix for the example.\nUsing the two-step approach illustrated above, write out the interpretation of the model parameters in the following regression equation:\nWhat happens if we include an additional dummy variable for “Single” in the model. In this case we would have \\(C = 4\\) dummies in the model rather than \\(C-1 = 3\\) dummies. Please take a moment to write down what you think will happen (Hint: something goes wrong)\n\n\n\n4.4.4 Summary\nIn reference-group coding with a single categorical variable:\n\nThe reference is group is chosen by the analyst – it is the group that is coded zero on all dummies, or the one that has its dummy left out of the \\(C-1\\) dummies used in the model.\nThe intercept is interpreted as the mean of the reference group.\nThe regression coefficients of the dummy variables are interpreted as the difference between the mean of the indicated group and the mean of the reference group."
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-deviation-coding-4",
    "href": "ch4_categorical_predictors.html#sec-deviation-coding-4",
    "title": "4  Categorical predictors",
    "section": "4.5 Deviation coding",
    "text": "4.5 Deviation coding\nIn some research scenarios, there is a clear reference group (e.g., in experimental conditions, comparisons are made to the control group). But in other cases, it is less clear what the reference group should be. In both of the examples we have considered so far, the choice of reference group was arbitrary.\nWhen there is not a clear reference group, it can be preferable to use other types of contrast coding than reference-group coding. One such approach is called deviation coding. In R, this is called sum-to-zero constrasts, or sum contrasts for short.\nThe main difference between deviation coding and reference-group coding is the interpretation of the intercept. In deviation coding, it is no longer an (arbitrarily chosen) reference-group, but instead represents the mean of the predicted values. When the groups have equal sample size, the deviation-coded intercept is also equal to overall mean on \\(Y\\).\nTo summarize deviation coding:\n\nThe intercept is equal to the mean of the predicted values for each category i.e.,\n\n\\[\nb_0 = \\frac{\\sum_{c=1}^C \\widehat Y_c} {C}\n\\tag{4.5}\\]\n\nWhen the groups have equal sample sizes (\\(n\\)), the intercept is also equal to the overall mean on the outcome variable:\n\n\\[\\begin{equation}\nb_0 = \\frac{\\sum_{c=1}^C \\widehat Y_c} {C}\n= \\frac{\\sum_{c=1}^C \\bar Y_c}{C} = \\frac{\\sum_{c=1}^C \\left(\\frac{\\sum_{i=1}^n Y_{ic}}{n}\\right)} {C} =  \\frac{\\sum_{c=1}^C \\sum_{i=1}^n Y_{ic}}{nC} = \\bar Y\n\\end{equation}\\]\n\nThe regression slopes compare the indicated group to the intercept. Consequently, when the groups have equal sample sizes, the regression slopes are interpreted as the deviation of each group mean from the overall mean, which is why it is called deviation coding.\n\nWhen the groups have unequal sample size, the situation is a bit more complicated. In particular, we have to weight the predicted values in Equation 4.5 by the group sample sizes. This is addressed in Section 4.5.4 (optional). To clarify that intercept in deviation coding is not always equal to \\(\\bar Y\\), we refer to it as an “unweighted mean” of group means / predicted scores.\nIt is important to note that there are still only \\(C-1\\) regression slopes So, one group gets left out of the analysis, and the researcher has to chose which one. This is a shortcoming of deviation coding, which is addressed in the Section Section 4.5.5 (optional).\n\n4.5.1 A hypothetical example\nThe International Development and Early Learning Assessment (IDELA) is an assessment designed to measure young children’s development in literacy, numeracy, social-emotional, and motor domains, in international settings. Figure 4.4 shows the countries in which the IDELA had been used as of 2017 (for more info, see https://www.savethechildren.net/sites/default/files/libraries/GS_0.pdf).\n\n\nCode\nknitr::include_graphics(\"files/images/idela_map.png\")\n\n\n\n\n\nFigure 4.4: IDELA Worldwide Usage, 2017.\n\n\n\n\nIf our goal was to compare countries’ IDELA scores, agreeing on which country should serve as the reference grou would politically fraught. Therefore, it would be preferable to avoid the problem of choosing a reference group altogether. In particular, deviation coding let’s us compare each country’s mean IDELA score to the (unweighted) mean over all of the countries.\nFigure 4.5 presents a toy example. The data show the IDELA scores and Country for 16 hypothetical individuals. The countries considered in this example are\n\nEthiopia\nVietnam\nBoliva\n\nThese 3 countries are represented by two binary variables, denoted \\(X_1\\) and \\(X_2\\).\n\n\\(X_1\\) is a indicator for Ethiopia\n\\(X_2\\) is a indicator for Vietnam\n\n\n\nCode\nknitr::include_graphics(\"files/images/idela1.png\")\n\n\n\n\n\nFigure 4.5: Toy IDELA Example.\n\n\n\n\nNote that the dummy variables are different than for the case of reference-group coding discussed Section 4.4. In deviation coding, the dummies always take on values \\(1, 0, -1\\). The same group must receive the code \\(-1\\) for all dummies. The group with the value \\(-1\\) is the group that gets left out of the analysis.\nPlease take a moment to write down the contrast matrix for the IDELA example.\n\n\n4.5.2 Interpreting the regression coefficients\nRegressing IDELA on the dummies we have:\n\\[\\widehat Y = b_0 + b_1 X_1 + b_2 X_2\\]\nIn order to interpret the regression coefficients, we proceed using the same two steps as in Section 4.3 and Section 4.4.\n\nStep 1. Plug the values for \\(X\\) into the regression equation to get the predicted values for each group.\n\n\\[\\begin{align}\n\\widehat Y (Ethiopia) & = b_0 + b_1 (1) + b_2 (0) = b_0 + b_1\\\\\n\\widehat Y (Vietnam) & = b_0 + b_1 (0) + b_2 (1) = b_0 + b_2 \\\\\n\\widehat Y (Bolivia) & = b_0 + b_1 (-1) + b_2 (-1) = b_0 - b_1 - b_2\n\\end{align}\\]\n\nStep 2. Solve for the model parameters in terms of the predicted values.\n\n\\[\\begin{align}\nb_1 &= \\widehat Y (Ethiopia) - b_0 \\\\\nb_2 &= \\widehat Y (Vietnam) - b_0 \\\\\nb_0 & = \\widehat Y (Bolivia) + b_1 + b_2 \\\\  \n\\end{align}\\]\nAt this point, we want to use the first two lines to substitute in for \\(b_1\\) and \\(b_2\\) in the last line:\n\\[\\begin{align}\nb_0 & = \\widehat Y (Bolivia) + \\widehat Y (Ethiopia) - b_0 + \\widehat Y (Vietnam) - b_0 \\\\\n\\implies & \\\\\n3b_0 & = \\widehat Y (Bolivia) + \\widehat Y (Ethiopia) + \\widehat Y (Vietnam) \\\\\n\n\\implies & \\\\\nb_0 & = \\frac{\\widehat Y (Bolivia) + \\widehat Y (Ethiopia) + \\widehat Y (Vietnam)}{3}\n\\end{align}\\]\nIn the last line we see that \\(b_0\\) is equal to the (unweighted) mean of the predicted values.\nUsing the above equations, please write down an interpretation of the regression parameters for the hypothetical example. (Note: this question is not asking for a numerical answer, it is just asking you to put the above equations into words.)\n\n\n4.5.3 Summary\nIn deviation coding with a single categorical variable:\n\nThe intercept is interpreted as the unweighted mean of the groups’ means, which is equal to the overall mean on the \\(Y\\) variable when the groups have equal sample sizes.\nThe regression slopes on the dummy variables are interpreted as the difference between the mean of the indicated group and the unweighted mean of the groups.\nThere are still only \\(C - 1\\) regression coefficients, so one group gets left out (see extra material for how to get around this).\n\n\n\n4.5.4 Extra: Deviation coding with unequal sample sizes*\nWhen groups have unequal sample size, the unweighted mean of the group means is not the overall mean of the \\(Y\\) variable. This is not always a problem. For example, in the IDELA example, it is reasonable that each country should receive equal weight, even if the size of the samples in each country differ.\nHowever, if you want to compare each groups’ mean to the overall mean on \\(Y\\), deviation coding can be adjusted by replacing the dummy-coded value \\(-1\\) with the ratio of indicated group’s sample size to the omitted group’s sample size. An example for 3 groups is shown below.\n\\[\n\\begin{matrix} & \\text{Dummy 1}& \\text{Dummy 2}\\\\\n                   \\text{Group 1} & 1 & 0  \\\\\n                   \\text{Group 2} & 0 & 1 \\\\\n                  \\text{Group 3}  & - n_1 /n_3 & - n_2 / n_3 \\\\\n\\end{matrix}\n\\]\nYou can use the 2-step procedure to show that this coding, called weighted deviation coding, results in\n\\[\\begin{equation}\nb_0 = \\frac{n_1 \\widehat Y( \\text{Group 1}) + n_2 \\widehat Y( \\text{Group 2}) + n_3 \\widehat Y( \\text{Group 3})}{n_1 + n_2 + n_3}\n\\end{equation}\\]\nReplacing \\(\\widehat Y( \\text{Group }c )\\) with \\(\\bar Y_c\\) you can also show that \\(b_0 = \\bar Y\\), using the rules of summation algebra. So, unlike the case for the deviation coding, the intercept in weighted deviation coding is always equal to the overall mean on the outcome variance, regardless of the sample sizes of the groups. In R, you can use the package wec for weighted effect coding.\n\n\n4.5.5 Extra: Deviation coding all groups included*\nAnother issue with deviation coding is that it requires leaving one group out of the model. This is a shortcoming of the approach. As a work around, one can instead use the following approach. Note that this approach will affect the statistical significance of R-squared, so you should only use it if you aren’t interested in reporting R-squared.\n\nStep A: Standardize the \\(Y\\) variable to have \\(M = 0\\).\nStep B: Compute binary dummy variables (reference-group coding) for all \\(C\\) groups, \\(X_1, X_2, \\dots, X_C\\)\nStep C: Regress \\(Y\\) on the dummy variables, without the intercept in the model:\n\n\\[\\hat Y = b_1X_1 +  b_2 X_2 + \\dots + b_cX_C.\\]\nUsing the two step approach, it is easy to show that the regression coefficients are just the means of the indicated group. Since the overall mean of \\(Y\\) is zero (because of Step A), the group means can be interpreted as deviations from the overall mean on \\(Y\\).\nNote that to omit the intercept in R, you can use the formula syntax\nY ~ -1 + X1 + ...\nin the lm function (the -1 removes the intercept from the model).\nAgain, keep in mind that this will affect the statistical significance of R-squared, as reported in summary(lm). So, you shouldn’t use this approach when you want to report R-squared. The next paragraph explains why, but it requires some material we won’t discuss until ?sec-chap-6.\nThe F-test of R-squared can be computed by comparing two different models, a reference model with no predictors and a focal model with the predictors. In the present context, the focal models fit the data the same, regardless of whether we use the intercept and \\(C-1\\) predictors or omit the intercept and use \\(C\\) predictors. However, the reference model used by lm depends on whether the intercept is included in the model. In the model with the intercept, the reference model also has an intercept. In the model that omits the intercept, the reference also omits the intercept. Consequently, the F-statistic for the test of R-squared (as reported by summary(lm)) is different for the two approaches. You can get the “correct” R-squared from R, but it takes a bit of fiddling around. If you are interested, just ask in class."
  },
  {
    "objectID": "ch4_categorical_predictors.html#relation-with-anova",
    "href": "ch4_categorical_predictors.html#relation-with-anova",
    "title": "4  Categorical predictors",
    "section": "4.6 Relation with ANOVA",
    "text": "4.6 Relation with ANOVA\nAs mentioned in Chapter 3, the F-test of R-squared is equivalent to simultaneously testing whether all of the regression slopes are equal to zero. In symbols:\nTesting\n\\[H_0: R^2 = 0\\]\nis equivalent to testing\n\\[H_0: b_1 = b_2 = \\cdots = b_{C-1} = 0\\] We have see above that the regression slopes can be interpreted in terms of group-mean differences. For example, under reference-group coding, we can re-write the null hypothesis above as:\n\\[H_0: \\bar Y_1 - \\bar Y_C = \\bar Y_2 - \\bar Y_C  = \\cdots = \\bar Y_{C-1} - \\bar Y_C  = 0\\] In this notation, we let \\(\\bar Y_C\\) denote the reference group. If we add \\(\\bar Y_C\\) to each equality in the null hypothesis, we have\n\\[H_0: \\bar Y_1 = \\bar Y_2  = \\cdots = \\bar Y_{C-1}  =  \\bar Y_C \\] Note that this is the same null hypothesis as one-way ANOVA – i.e., all the group means are equal. Consequently, the F-test of R-squared using a categorical predictor is equivalent to the F-test of the omnibus hypothesis in one-way ANOVA (assuming homoskedasticity). In the Section 4.9, we show how the two approaches produce the same numerical values.\nAlthough regression with a categorical predictor is mathematically equivalent to one-way ANOVA, you may have noticed that the two approahces are not used in the same way. In ANOVA, we first conduct an F-test of the omnibus hypothesis, and, if the test is significant, we follow-up by comparing the groups using procedures that control for familywise error rate. In regression, we use contrast coding and just report the F-test along with the test of the individual effects, without any discussion of familywise error rate control. Which approach is correct? Depends who you ask…."
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-categorical-results-4",
    "href": "ch4_categorical_predictors.html#sec-categorical-results-4",
    "title": "4  Categorical predictors",
    "section": "4.7 Some details about categorical data*",
    "text": "4.7 Some details about categorical data*\nCheck back later for:\n\nmean and variance of binary variables\ncovariance between binary variables and between binary and continuous\nregression coefficient when X is binary"
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-workbook-4",
    "href": "ch4_categorical_predictors.html#sec-workbook-4",
    "title": "4  Categorical predictors",
    "section": "4.8 Workbook",
    "text": "4.8 Workbook\nThis section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in Section 2.11. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 4.2\nPlease take a moment to write down your thoughts on the tensions that arise when conceptualizing social constructs such as gender or race as categorical, and I will invite you to share you thoughts in class.\nSection 4.3\nPlease take a moment to write down how the regression output is related to the Figure. In particular, what is \\(\\bar Y_0\\) equal to, what is equal \\(\\bar Y_0\\) to, and what is their difference equal to?\n\n\nCode\n# Don't read this unless you really like working on graphics :) \nbinary_gender &lt;- (gender == \"Male\") * 1 \nplot(binary_gender, achrdg08, \n     col = \"#4B9CD3\", \n     xlab = \"binary gender (Female = 0, Male = 1)\",\n     ylab = \"Reading (Grade 8)\")\n\nmeans &lt;- tapply(achrdg08, binary_gender, mean)\nlabels &lt;- c(expression(bar(Y)[0]), expression(bar(Y)[1]))\n\ntext(x = c(.1, .9), y = means, labels = labels, cex = 1.5) \ntext(x = c(0, 1), y = means, labels = c(\"_\", \"_\"), cex = 2) \n\n\n\n\n\n\n\n\n\n\n\nCode\n# convert \"Female / Male\" coding to binary\ngender &lt;- NELS$gender\nbinary_gender &lt;- (gender == \"Male\")*1\nmod_binary &lt;- lm(achrdg08 ~ binary_gender)\ncoef(mod_binary)\n\n\n  (Intercept) binary_gender \n   56.4678022    -0.9223396 \n\n\nSection 4.3.1\nWe will go through the comparison between these two outputs in class together. If you have any questions about the relation between these two sets of output, please note them now and be prepared ask them in class.\nRegression with a binary predictor:\n\n\nCode\nsummary(mod_binary)\n\n\n\nCall:\nlm(formula = achrdg08 ~ binary_gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.7278  -6.1472   0.3784   6.9765  15.0045 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.4678     0.5342 105.703   &lt;2e-16 ***\nbinary_gender  -0.9223     0.7928  -1.163    0.245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.827 on 498 degrees of freedom\nMultiple R-squared:  0.00271,   Adjusted R-squared:  0.0007076 \nF-statistic: 1.353 on 1 and 498 DF,  p-value: 0.2452\n\n\nIndependent samples t-test (with homogeneity of variance assumed):\n\n\nCode\nt.test(achrdg08 ~ binary_gender, var.equal = T)\n\n\n\n    Two Sample t-test\n\ndata:  achrdg08 by binary_gender\nt = 1.1633, df = 498, p-value = 0.2452\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.6353793  2.4800586\nsample estimates:\nmean in group 0 mean in group 1 \n       56.46780        55.54546 \n\n\nSection 4.4.2\nRegressing Age on the dummies we have:\n\\[ \\widehat Y = b_0 + b_1 X_1 + b_2 X_2 \\]\nIn order to interpret the regression coefficients we can use the same two steps as in Section 4.3\n\nStep 1. Plug the values for the \\(X\\) variables into the regression equation to get the predicted values for each group\n\n\\[\\begin{align}\n\\widehat Y (Single) & = b_0 + b_1 (0) + b_2 (0) = b_0 \\\\\n\\widehat Y (Married) & = b_0 + b_1 (1) + b_2 (0) = b_0 + b_1 \\\\\n\\widehat Y (Divorced) & = b_0 + b_1 (0) + b_2 (1) = b_0 + b_2\n\\end{align}\\]\n\nStep 2. Solve for the model parameters in terms the predicted values.\n\n\\[\\begin{align}\nb_0 & = \\widehat Y (Single) \\\\\nb_1 & = \\widehat Y (Married) - b_0 = \\widehat Y (Married) - \\widehat Y (Single) \\\\\nb_2 & = \\widehat Y (Divorced) - b_0 = \\widehat Y (Divorced) - \\widehat Y (Single)\n\\end{align}\\]\nUsing the above equations, please write down an interpretation of the regression parameters for the hypothetical example. (Note: this question is not asking for a numerical answer, it is just asking you to put the above equations into words.)\nSection 4.4.3\nThe Mstatus example with 4 categories:\n\n\nCode\nknitr::include_graphics(\"files/images/marital_status2.png\")\n\n\n\n\n\n\n\n\n\nHere we are interested in the model:\n\\[\\widehat Y = b_0 + b_1 X_1 + b_2 X_2 + b_3 X_3\\]\nPlease work through the following questions and be prepared to share your answers in class\n\nHow should \\(X_3\\) be coded so that “single” is the reference-group?\nWrite down the contrast matrix for the example.\nUsing the two-step approach illustrated above, write out the interpretation of the model parameters in the following regression equation:\nWhat happens if we include an additional dummy variable for “Single” in the model. In this case we would have \\(C = 4\\) dummies in the model rather than \\(C-1 = 3\\) dummies. Please take a moment to write down what you think will happen (Hint: something goes wrong)\n\nSection 4.5.1\nPlease take a moment to write down the contrast matrix for the IDELA example:\n\\[\n\\begin{matrix} &  X_1 & X_2 \\\\\n                  \\text{Ethiopia} & ? & ? \\\\\n                  \\text{Vietnam} &?  & ? \\\\\n                  \\text{Bolivia}  & ? & ? \\\\\n\\end{matrix}\n\\]\nSection 4.5.2\nRegressing IDELA on the dummies we have:\n\\[\\widehat Y = b_0 + b_1 X_1 + b_2 X_2\\]\nIn order to interpret the regression coefficients, we proceed using the same two steps as in Section 4.3 and Section 4.4.\n\nStep 1. Plug the values for \\(X\\) into the regression equation to get the predicted values for each group.\n\n\\[\\begin{align}\n\\widehat Y (Ethiopia) & = b_0 + b_1 (1) + b_2 (0) = b_0 + b_1\\\\\n\\widehat Y (Vietnam) & = b_0 + b_1 (0) + b_2 (1) = b_0 + b_2 \\\\\n\\widehat Y (Bolivia) & = b_0 + b_1 (-1) + b_2 (-1) = b_0 - b_1 - b_2\n\\end{align}\\]\n\nStep 2. Solve for the model parameters in terms of the predicted values.\n\n\\[\\begin{align}\nb_1 &= \\widehat Y (Ethiopia) - b_0 \\\\\nb_2 &= \\widehat Y (Vietnam) - b_0 \\\\\nb_0 & = \\widehat Y (Bolivia) + b_1 + b_2 \\\\  \n\\end{align}\\]\nAt this point, we want to use the first two lines to substitute in for \\(b_1\\) and \\(b_2\\) in the last line, which leads to\n\\[\\begin{align}\nb_0 & = \\frac{\\widehat Y (Bolivia) + \\widehat Y (Ethiopia) + \\widehat Y (Vietnam)}{3}\n\\end{align}\\]\nIn the last line we see that \\(b_0\\) is equal to the (unweighted) mean of the predicted values.\nUsing the above equations, please write down an interpretation of the regression parameters for the hypothetical example. (Note: this question is not asking for a numerical answer, it is just asking you to put the above equations into words.)"
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-exercises-4",
    "href": "ch4_categorical_predictors.html#sec-exercises-4",
    "title": "4  Categorical predictors",
    "section": "4.9 Exercises",
    "text": "4.9 Exercises\nThese exercises provide an overview of contrast coding with categorical predictors in R. Two preliminary topics are also discussed: linear regression with a binary predictor, and the factor data class in R.\n\n4.9.1 A single binary predictor\nRegression with a single binary predictor is equivalent to an independent groups t-test of a difference between means. Let’s illustrate this using reading achievement in grade 8 (achrdg08) and gender in the NELS dataset. Note that, although the construct gender need not be conceptualized as dichotomous or even categorical, the variable gender reported in NELS data is dichotomous, with values “Female” and “Male”.\nR treats variables like gender as “factors”. Factors have special properties that we are going to work with later on, but for now let’s recode gender to binary_gender by setting Female = 0 and Male = 1\n\n\n4.9.2 Recoding a factor to numeric\n\n\nCode\n# load(\"NELS.RData\")\n# attach(NELS)\n\n# Note that gender is non-numeric -- it is a factor with levels \"Female\" and \"Male\"\nhead(gender)\n\n\n[1] Male   Female Male   Female Male   Female\nLevels: Female Male\n\n\nCode\nclass(gender)\n\n\n[1] \"factor\"\n\n\nCode\n# A trick to create a binary indicator for males   \nbinary_gender &lt;- (gender == \"Male\") * 1 \n\n# Check that the two variables are telling us the same thing\ntable(gender, binary_gender)\n\n\n        binary_gender\ngender     0   1\n  Female 273   0\n  Male     0 227\n\n\nIt is often hassle to get from factor to numeric or vice versa. Above we used the trick of first creating from the factor a logical vector (gender == \"Male\") and then coercing the logical vector to binary by multiplying it by 1. Other strategies can be used. See help(factor) for more information.\n\n\n4.9.3 Binary predictors and independent samples t-tests\nNow let’s get back to regressing reading achievement (achrdg08) on binary_gender, and comparing this to an independent groups t-test using the same two variables.\n\n\nCode\n# Regression with a binary variable\nmod1 &lt;- lm(achrdg08 ~ binary_gender)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = achrdg08 ~ binary_gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.7278  -6.1472   0.3784   6.9765  15.0045 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.4678     0.5342 105.703   &lt;2e-16 ***\nbinary_gender  -0.9223     0.7928  -1.163    0.245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.827 on 498 degrees of freedom\nMultiple R-squared:  0.00271,   Adjusted R-squared:  0.0007076 \nF-statistic: 1.353 on 1 and 498 DF,  p-value: 0.2452\n\n\nCode\n# Compare to the output of t-test\nt.test(achrdg08 ~ binary_gender, var.equal = T)\n\n\n\n    Two Sample t-test\n\ndata:  achrdg08 by binary_gender\nt = 1.1633, df = 498, p-value = 0.2452\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.6353793  2.4800586\nsample estimates:\nmean in group 0 mean in group 1 \n       56.46780        55.54546 \n\n\nNote that:\n\nThe intercept in mod1 is equal the mean of the group coded 0 (females) in the t-test:\n\n\\[b_0 = \\bar Y_0 = 56.4678\\]\n\nThe b-weight in mod1 is equal to difference between the means:\n\n\\[b_1 = \\bar Y_1 - \\bar Y_0 = 55.54546 - 56.4678 = -0.9223\\]\n\nThe t-test of the b-weight, and its p-value, are equivalent to the t-test mean difference (except for the sign):\n\n\\[ t(498) = 1.1633, p = .245\\]\nIn summary, a t-test of a b-weight of a binary predictor is equal equivalent to a t-test of the mean difference.\n\n\n4.9.4 Reference-group coding\nAs discussed in Section @ref(reference-group-coding-5), the basic idea of contrast coding is to replace a categorical variable with \\(C\\) categories with \\(C-1\\) “dummy” variables. In reference-group coding, the dummy variables are binary, and the resulting interpretation is:\n\nThe intercept is interpreted as the mean of the reference group. The reference is group is chosen by the analyst – it is the group that is coded zero on all dummies, or the group whose dummy variable is “left out” of the \\(C-1\\) dummy variables.\nThe regression coefficients of the dummy variables are interpreted as the difference between the mean of the indicated group and the mean of the reference group\n\nReference-group coding is the default contrast coding in R. However, in order for contrast coding to be implemented, our categorical predictor needs to be represented in R as a factor.\n\n\n4.9.5 More about factors\nFactors are the way that R deals with categorical data. If you want to know if your variable is a factor or not, you can use the functions class or is.factor. Let’s illustrate this with the urban variable from NELS.\n\n\nCode\n# Two ways of checking what type a variable is \nclass(urban)\n\n\n[1] \"factor\"\n\n\nCode\nis.factor(urban)\n\n\n[1] TRUE\n\n\nCode\n# Find out the levels of a factor using \"levels\"\nlevels(urban)\n\n\n[1] \"Rural\"    \"Suburban\" \"Urban\"   \n\n\nCode\n# Find out what contrast coding R is using for a factor \"constrasts\"\ncontrasts(urban)\n\n\n         Rural Suburban\nRural        1        0\nSuburban     0        1\nUrban       -1       -1\n\n\nIn the above code, we see that urban is a factor with 3 levels and the default reference-group contrasts are set up so that Rural is the reference group. Below we will show how to change the contrasts for a variable.\nIf we are working with a variable that is not a factor, but we want R to treat it as a factor, we can use the factor command. Let’s illustrate this by turning binary_gender back into a factor.\n\n\nCode\n# Change a numeric variable into a factor\nclass(binary_gender)\n\n\n[1] \"numeric\"\n\n\nCode\nfactor_gender &lt;- factor(binary_gender)\nclass(factor_gender)\n\n\n[1] \"factor\"\n\n\nCode\nlevels(factor_gender)\n\n\n[1] \"0\" \"1\"\n\n\nWe can also use the levels function to tell R what labels we want it to use for our factor. levels should be assigned a text vector with length equal to the number of levels of the variable. The entries of the assigned vector will be the new level names of the factor.\n\n\nCode\n# Change the levels of factor_gender to \"F\" and \"M\"\nlevels(factor_gender) &lt;- c(\"Female\", \"Male\")\nlevels(factor_gender)\n\n\n[1] \"Female\" \"Male\"  \n\n\n\n\n4.9.6 Back to reference-group coding\nOK, back to reference coding. Let’s see what lm does when we regress achrdg08 on urban.\n\n\nCode\nmod2 &lt;- lm(achrdg08 ~ urban)\nsummary(mod2)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.2577     0.4021 139.897   &lt;2e-16 ***\nurbanRural     -1.2650     0.5654  -2.237   0.0257 *  \nurbanSuburban  -0.5975     0.5299  -1.128   0.2600    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nIn the output, we see that two regression coefficients are reported, one for Suburban and one for Urban. As discussed in Section @ref(reference-group-coding-5), these coefficients are the mean difference between the indicated group and the reference group (Rural).\nWe can see that Urban students scores significantly higher than Rural students (3.125 percentage points), but there was no significant difference between Rural and Suburban students.\nThe intercept is the mean of the reference group (rural) – about 55% on the reading test.\nNote the R-squared – Urbanicity accounts for about 2% of the variation reading achievement. As usual, the F-test of R-squared has degrees of freedom \\(K\\) and \\(N - K -1\\), but now \\(K\\) (the number of predictors) is equal to \\(C - 1\\) – the number of categories minus one.\n\n\n4.9.7 Changing the reference group\nWhat if we wanted to use a group other than Rural as the reference group? We can chose a different reference group using the cont.treatment function. This function takes two arguments\n\nn tells R how many levels there\nbase tells R which level should be the reference group\n\n\n\nCode\n# The current reference group is Rural\ncontrasts(urban)\n\n\n         Rural Suburban\nRural        1        0\nSuburban     0        1\nUrban       -1       -1\n\n\nCode\n# Chance the reference group to the Urban (i.e., the last level)\ncontrasts(urban) &lt;- contr.treatment(n = 3, base = 3)\ncontrasts(urban)\n\n\n         1 2\nRural    1 0\nSuburban 0 1\nUrban    0 0\n\n\nNote that when we first ran contrasts(urban), the column names were names of the levels. But after changing the reference group, the column names are just the numbers 1 and 2. To help interpret the lm output, it is helpful to name the contrast levels appropriately\n\n\nCode\n# Naming our new contrasts\ncolnames(contrasts(urban)) &lt;- c(\"Rural\", \"Suburban\")\ncontrasts(urban)\n\n\n         Rural Suburban\nRural        1        0\nSuburban     0        1\nUrban        0        0\n\n\nNow we are ready to run our regression again, this time using a different reference group.\n\n\nCode\nmod3 &lt;- lm(achrdg08 ~ urban)\nsummary(mod3)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    58.1202     0.7901  73.559  &lt; 2e-16 ***\nurbanRural     -3.1275     1.0480  -2.984  0.00298 ** \nurbanSuburban  -2.4600     0.9907  -2.483  0.01335 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nCompared to the output from mod2, note that\n\nThe intercept now represents the mean reading scores of the Urban group, because this is the new reference group.\nThe regression coefficients now represent the mean difference between the indicated group with the new reference group.\nThe R-square and F stay the same – in other words, the total amount of variation explained by the variable urban does not change, just because we changed the reference group.\n\n\n\n4.9.8 Deviation coding\nIt is possible to change R’s default contrast coding to one of the other built-in contrasts (see help(contrasts) for more information on the built in contrasts).\nFor instance, to change to deviation coding, we use R’s contr.sum function and tell it how many levels there are for the factor (n). In deviation coding, the intercept is equal to the unweighted mean of the predicted values, and the regression coefficients are difference between the indicated group and the unweighted mean.\n\n\nCode\ncontrasts(urban) &lt;- contr.sum(n = 3)\ncontrasts(urban)\n\n\n         [,1] [,2]\nRural       1    0\nSuburban    0    1\nUrban      -1   -1\n\n\nCode\n# As above, it is helpful to name the contrasts using \"colnames\"\ncolnames(contrasts(urban)) &lt;- c(\"Rural\", \"Suburban\")\n\n\nNow we are all set to use deviation coding with lm.\n\n\nCode\nmod4 &lt;- lm(achrdg08 ~  urban)\nsummary(mod4)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.2577     0.4021 139.897   &lt;2e-16 ***\nurbanRural     -1.2650     0.5654  -2.237   0.0257 *  \nurbanSuburban  -0.5975     0.5299  -1.128   0.2600    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nNote the following things about the output:\n\nThe regression coefficients compare each group’s mean to the unweighted mean of the groups. Rural and Suburban students are below the unweighted mean, but the difference is only significant for Rural.\nAlthough the output looks similar to that of mod3, the coefficients are all different and they all have different interpretations. The lm output doesn’t tell us this, we have to know what is going on under the hood.\nThe R-square does not change from mod2 – again, the type of contrast coding used doesn’t affect how much variation is explained by the predictor.\n\n\n\n4.9.9 Extra: Relation to ANOVA\nAs our next exercise, let’s compare the output of lm and the output of aov – R’s module for Analysis of Variance. If regression and ANOVA are really doing the same thing, we should be able to illustrate it with these two modules. Note that if you check out help(aov), it explicitly states that aov uses the lm function, so, finding that the two approaches give similar output shouldn’t be a big surprise!\n\n\nCode\n# Run our model as an ANOVA\naov1 &lt;- aov(achrdg08 ~ urban)\n\n# Compare the output with lm\nsummary(aov1)\n\n\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nurban         2    741   370.5   4.824 0.00841 **\nResiduals   497  38163    76.8                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nsummary(mod2)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.2577     0.4021 139.897   &lt;2e-16 ***\nurbanRural     -1.2650     0.5654  -2.237   0.0257 *  \nurbanSuburban  -0.5975     0.5299  -1.128   0.2600    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nIf we compare the output from aov to the F-test reported by lm we see that the F-stat, degrees of freedom, and p-value are all identical. If we compute eta-squared from aov, we also find that it is equal to the R-squared value from lm.\n\n\nCode\n# Compute eta-squared from aov output\n741 / (741 + 38163)\n\n\n[1] 0.01904688\n\n\nIn short, ANOVA and regression are doing the same thing: R-squared is the same as omega-squared and the ANOVA omnibus F-test is the same as the F-test of R-squared. The main difference is that lm focuses on contrasts for analyzing and interpreting the group differences, whereas ANOVA focuses on the F-test of the omnibus hypothesis and the procedures for analyzing group difference are conducted as a follow-up step.\n\n\n4.9.10 Extra: Group-mean coding\nAs a step towards addresses the issues with deviation coding, let’s consider another coding procedure. If we omit the intercept term, all of the reference-group coded dummies can be included and the regression coefficients now correspond to means of each group. We omit the intercept using -1 in the model formula.\n\n\nCode\n# Omit the intercept using -1\nmod5 &lt;- lm(achrdg08 ~ -1 + urban)\nsummary(mod5)\n\n\n\nCall:\nlm(formula = achrdg08 ~ -1 + urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \nurbanRural     54.9927     0.6885   79.88   &lt;2e-16 ***\nurbanSuburban  55.6602     0.5976   93.14   &lt;2e-16 ***\nurbanUrban     58.1202     0.7901   73.56   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.9763,    Adjusted R-squared:  0.9761 \nF-statistic:  6822 on 3 and 497 DF,  p-value: &lt; 2.2e-16\n\n\nNote the following things about the output:\n\nThe coefficients are no longer interpreted mean differences, just the raw means.\nThe R-square and F-test do change from mod2. When the intercept is omitted, R-squared can no longer interpretated as a proportion of variance unless the variable \\(Y\\) variable is centered. When the intercept is omitted, it also changes the degrees of freedom in the F-test, because there are now 3 predictors instead of 2. In general, when the intercept is omitted, R-square and its F-test do not have the usual interpretation. When reporting R-squared and its F-test, we should use a model with the intercept.\n\nBy itself, group mean coding is not very interesting uninteresting – we don’t usually want to test whether the group means are different from zero. However, we will see in the next section that it can be used to provide an alternative to deviation coding.\n\n\n4.9.11 Extra: Weighted versus unweighted deviation coding\nThis section presents a “hack” for addressing the two main issues with deviation coding noted in Section @ref(deviation-coding-5). This is a hack in the sense that we are working around R’s usual procedures rather than replacing them with a completely new procedure. The result of this hack is to provide deviation coding in which comparisons are made to grand mean on the outcome for all \\(C\\) categories, not just \\(C-1\\) categories. As a side effect, the F-test for the R-squared statistic is no longer correct, so you should not use this approach to report R-squared.\nFirst, note that the group sample sizes are not equal for ubran\n\n\nCode\ntable(urban)\n\n\nurban\n   Rural Suburban    Urban \n     162      215      123 \n\n\nBecause of this, the unweighted mean of the group means (i.e., the intercept in mod4 above) is not equal to the overall mean on achrdg08 – we can see that the overall mean and the unweighted group means are slightly different for these data:\n\n\nCode\ngroup_means &lt;- tapply(achrdg08, INDEX = urban, FUN = mean)\ngroup_means\n\n\n   Rural Suburban    Urban \n54.99265 55.66019 58.12016 \n\n\nCode\n# Compare the overall mean and the unweighted mean of the group means\n# Center the outcome variable\nybar &lt;- mean(achrdg08)\nybar\n\n\n[1] 56.04906\n\n\nCode\nmean(group_means)\n\n\n[1] 56.25767\n\n\nNote that the intercept in mod4 is not equal to the overall mean, ybar, but is instead equal to the unweighted average of the group means, mean(group_means). the difference isnt very big in this example, but it can be quite drastic with highly unequal sample sizes.\nIf you would like to compare the groups to the overall mean when the sample sizes are unequal, the simplest way to do this in R is by first centering the Y variable and then using group mean coding.\nAfter centering, the grand mean of \\(Y\\) is zero, and so the group means represent deviations from the grand mean (i.e., deviations from zero) and the tests of the regression coefficients are tests of whether the group means are different from the grand mean.\n\n\nCode\n# Change the contrasts back to reference group\ncontrasts(urban) &lt;- contr.treatment(n = 3, base  = 1)\ncolnames(contrasts(urban)) &lt;- c(\"Suburban\", \"Urban\")\n\n# Center the outcome variable\ndev_achrdg08 &lt;- achrdg08 - ybar\n\n# Run the regression with group mean coding\nmod6 &lt;- lm(dev_achrdg08 ~ -1 + urban)\nsummary(mod6)\n\n\n\nCall:\nlm(formula = dev_achrdg08 ~ -1 + urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \nurbanRural     -1.0564     0.6885  -1.534  0.12556   \nurbanSuburban  -0.3889     0.5976  -0.651  0.51554   \nurbanUrban      2.0711     0.7901   2.621  0.00903 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.01312 \nF-statistic: 3.216 on 3 and 497 DF,  p-value: 0.02264\n\n\nNote the following things about the output:\n\nThe regression coefficients compare each group’s mean to the overall mean on the outcome. Urban students are significantly above average, Rural and Suburban students are below average but the difference is not significant.\nThe R-square is the same as mod2 because the Y variable is centered, but the F test does change from mod2, because there are now 3 variables included in the model. In general, when the intercept is omitted, R-square and its F-test do not have the usual interpretation. So, when reporting R-squared and F-test, we should use a model with the intercept, rather than the approach outlined here."
  }
]