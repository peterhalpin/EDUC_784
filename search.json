[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDUC 784",
    "section": "",
    "text": "Preface\nThese are the course notes for EDUC 784. Readings are assigned before class. Sections denoted with an asterisk (*) are optional.\nThe notes contain questions that are written in bold font. The questions are also collected in a section called “Workbook” that appears towards the end of each chapter. During class time, we will discuss the Workbook questions, your answers, any additional question you have, etc. It is really important for you to do the readings, and write down your responses to the questions, before class. You won’t get much out of the lessons if you haven’t done this preparation.\nSome chapters contain a section called “Exercises” that collects all of the R code from that chapter into a single overall workflow. You don’t need to do the Exercises before class, but you can if you want to. If a chapter doesn’t have an Exercises section, that means we will be working on an assignment together instead.\nImage credit: Daniela Rodriguez-Mincey, Spring 2023"
  },
  {
    "objectID": "ch1_review.html#sec-summation-1",
    "href": "ch1_review.html#sec-summation-1",
    "title": "1  Review",
    "section": "1.1 Summation notation",
    "text": "1.1 Summation notation\nSummation notation uses the symbol \\(\\Sigma\\) to stand-in for summation. For example, instead of writing\n\\[ X_1 + X_2 + X_3 + .... + X_N\\]\nto represent the sum of the values of the variable \\(X\\) in a sample of size \\(N\\), we can instead write:\n\\[ \\sum_{i=1}^{N} X_i. \\]\nThe symbol \\(\\Sigma\\) means “add.” The symbol is called “Sigma” – it’s the capital Greek letter corresponding to the Latin letter “S”. The value \\(i\\) is called the index, and \\(1\\) is the starting value of the index and \\(N\\) is the end value of the index. You can choose whatever start and end values you want to sum over. For example, if we just want to add the second and third values of \\(X\\), we write\n\\[ \\sum_{i=2}^{3} X_i = X_2 + X_3. \\]\nWhen the start and end values are clear from context, we can use a shorthand notation that omits them. In the following, it is implicit that the sum is over all available values of \\(X\\) (i.e., from \\(1\\) to \\(N\\)):\n\\[ \\sum_i X_i. \\]"
  },
  {
    "objectID": "ch1_review.html#sec-rules-1",
    "href": "ch1_review.html#sec-rules-1",
    "title": "1  Review",
    "section": "1.2 Rules of summation",
    "text": "1.2 Rules of summation\nThere are rules for manipulating summation notation that are useful for deriving results in statistics. You don’t need to do mathematical proofs or derivations in this class, but you will occasionally see some derivations in these notes (mainly in the optional sections).\nHere are the rules:\nRule 1: Sum of a constant (multiplication). Adding a constant to itself is equivalent to multiplication.\n\\[\\begin{align}\n\\sum_{i = 1}^{N} c &= c + c +  .... \\\\\n& = Nc\n\\end{align}\\]\nRule 2: Distributive property. Multiplying each term of a sum by a constant is the same as multiplying the sum by a constant.\n\\[\\begin{align}\n\\sum_{i = 1}^{N} c X_i &= cX_1 + cX_2 + .... \\\\\n& = c(X_1 + X_2 + ....) \\\\\n& = c \\sum_{i = 1}^{N} X_i\n\\end{align}\\]\nRule 3: Associative property. It doesn’t matter what order we do addition in.\n\\[\\begin{align}\n\\sum_{i = 1}^{N} (X_i + Y_i) &= (X_1 + Y_1) + (X_2 + Y_2) + .... \\\\\n& = (X_1 + X_2 + ....) + (Y_1 + Y_2 + ....) \\\\\n& = \\sum_{i = 1}^{N} X_i  + \\sum_{i = 1}^{N} Y_i\n\\end{align}\\]"
  },
  {
    "objectID": "ch1_review.html#sec-stats-1",
    "href": "ch1_review.html#sec-stats-1",
    "title": "1  Review",
    "section": "1.3 Sample statistics",
    "text": "1.3 Sample statistics\nSummation notation is useful for writing the formulas of statistics. The main statistics we use in the class are the mean, standard deviation, variance, covariance, and correlation. These are the building blocks for regression. Their symbols and formulas are presented below (using the shorthand summation notation). If you don’t remember their interpretation, you will need to go back to your Stat 1 notes.\n\nThe mean\n\n\\[\\bar X = \\frac{\\sum_i X_i}{N}\\]\n\nThe variance can be written as \\(\\text{var}(X)\\) or sometimes using the symbol \\(s^2\\)\n\n\\[ \\text{var}(X) = \\frac{\\sum_i (X_i - \\bar X)^2}{N - 1} \\]\n\nThe standard deviation can be written \\(\\text{SD}(X)\\) or using the letter \\(s\\)\n\n\\[ \\text{SD}(X) = \\sqrt{\\text{var}(X)} \\]\n\nThe covariance is a generalization of the variance to two variables, it describes how they co-vary:\n\n\\[\\text{cov}(X, Y) = \\frac{\\sum_i (X_i - \\bar X) (Y_i - \\bar Y)}{N - 1} \\]\n\nThe correlation is the covariance divided by the product of the standard deviations of the variables. It takes on values between -1 and 1 and describes the strength and direction of the linear relationship between two variables.\n\n\\[\\text{cor}(X, Y) = \\frac{\\text{cov}(X, Y)}{\\sqrt{\\text{var}(X)} \\sqrt{\\text{var}(Y)}} \\]\nFor numerical examples see Section 1.10.8."
  },
  {
    "objectID": "ch1_review.html#sec-properties-1",
    "href": "ch1_review.html#sec-properties-1",
    "title": "1  Review",
    "section": "1.4 Properties of sample statistics",
    "text": "1.4 Properties of sample statistics\nThe following are some useful properties of the sample statistics reviewed above. The properties tell us what happens to means, variances, covariances, and correlations when a variable is linearly transformed. We often linearly transform data (e.g., to compute percentages, proportions, z-scores, and in linear regression), so these properties turn out to be quite handy.\nYou can derive the properties using the rules of summation. For each property, the beginning of the derivation is shown. You should know the properties but completing the derivations is optional.\nSum of deviations from the mean. If we subtract the mean from each data point, we have a deviation (or deviation score): \\(d_i = X_i - \\bar X\\). Deviation scores sum to zero: \\(\\sum_i d_i = 0\\).\n\nDerivation\n\n\\[ \\sum_i d_i = \\sum_i(X_i - \\bar X) = \\dots \\]\nMean of a linear transformation. If \\(Y_i = A + B X_i\\) with known constants \\(A\\) and \\(B\\), then \\(\\bar{Y} = A + B \\bar{X}\\)\n\nDerivation:\n\n\\[ \\bar{Y} =  \\frac{\\sum_i Y_i}{N} =  \\frac{\\sum_i( A + B X_i)}{N} = \\dots \\] Variance of a linear transformation. If \\(Y_i = A + B X_i\\) with known constants \\(A\\) and \\(B\\), then \\(\\text{var}(Y) = B^2\\text{var}(X)\\)\n\nDerivation\n\n\\[ \\text{var}(Y) =  \\frac{\\sum_i (Y_i - \\bar{Y})^2}{N-1} =  \\frac{\\sum_i ((A + B X_i) - (A + B \\bar{X}))^2}{N-1} = \\dots\\] Mean and variance of a z-score. The z-score (or standardized score) is defined as \\(Z_i = (X_i - \\bar{X}) / \\text{SD}(X)\\). Standardized scores are useful because \\(\\bar{Z} = 0\\) and \\(\\text{var}(Z) = 1\\).\n\nDerivation: use the rules for linear transformation with \\(A = -\\bar{X} / \\text{SD}(X)\\) and \\(B = 1/ \\text{SD}(X)\\).\n\nCovariance of linear transformations. If \\(Y_i = A + B X_i\\) and \\(W_i = C + D U_i\\) with known constants \\(A\\), \\(B\\), \\(C\\), \\(D\\), then \\(\\text{cov}(Y, W) = BD\\,\\text{cov}(X, U)\\)\n\nDerivation\n\n\\[ \\text{cov}(Y, W) =  \\frac{\\sum_i ((A + B X_i) - (A + B \\bar{X}))((C + D U_i) - (C + D \\bar{U}))}{N-1} = \\dots\\]\nCorrelation of linear transformations. If \\(Y_i = A + B X_i\\) and \\(W_i = C + D U_i\\) with known constants \\(A\\), \\(B\\), \\(C\\), \\(D\\), then \\(\\text{cor}(Y, W) = \\text{cor}(X, U)\\) – i.e., the correlation is not affected by linear transformations.\n\nDerivation: use the rules for variances and covariances of linear transformation and the formula for correlation."
  },
  {
    "objectID": "ch1_review.html#bias-and-precision",
    "href": "ch1_review.html#bias-and-precision",
    "title": "1  Review",
    "section": "1.5 Bias and precision",
    "text": "1.5 Bias and precision\nIn this section we consider two more important properties of sample statistics. These properties are defined in terms of sampling distributions. Recall that a sampling distribution arises from the following thought experiment:\n\nTake a random sample of size \\(N\\) from a population of interest.\nCompute a statistic using the sample data. It can be any statistic, but let’s say the mean, \\(\\bar X\\), for concreteness.\nWrite down the value of the mean, and then return the sample to the population.\n\nAfter doing these 3 steps many times, you will have many values the sample mean,\n\\[ \\bar{X}_1, \\bar{X}_2, \\bar{X}_3, \\bar{X}_4, \\bar{X}_5, ... \\]\nThe distribution of these sample means is called a sampling distribution (i.e., the sampling distribution of the mean). A sampling distribution is just like any other distribution – so it has its own mean, and its own variance, etc. These quantities, when computed for a sampling distribution, have special names.\n\nThe expected value of the mean, denoted \\(E(\\bar X)\\), is the mean of the sampling distribution of the mean. That is a mouthful! That is why we say the “expected value” or “expectation” of a statistic rather than the mean of a statistic. It’s called the expected value because it’s the average value over many samples.\nThe standard error of the mean, denoted \\(SE(\\bar X)\\), is the standard deviation of the sampling distribution of the mean. It describes the sample-to-sample variation of the mean around its expected value.\n\nNow for the two additional properties of sample statistics:\n\nBias: If the expected value of a statistic is equal to a population parameter, we say that the statistic is an unbiased estimate of that parameter. For example, the expected value of the sample mean is equal to the population mean (in symbols: \\(E(\\bar{X}) = \\mu)\\), so we say that the sample mean is an unbiased estimate of the population mean.\nPrecision: The inverse of the squared standard error (i.e., \\(1 / \\text{SE}(\\bar{X})^2\\)) is called the precision of a statistic. So, the less a statistic varies from sample to sample, the more precise it is. That should hopefully make intuitive sense. The main thing to know about precision is that it is usually increasing in the sample size – i.e., we get more precise estimates by using larger samples. Again, this should feel intuitive.\n\nBelow is a figure that is often used to illustrate the ideas of bias and precision. The middle of the concentric circles represent the target parameter (like a bull’s eye) and the dots represent the sampling distribution of a statistic. You should be able to describe each panel in terms of the bias and precision of the statistic.\n\n\n\nFigure 1.1: Bias and Precision"
  },
  {
    "objectID": "ch1_review.html#t-tests",
    "href": "ch1_review.html#t-tests",
    "title": "1  Review",
    "section": "1.6 t-tests",
    "text": "1.6 t-tests\nThe \\(t\\)-test is used to make an inference about the value of an unknown population parameter. The test compares the value of an unbiased estimate of the parameter to a hypothesized value of the parameter. It is assumed that the sampling distribution of the estimate is a normal distribution, so the \\(t\\)-test applies to statistics like means and regression coefficients.\nThe conceptual formula for a \\(t\\)-test is\n\\[ t = \\frac{\\text{unbiased estimate} - \\text{hypothesized value }}{\\text{standard error}} \\] When we conduct a \\(t\\)-test, the basic rationale is as follows: If the “true” population parameter is equal to the hypothesized value, then the estimate should be close to the hypothesized value, and so \\(t\\) should be close to zero.\nIn order to determine what values of \\(t\\) are “close to zero”, we refer to its sampling distribution, which is called the \\(t\\)-distribution. The \\(t\\)-distribution tells what values of \\(t\\) are typical if the hypothesis is true. Some examples of the \\(t\\)-distribution are shown in the figure below. The x-axis denotes values of the statistic \\(t\\) shown above, and \\(\\nu\\) is parameter called the “degrees of freedom” (more on this soon).\n\n\n\nFigure 1.2: t-distribution (source: https://en.wikipedia.org/wiki/Student%27s_t-distribution)\n\n\nYou can see that the \\(t\\)-distribution looks like a normal distribution centered a zero. So, when the hypothesis is true, the expected value of \\(t\\) is zero. Informally, we could say that, if the hypothesis is true, values of \\(t\\) greater than \\(\\pm 2\\) are pretty unlikely, and values greater than \\(\\pm 4\\) are very unlikely.\nMore formally, we can make an inference about whether the population parameter is equal to the hypothesized value by comparing the value of \\(t\\) computed in our sample, denoted as \\(t_{\\text{obs}}\\), to a “critical value”, denoted as \\(t^*\\). The critical value is chosen so that the “significance level”, defined as \\(\\alpha = \\text{Prob}(|t| \\geq t^*)\\), is sufficiently small. The significance level is chosen by the researcher. Often it is set to \\(.05\\) or \\(.01.\\)\nThere are two equivalent ways of “formally” conducting a \\(t\\)-test.\n\nCompare the observed value of \\(t\\) to the critical value. Specifically: if \\(|t_{\\text{obs}}| &gt; t^*\\), reject the hypothesis.\nCompare the significance level chosen by the researcher, \\(\\alpha\\), to the “p-value” of the test, computed as \\(p = \\text{Prob}(|t| \\geq |t_{\\text{obs}}|)\\). Specifically: if \\(p &lt; \\alpha\\), reject the hypothesis.\n\nInformally, both of these just mean that the absolute value of \\(t\\) should be pretty big (i.e., greater than \\(t^*\\)) before we reject the hypothesis.\nA couple more things before moving on.\nFirst, the hypothesized value of the population parameter is often zero, in which case it is called a null hypothesis. The null hypothesis usually translates into a research hypothesis of “no effect” or “no relationship.” So, if we reject the null hypothesis, we conclude that there was an effect.\nSecond, the t-distribution has a single parameter called its “degrees of freedom”, which is denoted as \\(\\nu\\) in Figure 1.2. The degrees of freedom are an increasing function of the sample size, with larger samples leading to more degrees of freedom. When the degrees of freedom approach \\(\\infty\\), the \\(t\\)-distribution approaches a normal distribution. This means that the difference between a \\(t\\)-test and a \\(z\\)-test is pretty minor in large samples (say \\(N \\geq 100\\))."
  },
  {
    "objectID": "ch1_review.html#confidence-intervals",
    "href": "ch1_review.html#confidence-intervals",
    "title": "1  Review",
    "section": "1.7 Confidence intervals",
    "text": "1.7 Confidence intervals\nA confidence interval uses the same equation as a \\(t\\)-test, except we solve for the population parameter rather than the value of \\(t\\). Whereas a \\(t\\)-test lets us make a guess about specific value of the parameter of interest (i.e., the null-hypothesized value), a confidence interval gives us a range of values that include the parameter of interest, with some degree of “confidence.”\nConfidence intervals have the general formula:\n\\[\\text{Interval} = \\text{unbiased estimate} \\pm t \\times {\\text{standard error}}. \\] We get the value of \\(t\\) from the \\(t\\)-distribution. In particular, if we want the interval to include the true population parameter \\((1-\\alpha) \\times 100\\%\\) of the time, then we choose \\(t\\) to be the \\(\\alpha/2 \\times 100\\) percentile of the \\(t\\)-distribution. For example, if we set \\(\\alpha = .05\\), we will have a \\((1-\\alpha) \\times 100 = 95\\%\\) confidence interval by choosing \\(t\\) to be the \\(\\alpha/2 \\times 100 = 2.5\\)-th percentile of the \\(t\\)-distribution.\nAs mentioned, \\(t\\)-tests and confidence intervals are closely related. In particular, if the confidence interval includes the value \\(0\\), this is the same as retaining the null hypothesis that the parameter is equal to \\(0\\). This should make intuitive sense. If the confidence interval includes \\(0\\), we are saying that it is a reasonable value of the population parameter, so we should not reject that value. This equivalence between tests and confidence intervals assumes we use the same level of \\(\\alpha\\) for both of them.\nIn summary, if the confidence interval includes zero, we retain the null hypothesis at the stated level of \\(\\alpha\\). If the confidence interval does not include zero, we reject the null hypothesis at the stated level of \\(\\alpha\\)."
  },
  {
    "objectID": "ch1_review.html#f-tests",
    "href": "ch1_review.html#f-tests",
    "title": "1  Review",
    "section": "1.8 F-tests",
    "text": "1.8 F-tests\nThe \\(F\\)-test is used to infer if two independent variances have the same expected value. This turns out to be useful when we analyze the variance of a variable into different sources (i.e., Analysis of Variance or ANOVA).\nA variance can be defined as a sum-of-squares divided by its degrees of freedom. For example, the sample variance is just a sum-of-squared deviations from the sample mean (a sum of squares) divided by \\(N - 1\\) (its degrees of freedom).\nThe generic formula for an F-test is the ratio of two variances:\n\\[F = \\frac{SS_A / df_A}{SS_B / df_B}, \\]\nwhere \\(SS\\) denotes sums-of-squares and \\(df\\) denotes degrees of freedom.\nJust the like \\(t\\)-test, the \\(F\\)-test is called by the letter “F” because it has an \\(F\\)-distribution when the null hypothesis is true (i.e., when the variances have the same expected value). The plot below shows some examples of \\(F\\)- distributions. These distributions tell us the values of \\(F\\) that are likely, if the null hypothesis is true.\n\n\n\nFigure 1.3: F-distribution (source: https://en.wikipedia.org/wiki/F-distribution)\n\n\nThe F distribution has two parameters, which are referred to as the “degrees of freedom in the numerator” and the “degrees of freedom in the denominator” (in the figure, d1 and d2, respectively). We always write the numerator \\(df\\) first and then the denominator \\(df\\). So, the green line in the figure is “an \\(F\\)-distribution on 10 and 1 degrees of freedom”, which means the \\(df\\) in the numerator is 10 and the \\(df\\) in the denominator is 1.\nWe use an \\(F\\)-test the same way we use a \\(t\\)-test – we set a significance level and use this level to determine how large the value of \\(F\\) needs to be for us to reject the null hypothesis. The main difference is that \\(F\\) is non-negative, because it is the ratio of squared numbers. We don’t usually compute confidence intervals for statistics with an \\(F\\)-distribution."
  },
  {
    "objectID": "ch1_review.html#apa-reporting",
    "href": "ch1_review.html#apa-reporting",
    "title": "1  Review",
    "section": "1.9 APA reporting",
    "text": "1.9 APA reporting\nIt is important to write up the results of statistical analyses in a way that other people will understand. For this reason, there are conventions about how to report statistical results. In this class, we will mainly use tables and figures (formatted in R) rather than inline text. But sometimes reporting statistics using inline text unavoidable, in which case this course will use APA formatting. You don’t need to use APA in this class, but you should be familiar with some kind of conventions for reporting statistical results in your academic writing.\nThe examples below illustrate APA conventions. We haven’t covered the examples, they are just illustrative of the formatting (spacing, italics, number of decimal places, whether or not to use a leading zero before a decimal, etc). More details are available online (for example, here).\n\nJointly, the two predictors explained about 22% of the variation in Academic Achievement, which was statistically significant at the .05 level (\\(R^2 = .22, F(2, 247) = 29.63, p &lt; .001\\)).\nAfter controlling for SES, a one unit of increase in Maternal Education was associated with \\(b = 1.33\\) units of increase in Academic Achievement (\\(t(247) = 5.26, p &lt; .001\\)).\nAfter controlling for Maternal Education, a one unit of increase in SES was associated with \\(b = 0.32\\) units of increase in Academic Achievement. This was a statistically significant relationship (\\(t(247) = 2.91, p &lt; .01\\))."
  },
  {
    "objectID": "ch1_review.html#sec-exercizes-1",
    "href": "ch1_review.html#sec-exercizes-1",
    "title": "1  Review",
    "section": "1.10 Exercises",
    "text": "1.10 Exercises\nThis section will walk through some basics of programming with R. We will get started with this part of the review in the first class. You don’t need to do it before class.\nIf you are already familiar with R, please skim through the content and work on getting the NELS data loaded. If you are not familiar with R, or would like to brush up your R skills, you should work through this section.\n\n1.10.1 General info about R\nSome things to know about R before getting started:\n\nR is case sensitive. It matters if you useCAPS or lowercase in your code.\nEach new R command should begin on its own line.\nUnlike many other programming languages, R commands do not need to end with punctuation (e.g., ; or .).\nR uses the hash tag symbol (#) for comments. Comments are ignored by R but can be helpful for yourself and others to understand what your code does. An example is below.\n\n\n# This is a comment. R doesn't read it.\n# Below is a code snippet. R will read it and return the result. \n2 + 2\n\n[1] 4\n\n\n\nR’s working memory is cumulative. This means that you have to run code in order, one line after the next. It also means that any code you run is still hanging around in R’s memory until you clear it away using rm or the brush icon in R Studio - make sure to ask about how to do this in class if you aren’t sure.\n\n\n\n1.10.2 The basics\nAs we have just seen, R can do basic math like a calculator. Some more examples are presented in the code snippets below. R’s main math symbols are\n\n+ addition\n- subtraction or negative numbers\n* multiplication\n/ division (don’t use \\)\n^ or ** exponentiation\n\n\n2 * 2\n\n[1] 4\n\n\n\n# Remember pedmas? Make sure to use parentheses \"()\", \n# not brackets \"[]\" or braces \"{}\"\n(2 - 3) * 4 / 5 \n\n[1] -0.8\n\n\n\n# Exponentiation can be done two ways\n2^3\n\n[1] 8\n\n2**3\n\n[1] 8\n\n\n\n# Square roots are \"squirt\". Again, make sure to use \"()\", \n# not brackets \"[]\" or braces \"{}\"\nsqrt(25)\n\n[1] 5\n\n\n\n# Logs and exponents, base e (2.718282....) by default \nlog(100)\n\n[1] 4.60517\n\nexp(1)\n\n[1] 2.718282\n\n\n\n# We can override the default log by using the \"base\" option\nlog(100, base = 2)\n\n[1] 6.643856\n\n\n\n# Special numbers...\npi \n\n[1] 3.141593\n\n\n\n\n1.10.3 The help function\nThe help function is your best friend when using R. If we want more info on how to use an R function (like log), type:\n\nhelp(log)\n\nIf you don’t exactly remember the name of the function, using ??log will open a more complete menu of options.\n\n\n1.10.4 Logicals and strings\nR can also work with logical symbols that evaluate to TRUE or FALSE. R’s main logical symbols are\n\n== is equal to\n!= is not equal to\n&gt; greater than\n&lt; less than\n&gt;= greater than or equal to\n&lt;= less than or equal to\n\nHere are some examples:\n\n2 + 2 == 4\n\n[1] TRUE\n\n\n\n2 + 2 == 5\n\n[1] FALSE\n\n\n\n2 + 3 &gt; 5\n\n[1] FALSE\n\n\n\n2 + 3 &gt;= 5\n\n[1] TRUE\n\n\nThe main thing to note is that the logical operators return TRUE or FALSE as their output, not numbers. There are also symbols for logical conjunction (&) and disjunction (|), but we won’t get to those until later.\nIn addition to numbers and logicals, R can work with text (also called “strings”). We wont use strings a lot but they are worth knowing about.\n\n\"This is a string in R. The quotation marks tell R the input is text.\"\n\n[1] \"This is a string in R. The quotation marks tell R the input is text.\"\n\n\n\n\n1.10.5 Assignment (naming)\nOften we want to save the result of a calculation so that we can use it later on. In R, this means we need to assign the result a name. Once we assign the result a name, we can use that name to refer to the result, without having to re-do the calculation that produced the result. For example:\n\nx &lt;- 2 + 2\n\nNow we have given the result of 2 + 2 the name “x” using the assignment operator, &lt;-.\nNote that R no longer prints the result of the calculation to the console. If we want to see the result, we can type x\n\n# To see the result a name refers to, just type the name\nx \n\n[1] 4\n\n\nWe can also do assignment with the = operator.\n\ny = 2 + 2\ny\n\n[1] 4\n\n\nIt’s important to note that the = operator also gets used in other ways (e.g., to override default values in functions like log). Also, the math interpretation of “=” doesn’t really capture what is happening with assignment in computer code. In the above code, we are not saying that “2 + 2 equals y.” Instead, we are saying, “2 + 2 equals 4 and I want to refer to 4 later with the name ‘y’.”\nAlmost anything in R can be given a name and thereby saved in memory for later use. Assignment will become a lot more important when we name things like datasets, so that we can use the data for other things later on.\nA few other side notes:\n\nNames cannot include spaces or start with numbers. If you want separate words in a name, consider using a period ., an underscore _, or CamelCaseNotation.\nYou can’t use the same name twice. If you use a name, and then later on re-assign that same name to a different result, the name will now only represent the new result. The old result will no longer have a name, it will be lost in the computer’s memory and will be cleaned up by R’s garbage collector. Because R’s memory is cumulative, it’s important to keep track of names to make sure you know what’s what.\nR has some names that are reserved for built-in stuff, like log and exp and pi. You can override those names, but R will give a warning. If you override the name, this means you can’t use the built-in functions until you delete that name (e.g., rm(x)).\n\n\n\n1.10.6 Pop-quiz\n\nIn words, describe what the following R commands do.\n\nx &lt;- 7\nx = 7\nx == 7\n7 -&gt; x\n7 &gt; x\n\nAnswers: Check the commands in R.\n\n\n\n1.10.7 Vectors\nIn research settings, we often want to work with multiple numbers at once (surprise!). R has many data types or “objects” for doing this, for example, vectors, matrices, arrays, data.frames, and lists. We will start by looking at vectors.\nHere is an example vector, containing the sequence of integers from 15 to 25.\n\n# A vector containing the sequence of integers from 15 to 25\ny &lt;- 15:25\ny\n\n [1] 15 16 17 18 19 20 21 22 23 24 25\n\n\nWhen we work with a vector of numbers, sometimes we only want to access a subset of them. To access elements of a vector we use the square bracket notation []. Here are some examples of how to index a vector with R:\n\n# Print the first element of the vector y\n# Note: use brackets \"[]\" not parens\"()\"\ny[1]\n\n[1] 15\n\n\n\n# The first 3 elements\ny[1:3]\n\n[1] 15 16 17\n\n\n\n# The last 5\ny[6:11]\n\n[1] 20 21 22 23 24 25\n\n\nWe can also access elements of a vector that satisfy a given logical condition.\n\n# Print the elements of the vector y that are greater than the value 22\ny[y &gt; 22]\n\n[1] 23 24 25\n\n\nThis trick often comes in handy so its worth understanding how it works. First let’s look again at what y is, and what the logical statement y &gt; 22 evaluates to:\n\n# This is the vector y\ny\n\n [1] 15 16 17 18 19 20 21 22 23 24 25\n\n# This is the logical expression y &gt; 22\ny &gt; 22\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nWe can see that y &gt; 22 evaluates to TRUE or FALSE depending on whether the correspond number in the vector y is greater than 22. When we use the logical vector as an index – R will then return all the values for which y &gt; 22 is TRUE.\nIn general, we can index a vector y with any logical vector of the same length as y. The result will return only the values for which the logical vector is TRUE.\n\n\n1.10.8 Computing sample stats\nThe following are examples of statistical operations you can do with vectors of numbers. These examples follow closely to Section 1.1 to Section 1.4\n\n# Making a vector with the \"c\" command (combine) \nx &lt;- c(10, 9, 15, 15, 20, 17)\n\n# Find out how long a vector is (i.e., the sample size)\nlength(x)\n\n[1] 6\n\n# Add up the elements of a vector\nsum(x)\n\n[1] 86\n\n# Add up the elements of a subset of a vector\nsum(x[2:3])\n\n[1] 24\n\n# Check the distributive rule\nsum(x*2) == sum(x) * 2 \n\n[1] TRUE\n\n# Check the associative rule\ny &lt;- c(5, 11, 11, 19, 13, 15)\nsum(x) + sum(y) == sum(x + y) \n\n[1] TRUE\n\n# Compute the mean\nmean(x)\n\n[1] 14.33333\n\n# Compute the variance\nvar(x)\n\n[1] 17.46667\n\n# Compute the standard deviation\nsd(x)\n\n[1] 4.179314\n\n# Compute the covariance\ncov(x, y)\n\n[1] 10.66667\n\n# Compute the correlation\ncor(x, y)\n\n[1] 0.5457986\n\n\n\n\n1.10.9 Working with datasets\nMost of the time, we will be reading-in data from an external source. The easiest way to do this is if the data is in the .RData file format. Then we can just double-click the .Rdata file and Rstudio will open the file, or we can use the load command in the console – both do the same thing.\nTo get started, let’s load the NELS data. The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see https://nces.ed.gov/surveys/nels88/).\nThis data and codebook are available on the Canvas site of the course under “Files/Data/NELS” and are linked in the “Module” for Week 1. You need to download the data onto your machine and then open the data file (e.g., by clicking it, or double-clicking, or whatever you do to open files on your computer). That will do the same thing as the following line of code\n\n#This is what happens when you double-click NELS.RData\nload(\"NELS.RData\")\n\nThe function dim reports the number of rows (500 persons) and columns (48 variables) for the data set.\n\ndim(NELS)\n\n[1] 500  48\n\n\nIf you want to look at the data in a spreadsheet, use the following command. It won’t render anything in this book, but you can see what it does in R. (You may need to install XQuartz from https://www.xquartz.org if you are using a Mac.)\n\nView(NELS)\n\nIf you want to edit the data set using the spreadsheet, use edit(NELS). However, R’s spreadsheet editor is pretty wimpy, so if you want to edit data in spreadsheet format, use Excel or something.\nWorking with data is often made easier by “attaching”” the dataset. When a dataset it attached, this means that we can refer to the columns of the dataset by their names.\n\n# Attach the data set\nattach(NELS)\n\n# Print the first 10 values of the NELS gender variable\ngender[1:10]\n\n [1] Male   Female Male   Female Male   Female Female Female Female Male  \nLevels: Female Male\n\n\nWarning about attaching datasets. Once you attach a dataset, all of the column names in that dataset enter R’s working memory. If the column names in your dataset were already used, the old names are overwritten. If you attach the same dataset more than once in the same session, R will print a warning telling you that the previously named objects have been “masked” – this won’t affect your analyses, but it can be irritating.\nThe basic point: we should only attach each dataset once per R session. Once you are done using a data set it is good practice to detach it:\n\ndetach(NELS)\n\n\n\n1.10.10 Preview of next week\nFigure 1.4 shows the relationship between Grade 8 Math Achievement (percent correct on a math test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). Once you have reproduced this figure, you are ready to start the next chapter.\n\n# Load and attach the NELS data\nload(\"NELS.RData\")\nattach(NELS)\n\n# Scatter plot\nplot(x = ses, \n     y = achmat08, \n     col = \"#4B9CD3\", \n     ylab = \"Math Achievement (Grade 8)\", \n     xlab = \"SES\")\n\n# Run a simple linear regression \nmod &lt;- lm(achmat08 ~ ses)\n\n# Add the regression line to the plot\nabline(mod)\n\n# Detach the data set\ndetach(NELS)\n\n\n\n\nFigure 1.4: Math Achievement and SES (NELS88)."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-example-2",
    "href": "ch2_simple_regression.html#sec-example-2",
    "title": "2  Simple regression",
    "section": "2.1 An example from NELS",
    "text": "2.1 An example from NELS\nLet’s begin by considering an example. Figure 2.1 shows the relationship between Grade 8 Math Achievement (percent correct on a math test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see https://nces.ed.gov/surveys/nels88/).\n\n\nCode\n# Load and attach the NELS88 data\nload(\"NELS.RData\")\nattach(NELS)\n\n# Scatter plot\nplot(x = ses, \n     y = achmat08, \n     col = \"#4B9CD3\", \n     ylab = \"Math Achievement (Grade 8)\", \n     xlab = \"SES\")\n\n# Run the regression model\nmod &lt;- lm(achmat08 ~ ses)\n\n# Add the regression line to the plot\nabline(mod) \n\n\n\n\n\nFigure 2.1: Math Achievement and SES (NELS88).\n\n\n\n\nThe strength and direction of the linear relationship between the two variables is summarized by their correlation. In this sample, the value of the correlation is:\n\n\nCode\ncor(achmat08, ses)\n\n\n[1] 0.3182484\n\n\nThis is a moderate, positive correlation between Math Achievement and SES. This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in math (higher Math Achievement).\nThe relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., https://www.apa.org/pi/ses/resources/publications/education). Please look over this web page and be prepared to share your thoughts/questions about the relationship between SES and academic achievement and its relevance for education research."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-regression-line-2",
    "href": "ch2_simple_regression.html#sec-regression-line-2",
    "title": "2  Simple regression",
    "section": "2.2 The regression line",
    "text": "2.2 The regression line\nThe section presents three interchangeable ways of writing the regression line in Figure 2.1. You should be familiar with all three ways of presenting regression equations and you are welcome to use whichever approach you like best in your writing for this class.\nThe regression line in Figure 2.1 can be represented mathematically as\n\\[\n\\widehat Y = a + b X\n\\tag{2.1}\\]\nwhere\n\n\\(Y\\) denotes Math Achievement\n\\(X\\) denotes SES\n\\(a\\) represents the regression intercept (the value of \\(\\widehat Y\\) when \\(X = 0\\))\n\\(b\\) represents the regression slope (how much \\(\\widehat Y\\) changes for each unit of increase in \\(X\\))\n\nIn this equation, the symbol \\(\\widehat Y\\) represents the predicted value of Math Achievement for a given value of SES. In Figure 2.1, the predicted values are represented by the regression line. The observed values of Math Achievement are denoted as \\(Y\\). In Figure 2.1, these values are represented by the points in the scatter plot.\nSome more terminology: the \\(Y\\) variable is often referred to as the outcome or the dependent variable. The \\(X\\) variable is often referred to as the predictor, independent variable, explanatory variable, or covariate. Different areas of research have different conventions about terminology for regression. We talk more about “big picture” interpretations of regression in Chapter 3.\nThe difference between an observed value \\(Y\\) and its predicted value \\(\\widehat Y\\) is called a residual. Residuals are denoted as \\(e = Y - \\widehat Y\\). The residuals for a subset of the data points in Figure 2.1 are shown in pink in Figure 2.2\n\n\nCode\n# Get predicted values from regression model\nyhat &lt;- mod$fitted.values\n\n# select a subset of the data\nset.seed(10)\nindex &lt;- sample.int(500, 30)\n\n# plot again\nplot(x = ses[index], \n     y = achmat08[index], \n     ylab = \"Math Achievement (Grade 8)\", \n     xlab = \"SES\")\n\nabline(mod)\n\n# Add pink lines\nsegments(x0 = ses[index], \n         y0 = yhat[index], \n         x1 = ses[index], \n         y1 = achmat08[index], \n         col = 6, lty = 3)\n\n# Overwrite dots to make it look at bit better\npoints(x = ses[index], \n       y = achmat08[index], \n       col = \"#4B9CD3\", \n       pch = 16)\n\n\n\n\n\nFigure 2.2: Residuals for a Subsample of the Example.\n\n\n\n\nNotice that \\(Y = \\widehat Y + e\\) by definition:\n\\[\nY = \\widehat Y + e = \\widehat Y + (Y - \\widehat Y ) = Y.\n\\]\nThis leads to a second way of writing out a regression model:\n\\[\nY = a + bX + e.  \n\\tag{2.2}\\]\nThe difference between Equation 2.1 and Equation 2.2 is that the former lets us talk about the predicted values (\\(\\hat Y\\)), whereas the latter lets us talk about the observed data points (\\(Y\\)).\nA third way to write out the model is using the variable names (or abbreviations) in place of the more generic “X, Y” notation. For example,\n\\[MATH = a + b(SES) + e \\tag{2.3}\\]\nThis notation is useful when talking about a specific example, because we don’t have to remember what \\(Y\\) and \\(X\\) stand for. But this notation is more clunky and doesn’t lend itself talking about regression in general or writing other mathematical expressions related to regression.\nYou should be familiar with all three ways of presenting regression equations (Equation 2.1, Equation 2.2, and Equation 2.3) and you are welcome to use whichever approach you like best in this class."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-ols-2",
    "href": "ch2_simple_regression.html#sec-ols-2",
    "title": "2  Simple regression",
    "section": "2.3 OLS",
    "text": "2.3 OLS\nThis section talks about how to estimate the regression intercept (denoted as \\(a\\) in Equation 2.1) and the regression slope (denoted as \\(b\\) in Equation 2.1). The intercept and slope are collectively referred to as the parameters of the regression line. They are also referred to as regression coefficients.\nOur overall goal in this section is to “fit a line to the data” – i.e., we want to select the values of the regression coefficients that best represent our data. An intuitive way to approach this problem is by minimizing the residuals – i.e., minimizing the total amount of pink in Figure 2.2. We can operationalize this intuitive idea by minimizing the sum of squared residuals:\n\\[\nSS_{\\text{res}} = \\sum_{i=1}^{N} e_i^2 = \\sum_{i=1}^{N} (Y_i - a - b X_i)^2\n\\]\nwhere \\(i = 1 \\dots N\\) indexes the respondents in the sample. When we estimate the regression coefficients by minimizing \\(SS_{\\text{res}}\\), this is called ordinary least squares (OLS) regression. OLS is very widely used and is the main focus of this course, although we will visit some other approaches in the second half of the course.\nThe values of the regression coefficients that minimize \\(SS_{\\text{res}}\\) can be found using calculus (i.e., compute the derivatives of \\(SS_{\\text{res}}\\) and set them to zero). This approach leads to the following equations for the regression coefficients:\n\\[\na = \\bar Y - b \\bar X \\quad \\quad \\quad \\quad b = \\frac{\\text{cov}(X, Y)}{s^2_X} = \\text{cor}(X, Y) \\frac{s_Y}{s_X}\n\\tag{2.4}\\]\n(If you aren’t familiar with the symbols in these equations, check out the review materials in Chapter 1 for a refresher.)\nThe formulas in Equation 2.4 tell us how to compute the regression coefficients using our sample data. However, on face value, these formulas don’t tell us much about how to interpret the regression coefficients. For interpreting the regression coefficients, it is more straightforward to refer to Equation 2.1.\nTo clarify:\n\nTo interpret the regression intercept, use Equation 2.1: It is the value of \\(\\hat Y\\) when \\(X = 0\\). Similarly, the regression slope is how much \\(\\hat Y\\) changes for a one-unit increase in \\(X\\).\nTo compute the regression coefficients, use Equation 2.4. These formulas are not very intuitive – they are just what we get when we fit a line to the data using OLS.\n\nIt is important to emphasize that the formulas in Equation 2.4 do lead to some useful mathematical results about regression. Section 2.9, which is optional, derives some of the main results. If you want a deeper mathematical understanding of regression, make sure to check out this section. If you prefer to skip the math and just learn about the results as they become relevant, that is OK too.\n\n2.3.1 Correlation and regression\nBefore moving on, it is worth noting something that we can learn from Equation 2.4 without too much math: the regression slope is just a re-packaging of the correlation coefficient. In particular, if we assume that \\(X\\) and \\(Y\\) are z-scores (i.e., they are standardized to have mean of zero and variance of one), then Equation 2.4 reduces to:\n\n\\(a = 0\\)\n\\(b = \\text{cov}(X, Y) = \\text{cor}(X, Y)\\)\n\nThere are two important things to note here.\nFirst, the difference between correlation and simple regression depends on the scale of the variables. Otherwise stated, if we standardize both \\(Y\\) and \\(X\\), then regression is just correlation. In particular, if the correlation is equal to zero, then the regression slope is also equal to zero – these are just two equivalent ways of saying that the variables are not (linearly) related.\nSecond, this relationship between correlation and regression holds only for simple regression (i.e., one predictor). When we get to multiple regression, we will see that relationship between regression and correlation (and covariance) gets more complicated.\nFor the NELS example in Figure 2.1, the regression intercept and slope are, respectively:\n\n\nCode\ncoef(mod)\n\n\n(Intercept)         ses \n 48.6780338   0.4292604 \n\n\nPlease write down an interpretation of these numbers and be prepared to share your answers in class. How would your interpretation change if, rather than the value of the slope shown above, we had \\(b = 0\\)?"
  },
  {
    "objectID": "ch2_simple_regression.html#sec-rsquared-2",
    "href": "ch2_simple_regression.html#sec-rsquared-2",
    "title": "2  Simple regression",
    "section": "2.4 R-squared",
    "text": "2.4 R-squared\nIn this section we introduce another statistic that is commonly used in regression, called “R-squared” (in symbols: \\(R^2\\)). First we will talk about its interpretation, then we will show how it is computed.\nR-squared is the proportion of variance in the outcome variable that is associated with, or “explained by”, the predictor variable. In terms of the NELS example, the variance of the outcome can be interpreted in terms of individual differences in Math Achievement – i.e., how students deviate from, or vary around, the mean level of Math Achievement. R-squared tells us the extent to which these individual differences in Math Achievement are associated with, or explained by, individual differences in SES.\nAs mentioned, R-squared is a proportion. Because it is a proportion, it takes on values between \\(0\\) and \\(1\\). If \\(R^2 = 0\\) then a student’s SES doesn’t tell us anything about their Math Achievement – this is the same as saying the two variables aren’t correlated, or that there is no (linear) relationship between Math Achievement and SES. If \\(R^2 = 1\\), then all of the data points fall exactly on the regression line, and we can perfectly predict each student’s Math Achievement from their SES.\nYou might be asking – why do we need R-squared? We already have the regression coefficient (which is just a repackaging of the correlation), so why do we need yet another way of describing the relationship between Math Achievement and SES? This is very true for simple regression! However, when we move on to multiple regression, we will see that R-squared lets us talk about the relationship between the outcome and all of the predictors, or any subset of the predictors, whereas the regression coefficient only lets us talk about the relationship with one predictor at a time.\nTo see how R-squared is computed for the NELS example, let’s consider Figure 2.3. The horizontal grey line denotes the mean of Math Achievement. Recall that the variance of \\(Y\\) is computed using the sum-of-squared deviations from the mean. For each student, these deviations from the mean can be divided into two parts. The Figure shows these two parts for a single student, using black and pink dashed lines:\n\nThe black dashed line represents the extent to which the student’s deviation from the mean level of Math Achievement is explained by the linear relationship between Math Achievement and SES.\nThe pink dashed line is the regression residual, which was introduced in Section 2.2. This is the variation in Math Achievement that is “left over” after considering the linear relationship with SES.\n\n\n\n\n\n\nFigure 2.3: The Idea Behind R-squared.\n\n\n\n\nThe R-squared statistic averages the variation in Math Achievement associated with SES (i.e., the black dashed line) for all students in the sample, and then divides by the total variation in Math Achievement (i.e., black + pink).\nThe derivation of the R-squared statistic is not very complicated and provides some useful notation. To simplify the derivation, we can work the numerator of the variance, which is called the “total sum of squares:”\n\\[SS_{\\text{total}} = \\sum_{i = 1}^N (Y_i - \\bar Y)^2. \\]\nNext we add and subtract the predicted values (that old trick!):\n\\[SS_{\\text{total}} = \\sum_{i = 1}^N [(Y_i - \\widehat Y_i) + (\\widehat Y_i - \\bar Y)]^2. \\]\nThe right-hand-side can be reduced to two other sums of squares using the rules of summation algebra (see Section 1.2 – the derivation is long but not complicated).\n\\[\nSS_{\\text{total}} = \\sum_{i = 1}^N (Y_i - \\widehat Y_i)^2 + \\sum_{i = 1}^N (\\widehat Y_i - \\bar Y)^2.\n\\]\nThe first term on the right-hand-side is just the sum of squared residuals (\\(SS_\\text{res}\\)) from Section 2.3. The second term is called the sum of squared regression and denoted \\(SS_\\text{reg}\\). Using this notation we can re-write the previous equation as\n\\[ SS_{\\text{total}} = SS_\\text{res} + SS_\\text{reg} \\]\nand the R-squared statistic is computed as\n\\[R^2 = SS_{\\text{reg}} / SS_{\\text{total}}. \\]\nAs discussed above, this quantity can be interpreted as the proportion of variance in \\(Y\\) that is explained by its linear relationship with \\(X\\).\nFor the NELS example, the R-squared statistic is:\n\n\nCode\nsummary(mod)$r.squared\n\n\n[1] 0.1012821\n\n\nPlease write down an interpretation of this number and be prepared to share your answer in class. Hint: Instead of talking about proportions, it is often helpful to multiply by 100 and talk about percentages instead."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-population-model-2",
    "href": "ch2_simple_regression.html#sec-population-model-2",
    "title": "2  Simple regression",
    "section": "2.5 The population model",
    "text": "2.5 The population model\nUp to this point we have discussed simple linear regression as a way of describing the relationship between two variables in a sample. The next step is to discuss statistical inference. Recall that statistical inference involves generalizing from a sample to the population from which the sample was drawn.\nIn the NELS example, the population of interest is U.S. eighth graders in 1988. We want to be able to draw conclusions about that population based on the sample of eighth graders that participated in NELS. In order to do that, we make some statistical assumptions about the population, which are collectively referred to as the population model.\nThe population model for simple linear regression is summarized in Figure 2.4. The three assumptions associated with this model are written below. We talk about how to check the plausibility of these assumptions in ?sec-chapter-7.\n\n\n\n\n\nFigure 2.4: The Regression Population Model.\n\n\n\n\nThe three assumptions:\n\nNormality: The values of \\(Y\\) conditional on \\(X\\), denoted \\(Y|X\\), are normally distributed. The figure shows these distributions for three values of \\(X\\). We can write this assumption formally as\n\n\\[Y | X \\sim  N(\\mu_{Y | X} , \\sigma^2_{Y | X}) \\]\n\n(This notation should be familiar from EDUC 710. In general, we write \\(Y \\sim N(\\mu, \\sigma^2)\\) to denote that the variable \\(Y\\) has a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\).)\n\n\nHomoskedasticity: The conditional distributions have equal variances (also called “homogeneity of variance”, or simply “equal variances”).\n\n\\[ \\sigma^2_{Y| X} = \\sigma^2 \\]\n\nLinearity: The means of the conditional distributions are a linear function of \\(X\\).\n\n\\[ \\mu_{Y| Χ} = a + bX \\]\nThese three assumptions are summarized by writing\n\\[ Y|X \\sim N(a + bX, \\sigma^2). \\] Sometimes it will be easier to state the assumptions in terms of the population residuals, \\(\\epsilon = Y - \\mu_{Y|X}\\). The residuals have distribution \\(\\epsilon \\sim N(0, \\sigma^2)\\).\nSometimes it will also be easier to write the population regression line using expected values, \\(E(Y|X)\\), rather than \\(\\mu_{Y|X}\\). Both of these are interpreted the same way – they denote the mean of \\(Y\\) for a given value of \\(X\\).\nAn additional assumption is usually made about the data in the sample – that they were obtained as a simple random sample from the population. We will see some ways of dealing with other types of samples later on this course, but for now we can consider this a background assumption that applies to OLS regression.\nFrom a mathematical perspective, these assumptions are important because they can be used to prove (a) that OLS regression provides unbiased estimates of the population regression coefficients and (b) that the OLS estimates are more precise than any other unbiased estimates of the population regression coefficients. There are other variations on these assumptions, which are sometimes called the Gauss-Markov assumptions see https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem.\nFrom a practical perspective, these assumptions are important conditions that we should check when conducting data analyses. If the assumptions are violated – particularly the linearity assumption – then our statistical model may not be a good representation of the population. If the model is not a good representation of the population, then inferences based on the model may provide misleading conclusions about the population."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-notation-2",
    "href": "ch2_simple_regression.html#sec-notation-2",
    "title": "2  Simple regression",
    "section": "2.6 Clarifying notation",
    "text": "2.6 Clarifying notation\nAt this point we have used the mathematical symbols for regression (e.g., \\(a\\), \\(b\\)) in two different ways:\n\nIn Section 2.2 they denoted sample statistics.\nIn Section 2.5 they denoted population parameters.\n\nThe population versus sample notation for regression is a bit of a hot mess, but the following conventions are used.\n\n\n\n\n\n\n\n\nConcept\nSample statistic\nPopulation parameter\n\n\n\n\nregression line\n\\(\\widehat Y\\)\n\\(\\mu_{Y|X}\\) or \\(E(Y|X)\\)\n\n\nslope\n\\(\\widehat b\\)\n\\(b\\)\n\n\nintercept\n\\(\\widehat a\\)\n\\(a\\)\n\n\nresidual\n\\(e\\)\n\\(\\epsilon\\)\n\n\nvariance explained\n\\(\\widehat R^2\\)\n\\(R^2\\)\n\n\n\nThe “hats” always denote sample quantities, and the Greek letters always denote population quantities, but there is some lack of consistency. For example, why not use \\(\\beta\\) instead of \\(b\\) for the population slope? Well, \\(\\beta\\) is conventionally used to denote standardized regression coefficients in the sample, so its already taken (more on this in Chapter 4).\nIf it is clear from context that we are talking about the sample rather than the population, then the hats are usually omitted from the statistics \\(\\widehat a\\), \\(\\widehat b\\), and \\(\\widehat R^2\\). This doesn’t apply to \\(\\widehat Y\\), because the hat is required to distinguish the predicted values from the data points.\nAnother thing to note is that while \\(\\widehat Y\\) is often called a predicted value, \\(E(Y|X)\\) is not usually referred to this way. It is called the conditional mean function or the conditional expectation function. Using this language, we can say that regression is about estimating the conditional mean function.\nPlease be prepared for a pop quiz on notation during class!"
  },
  {
    "objectID": "ch2_simple_regression.html#sec-inference-2",
    "href": "ch2_simple_regression.html#sec-inference-2",
    "title": "2  Simple regression",
    "section": "2.7 Inference",
    "text": "2.7 Inference\nThis section reviews the main inferential procedures for regression. The formulas presented in this section are used to produce standard errors, t-tests, p-values, and confidence intervals for the regression coefficients, as well as an F-test for R-squared. It is very unlikely that you will ever need to compute these formulas by hand, so don’t worry about memorizing them.\nHowever, it is important that you can interpret the numerical results in research settings. The interpretations of these procedures were reviewed in Chapter 1 and should be familiar from EDUC 710. This sections documents the formulas for simple regression and then asks you to interpret the results in the context of the NELS example.\nIt is worth noting that the regression intercept is often not of interest in simple regression. Recall that the intercept is the value of \\(\\widehat Y\\) when \\(X = 0\\). So, unless we have a hypothesis or research question about this particular value of \\(X\\) (e.g., eighth graders with \\(SES = 0\\)), we won’t be interested in a test of the regression intercept. When we get into to multiple regression, we will see some situations where the intercept is of interest.\n\n2.7.1 Inference for coefficients\nWhen the population model is true, \\(\\widehat b\\) is an unbiased estimate of \\(b\\) (in symbols: \\(E(\\hat b) = b\\)). The standard error of \\(\\widehat b\\) is equal to (see (fox-2016?), Section 6.1):\n\\[ SE(\\widehat b) = \\frac{s_Y}{s_X} \\sqrt{\\frac{1-R^2}{N-2}} . \\] Using these two results, we can compute t-tests and confidence intervals for the regression slope in the usual way.\nt-tests\nThe null hypothesis \\(H_0: \\widehat b = b_0\\) can be tested against the alternative \\(H_A: \\widehat b \\neq b_0\\) using the test statistic:\n\\[ t = \\frac{\\widehat b - b_0}{SE(\\widehat b)}, \\]\nwhich has a t-distribution on \\(N-2\\) degrees of freedom when the null hypothesis is true.\nThe test assumes that the population model is correct. The null hypothesized value of the parameter is usually chosen to be \\(b_0 = 0\\), in which case the test is interpreted in terms of the “statistical significance” of the regression slope.\nConfidence intervals\nFor a given Type I Error rate, \\(\\alpha\\), the corresponding \\((1-\\alpha) \\times 100\\%\\) confidence interval is\n\\[ b_0 = \\widehat b \\pm t_{\\alpha/2} \\times SE(\\widehat b), \\]\nwhere \\(t_{\\alpha/2}\\) denotes the \\(\\alpha/2\\) quantile of the \\(t\\)-distribution with \\(N-2\\) degrees of freedom. For example, if \\(\\alpha\\) is chosen to be \\(.05\\), the corresponding \\(95\\%\\) confidence interval uses \\(t_{.025}\\), or the 2.5-th percentile of the t-distribution.\nThe standard error for the regression intercept, presented below, can be used to compute t-tests and confidence intervals for \\(\\hat a\\):\n\\[\nSE(\\widehat a) = \\sqrt{\\frac{SS_{\\text{res}}}{N-2} \\left(\\frac{1}{N} + \\frac{\\bar X^2}{(N-1)s^2_X}\\right)}.\n\\]\n\n\n2.7.2 Inference for R-squared\nThe null hypothesis \\(H_0: R^2 = 0\\) can be tested against the alternative \\(H_A: R^2 \\neq 0\\) using the F-test:\n\\[ F = (N-2) \\frac{\\widehat R^2}{1-\\widehat R^2}, \\]\nwhich has a F-distribution on \\(1\\) and \\(N – 2\\) degrees of freedom when the null is true. The test assumes that the population model is true. Confidence intervals for R-squared are generally not reported.\n\n\n2.7.3 The NELS example\nFor the NELS example, the standard errors, t-test, and p-values of the regression coefficients are shown in the table below (along with the OLS estimates). The F-test appears in the text below the table. Note that the output uses the terminology “multiple R-squared” to refer to R-squared.\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = achmat08 ~ ses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5995  -6.5519  -0.1475   6.0226  27.6634 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  48.6780     1.1282  43.147  &lt; 2e-16 ***\nses           0.4293     0.0573   7.492 3.13e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.863 on 498 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09948 \nF-statistic: 56.12 on 1 and 498 DF,  p-value: 3.127e-13\n\n\nThe \\(95\\%\\) confidence intervals for the regression coefficients are:\n\n\nCode\nconfint(mod)\n\n\n                 2.5 %     97.5 %\n(Intercept) 46.4614556 50.8946120\nses          0.3166816  0.5418392\n\n\nPlease write down your interpretation of the t-tests, confidence intervals, and F-test, and be prepared to share your answers in class. Hint: state whether the tests are statistically significant and the corresponding conclusions are about the population parameters. For confidence intervals, report the range of values we are “confident” about. For practice, you might want to try using APA notation in your answer (or whatever style conventions are used in your field)."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-power-2",
    "href": "ch2_simple_regression.html#sec-power-2",
    "title": "2  Simple regression",
    "section": "2.8 Power analysis*",
    "text": "2.8 Power analysis*\nStatistical power is the probability of rejecting the null hypothesis, when it is indeed false. Rejecting the null hypothesis when it is false is sometimes called a “true positive”, meaning we have correctly inferred that a parameter of interest is not zero. Power analysis is useful for designing studies so that the statistical power / true positive rate is satisfactory.\nIn practice, statistical power comes down to having a large enough sample size. Consequently, power analysis is important when planning studies (e.g., in research grants proposals). In this class, we will not be planning any studies – rather, we will be working with secondary data analyses. In this context, power analysis is not very interesting, and so we do not mention it much. Nonetheless, power analysis is important for research and so we review the basics here.\nPower analysis in regression is very similar to power analysis for the tests we studied last semester. There are five ingredients that go into a power analysis:\n\nThe desired Type I Error rate, \\(\\alpha\\).\nThe desired level of statistical power.\nThe sample size, \\(N\\).\nThe number of predictors.\nThe effect size, which is Cohen’s f-squared statistic (AKA the signal to noise ratio):\n\n\\[ f^2 = {\\frac{R^2}{1-R^2}}. \\]\nIn principal, we can plug-in values for any four of these ingredients and then solve for the fifth. But, as mentioned, power analysis is most useful when we solve for \\(N\\) while planning a study. When solving for \\(N\\) “prospectively,” the effect size \\(f^2\\) should be based on reports of R-squared in past research. Power and \\(\\alpha\\) are usually chosen to be .8 and .05, respectively.\nThe example below shows the sample size required to detect an effect size of \\(R^2 = .1\\). This effect size was based on the NELS example discussed above. Note that the values \\(u\\) and \\(v\\) denote the degrees of freedom in the numerator and denominator of the F-test of R-squared, respectively. The former provides information about the number of predictors in the model, the latter about sample size.\n\n\nCode\n# Install package\n# install.packages(\"pwr\")\n\n# Load library\nlibrary(pwr)\n\n# Run power analysis\npwr.f2.test(u = 1, f2 = .1 / (1 - .1), sig.level = .05, power = .8)\n\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 70.61137\n             f2 = 0.1111111\n      sig.level = 0.05\n          power = 0.8\n\n\nRounding up, we would require 72 persons in the sample in order to have an 80% chance of detecting an effect size of \\(R^2 = .1\\) with simple regression.\nAnother use of power analysis is to solve for the effect size. This can be useful when the sample size is constrained by external factors (e.g., budget). In this situation, we can use power analysis to address whether the sample size is sufficient to detect an effect that is “reasonable” (again, based on past research). In the NELS example, we have \\(N=500\\) observations. The output below reports the smallest effect size we can detect with a power of \\(.8\\) and \\(\\alpha = .05\\). This is sometimes called the “minimum detectable effect size” (MDES).\n\n\nCode\npwr.f2.test(u = 1, v = 498, sig.level = .05, power = .8)\n\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 498\n             f2 = 0.01575443\n      sig.level = 0.05\n          power = 0.8\n\n\nWith a sample size of 500, and power of 80%, the MDES for simple regression is \\(R^2 = f^2 / (1 + f^2) \\approx .03\\). Based on this calculation, we can conclude that this sample size is sufficient for applications of simple linear regression in which we expect to explain at least 3% of the variance in the outcome."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-properties-2",
    "href": "ch2_simple_regression.html#sec-properties-2",
    "title": "2  Simple regression",
    "section": "2.9 Properties of OLS*",
    "text": "2.9 Properties of OLS*\nThis section summarizes some properties of OLS regression that will be used later on. You can skip this section if you aren’t interested in the math behind regression – the results will be mentioned again when needed.\nThe derivations in this section follow from the three rules of summation reviewed Section 1.2 and make use of the properties of means, variances, and covariances already derived in Section 1.4. If you have any questions about the derivations, I would be happy to address them in class during open lab time.\n\n2.9.1 Residuals\nWe start with two important implications of Equation 2.4 for the OLS residuals. In particular, OLS residuals always have mean zero and are uncorrelated with the predictor variable. These properties generalize to multiple regression.\nFirst we show that\n\\[\\text{mean} (e) = 0. \\tag{2.5}\\]\nFrom Equation 2.4 we have\n\\[a = \\bar Y - b \\bar X \\] Solving for \\(\\bar Y\\) gives\n\\[\\bar Y = a + b \\bar X. \\] Since \\(\\hat Y\\) is a linear transformation of \\(X\\), we know from Section 1.4 that\n\\[ \\text{mean} (\\hat Y) = a + b \\bar X. \\]\nThe previous two equations imply that \\(\\text{mean} (\\hat Y) = \\bar Y\\). Consequently,\n\\[\\text{mean}(e) = \\text{mean}(Y - \\hat Y) = \\text{mean}(Y) - \\text{mean}(\\hat Y) = \\bar Y - \\bar Y = 0\\]\nNext we show that\n\\[\\text{cov}(X, e) = 0.  \\tag{2.6}\\]\nThe derivation is:\n\\[\\begin{align}\n\\text{cov}(X, e) & = \\text{cov}(X, Y - \\hat Y) \\\\\n& = \\text{cov}(X, Y)  - \\text{cov}(X, \\hat Y) \\\\\n& = \\text{cov}(X, Y)  - \\text{cov}(X, a + b X) \\\\\n& = \\text{cov}(X, Y)  - b \\, \\text{var}(X) \\\\\n& = \\text{cov}(X, Y)  - \\left(\\frac{\\text{cov}(X, Y)} {\\text{var}(X)} \\right) \\text{var}(X) \\\\\n& = 0\n\\end{align}\\]\nThe second last line uses the expression for the slope in Equation 2.4.\n\n\n2.9.2 Multiple correlation (\\(R\\))\nAbove we defined \\(R^2\\) as a proportion of variance. This was a bit lazy. Instead, we can start with the definition of the multiple correlation\n\\[R = \\text{cor}(Y, \\hat Y)\\]\nand from this definition derive the result, shown above, that \\(R^2\\) is the proportion of variance in \\(Y\\) associated with \\(\\hat Y\\).\nLet’s start by showing that\n\\[\\text{cov}(Y, \\hat Y) = \\text{var}(\\hat Y) \\tag{2.7}\\]\nBefore deriving this result, note that Equation 2.4 implies\n\\[\\text{cov}(X, Y) = b \\, \\text{var}(X),\\]\nand, using the the variance of a linear transformation (Section 1.4), we have\n\\[ \\text{var} (\\hat Y) = \\text{var}(a + b X) = b^2 \\text{var}(X). \\]\nThese two results are used on the third and fourth lines of the following derivation, respectively.\n\\[\\begin{align}\n\\text{cov}(Y, \\hat Y)  & = \\text{cov}(Y, a + b X) \\\\\n& = b \\,\\text{cov}(Y, X) \\\\\n& = b^2 \\,\\text{var}(X) \\\\\n& = \\text{var}(\\hat Y) \\\\\n\\end{align}\\]\nNext we show that \\(R^2 = \\text{var}(\\hat Y) / \\text{var}(Y)\\):\n\\[\\begin{align}\nR^2 & = [\\text{cor}(Y, \\hat Y)]^2 \\\\\n& = \\frac{[\\text{cov}(Y, \\hat Y)]^2}{\\text{var}(Y) \\; \\text{var}(\\hat Y)} \\\\\n& = \\frac{[\\text{var}(\\hat Y)]^2}{\\text{var}(Y) \\; \\text{var}(\\hat Y)} \\\\\n& = \\frac{\\text{var}(\\hat Y)}{\\text{var}(Y)}. \\\\\n\\end{align}\\]\nThis derivation is nicer than the one in Section 2.4 because it obtains a result about \\(R^2\\) using the definition of \\(R\\). However, this derivation does not show that the resulting ratio is a proportion, which requires a second step (which also uses Equation 2.7):\n\\[\\begin{align}\n\\text{var}(e) & = \\text{var}(Y - \\hat Y) \\\\\n& = \\text{var}(Y) + \\text{var}(\\hat Y) - 2 \\text{cov}(Y, \\hat Y) \\\\\n& = \\text{var}(Y) + \\text{var}(\\hat Y) - 2 \\text{var}(\\hat Y) \\\\\n& = \\text{var}(Y) - \\text{var}(\\hat Y).\n\\end{align}\\]\nRe-arranging gives\n\\[ \\text{var}(Y) =  \\text{var}(\\hat Y) + \\text{var}(e),\\] which shows that \\(0 \\leq \\text{var}(\\hat Y) \\leq \\text{var}(Y).\\)\nOne last detail concerns the relation between the multiple correlation \\(R\\) and the regular correlation coefficient \\(r = \\text{cor}(Y, X)\\). Using the invariance of the correlation under linear transformation (Section 1.4), we have\n\\[ R = \\text{cor}(Y, \\widehat Y) = \\text{cor}(Y, a + bX) = \\text{cor}(Y, X) = r\\]\nConsequently, in simple regression, \\(R^2 = r^2\\) – i.e., the proportion of variance explained by the predictor is just the squared Pearson product-moment correlation. When we add multiple predictors, this relationship between \\(R^2\\) and \\(r^2\\) no longer holds."
  },
  {
    "objectID": "ch2_simple_regression.html#workbook-2",
    "href": "ch2_simple_regression.html#workbook-2",
    "title": "2  Simple regression",
    "section": "2.10 Workbook",
    "text": "2.10 Workbook\nThis section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in Section 2.11. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 2.1\n\n\nCode\n# Scatter plot\nplot(x = ses, \n     y = achmat08, \n     col = \"#4B9CD3\", \n     ylab = \"Math Achievement (Grade 8)\", \n     xlab = \"SES\")\n\n# Run the regression model\nmod &lt;- lm(achmat08 ~ ses)\n\n# Add the regression line to the plot\nabline(mod) \n\n\n\n\n\nMath Achievement and SES (NELS88).\n\n\n\n\nThe strength and direction of the linear relationship between the two variables is summarized by their correlation. In this sample, the value of the correlation is:\n\n\nCode\ncor(achmat08, ses)\n\n\n[1] 0.3182484\n\n\nThis correlation means that eighth graders from more well-off families (higher SES) also tended to do better in Math (higher Math Achievement). This relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., https://www.apa.org/pi/ses/resources/publications/education). Please look over this web page and be prepared to share your thoughts about this relationship.\nSection 2.3\nFor the NELS example, the regression intercept and slope are, respectively:\n\n\nCode\ncoef(mod)\n\n\n(Intercept)         ses \n 48.6780338   0.4292604 \n\n\nPlease write down an interpretation of these numbers and be prepared to share your answers in class. How would your interpretation change if, rather than the value of the slope shown above, we had \\(b = 0\\)?\nSection 2.4\nFor the NELS example, the R-squared statistic is:\n\n\nCode\nsummary(mod)$r.squared\n\n\n[1] 0.1012821\n\n\nPlease write down an interpretation of this number and be prepared to share your answer in class. Hint: Instead of talking about proportions, it is often helpful to multiply by 100 and talk about percentages instead.\nSection 2.6\nPlease be prepared for a pop quiz on notation during class!\n\n\n\nConcept\nSample statistic\nPopulation parameter\n\n\n\n\nregression line\n\n\n\n\nslope\n\n\n\n\nintercept\n\n\n\n\nresidual\n\n\n\n\nvariance explained\n\n\n\n\n\nSection 2.7\nFor the NELS example, the standard errors, t-test, and p-values of the regression coefficients are shown in the table below (along with the OLS estimates). The F-test appears in the text below the table. Note that the output uses the terminology “multiple R-squared” to refer to R-squared.\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = achmat08 ~ ses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5995  -6.5519  -0.1475   6.0226  27.6634 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  48.6780     1.1282  43.147  &lt; 2e-16 ***\nses           0.4293     0.0573   7.492 3.13e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.863 on 498 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09948 \nF-statistic: 56.12 on 1 and 498 DF,  p-value: 3.127e-13\n\n\nThe \\(95\\%\\) confidence intervals for the regression coefficients are:\n\n\nCode\nconfint(mod)\n\n\n                 2.5 %     97.5 %\n(Intercept) 46.4614556 50.8946120\nses          0.3166816  0.5418392\n\n\nPlease write down your interpretation of the t-tests, confidence intervals, and F-test, and be prepared to share your answers in class. Hint: state whether the tests are statistically significant and the corresponding conclusions are about the population parameters. For confidence intervals, report the range of values we are “confident” about. For practice, you might want to try using APA notation in your answer (or whatever style conventions are used in your field)."
  },
  {
    "objectID": "ch2_simple_regression.html#sec-exercises-2",
    "href": "ch2_simple_regression.html#sec-exercises-2",
    "title": "2  Simple regression",
    "section": "2.11 Exercises",
    "text": "2.11 Exercises\nThese exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)\nBefore staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\n\n2.11.1 The lm function\nThe functionlm, short for “linear model”, is used to estimate linear regressions using OLS. It also provides a lot of useful output.\nThe main argument that the we provides to the lm function is a formula. For the simple regression of Y on X, a formula has the syntax:\nY ~ X\nHere Y denotes the outcome variable and X is the predictor variable. The tilde ~ just means “equals”, but the equals sign = is already used to for other stuff in R, so ~ is used instead. We will see more complicated formulas as we go through the course. For more information on R’s formula syntax, see help(formula).\nLet’s take a closer look using the following two variables from the NELS data.\n\nachmat08: eighth grade math achievement (percent correct on a math test)\nses: a composite measure of socio-economic status, on a scale from 0-35\n\n\n\nCode\n# Load the data. Note that you can click on the .RData file and RStudio will load it\n# load(\"NELS.RData\") #Un-comment this line to run\n\n# Attach the data: will discuss this in class\n# attach(NELS) #Un-comment this line to run!\n\n# Scatter plot of math achievement against SES\nplot(x = ses, y = achmat08, col = \"#4B9CD3\")\n\n# Regress math achievement on SES; save output as \"mod\"\nmod &lt;- lm(achmat08 ~ ses)\n\n# Add the regression line to the plot\nabline(mod)\n\n\n\n\n\nCode\n# Print the regression coefficients\ncoef(mod)\n\n\n(Intercept)         ses \n 48.6780338   0.4292604 \n\n\nLet’s do some quick calculations to check that the lm output corresponds the formulas for the slope and intercept in Section 2.3:\n\\[ a = \\bar Y - b \\bar X \\quad \\text{and} \\quad b = \\frac{\\text{cov}(X, Y)}{\\text{var}(X)}. \\]\nWe won’t usually do this kind of “manual” calculation, but it is a good way consolidate knowledge presented in the readings with the output presented by R. It is also useful to refresh our memory about some useful R functions and how the R language works.\n\n\nCode\n# Compute the slope as the covariance divided by the variance of X\ncov_xy &lt;- cov(achmat08, ses)\nvar_x &lt;- var(ses)\nb &lt;- cov_xy / var_x\n\n# Compare the \"manual\" calculation to the output from lm. \nb\n\n\n[1] 0.4292604\n\n\nCode\n# Compute the y-intercept using from the two means and the slope\nxbar &lt;- mean(ses)\nybar &lt;- mean(achmat08)\n\na &lt;- ybar - b * xbar\n\n# Compare the \"manual\" calculation to the output from lm. \na\n\n\n[1] 48.67803\n\n\nLet’s also check our interpretation of the parameters. If the answers to these questions are not clear, please make sure to ask in class!\n\nWhat is the predicted value of achmat08 when ses is equal to zero?\nHow much does the predicted value of achmat08 increase for each unit of increase in ses?\n\n\n\n2.11.2 Variance explained\nAnother way to describe the relationship between the two variables is by considering the amount of variation in \\(Y\\) that is associated with (or explained by) its relationship with \\(X\\). Recall that one way to do this is via the “variance” decomposition\n\\[ SS_{\\text{total}} = SS_{\\text{res}} + SS_{\\text{reg}}\\]\nfrom which we can compute the proportion of variation in Y that is associated with the regression model:\n\\[R^2 = \\frac{SS_{\\text{reg}}}{SS_{\\text{total}}}.\\]\nThe R-squared for the example is presented in the output below. You should be able to provide an interpretation of this number, so if it’s not clear make sure to ask in class!\n\n\nCode\n# R-squared from the example\nsummary(mod)$r.squared\n\n\n[1] 0.1012821\n\n\nAs above, let’s compute \\(R^2\\) “by hand” for our example.\n\n\nCode\n# Compute the sums of squares\nybar &lt;- mean(achmat08)\nss_total &lt;- sum((achmat08 - ybar)^2)\nss_reg &lt;- sum((yhat - ybar)^2)\nss_res &lt;-  sum((achmat08 - yhat)^2)\n\n# Check that SS_total = SS_reg + SS_res\nss_total\n\n\n[1] 43526.91\n\n\nCode\nss_reg + ss_res\n\n\n[1] 43526.91\n\n\nCode\n# Compute R-squared (compare to value from lm)\nss_reg/ss_total\n\n\n[1] 0.1012821\n\n\nCode\n# Also check that R-squared is really equal to the square of the PPMC\ncor(achmat08, ses)^2\n\n\n[1] 0.1012821\n\n\n\n\n2.11.3 Predicted values and residuals\nThe lm function returns the predicted values \\(\\widehat{Y_i}\\) and residuals \\(e_i\\) and which we can access using the $ operator. These are useful for various reasons, especially model diagnostics, which we discuss later in the course. For now, lets just take a look at the residual vs fitted plot to illustrate the code.\n\n\nCode\nyhat &lt;- mod$fitted.values\nres &lt;- mod$resid\n\nplot(yhat, res, col = \"#4B9CD3\")\n\n\n\n\n\nAlso note that the residuals values have mean zero and are uncorrelated with the predictor – this is always the case in OLS (See Section 2.9})\nmean(res)\ncor(yhat, res)\n\n\n2.11.4 Inference\nNext let’s address statistical inference, or how we can make conclusions about a population based on a sample from that population.\nWe can use the summary function to test the coefficients in our model.\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = achmat08 ~ ses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.5995  -6.5519  -0.1475   6.0226  27.6634 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  48.6780     1.1282  43.147  &lt; 2e-16 ***\nses           0.4293     0.0573   7.492 3.13e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.863 on 498 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09948 \nF-statistic: 56.12 on 1 and 498 DF,  p-value: 3.127e-13\n\n\nIn the table, the t-test and p-values are for the null hypothesis that the corresponding coefficient is zero in the population. We can see that the intercept and slope are both significantly different from zero at the .05 level. However, the test of the intercept is not very meaningful (why?).\nThe text below the table summarizes the output for R-squared, including its F-test, it’s degrees of freedom, and the p-value. (We will talk about adjusted R-square in Chapter 4)\nWe can also use the confint function to obtain confidence intervals for the regression coefficients. Use help to find out more about the confint function.\n\n\nCode\nconfint(mod)\n\n\n                 2.5 %     97.5 %\n(Intercept) 46.4614556 50.8946120\nses          0.3166816  0.5418392\n\n\nBe sure to remember the correct interpretation of confidence intervals: there is a 95% chance that the interval includes the true parameter value (not: there is a 95% chance that the parameter falls in the interval). For example, there is a 95% chance that the interval [.32, .54] includes the true regression coefficient for SES.\n\n\n2.11.5 Writing up results\nWe could write up the results from this analysis in APA format as follows. You should practice doing this kind of thing, because it is important to be able to write up the results of your analyses in a way that people in your area of research will understand.\nIn this analysis, we considered the relationship between Math Achievement in Grade 8 (percent correct on a math test) and SES (a composite on a scale from \\(0-35\\)). Regressing Math Achievement on SES, the relationship was positive and statistically significant at the \\(.05\\) level (\\(b = 0.43\\), \\(t(498) = 7.49\\), \\(p &lt; .001\\), \\(95\\% \\text{ CI: } [0.32, 0.54]\\)). SES explained about \\(10\\%\\) of the variation in Math Achievement (\\(R^2 = .10\\), \\(F(1, 498) = 56.12\\), \\(p &lt; .001\\)).\n\n\n2.11.6 Additional exercises\nIf time permits, we will address these additional exercises in class.\nThese exercises replace achmat08 with\n\nachrdg08: eighth grade Reading Achievement (percent correct on a reading test)\n\nPlease answer the following questions using R.\n\nPlot achrdg08 against ses.\nWhat is the correlation between achrdg08 and ses? How does it compare to the correlation with Math and SES?\nHow much variation in Reading is explained by SES? Is the proportion of variance explained significant at the .05 level?\nHow much do predicted Reading scores increase for a one unit of increase in SES? Is this a statistically significant at the .05 level?\nWhat are your overall conclusions about the relationship between Academic Achievement and SES in the NELS data? Write up your results using APA formatting or whatever conventions are used in your area of research."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-interpretations-3",
    "href": "ch3_two_predictors.html#sec-interpretations-3",
    "title": "3  Two predictors",
    "section": "3.1 Interpretations",
    "text": "3.1 Interpretations\nMultiple regression has three main interpretations:\n\nPrediction (focus on \\(\\hat Y\\))\nCausation (focus on \\(b\\))\nExplanation (focus on \\(R^2\\))\n\nBy understanding these interpretations, we will have a better idea of how multiple regression is used in research. Each interpretation also provides a different perspective on the importance of using multiple predictor variables, rather than only a single predictor.\n\n3.1.1 Prediction\nPrediction was the original use of regression (https://en.wikipedia.org/wiki/Regression_toward_the_mean#History). In the context of simple regression, prediction means using observations of \\(X\\) to make a guess about yet unobserved values of \\(Y\\). Our guess is \\(\\hat Y\\), and this is why \\(\\hat Y\\) is called the “predicted value” of \\(Y\\).\nWhen making predictions, we usually want some additional information about how precise the predictions are. In OLS regression, this information is provided by the standard error of prediction (fox-2016?):\n\\[\\text{SE}({\\hat Y_i}) = \\sqrt{\\frac{SS_{\\text{res}}}{N - 2} \\left(1 +  \\frac{1}{N} + \\frac{(X_i - \\bar X)^2}{\\sum_j(X_j - \\bar X)^2} \\right)} \\tag{3.1}\\]\nThis statistic quantifies our uncertainty when making predictions based on observations of \\(X\\) that were not in our original sample. The prediction errors for the NELS example in Chapter 2 are represented in Figure 3.1 as a gray band around the regression line.\n\n\nCode\n# Plotting library\nlibrary(ggplot2)\n\n# Load data\nload(\"NELS.RData\")\n\n# Run regression \nmod &lt;- lm(achmat08 ~ ses, data = NELS)\n\n# Compute SE(Y-hat)\nn &lt;- nrow(NELS)\nms_res &lt;- var(mod$residuals) * (n-1) / (n-2)\nd_ses &lt;- NELS$ses - mean(NELS$ses) \nse_yhat &lt;- sqrt(ms_res * (1 + 1/n + d_ses^2 / sum(d_ses^2)))\n\n# Plotting\ngg_data &lt;- data.frame(\n             achmat08 = NELS$achmat08,\n             ses = NELS$ses,\n             y_hat = mod$fitted.values,\n             lwr = mod$fitted.values - 1.96 * se_yhat,\n             upr = mod$fitted.values + 1.96 * se_yhat)\n\nggplot(gg_data, aes(x = ses, y = achmat08))+\n    geom_point(color='#3B9CD3', size = 2) +\n    geom_line(aes(x = ses, y = y_hat), color = \"grey35\") +\n    geom_ribbon(aes(ymin=lwr,ymax=upr),alpha=0.3) + \n    ylab(\"Math Achievement (Grade 8)\") +\n    xlab(\"SES\") +\n    theme_bw()\n\n\n\n\n\nFigure 3.1: Prediction Error for NELS Example.\n\n\n\n\nWe can see in the figure that the error band is quite wide. So, we might wonder how to make our predictions more precise. On way to do this is by including more predictors in the regression model – i.e., multiple regression.\nTo see why including more predictors improves the precision of predictions, note that the standard error of prediction shown in Equation 3.1 increases with \\(SS_{\\text{res}}\\), which is the variation in the outcome that is not explained by the predictor (see Section 2.4). In most situations, \\(SS_{\\text{res}}\\) is the largest contributor the prediction error. As we will see below, one way to reduce \\(SS_{\\text{res}}\\) is by adding more predictors to the model.\n\n3.1.1.1 More about prediction\nRegression got its name from a statistical property of predicted scores called “regression toward the mean.” To explain this property, let’s assume \\(Y\\) and \\(X\\) are z-scores (i.e., both variables have \\(M = 0\\) and \\(SD = 1\\)). Recall that this implies that \\(a = 0\\) and \\(b = r_{XY}\\), so the regression equation reduces to\n\\[\\hat Y = r_{XY} X\\]\nSince \\(|r_{XY} | ≤ 1\\), the absolute value of the \\(\\hat Y\\) must be less than or equal to that of \\(X\\). And, since both variables have \\(M = 0\\), this implies that \\(\\hat Y\\) is closer to the mean of \\(Y\\) than \\(X\\) is to the mean of \\(X\\). This is sometimes called regression toward the mean.\nAlthough prediction was the original use of regression, many research problems do not involve prediction. For instance, there are no students in the NELS data for whom we need to predict Math Achievement – all of the test scores are already in the data! However, there has been a resurgence of interest in prediction in recent years, especially in machine learning. Although the methods used in machine learning are often more complicated than OLS regression, the basic problem is the same. Because the models are more complicated, theoretical results like Equation 3.1 are more difficult to obtain. Consequently, machine learning uses data-driven procedures like cross-validation to evaluate model predictions. As one example, we could evaluate the accuracy and precision of out-of-sample predictions by splitting our data into two samples, fitting the model in one sample (the “training data”), and then making predictions in the other sample (the “test data”). Equation 3.1 is a theoretical result saves us the trouble of doing this with OLS. Machine learning has also introduced some new techniques for choosing which predictors to include in a model (“variable selection” methods like the lasso). We will touch on these topics later in the course when we get to model building.\n\n\n\n3.1.2 Causation\nA causal interpretation of regression means that that changing \\(X\\) by one unit will change \\(E(Y|X)\\) by \\(b\\) units. This is interpreted as a claim about the expected value of \\(Y\\) “in real life”, not simply a claim about the mechanics of the regression line. In terms of our example, a causal interpretation would state that improving students’ SES by one unit will, on average, cause Math Achievement to increase by about half a percentage point.\nThe gold standard for inferring causality is to randomly assign people to different treatment conditions. In a regression context, treatment is represented by the independent variable, or the \\(X\\) variable. While randomized experiments are possible in some settings, there are many types of variables that we cannot feasibly randomly assign (e.g., SES).\nThe concept of an omitted variable is used to describe what happens when we can’t (or don’t) randomly assign people to treatment conditions. An omitted variable is any variable that is correlated with both \\(Y\\) and \\(X\\). In our example, this would be any variable correlated with both Math Achievement and SES (e.g., School Quality). When we use random assignment, we ensure that \\(X\\) is uncorrelated with all pre-treatment variables – i.e., randomization ensure that there are no omitted variables. However, when we don’t use random assignment, our results may be subject to omitted variable bias.\nThe overall idea of omitted variable bias is the same as “correlation \\(\\neq\\) causation”. The take-home message is summarized in the following points, which are stated in terms of the our NELS example.\n\nAny variable that is correlated with Math Achievement and with SES is called an omitted variable. One example is School Quality. This is an omitted variable because we did not include it as a predictor in our simple regression model.\nThe problem is not just that we have an incomplete picture of how School Quality is related to Math Achievement.\nOmitted variable bias means that the predictor variable that was included in the model ends up having the wrong regression coefficient. Otherwise stated, the regression coefficient of SES is biased because we did not consider School Quality.\nIn order to mitigate omitted variable bias, we want to include plausible omitted variables in our regression models – i.e., multiple regression.\n\n\n3.1.2.1 Omitted variable bias*\nOmitted variable bias is nicely explained by Gelman and Hill (gelman-2007?), and a modified version of their discussion is provided below. We start by assuming a “true” regression model with two predictors. In the context of our example, this means that there is one other variable, in addition to SES, that is important for predicting Math Achievement. Of course, there are many predictors of Math Achievement (see Section Section 2.1), but we only need two to explain the problem of omitted variable bias.\nWrite the “true” model as:\n\\[\nY = a + b_1 X_1 + b_2 X_2 + \\epsilon\n\\tag{3.2}\\]\nwhere \\(X_1\\) is SES and \\(X_2\\) is any other variable that is correlated with both \\(Y\\) and \\(X_1\\) (e.g., School Quality).\nNext, imagine that instead of using the model in Equation 3.2, we analyze the data using the model with just SES, leading to the usual simple regression:\n\\[\n\\hat Y = a^* + b^*_1 X_1 + \\epsilon^*\n\\tag{3.3}\\]\nThe problem of omitted variable bias is that \\(b_1 \\neq b^*_1\\) – i.e., the regression coefficient in the true model is not the same as the regression coefficient in the model with only one predictor. This is perhaps surprising – leaving out School Quality gives us the wrong regression coefficient for SES!\nTo see why, start by writing \\(X_2\\) as a function of \\(X_1\\).\n\\[\nX_2 = \\alpha + \\beta X_1 + \\nu\n\\tag{3.4}\\]\nNext we use Equation 3.4 to substitute for \\(X_2\\) in Equation 3.2,\n\\[\\begin{align}\nY & = a + b_1 X_1 + b_2 X_2 + \\epsilon \\\\\n  & = a + b_1 X_1 + b_2 (\\alpha + \\beta X_1 + \\nu)  + \\epsilon \\\\\n  & = \\color{orange}{(a + \\alpha)} + \\color{green}{(b_1 + b_2\\beta)} X_1 + (e + \\nu) \\label{eq-3parm}\n\\end{align}\\]\nNotice that in the last line, \\(Y\\) is predicted using only \\(X_1\\), so it is equivalent to Equation 3.3. Based on this comparison, we can write\n\n\\(a^* = \\color{orange}{a + \\alpha}\\)\n\\(b^*_1 = \\color{green}{b_1 + b_2\\beta}\\)\n\\(\\epsilon^* = \\epsilon + \\nu\\)\n\nThe equation for \\(b^*_1\\) is what we are most interested in. It shows that the regression parameter in our one-parameter model (\\(b^*_1\\)) is not equal to the “true” regression parameter using both predictors (\\(b_1\\)).\nThis is what omitted variable bias means – leaving out \\(X_2\\) in Equation Equation 3.3 gives us the wrong regression parameter for \\(X_1\\). This is one of the main motivations for including more than one predictor variable in a regression model – i.e., to avoid omitted variable bias.\nNotice that there two special situations in which omitted variable bias is not a problem:\n\nWhen the two predictors are not related – i.e., \\(\\beta = 0\\).\nWhen the second predictor is not related to \\(Y\\) – i.e., \\(b_2 = 0\\).\n\n\n\n\n3.1.3 Explanation\nMany uses of regression fall somewhere between prediction and causation. We want to do more than just predict outcomes of interest, but we often don’t have a basis for making the strong assumptions required for a causal interpretation of regression coefficients. This grey area between prediction and causation can be referred to as explanation.\nIn terms of our example, we might want to explain why eighth graders differ in their Math Achievement. There are large number of potential reasons for individual difference in Math Achievement, such as\n\nStudent factors\n\nattendance\npast academic performance in Math\npast academic performance in other subjects (Question: why include this?)\n…\n\nSchool factors\n\ntheir ELA teacher\nthe school they attend\ntheir peers\n…\n\nHome factors\n\nSES\nmaternal education\npaternal education\nparental expectations\n…\n\n\nWhen the goal of an analysis is explanation, it usual to focus on the proportion of variation in the outcome variable that is explained by the predictors, i.e., R-squared (see Section 2.4). Later in the course we will see how to systematically study the variance explained by individual predictors, or blocks of several predictors (e.g., student factors).\nNote that even a long list of predictors such as that above leaves out potential omitted variables. While the addition of more predictors can help us explain more of the variation in Math Achievement, it is rarely the case that we can claim that all relevant variables have been included in the model."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-ecls-3",
    "href": "ch3_two_predictors.html#sec-ecls-3",
    "title": "3  Two predictors",
    "section": "3.2 An example from ECLS",
    "text": "3.2 An example from ECLS\nIn the remainder of this chapter we will consider a new example from the 1998 Early Childhood Longitudinal Study (ECLS; https://nces.ed.gov/ecls/). Below is a description of the data from the official NCES codebook (page 1-1 of https://nces.ed.gov/ecls/data/ECLSK_K8_Manual_part1.pdf):\nThe ECLS-K focuses on children’s early school experiences beginning with kindergarten and ending with eighth grade. It is a multisource, multimethod study that includes interviews with parents, the collection of data from principals and teachers, and student records abstracts, as well as direct child assessments. In the eighth-grade data collection, a student paper-and-pencil questionnaire was added. The ECLS-K was developed under the sponsorship of the U.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics (NCES). Westat conducted this study with assistance provided by Educational Testing Service (ETS) in Princeton, New Jersey.\nThe ECLS-K followed a nationally representative cohort of children from kindergarten into middle school. The base-year data were collected in the fall and spring of the 1998–99 school year when the sampled children were in kindergarten. A total of 21,260 kindergartners throughout the nation participated.\nThe subset of the ECLS-K data used in this class was obtained from the link below.\nhttp://routledgetextbooks.com/textbooks/_author/ware-9780415996006/data.php\nThe codebook for this subset of data is available on our course website. In this chapter, we will be using a even smaller subset of \\(N = 250\\) cases from the example data set (the ECLS250.RData data)\nWe focus on the following three variables.\n\nMath Achievement in the first semester of Kindergarten. This variable can be interpreted as the number of questions (out of 60) answered correctly on a math test. Don’t worry – the respondents in this study did not have to write a 60-question math test in the first semester of K! Students only answered a few of the questions and their scores were re-scaled to be out a total of 60 questions afterwards.\nSocioecomonic Status (SES), which is a composite of household factors (e.g., parental education, household income) ranging from 30-72.\nApproaches to Learning (ATL), which is a teacher-reported measure of behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), with higher values representing more positive responses, and ATL is scored as an unweighted average the six items.\n\n\n3.2.1 Correlation matrices\nAs was the case in simple regression, the correlation coefficient is a building block of multiple regression. So, we will start by examining the correlations in our example. We also introduce a new way of presenting correlations, the correlation matrix. The notation developed in this section will appear throughout the rest of the chapter.\nIn the scatter plots below, the panels are arranged in matrix format. The variables named in the diagonal panels correspond to the vertical (\\(Y\\)) axis in that row and the horizontal (\\(X\\)) axis in that column. For example, Math is in the first diagonal, so it is the variable on vertical axis in the first row and the horizontal axis in the first column. This can be a bit confusing at first, so take a moment to make sure you know which variable is on which axis in each plot. Also notice that plots below the diagonal are just mirror image of the plots above the diagonal.\n\n\nCode\nload(\"ECLS250.RData\")\nattach(ecls)\nexample_data &lt;- data.frame(c1rmscal, wksesl, t1learn)\nnames(example_data) &lt;- c(\"Math\", \"SES\", \"ATL\")\npairs(example_data , col = \"#4B9CD3\")\n\n\n\n\n\nFigure 3.2: ECLS Example Data.\n\n\n\n\nThe format of Figure 3.2 is the same as that of the correlation matrix among the variables, which is shown below.\n\n\nCode\ncor(example_data)\n\n\n          Math       SES       ATL\nMath 1.0000000 0.4384619 0.3977048\nSES  0.4384619 1.0000000 0.2877015\nATL  0.3977048 0.2877015 1.0000000\n\n\nAgain, notice that the entries below the diagonal are mirrored by the entries above the diagonal. We can see that SES and ATL have similar correlations with Math Achievement (0.4385 and 0.3977, respectively), and are also moderately correlated with each other (0.2877).\nIn order to represent the correlation matrix among a single outcome variable (\\(Y\\)) and two predictors (\\(X_1\\) and \\(X_2\\)) we will use the following notation:\n\\[\n\\begin{array}{c}\n\\text{var } Y \\\\ \\text{var } X_1 \\\\ \\text{var } X_2\n\\end{array}\n\\quad\n\\left[\n\\begin{array}{ccc}\n1       & r_{Y1}  & r_{Y2}  \\\\\nr_{1Y}  & 1       & r_{12}  \\\\\nr_{2Y}  & r_{21}  & 1\n\\end{array}\n\\right]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this notation, \\(r_{Y1} = \\text{cor}(Y,X_1)\\) is the correlation between \\(Y\\) and \\(X_1\\). Note that each correlation coefficient (“\\(r\\)”) has two subscripts that tell us which two variables are being correlated. For the outcome variable we use the subscript \\(Y\\), and for the two predictors we use the subscripts \\(1\\) and \\(2\\). The order of the predictors doesn’t matter but we use the subscripts to keep track of which is which. In our example, \\(X_1\\) is SES and \\(X_2\\) is ATL.\nAs with the numerical examples, the values below the diagonal mirror the values above the diagonal. So, we really just need the three correlations shown in the matrix below.\n\\[\n\\begin{array}{c}\n\\text{var } Y \\\\ \\text{var } X_1 \\\\ \\text{var } X_2\n\\end{array}\n\\quad\n\\left[\n\\begin{array}{ccc}\n-       & r_{Y1}  & r_{Y2}  \\\\\n-  & -       & r_{12}  \\\\\n-  & -  & -\n\\end{array}\n\\right]\n\\]\nThe three correlations are interpreted as follows:\n\n\\(r_{Y1}\\) - the correlation between the outcome (\\(Y\\)) and the first predictor (\\(X_1\\)).\n\\(r_{Y2}\\) - the correlation between the outcome (\\(Y\\)) and the second predictor (\\(X_2\\)).\n\\(r_{12}\\) - the correlation between the two predictors.\n\nIf you have questions about how scatter plots and correlations can be presented in matrix format, please write them down now and share them class."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-model-3",
    "href": "ch3_two_predictors.html#sec-model-3",
    "title": "3  Two predictors",
    "section": "3.3 The two-predictor model",
    "text": "3.3 The two-predictor model\nIn the ECLS example, we can think of Kindergarteners’ Math Achievement as the outcome variable, with SES and Approaches to Learning as potential predictors / explanatory variables. The multiple regression model for this example can be written as\n\\[\n\\widehat Y = b_0 + b_1 X_1 + b_2 X_2\n\\tag{3.5}\\]\nwhere\n\n\\(\\widehat Y\\) denotes the predicted Math Achievement\n\\(X_1 = \\;\\) SES and \\(X_2 = \\;\\) ATL (it doesn’t matter which predictor we denote as \\(1\\) or \\(2\\))\n\\(b_1\\) and \\(b_2\\) are the regression slopes\nThe intercept is denoted by \\(b_0\\) (rather than \\(a\\)).\n\nJust like simple regression, the residual for Equation 3.5 is defined as \\(e = Y - \\widehat Y\\) and the model can be equivalently written as \\(Y = \\widehat Y + e\\). Also, remember that you can write out the model using the variable names in place of \\(Y\\) and \\(X\\) if that helps keep track of all the notation. For example,\n\\[\nMATH = b_0 + b_1 SES + b_2 ATL + e.\n\\]\nAs mentioned in Chapter 2, feel free to use whatever notation works best for you.\nYou might be wondering, what is the added value of multiple regression compared to the correlation co-efficients reported in the previous section? Well, correlations only consider two-variables-at-a-time. Multiple regression let’s us further consider how the predictors work together to explain variation in the outcome, and to consider the relationship between each predictor and the outcome while holding the other predictors constant. In the context of our example, multiple regression let’s us address the following questions:\n\nHow much of variation in Math Achievement do both predictors explain together?\nWhat is the relationship between Math Achievement and ATL if we hold SES constant?\nSimilarly, what is the relationship between Math Achievement and SES if we hold ATL constant?\n\nNotice that this is different from simple regression – simple regression was just a repackaging of correlation, but multiple regression is something new."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-ols-3",
    "href": "ch3_two_predictors.html#sec-ols-3",
    "title": "3  Two predictors",
    "section": "3.4 OLS with two predictors",
    "text": "3.4 OLS with two predictors\nWe can estimate the parameters of the two-predictor regression model in Equation 3.5 model using same approach as for simple regression. We do this by choosing the values of \\(b_0, b_1, b_2\\) that minimize\n\\[SS_\\text{res} = \\sum_i e_i^2.\\]\nSolving the minimization problem (setting derivatives to zero) leads to the following equations for the regression coefficients. Remember, the subscript \\(1\\) denotes the first predictor and the subscript \\(2\\) denotes the second predictor – see Section 3.2 for notation. Also note that \\(s\\) represents standard deviations.\n\\[\\begin{align}\nb_0 & = \\bar Y - b_1 \\bar X_1 - b_2 \\bar X_2 \\\\ \\\\\nb_1 & = \\frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}} \\frac{s_Y}{s_1} \\\\ \\\\\nb_2 & = \\frac{r_{Y2} - r_{Y1} r_{12}}{1 - r^2_{12}} \\frac{s_Y}{s_2}\n\\end{align}\\]\nAs promised, these equations are more complicated than for simple regression :) The next section addresses the interpretation of the regression coefficients."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-interpretation-3",
    "href": "ch3_two_predictors.html#sec-interpretation-3",
    "title": "3  Two predictors",
    "section": "3.5 Interpreting the coefficients",
    "text": "3.5 Interpreting the coefficients\nAn important part of using multiple regression is getting the correct interpretation of the regression coefficients. The basic interpretation is that the slope for SES represents how much predicted Math Achievement changes for a one unit increase of SES, while holding ATL constant. (The same interpretation holds when switching the predictors.) The important difference with simple regression is the “holding the other predictor constant” part, so let’s dig into it.\n\n3.5.1 “Holding the other predictor constant”\nLet’s start with the regression model for the predicted values:\n\\[ \\widehat {MATH} = b_0 + b_1 SES + b_2 ATL\\]\nIf we increase \\(SES\\) by one unit and hold \\(ATL\\) constant, we get new predicted value (denoted with an asterisk):\n\\[\\widehat {MATH^*} = b_0 + b_1 (SES + 1) + b_2 ATL\\]\nThe difference between \\(\\widehat{MATH^*}\\) and \\(\\widehat{MATH}\\) is how much the predicted value changes for a one unit increase in SES, while holding ATL constant:\n\\[\\widehat{MATH^*} - \\widehat{MATH}  = b_1\\] In words: the multiple regression coefficient tells us how the much the predicted value changes for a one unit increase in the predictor, while holding the other predictor(s) constant.\nThis why we interpret the regression coefficients in multiple regression differently than simple regression. In simple regression, the slope is just a re-scaled version of the correlation. In multiple regression, the slope of each predictor is interpreted in terms of the “effect” of that predictor, while holding the other predictor(s) constant. This is sometimes referred to as “ceteris paribus,” which is Latin for “with other conditions remaining the same.” So, we could say that multiple regression is a statistical way of making ceteris paribus arguments.\nNote that the interpretation of the regression intercept is basically the same as for simple regression: it is the value of \\(\\widehat Y\\) when \\(X_1 = 0\\) and \\(X_2 = 0\\) (i.e., still not very interesting).\n\n\n3.5.2 “Controlling for the other predictor”\nAnother interpretation of the regression coefficients is in terms of the equations for \\(b_1\\) and \\(b_2\\) presented in Section 3.4. For example, the equation for \\(b_1\\) is\n\\[\\begin{equation}\nb_1  = \\frac{r_{Y1} - r_{Y2} \\color{red}{r^2_{12}}}{1 -\\color{red}{r^2_{12}}}\\frac{s_Y}{s_1}\n\\end{equation}\\]\nThis is the same equation as from Section 3.4, but the correlation between the predictors is shown in red. Note that if the predictors are uncorrelated (i.e., \\(\\color{red}{r^2_{12}}\\) = 0) then\n\\[b_1 = r_{Y1} \\frac{s_Y}{s_1},\\]\nwhich is just the regression coefficient from simple regression (Section 2.3).\nIn general, the formulas for the regression slopes in the two-predictor model are more complicated because they “control for” or “account for” the relationship between the predictors. In simple regression, we only had one predictor, so we didn’t need to account for how the predictors were related to each other.\nThe equations for the regression coefficients show that, if the predictors are uncorrelated, then doing a multiple regression is just the same thing as doing simple regression multiple times. However, most of the time our predictors will be correlated, and multiple regression “controls for” the relationship between the predictors when examining the relationship between each predictor and the outcome.\n\n\n3.5.3 The ECLS example\nBelow, the R output from the ECLS example is reported. Please provide a written explanation of the regression coefficients for SES and ATL. If you have questions about how to interpret the coefficients, please also note them now and be prepared to share them in class. Note that this question is not asking about whether the coefficients are significant – it is asking what the numbers in the first column of the Coefficients table mean.\n\n\nCode\n# Run the regression model and print output\nmod1 &lt;- lm(c1rmscal ~ wksesl + t1learn)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.5.4 Standardized coefficients\nOne question that arises in the interpretation of the example is the relative contribution of the two predictors to Kindergartener’s Math Achievement. In particular, the regression coefficient for ATL is 10 times larger than the regression coefficient for SES – does this mean that ATL is 10 times more important than SES?\nThe short answer is, “no.” ATL is on a scale of 1-4 whereas SES ranges from 30-72. In order to make the regression coefficients more comparable, we can standardize the \\(X\\) variables so that they have the same variance. Many researchers go a step further and standardize all of the variables \\(Y, X_1, X_2\\) to be z-scores with M = 0 and SD = 1. The resulting regression coefficients are often called \\(\\beta\\)-coefficients or \\(\\beta\\)-weights (\\(\\beta\\) is pronounced “beta”).\nThe \\(\\beta\\)-weights are related to the regular regression coefficients from Section 3.4:\n\\[\\beta_1 = b_1 \\frac{s_1}{s_Y} = \\frac{r_{Y1} - r_{Y2} r_{12}}{1 - r^2_{12}}\\] A similar expression holds for \\(\\beta_2\\).\nNote that the lm function in R does not provide an option to report standardized output. So, if you want to get the \\(\\beta\\)-coefficients in R, it’s easiest to just standardized the variables first and then do the regression with the standardized variables.\nRegardless of how you compute them, the interpretation of the \\(\\beta\\)-coefficients is in terms of the standard deviation units of both the \\(Y\\) variable and the \\(X\\) variable – e.g., increasing \\(X_1\\) by one standard deviation changes \\(\\hat Y\\) by \\(\\beta_1\\) standard deviations (holding the other predictors constant).\n\n\nCode\n# Unlike other software, R doesn't have a convenience functions for beta coefficients. \nz_example_data &lt;- as.data.frame(scale(example_data))\nz_mod &lt;- lm(Math ~ SES  + ATL, data = z_example_data)\nsummary(z_mod)\n\n\n\nCall:\nlm(formula = Math ~ SES + ATL, data = z_example_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9590 -0.5604 -0.1493  0.4569  4.1043 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.337e-15  5.416e-02   0.000        1    \nSES         3.533e-01  5.666e-02   6.235 1.94e-09 ***\nATL         2.961e-01  5.666e-02   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8563 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\nWe should be careful when using beta-coefficients to “ease” the comparison of predictors. In the context of our example, we might wonder whether the overall cost of raising a child’s Approaches to Learning by 1 SD is comparable to the overall cost of raising their family’s SES by 1 SD. In general, putting variables on the same scale is only a superficial way of making comparisons among their regression coefficients.\nPlease write down an interpretation of the of beta (standardized) regression coefficients in the above output. Your interpretation should include reference to the fact that the variables have been standardized. Based on this analysis, do you think one predictor is more important than the other? Why or why not? Please be prepared to share your interpretations / questions in class!"
  },
  {
    "objectID": "ch3_two_predictors.html#sec-rsquared-3",
    "href": "ch3_two_predictors.html#sec-rsquared-3",
    "title": "3  Two predictors",
    "section": "3.6 (Multiple) R-squared",
    "text": "3.6 (Multiple) R-squared\nR-squared in multiple regression has the same general formula and interpretation as in simple regression. The formula is\n\\[R^2 = \\frac{SS_{\\text{reg}}} {SS_{\\text{total}}} \\]\nand it is interpreted as the proportion of variance in the outcome variable that is “associated with” or “explained by” its linear relationship with the predictor variables.\nAs discussed below, we can also say a bit more about R-squared in multiple regression.\n\n3.6.1 Relation with simple regression\nLike the regression coefficients in Section 3.4, the equation for R-squared can also be written in terms of the correlations among the three variables:\n\\[R^2 = \\frac{r^2_{Y1} + r^2_{Y2} - 2 r_{12}r_{Y1}r_{Y2}}{1 - r^2_{12}}.\\]\nIf the correlation between the predictors is zero, then this equation simplifies to\n\\[R^2 = r^2_{Y1} + r^2_{Y2}.\\] In words: When the predictors are uncorrelated, their total contribution to variance explained is just the sum of their individual contributions.\nHowever, when the predictors are correlated, either positively or negatively, it can be show that\n\\[ R^2 &lt; r^2_{Y1} + r^2_{Y2}.\\]\nIn other words: correlated predictors jointly explain less variance than if we added the contributions of each predictor considered separately. Intuitively, this is because correlated predictors share some variation with each other. If we considered the predictors one at a time, we double-count their shared variation.\nThe interpretation of R-squared for one versus two predictors can be explained in terms of the following Venn diagram.\n\n\n\n\n\nFigure 3.3: Shared Variance Among \\(Y\\), \\(X_1\\), and \\(X_2\\).\n\n\n\n\nIn the diagram, the circles represent the variance of each variable and the overlap between circles represents their shared variance (i.e., the R-squared for each pair of variables). When we conduct a multiple regression, the variance in the outcome explained by both predictors is equal to the sum of the areas A + B + C. If we instead conduct two simple regressions and then add up the R-squared values, we would double count the area labelled “B”.\nThe Venn diagram in Figure 3.3 is also useful for understanding other aspects of multiple regression. In the lesson we will discuss the following questions. Please write down your answers now so you are prepared to contribute to the discussion:\n\nWhich area represents the correlation between the predictors? \nWhich areas represent the regression coefficients from multiple regression?\nWhich areas represent the regression coefficients from simple regression?\n\n\n\n\n\n3.6.2 Adjusted R-squared\nThe sample R-squared is an upwardly biased estimate of the population R-squared. The adjusted R-squared corrects this bias. This section explains the main ideas.\nThe bias of R-squared is illustrated in the figure below. In the example, we are considering simple regression (one predictor), and we assume that the population correlation between the predictor and the outcome is zero (i.e., \\(\\rho = 0\\)).\n\n\n\n\n\nFigure 3.4: Sampling Distribution of \\(r\\) and \\(r^2\\) when $ ho = 0$.\n\n\n\n\nIn the left panel, we can see that “un-squared” correlation, \\(r\\), has a sampling distribution that is centered at the true value \\(\\rho = 0\\). This means that \\(r\\) is an unbiased estimate of \\(\\rho\\).\nBut in the right panel, we can see that the sampling distribution of the squared correlation, \\(r^2\\), must have a mean greater than zero. This is because all of the sample-to-sample deviations in left panel are now positive (because they have been squared). Since the average value of \\(r^2\\) is greater than 0, \\(r^2\\) is an upwardly biased estimate of \\(\\rho^2\\).\nThe adjusted R-squared corrects this bias. The formula for the adjustment is:\n\\[\\tilde R^2 = 1 - (1 - R^2) \\frac{N-1}{N - K - 1}\\]\nwhere \\(K\\) is the number of predictors in the model.\nThe formula contains two main terms, the proportion of residual variance, \\((1 - R^2)\\), and the adjustment factor (the ratio of \\(N-1\\) to \\(N-K-1\\)). We can understand how the adjustment works by considering these two terms.\nFirst, it can be seen that the adjustment factor is larger when the number of predictors, \\(K\\), is large relative to the sample size, \\(N\\). So, roughly speaking, the adjustment will be more severe when there are a lot of predictors in the model relative to the sample size.\nSecond, it can also be seen that the adjustment proportional to \\((1 - R^2)\\). This means that the adjustment is more severe if the model explains less variance in the outcome. For example, if \\(R^2 = .9\\) and the adjustment factor is \\(1.1\\), then adjusted \\(R^2 = .89\\). In this case the adjustment is a decrease of 1% of variance explained. But if we start off explaining less variance, say \\(R^2 = .1\\) and use the same adjustment factor, then adjusted \\(R^2 = .01\\). Now the adjustment is a decrease of 9% variance explained, even though we didn’t change the adjustment factor.\nIn summary, the overall interpretation of adusted R-squared is as follows: the adjustment will be larger when there are lots of predictors in the model but they don’t explain much variance in the outcome. This situation is sometimes called “overfitting” the data, so we can think of adjusted R-squared as a correction for overfitting.\nThere is no established standard for when you should reported R-squared or adjusted R-squared. I recommend that you report both whenever they would would lead to different substantive conclusions. We can discuss this more in class.\n\n\n3.6.3 The ECLS example\nAs shown in Section 3.5.4, the R-squared for the ECLS example is equal to .2726 and the adjusted R-squared is equal to .2668. Please write down your interpretation of these value and be prepared to share your answer in class."
  },
  {
    "objectID": "ch3_two_predictors.html#sec-inference-3",
    "href": "ch3_two_predictors.html#sec-inference-3",
    "title": "3  Two predictors",
    "section": "3.7 Inference",
    "text": "3.7 Inference\nThere isn’t really any thing new that about inference with multiple regression, except the formula for the standard errors (see (fox2016?) chap.6). We present the formulas for an abribrary number of predictors, denoted \\(k = 1, \\dots K\\).\n\n3.7.1 Inference for the coefficients\nIn multiple regression\n\\[SE({\\widehat b_k}) = \\frac{\\text{SD}(Y)}{\\text{SD}(X)} \\sqrt{\\frac{1 - R^2}{N - K - 1}} \\times \\sqrt{\\frac{1}{1 - R_k^2}}\n\\tag{3.6}\\]\nIn this formula, \\(K\\) denotes the number of predictors and \\(R^2_k\\) is the R-squared that results from regressing predictor \\(k\\) on the other \\(K-1\\) predictors (without the \\(Y\\) variable).\nNotice that the first part of the standard error (before the “\\(\\times\\)”) is the same as simple regression (see Section 2.7). The last part, which includes \\(R^2_k\\), is different and we talk about it more below.\nThe standard errors can be used to construct t-tests and confidence intervals using the same approach as for simple regression (see Section 2.7). The degrees of freedom for the t-distribution is \\(N - K -1\\). This formula for the degrees of freedom applies to simple regression too, where \\(K = 1\\).\n\n\n3.7.2 Precision of \\(\\hat b\\)\nWe can use Equation 3.6 to understand the factors that influence the size of the standard errors of the regression coefficients. Recall that standard errors describe the sample-to-sample variability of a statistic. If there is a lot sample-to-sample variability, the statistic is said to be imprecise. Equation 3.6 shows us what factors make \\(\\hat b\\) more or less precise.\n\nThe standard errors decrease with\n\nThe sample size, \\(N\\)\nThe proportion of variance in the outcome explained by the predictors, \\(R^2\\)\n\nThe standard errors increase with\n\nThe number of predictors, \\(K\\)\nThe proportion of variance in the predictor that is explained by the other predictors, \\(R^2_k\\)\n\n\nSo, large sample sizes and a large proportion of variance explained lead to precise estimates of the regression coefficients. On the other hand, including many predictors that are highly correlated with each other leads to less precision. In particular, the situation where \\(R^2_k\\) approaches the value of \\(1\\) is called multicollinearity. We will talk about multicollinearity in more detail in Chapter 5.\n\n\n3.7.3 Inference for R-squared\nThe R-squared statistic in multiple regression tells us how much variation in the outcome is explained by all of the predictors together. If the predictors do not explain any variation, then the population R-squared is equal to zero.\nNotice that \\(R^2 = 0\\) implies \\(b_1 = b_2 = ... = b_K = 0\\) (in the population). So, testing the significance of R-squared is equivalent to testing whether any of the regression parameters are non-zero. When we addressed ANOVA last semester, we called this the omnibus hypothesis. But in regression analysis, it is usually just referred to as a test of R-squared.\nThe null hypothesis \\(H_0 : R^2 = 0\\) can be tested using the statistic\n\\[F = \\frac{\\widehat R^2 / K}{(1 - \\widehat R^2) / (N - K - 1)},\\]\nwhich has an F-distribution on \\(K\\) and \\(N - K -1\\) degrees of freedom when the null hypothesis is true.\n\n\n3.7.4 The ECLS example\nThe R output for the ECLS example is presented (again) below. Please write down your conclusions about the statistical significance of the predictors and the R-squared statistic, and be prepared to share your answer in class. Please also write down the factors that affect the precision of the regression coefficients. This would be a good opportunity to practice APA formatting.\n\n\nCode\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "ch3_two_predictors.html#workbook",
    "href": "ch3_two_predictors.html#workbook",
    "title": "3  Two predictors",
    "section": "3.8 Workbook",
    "text": "3.8 Workbook\nThis section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in Section 3.9. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 3.2\nIf you have questions about the interpretation of a correlation matrix (below) or pairwise plots (see Section 3.2), please write them down now and share them class.\nNumerical output for the ECLS example:\n\n\nCode\ncor(example_data)\n\n\n          Math       SES       ATL\nMath 1.0000000 0.4384619 0.3977048\nSES  0.4384619 1.0000000 0.2877015\nATL  0.3977048 0.2877015 1.0000000\n\n\nMathematical notation for formulas\n\\[\n\\begin{array}{c}\n\\text{var } Y \\\\ \\text{var } X_1 \\\\ \\text{var } X_2\n\\end{array}\n\\quad\n\\left[\n\\begin{array}{ccc}\n1       & r_{Y1}  & r_{Y2}  \\\\\nr_{1Y}  & 1       & r_{12}  \\\\\nr_{2Y}  & r_{21}  & 1\n\\end{array}\n\\right]\n\\]\nSection 3.5\nBelow, the R output from the ECLS example is reported. Please provide a written explanation of the regression coefficients for SES and ATL. If you have questions about how to interpret the coefficients, please also note them now and be prepared to share them in class. Note that this question is not asking about whether the coefficients are significant – it is asking what the numbers in the first column of the Coefficients table mean.\n\n\nCode\n# Run the regression model and print output\nmod1 &lt;- lm(c1rmscal ~ wksesl + t1learn)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\nSection 3.5.4\nPlease write down an interpretation of the of beta (standardized) regression coefficients in the output below. Your interpretation should include reference to the fact that the variables have been standardized. Based on this analysis, do you think one predictor is more important than the other? Why or why not?\n\n\nCode\n# Unlike other software, R doesn't have a convenience functions for beta coefficients. \nz_example_data &lt;- as.data.frame(scale(example_data))\nz_mod &lt;- lm(Math ~ SES  + ATL, data = z_example_data)\nsummary(z_mod)\n\n\n\nCall:\nlm(formula = Math ~ SES + ATL, data = z_example_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9590 -0.5604 -0.1493  0.4569  4.1043 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.337e-15  5.416e-02   0.000        1    \nSES         3.533e-01  5.666e-02   6.235 1.94e-09 ***\nATL         2.961e-01  5.666e-02   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8563 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\nSection 3.6\nThe Venn diagram below is useful for understanding multiple regression. In the lesson we will discuss the following questions. Please write down your answers now so you are prepared to contribute to the discussion:\n\nWhich area represents the correlation between the predictors?\nWhich areas represent the R-squared from multiple regression?\nWhich areas represent the R-squared from simple regression?\nWhich areas represent the regression coefficients from multiple regression?\nWhich areas represent the regression coefficients from simple regression?\n\n\n\n\n\n\nShared Variance Among \\(Y\\), \\(X_1\\), and \\(X_2\\).\n\n\n\n\n\nLast question: The R-squared for the ECLS example is equal to .2726 and the adjusted R-squared is equal to .2668. Please write down your interpretation of these value and be prepared to share your answer in class.\n\nSection 3.7\nThe R output for the ECLS example is presented (again) below. Please write down your conclusions about the statistical significance of the predictors and the R-squared statistic, and be prepared to share your answer in class. This would be a good opportunity to practice APA formatting. Please also write down the factors that negatively affect the precision of the regression coefficients and address whether you think they are problematic for the example.\n\n\nCode\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "ch3_two_predictors.html#sec-exercises-3",
    "href": "ch3_two_predictors.html#sec-exercises-3",
    "title": "3  Two predictors",
    "section": "3.9 Exercises",
    "text": "3.9 Exercises\nThese exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)\nBefore staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\n\n3.9.1 The ECLS250 data\nLet’s start by getting our example data loaded into R.\nMake sure to download the file ECLS250.RData from Canvas and then double click the file to open it\n\n\nCode\nload(\"ECLS250.RData\") # load new example\nattach(ecls) # attach \n\n# knitr and kable are just used to print nicely -- you can just use head(ecls[, 1:5]) \nknitr::kable(head(ecls[, 1:5]))\n\n\n\n\n\ncaseid\ngender\nrace\nc1rrscal\nc1rrttsco\n\n\n\n\n960\n2\n1\n28\n58\n\n\n113\n1\n8\n14\n39\n\n\n1828\n1\n1\n22\n50\n\n\n1693\n1\n1\n21\n50\n\n\n643\n2\n1\n14\n39\n\n\n772\n1\n1\n21\n49\n\n\n\n\n\nThe naming conventions for these data are bit challenging.\n\nVariable names begin with c, p, or t depending on whether the respondent was the child, parent, or teacher. Variables that start with wk were created by the ECLS using other data sources available in during the kindergarten year of the study.\nThe time points (1-4 denoting fall and spring of K and Gr 1) appear as the second character.\nThe rest of the name describes the variable.\n\nThe variables we will use for this illustration are:\n\nc1rmscal: Child’s score on a math assessment, in first semester of Kindergarten . The scores can be interpreted as number of correct responses out of a total of approximately 60 math exam questions.\nwksesl: An SES composite of household factors (e.g., parental education, household income) ranging from 30-72.\nt1learn: Approaches to Learning Scale (ATLS), teacher reported in first semester of kindergarten. This scale measures behaviors that affect the ease with which children can benefit from the learning environment. It includes six items that rate the child’s attentiveness, task persistence, eagerness to learn, learning independence, flexibility, and organization. The items have 4 response categories (1-4), so that higher values represent more positive responses, and the scale is an unweighted average the six items.\n\nTo get started lets produce the simple regression of Math with SES. This is another look at the relationship between Academic Achievement and SES that we discussed in Chapter Chapter 2). If you do not feel comfortable running this analysis or interpreting the output, take another look at Section 2.11.\n\n\nCode\nplot(x = wksesl, \n     y = c1rmscal, \n     col = \"#4B9CD3\")\n\nmod &lt;- lm(c1rmscal ~ wksesl)\nabline(mod)\n\n\n\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.1314  -4.3549  -0.8486   3.6775  31.5358 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.61595    2.73925   0.225    0.822    \nwksesl       0.43594    0.05674   7.683 3.61e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.482 on 248 degrees of freedom\nMultiple R-squared:  0.1922,    Adjusted R-squared:  0.189 \nF-statistic: 59.03 on 1 and 248 DF,  p-value: 3.612e-13\n\n\nCode\ncor(wksesl, c1rmscal)\n\n\n[1] 0.4384619\n\n\n\n\n3.9.2 Multiple regression with lm\nFirst, let’s tale a look at the “zero-order” relationship among the three variables. This type of descriptive, two-way analysis is a good way to get familiar with your data before getting into multiple regression. We can see that the variables are all moderately correlated and their relationships appear reasonably linear.\n\n\nCode\n# Use cbind to create a data.frame with just the 3 variables we want to examine\ndata &lt;- cbind(c1rmscal, wksesl, t1learn)\n\n# Correlations\ncor(data)\n\n\n          c1rmscal    wksesl   t1learn\nc1rmscal 1.0000000 0.4384619 0.3977048\nwksesl   0.4384619 1.0000000 0.2877015\nt1learn  0.3977048 0.2877015 1.0000000\n\n\nCode\n# Scatterplots\npairs(data, col = \"#4B9CD3\") \n\n\n\n\n\nIn terms of input, multiple regression with lm is similar to simple regression. The only difference is the model formula. To include more predictors in a formula, just include them on the right hand side, separated by at + sign.\n\ne.g, Y ~ Χ1 + Χ2\n\nFor our example, let’s consider the regression of math achievement on SES and Approaches to Learning. We’ll save our result as mod1 which is short for “model one”.\n\n\nCode\nmod1 &lt;- lm(c1rmscal ~ wksesl + t1learn)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see from the output that regression coefficient for t1learn is about 3.5. This means that, as the predictor increases by a single unit, children’s predicted math scores increase by 3.5 points (out of 60), after controlling for the SES. You should be able to provide a similar interpretation of the regression coefficient for wksesl. Together, both predictors accounted for about 27% of the variation in students’ math scores. In education, this would be considered a pretty strong relationship.\nWe will talk about the statistical tests later on. For now let’s consider the relationship with simple regression.\n\n\n3.9.3 Relations between simple and multiple regression\nFirst let’s consider how the two simple regression compare to the multiple regression with two variables. Here is the relevant output:\n\n\nCode\n# Compare the multiple regression output to the simple regressions\nmod2a &lt;- lm(c1rmscal ~ wksesl)\nsummary(mod2a)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.1314  -4.3549  -0.8486   3.6775  31.5358 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.61595    2.73925   0.225    0.822    \nwksesl       0.43594    0.05674   7.683 3.61e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.482 on 248 degrees of freedom\nMultiple R-squared:  0.1922,    Adjusted R-squared:  0.189 \nF-statistic: 59.03 on 1 and 248 DF,  p-value: 3.612e-13\n\n\nCode\nmod2b &lt;- lm(c1rmscal ~ t1learn)\nsummary(mod2b)\n\n\n\nCall:\nlm(formula = c1rmscal ~ t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.399  -4.211  -0.997   3.770  31.844 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.0394     2.1485   3.276   0.0012 ** \nt1learn       4.7301     0.6929   6.826 6.66e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.618 on 248 degrees of freedom\nMultiple R-squared:  0.1582,    Adjusted R-squared:  0.1548 \nF-statistic:  46.6 on 1 and 248 DF,  p-value: 6.665e-11\n\n\nThe important things to note here are\n\nThe regression coefficients from the simple models (\\(b_{ses} = 0.44\\) and \\(b_{t1learn} = 4.73\\)) are larger than the regression coefficients from the two-predictor model. Can you explain why? (Hint: see Section Section 3.5.\nThe R-squared values in the two simple models (.192 + .158 = .350) add up to more than the R-squared in the two-predictor model (.273). Again, take a moment to think about why before reading on. (Hint: see Section Section 3.6.)\n\n\n\n3.9.4 Inference with 2 predictors\nLet’s move on now to consider the statistical tests and confidence intervals provided with the lm summary output.\nFor regression with more than one predictor, both the t-tests and F-tests have a very similar construction and interpretation as with simple regression. The main differences are the formulas, not so much the interpretations of the procedures. Some differences:\n\nThe degrees of freedom for both tests now involve \\(K\\), the number of predictors.\nThe standard error of the b-weight is more complicated, because it involves the inter-correlation among the predictors.\n\nWe can see for mod1 that both b-weights are significant at the .05 level, and so is the R-square. As mentioned previously, it is not usual to interpret or report results on the regression intercept unless you have a special reason to do so (e.g., see the next chapter).\n\n\nCode\n# Revisting the output of mod1\nsummary(mod1)\n\n\n\nCall:\nlm(formula = c1rmscal ~ wksesl + t1learn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.101  -4.034  -1.075   3.289  29.543 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.05016    2.90027  -2.086    0.038 *  \nwksesl       0.35125    0.05633   6.235 1.94e-09 ***\nt1learn      3.52125    0.67390   5.225 3.70e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 247 degrees of freedom\nMultiple R-squared:  0.2726,    Adjusted R-squared:  0.2668 \nF-statistic: 46.29 on 2 and 247 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.9.5 APA reporting of results\nThis section shows how we might write out the results of our regression using APA format. When we have a regression model with many predictors, or are comparing among different models, it is more usual to put all the relevant statistics in a table rather than writing them out one by one. We will see how to do that later on in the course. For more info on APA format, see the APA publications manual: (https://www.apastyle.org/manual).\n\nThe regression of Math Achievement on SES was positive and statistically significant at the .05 level (\\(b = 0.35, t(247) = 6.24, p &lt; .001\\)).\nThe regression of Math Achievement on Approaches to Learning was also positive and statistically significant at the .05 level (\\(b = 3.52, t(247) = 5.22, p &lt; .001\\)).\nTogether both predictors accounted for about 27% of the variation in Math Achievement (\\(R^2 = .273\\), \\(\\text{adjusted} R^2 = .267\\)), which was also statistically significant at the .05 level (\\(F(2, 247) = 46.29, p &lt; .001\\))."
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-interpretations-4",
    "href": "ch4_categorical_predictors.html#sec-interpretations-4",
    "title": "4  Categorical predictors",
    "section": "4.1 Focus on interpretation",
    "text": "4.1 Focus on interpretation\nCategorical predictors can be challenging to understand because, depending on the contrast coding used, the model results can appear quite different.\nFor example, the two models below use the same data and the same variables (Math Achievement regressed on Urbanicity), but their regression coefficients have different values. Why? Because the models used different contrast coding for Urbanicity. In the first model, Urban and Suburban students are compared to Rural students. In the second model, Rural and Suburban students are compared to the unweighted average across all three groups. We talk in more detail about these interpretations in the following sections. For now, the main thing to notice is just that “different contrast coding = different results”.\n\n\nCode\nload(\"NELS.RData\")\nattach(NELS)\n\n# run model with default contrast (treatment / dummy coding)\negA &lt;- lm(achrdg08 ~ urban)\n\n# change to sum / deviation contrasts and run again\ncontrasts(urban) &lt;- contr.sum(n = 3)\ncolnames(contrasts(urban)) &lt;- c(\"Rural\", \"Suburban\")\negB &lt;- lm(achrdg08 ~ urban)\n\n# print\nsummary(egA)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    54.9927     0.6885  79.876  &lt; 2e-16 ***\nurbanSuburban   0.6675     0.9117   0.732  0.46439    \nurbanUrban      3.1275     1.0480   2.984  0.00298 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nCode\nsummary(egB)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.2577     0.4021 139.897   &lt;2e-16 ***\nurbanRural     -1.2650     0.5654  -2.237   0.0257 *  \nurbanSuburban  -0.5975     0.5299  -1.128   0.2600    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nCode\n# Reset to original contrasts\ncontrasts(urban) &lt;- contr.treatment(n = 3)\ncolnames(contrasts(urban)) &lt;- c(\"Suburban\", \"Urban\")\n\n\nNote that the lm output doesn’t explicitly tell us what kind of coding was used. So, we need to know what is going on “under the hood” in order to interpret the output correctly. Also note that the while the Coefficients tables of the two models are different, they both explain the same amount of variance in the outcome. So, we could say that both models “fit” the data equally well, but they provide different interpretations of the relationship between Math Achievement and Urbanicity.\nThe choice of which contrast to use is up to the researcher. This choice should reflect the research questions you want to address. For example, if we wanted to know how Urban and Suburban students differ from Rural students, that is what the first output shows us. If we wanted to know how Suburban and Rural students differ from the overall average, that is what the second output shows us. As we will discuss below, we can’t make all possible comparisons among groups in a single model, so we have to make some compromises when choosing contrasts. Understanding how to align our research questions with the choice of contrasts is a big part of what this chapter is about!"
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-social-constructs-4",
    "href": "ch4_categorical_predictors.html#sec-social-constructs-4",
    "title": "4  Categorical predictors",
    "section": "4.2 Data and social constructs",
    "text": "4.2 Data and social constructs\nBefore getting into the math, let’s consider some conceptual points.\nFirst, some terminology. Binary means that a variable can take on only two values: 1 and 0. If a variable takes on two values but these are represented with other numbers (e.g., 1 and 2) or with non-numeric values (“male”, “female”), it is called dichotomous rather than binary. Otherwise stated, a binary variable is a dichotomous variable whose values are 1 and 0. The term categorical is used to describe variables with two or more categories – it includes binary and dichotomous variables, as well as variables with more categories.\nNote that encoding a variable as dichotomous does not imply that the underlying social construct is dichotomous. For example, we can encode educational attainment as a dichotomous variable indicating whether or not a person has graduated high school. This does not imply that educational attainment has only two values “irl”, or even that educational attainment is best conceptualized in terms of years of formal education. Nonetheless, for many outcomes of interest it can be meaningful to consider whether individuals have completed high school (e.g., https://www.ssa.gov/policy/docs/research-summaries/education-earnings.html).\nIn general, the way that a variable is encoded in a dataset is not a statement about reality – it reflects a choice made by researchers about how to represent social constructs. When conducting quantitative research, we are often faced with less-than-ideal encodings of social constructs, especially demographic variables. For example, both NELS and ECLS conceptualize gender as dichotomous and use a limited set of mutually exclusive categories for race. These representations are not well aligned with current literature on gender and racial identity. Some recent perspectives on how these issues play into quantitative research are available here: https://www.sree.org/critical-perspectives.\nDespite the conceptual issues inherent in categorizing social constructs, I would argue that categorical variables often have utility, especially in the study of social inequality. Here is an example of why I think gender qua “male/female” is a flawed but important consideration in global education: https://www.unicef.org/education/girls-education.\nPlease take a moment to write down your thoughts on the tensions that arise when conceptualizing social constructs such as gender or race as categorical, and I will invite you to share you thoughts in class."
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-binary-predictors-4",
    "href": "ch4_categorical_predictors.html#sec-binary-predictors-4",
    "title": "4  Categorical predictors",
    "section": "4.3 Binary predictors",
    "text": "4.3 Binary predictors\nLet’s start our interpretation of categorical predictors with the simplest case: a single binary predictor.\nFigure 4.1 uses the NELS data to illustrate the regression of Reading Achievement in Grade 8 (achrdg08) on a binary encoding of Gender (female = 0, male = 1). There isn’t a lot going on the plot! However, we can see the conditional distributions of Reading Achievement for each value of Gender, and the means of the two groups are indicated.\n\n\nCode\n# Don't read this unless you really like working on graphics :) \nbinary_gender &lt;- (gender == \"Male\") * 1 \nplot(binary_gender, achrdg08, \n     col = \"#4B9CD3\", \n     xlab = \"binary gender (Female = 0, Male = 1)\",\n     ylab = \"Reading (Grade 8)\")\n\nmeans &lt;- tapply(achrdg08, binary_gender, mean)\nlabels &lt;- c(expression(bar(Y)[0]), expression(bar(Y)[1]))\n\ntext(x = c(.1, .9), y = means, labels = labels, cex = 1.5) \ntext(x = c(0, 1), y = means, labels = c(\"_\", \"_\"), cex = 2) \n\n\n\n\n\nFigure 4.1: Reading Achievement on Binary Gender.\n\n\n\n\nWe can represent the relationship between Reading Achievement and Gender using the same simple regression equation from Section 2.2\n\\[\\widehat Y = b_0 + b_1 X, \\]\nand we can still interpret the regression slope in terms of a one-unit increase in \\(X\\). However, since \\(X\\) can only take on two values (0 or 1), there are also other interpretations of the regression coefficients. In this section we are interested in the these “more specific” interpretations of regression coefficients when \\(X\\) is binary.\nThe general strategy for approaching the interpretation of regression coefficients with categorical predictors has two steps:\n\nStep 1. Plug the values for \\(X\\) into the regression equation to get the predicted values for each group \\[\n\\begin{align}\n\\widehat Y (Female) & = b_0 + b_1 (0) = b_0 \\\\\n\\widehat Y (Male) & = b_0 + b_1 (1) = b_0 + b_1\n\\end{align}\n\\]\nStep 2. Solve for the coefficients in terms the predicted values.\n\n\\[ b_0 = \\widehat Y (Female) \\tag{4.1}\\] \\[ b_1  = \\widehat Y (Male) - b_0 = \\widehat Y (Male) - \\widehat Y (Female)\n\\tag{4.2}\\]\nLooking at Equation 4.1 we can see that intercept (\\(b_0\\)) is equal to the predicted value of Reading Achievement for Females, and Equation 4.2 shows that the regression slope (\\(b_1\\)) is equal to the difference between predicted Reading Achievement for Males and Females.\nThere is one last detail that is important for interpreting these equations: for a single categorical predictor, the predicted values for each category are just the group means on the \\(Y\\) variable (see Section 4.7, which is optional). So, using the notation of Figure 4.1, we can re-write Equation 4.1 and Equation 4.2 as\n\\[b_0 = \\bar Y_0   \\tag{4.3}\\]\n\\[ b_1 = \\bar Y_1 - \\bar Y_0  \\tag{4.4}\\]\nWe will use the equivalence between predicted values and group means throughout this chapter. However, it is important to note this equivalence holds only when there is single categorical predictor in the model, and no other predictors. Additional predictors are discussed in Section 5.2\nFor the example data, regression coefficients are:\n\n\nCode\n# convert \"Female / Male\" coding to binary\ngender &lt;- NELS$gender\nbinary_gender &lt;- (gender == \"Male\")*1\nmod_binary &lt;- lm(achrdg08 ~ binary_gender)\ncoef(mod_binary)\n\n\n  (Intercept) binary_gender \n   56.4678022    -0.9223396 \n\n\nPlease take a moment and write down how these two numbers are related to Figure 4.1. In particular, what is \\(\\bar Y_0\\) equal to, what is \\(\\bar Y_1\\) equal to, and what is their difference equal to?\n\n4.3.1 Relation with t-tests of means\nSimple regression with a binary predictor is equivalent to conducting an independent samples t-test in which the \\(X\\) variable (Gender) is the grouping variable and the \\(Y\\) variable (Reading Achievement) is the outcome. The following output illustrates this.\nFor the regression model (same as above):\n\n\nCode\nsummary(mod_binary)\n\n\n\nCall:\nlm(formula = achrdg08 ~ binary_gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.7278  -6.1472   0.3784   6.9765  15.0045 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.4678     0.5342 105.703   &lt;2e-16 ***\nbinary_gender  -0.9223     0.7928  -1.163    0.245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.827 on 498 degrees of freedom\nMultiple R-squared:  0.00271,   Adjusted R-squared:  0.0007076 \nF-statistic: 1.353 on 1 and 498 DF,  p-value: 0.2452\n\n\nFor the independent samples t-test (with homogeneity of variance assumed):\n\n\nCode\nt.test(achrdg08 ~ binary_gender, var.equal = T)\n\n\n\n    Two Sample t-test\n\ndata:  achrdg08 by binary_gender\nt = 1.1633, df = 498, p-value = 0.2452\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.6353793  2.4800586\nsample estimates:\nmean in group 0 mean in group 1 \n       56.46780        55.54546 \n\n\nWe will go through the comparison between these two outputs in class together. If you have any questions about the relation between these two sets of output, please note them now and be prepared ask them in class.\n\n\n4.3.2 Summary\nWhen doing regression with a binary predictor:\n\nThe regression intercept is equal to the mean of the group coded “0”.\nThe regression slope is equal to the mean of the group coded “0” subtracted from the mean of the groups coded “1”.\nTesting \\(H_0: b_1 = 0\\) is equivalent to testing the mean difference \\(H_0: \\mu_1 – \\mu_0 = 0\\)\n\ni.e., regression with a binary variable is the same as a t-test of means for independent samples"
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-reference-group-coding-4",
    "href": "ch4_categorical_predictors.html#sec-reference-group-coding-4",
    "title": "4  Categorical predictors",
    "section": "4.4 Reference-group coding",
    "text": "4.4 Reference-group coding\nNow that we know about regression with a binary predictor, let’s consider how to extend this approach to contrast coding of categorical predictors with \\(C ≥ 2\\) categories.\nThe basic idea is to represent the \\(C\\) categories of the predictor in terms of \\(C – 1\\) “dummy variables.” Binary coding of a dichotomous predictor is one example of this: We represented a categorical variable with \\(C = 2\\) categories using \\(1\\) binary predictor.\nFor predictors with \\(C ≥ 2\\) categories, the most common approach to contrast coding is called reference-group coding. In R, the approach is called treatment contrasts and is the default coding for categorical predictors.\nIt is called reference-group coding because:\n\nThe researcher chooses a reference-group.\nThe intercept is interpreted as the mean of the reference-group.\nThe \\(C – 1\\) regression slopes are interpreted as the mean differences between the \\(C – 1\\) other groups and the reference-group.\n\nNote that reference-group coding is a generalization of binary coding. In the example from Section 4.3:\n\nFemales were the reference-group.\nThe intercept was equal to the mean Reading Achievement for females.\nThe regression coefficient was equal to the mean difference between males and females.\n\nThe rest of this section considers how to generalize this approach to more than 2 groups.\n\n4.4.1 A hypothetical example\nFigure 4.2 presents a toy example. The data show the Age and marital status (Mstatus) of 16 hypothetical individuals. Marital status is encoded as\n\nSingle\nMarried\nDivorced\n\n\n\nCode\nknitr::include_graphics(\"files/images/marital_status1.png\")\n\n\n\n\n\nFigure 4.2: Toy Martital Status Example.\n\n\n\n\nThese 3 categories are represented by two binary variables, denoted \\(X_1\\) and \\(X_2\\).\n\n\\(X_1\\) is a binary variable that is equal to 1 when Mstatus is “married”, and equal to 0 otherwise.\n\\(X_2\\) is a binary variable that is equal to 1 when Mstatus is “divorced”, and equal to 0 otherwise.\n\nThe binary variables are often called dummies or indicators. For example, \\(X_1\\) is a dummy or indicator for married respondents.\nIn reference-group coding, the group that does not have a dummy variable is the reference group. It is also the group that is coded zero on all of the included dummies. In this example, “Single” is the reference group.\nWe can more compactly write down the contrast coding in the example using a contrast matrix .\n\\[\n\\begin{matrix} & X_1 & X_2\\\\\n                 \\text{Married} & 1 & 0  \\\\\n                 \\text{Divorced} & 0 & 1 \\\\\n                \\text{Single}  & 0 & 0 \\\\\n\\end{matrix}\n\\]\nThis notation is an abbreviated version of the data matrix in Figure 4.2. It summarizes how the two indicator variables correspond to the different levels of the categorical variable. As we will see in the Section 4.9, contrast matrices are useful for programming in R.\n\n\n4.4.2 Interpreting the regression coefficients\nRegressing Age on the dummies we have:\n\\[ \\widehat Y = b_0 + b_1 X_1 + b_2 X_2. \\]\nIn order to interpret the regression coefficients we can use the same two steps as in Section 4.3\n\nStep 1. Plug the values for the \\(X\\) variables into the regression equation to get the predicted values for each group\n\n\\[\\begin{align}\n\\widehat Y (Single) & = b_0 + b_1 (0) + b_2 (0) = b_0 \\\\\n\\widehat Y (Married) & = b_0 + b_1 (1) + b_2 (0) = b_0 + b_1 \\\\\n\\widehat Y (Divorced) & = b_0 + b_1 (0) + b_2 (1) = b_0 + b_2\n\\end{align}\\]\n\nStep 2. Solve for the model parameters in terms the predicted values.\n\n\\[\\begin{align}\nb_0 & = \\widehat Y (Single) \\\\\nb_1 & = \\widehat Y (Married) - b_0 = \\widehat Y (Married) - \\widehat Y (Single) \\\\\nb_2 & = \\widehat Y (Divorced) - b_0 = \\widehat Y (Divorced) - \\widehat Y (Single)\n\\end{align}\\]\nUsing the above equations, please write down an interpretation of the regression parameters for the hypothetical example. (Note: this question is not asking for a numerical answer, it is just asking you to put the above equations into words.)\n\n\n4.4.3 More than 3 categories\nFigure 4.3 extends the example by adding another category for Mstatus (“widowed”).\n\n\nCode\nknitr::include_graphics(\"files/images/marital_status2.png\")\n\n\n\n\n\nFigure 4.3: Toy Martital Status Example, Part 2.\n\n\n\n\nHere we are interested in the model\n\\[\\widehat Y = b_0 + b_1 X_1 + b_2 X_2 + b_3 X_3\\]\nPlease work through the following questions and be prepared to share your answers in class\n\nHow should \\(X_3\\) be coded so that “single” is the reference-group?\nWrite down the contrast matrix for the example.\nUsing the two-step approach illustrated above, write out the interpretation of the regression coefficients.\nLet’s imagine that we included an additional dummy variable for “Single” in the model. In this case we would have \\(C = 4\\) dummies in the model rather than \\(C-1 = 3\\) dummies. Please take a moment to write down what you think will happen (Hint: something goes wrong)\n\n\n\n4.4.4 Summary\nIn reference-group coding with a single categorical variable:\n\nThe reference is group is chosen by the analyst – it is the group that is coded zero on all dummies, or the one that has its indicator omitted from the model.\nThe intercept is interpreted as the mean of the reference group.\nThe regression coefficients of the dummy variables are interpreted as the difference between the mean of the indicated group and the mean of the reference group."
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-deviation-coding-4",
    "href": "ch4_categorical_predictors.html#sec-deviation-coding-4",
    "title": "4  Categorical predictors",
    "section": "4.5 Deviation coding",
    "text": "4.5 Deviation coding\nIn some research scenarios, there is a clear reference group. For example, in an experiment, all treatment groups are can be compared to the control group. But in other cases, it is less clear what the reference group should be. In both of the examples we have considered so far (gender and marital status), the choice of reference group was arbitrary.\nWhen there is not a clear reference group, it can be preferable to use other types of contrast coding than reference-group coding. One such approach is called deviation coding. In R, this is called sum-to-zero constrasts, or sum contrasts for short.\nThe main difference between deviation coding and reference-group coding is the interpretation of the intercept. In deviation coding, it is no longer the mean of the reference-group, but instead represents the mean of all groups’ predicted values. When the groups have equal sample size, the deviation-coded intercept is also equal to overall mean on \\(Y\\).\nMore specifically, in deviation coding:\n\nThe intercept is equal to the mean of the predicted values for each category i.e.,\n\n\\[\nb_0 = \\frac{\\sum_{c=1}^C \\widehat Y_c} {C}\n\\tag{4.5}\\]\n\nWhen the groups have equal sample sizes (\\(n\\)), the intercept is also equal to the overall mean on the outcome variable:\n\n\\[\\begin{equation}\nb_0 = \\frac{\\sum_{c=1}^C \\widehat Y_c} {C}\n= \\frac{\\sum_{c=1}^C \\bar Y_c}{C} = \\frac{\\sum_{c=1}^C \\left(\\frac{\\sum_{i=1}^n Y_{ic}}{n}\\right)} {C} =  \\frac{\\sum_{c=1}^C \\sum_{i=1}^n Y_{ic}}{nC} = \\bar Y\n\\end{equation}\\]\nThe regression slopes compare the each group to the intercept. Consequently, when the groups have equal sample sizes, the regression slopes are interpreted as the deviation of each group mean from the overall mean, which is why it is called deviation coding.\nWhen the groups have unequal sample size, the situation is a bit more complicated. In particular, we have to weight the predicted values in Equation 4.5 by the group sample sizes. This is addressed in Section 4.5.4 (optional). To clarify that intercept in deviation coding is not always equal to \\(\\bar Y\\), we refer to it as an “unweighted mean” of group means / predicted scores.\nIt is important to note that there are still only \\(C-1\\) regression slopes So, one group gets left out of the analysis, and the researcher has to chose which one. This is a shortcoming of deviation coding, which is addressed in Section 4.5.5 (optional).\n\n4.5.1 A hypothetical example\nThe International Development and Early Learning Assessment (IDELA) is an assessment designed to measure young children’s development in literacy, numeracy, social-emotional, and motor domains, in international settings. Figure 4.4 shows the countries in which the IDELA had been used as of 2017 (for more info, see https://www.savethechildren.net/sites/default/files/libraries/GS_0.pdf).\n\n\nCode\nknitr::include_graphics(\"files/images/idela_map.png\")\n\n\n\n\n\nFigure 4.4: IDELA Worldwide Usage, 2017.\n\n\n\n\nIf our goal was to compare countries’ IDELA scores, agreeing on which country should serve as the reference group would politically fraught. Therefore, it would be preferable to avoid the problem of choosing a reference group altogether. In particular, deviation coding let’s us compare each country’s mean IDELA score to the (unweighted) mean over all of the countries.\nFigure 4.5 presents a toy example. The data show the IDELA scores and Country for 16 hypothetical individuals. The countries considered in this example are\n\nEthiopia\nVietnam\nBoliva\n\nThese 3 countries are represented by two binary variables, denoted \\(X_1\\) and \\(X_2\\).\n\n\\(X_1\\) is a indicator for Ethiopia\n\\(X_2\\) is a indicator for Vietnam\n\n\n\nCode\nknitr::include_graphics(\"files/images/idela1.png\")\n\n\n\n\n\nFigure 4.5: Toy IDELA Example.\n\n\n\n\nNote that the dummy variables are different than for the case of reference-group coding discussed in Section 4.4. In deviation coding, the dummies always take on values \\(1, 0, -1\\). The same group must receive the code \\(-1\\) for all dummies. The group with the value \\(-1\\) is the group that gets left out of the analysis.\nPlease take a moment to write down the contrast matrix for the IDELA example.\n\n\n4.5.2 Interpreting the regression coefficients\nRegressing IDELA on the dummies we have:\n\\[\\widehat Y = b_0 + b_1 X_1 + b_2 X_2\\]\nIn order to interpret the regression coefficients, we proceed using the same two steps as in Section 4.3 and Section 4.4.\n\nStep 1. Plug the values for \\(X\\) into the regression equation to get the predicted values for each group.\n\n\\[\\begin{align}\n\\widehat Y (Ethiopia) & = b_0 + b_1 (1) + b_2 (0) = b_0 + b_1\\\\\n\\widehat Y (Vietnam) & = b_0 + b_1 (0) + b_2 (1) = b_0 + b_2 \\\\\n\\widehat Y (Bolivia) & = b_0 + b_1 (-1) + b_2 (-1) = b_0 - b_1 - b_2\n\\end{align}\\]\n\nStep 2. Solve for the model parameters in terms of the predicted values.\n\n\\[\\begin{align}\nb_1 &= \\widehat Y (Ethiopia) - b_0 \\\\\nb_2 &= \\widehat Y (Vietnam) - b_0 \\\\\nb_0 & = \\widehat Y (Bolivia) + b_1 + b_2 \\\\  \n\\end{align}\\]\nAt this point, we want to use the first two lines to substitute in for \\(b_1\\) and \\(b_2\\) in the last line:\n\\[\\begin{align}\nb_0 & = \\widehat Y (Bolivia) + \\widehat Y (Ethiopia) - b_0 + \\widehat Y (Vietnam) - b_0 \\\\\n\\implies & \\\\\n3b_0 & = \\widehat Y (Bolivia) + \\widehat Y (Ethiopia) + \\widehat Y (Vietnam) \\\\\n\n\\implies & \\\\\nb_0 & = \\frac{\\widehat Y (Bolivia) + \\widehat Y (Ethiopia) + \\widehat Y (Vietnam)}{3}\n\\end{align}\\]\nIn the last line we see that \\(b_0\\) is equal to the (unweighted) mean of the predicted values.\nUsing the above equations, please write down an interpretation of the regression parameters for the hypothetical example. (Note: this question is not asking for a numerical answer, it is just asking you to put the above equations into words.)\n\n\n4.5.3 Summary\nIn deviation coding with a single categorical variable:\n\nThe intercept is interpreted as the unweighted mean of the groups’ means, which is equal to the overall mean on the \\(Y\\) variable when the groups have equal sample sizes.\nThe regression slopes on the dummy variables are interpreted as the difference between the mean of the indicated group and the unweighted mean of the groups.\nThere are still only \\(C - 1\\) regression coefficients, so one group gets left out (see extra material for how to get around this).\n\n\n\n4.5.4 Extra: Deviation coding with unequal sample sizes*\nWhen groups have unequal sample size, the unweighted mean of the group means is not equial the overall mean of the \\(Y\\) variable. This is not always a problem. In particular, in the IDELA example, it is reasonable that each country should receive equal weight, even if the size of the samples in each country differed.\nHowever, if you want to compare each groups’ mean to the overall mean on \\(Y\\), deviation coding can be adjusted by replacing the dummy-coded value \\(-1\\) with the ratio of indicated group’s sample size to the omitted group’s sample size. An example for 3 groups is shown below.\n\\[\n\\begin{matrix} & \\text{Dummy 1}& \\text{Dummy 2}\\\\\n                   \\text{Group 1} & 1 & 0  \\\\\n                   \\text{Group 2} & 0 & 1 \\\\\n                  \\text{Group 3}  & - n_1 /n_3 & - n_2 / n_3 \\\\\n\\end{matrix}\n\\]\nYou can use the 2-step procedure to show that this coding, called weighted deviation coding, results in\n\\[\\begin{equation}\nb_0 = \\frac{n_1 \\widehat Y( \\text{Group 1}) + n_2 \\widehat Y( \\text{Group 2}) + n_3 \\widehat Y( \\text{Group 3})}{n_1 + n_2 + n_3}\n\\end{equation}\\]\nReplacing \\(\\widehat Y( \\text{Group }c )\\) with \\(\\bar Y_c\\) you can also show that \\(b_0 = \\bar Y\\), using the rules of summation algebra.\nThe punchline: unlike the case for the deviation coding, the intercept in weighted deviation coding is always equal to the overall mean on the outcome variance, regardless of the sample sizes of the groups. In R, you can use the package wec for weighted effect coding.\n\n\n4.5.5 Extra: Deviation coding with all groups included*\nAnother shortcoming of deviation coding is that it requires leaving one group out of the model. As a work around, one can instead use the following approach. Note that this approach will affect the statistical significance of R-squared, so you should only use it if you aren’t interested in reporting R-squared (or are willing to do some more fiddling around with R).\n\nStep 1: Center the \\(Y\\) variable so that \\(\\bar Y = 0\\).\nStep 2: Compute binary dummy variables (reference-group coding) for all \\(C\\) groups, \\(X_1, X_2, \\dots, X_C\\)\nStep 3: Regress \\(Y\\) on the all \\(C\\) dummy variables, without the intercept in the model:\n\n\\[\\hat Y = b_1X_1 +  b_2 X_2 + \\dots + b_cX_C.\\]\nUsing the two step approach, it is easy to show that the regression coefficients are just the means of the indicated group. Since the overall mean of \\(Y\\) is zero (because of Step 1), the group means can be interpreted as deviations from the overall mean on \\(Y\\).\nUsing the lm function in R, you can remove the intercept from the model using -1 in the formula syntax:\nY ~ -1 + X1 + ...\nAs noted, this will affect the statistical significance of R-squared, as reported in summary(lm). So, you shouldn’t use this approach when you want to report R-squared. The next two paragraphs explain why, but the explanation requires some material we won’t discuss until Chapter 6.\nThe F-test of R-squared can be computed by comparing two different models, a reference model with no predictors and a focal model with the predictors. The reference model used by lm depends on whether the intercept is included in the focal model. If the focal model has an intercept, the reference model also has an intercept. In the focal model omits the intercept, the reference also omits the intercept.\nIn the present context, the focal model omits the intercept and use \\(C\\) dummies This model has the same R-squared as if we had used the intercept and \\(C-1\\) dummies. However, the F-statistic for the test of R-squared (as reported by summary(lm)) is different for the two approaches, because of the choice of reference model. You can change the reference model, but it takes a bit of fiddling around. If you are interested, just ask in class."
  },
  {
    "objectID": "ch4_categorical_predictors.html#relation-with-anova",
    "href": "ch4_categorical_predictors.html#relation-with-anova",
    "title": "4  Categorical predictors",
    "section": "4.6 Relation with ANOVA",
    "text": "4.6 Relation with ANOVA\nAs mentioned in Chapter 3, the F-test of R-squared is equivalent to simultaneously testing whether all of the regression slopes are equal to zero. In symbols:\nTesting\n\\[H_0: R^2 = 0\\]\nis equivalent to testing\n\\[H_0: b_1 = b_2 = \\cdots = b_{C-1} = 0\\]\nWe have see above that the regression slopes the regression coefficient of dummy variables can be interpreted in terms of group-mean differences. For example, under reference-group coding, we can re-write the null hypothesis above as:\n\\[H_0: \\mu_1 - \\mu_C = \\mu_2 - \\mu_C  = \\cdots = \\mu_{C-1} - \\mu_C  = 0\\] In this notation, we let \\(C\\) denote the reference group. If we add $_C $ to each equality in the null hypothesis, we have\n\\[H_0: \\mu_1  = \\mu_2  = \\cdots = \\mu_{C-1} = \\mu_C\\]\nNote that this is the same null hypothesis as one-way ANOVA – i.e., all the group means are equal. Consequently, the F-test of R-squared using a categorical predictor is equivalent to the F-test of the omnibus hypothesis in one-way ANOVA (assuming homoskedasticity). In Section 4.9, we show how the two approaches produce the same numerical values.\nAlthough regression with a categorical predictor is mathematically equivalent to one-way ANOVA, you may have noticed that the two approahces are not used in the same way. In ANOVA, we first conduct an F-test of the omnibus hypothesis, and, if the test is significant, we follow-up by comparing the groups using procedures that control for familywise error rate. In regression, we just report the F-test along with the test of the individual contrasts, without any discussion of familywise error rate control. Which approach is correct? Depends who you ask…."
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-categorical-results-4",
    "href": "ch4_categorical_predictors.html#sec-categorical-results-4",
    "title": "4  Categorical predictors",
    "section": "4.7 More about categorical data*",
    "text": "4.7 More about categorical data*\nCheck back later for:\n\nmean and variance of binary variables\ncovariance between binary variables and between binary and continuous\nregression coefficient when X is binary"
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-workbook-4",
    "href": "ch4_categorical_predictors.html#sec-workbook-4",
    "title": "4  Categorical predictors",
    "section": "4.8 Workbook",
    "text": "4.8 Workbook\nThis section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in Section 4.9. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 4.2\nPlease take a moment to write down your thoughts on the tensions that arise when conceptualizing social constructs such as gender or race as categorical, and I will invite you to share you thoughts in class.\nSection 4.3\nPlease take a moment to write down how the regression output is related to the Figure. In particular, what is \\(\\bar Y_0\\) equal to, what is equal \\(\\bar Y_0\\) to, and what is their difference equal to?\n\n\nCode\n# Don't read this unless you really like working on graphics :) \nbinary_gender &lt;- (gender == \"Male\") * 1 \nplot(binary_gender, achrdg08, \n     col = \"#4B9CD3\", \n     xlab = \"binary gender (Female = 0, Male = 1)\",\n     ylab = \"Reading (Grade 8)\")\n\nmeans &lt;- tapply(achrdg08, binary_gender, mean)\nlabels &lt;- c(expression(bar(Y)[0]), expression(bar(Y)[1]))\n\ntext(x = c(.1, .9), y = means, labels = labels, cex = 1.5) \ntext(x = c(0, 1), y = means, labels = c(\"_\", \"_\"), cex = 2) \n\n\n\n\n\n\n\n\n\n\n\nCode\n# convert \"Female / Male\" coding to binary\ngender &lt;- NELS$gender\nbinary_gender &lt;- (gender == \"Male\")*1\nmod_binary &lt;- lm(achrdg08 ~ binary_gender)\ncoef(mod_binary)\n\n\n  (Intercept) binary_gender \n   56.4678022    -0.9223396 \n\n\nSection 4.3.1\nWe will go through the comparison between these two outputs in class together. If you have any questions about the relation between these two sets of output, please note them now and be prepared ask them in class.\nRegression with a binary predictor:\n\n\nCode\nsummary(mod_binary)\n\n\n\nCall:\nlm(formula = achrdg08 ~ binary_gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.7278  -6.1472   0.3784   6.9765  15.0045 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.4678     0.5342 105.703   &lt;2e-16 ***\nbinary_gender  -0.9223     0.7928  -1.163    0.245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.827 on 498 degrees of freedom\nMultiple R-squared:  0.00271,   Adjusted R-squared:  0.0007076 \nF-statistic: 1.353 on 1 and 498 DF,  p-value: 0.2452\n\n\nIndependent samples t-test (with homogeneity of variance assumed):\n\n\nCode\nt.test(achrdg08 ~ binary_gender, var.equal = T)\n\n\n\n    Two Sample t-test\n\ndata:  achrdg08 by binary_gender\nt = 1.1633, df = 498, p-value = 0.2452\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.6353793  2.4800586\nsample estimates:\nmean in group 0 mean in group 1 \n       56.46780        55.54546 \n\n\nSection 4.4.2\nRegressing Age on the dummies we have:\n\\[ \\widehat Y = b_0 + b_1 X_1 + b_2 X_2 \\]\nIn order to interpret the regression coefficients we can use the same two steps as in Section 4.3\n\nStep 1. Plug the values for the \\(X\\) variables into the regression equation to get the predicted values for each group\n\n\\[\\begin{align}\n\\widehat Y (Single) & = b_0 + b_1 (0) + b_2 (0) = b_0 \\\\\n\\widehat Y (Married) & = b_0 + b_1 (1) + b_2 (0) = b_0 + b_1 \\\\\n\\widehat Y (Divorced) & = b_0 + b_1 (0) + b_2 (1) = b_0 + b_2\n\\end{align}\\]\n\nStep 2. Solve for the model parameters in terms the predicted values.\n\n\\[\\begin{align}\nb_0 & = \\widehat Y (Single) \\\\\nb_1 & = \\widehat Y (Married) - b_0 = \\widehat Y (Married) - \\widehat Y (Single) \\\\\nb_2 & = \\widehat Y (Divorced) - b_0 = \\widehat Y (Divorced) - \\widehat Y (Single)\n\\end{align}\\]\nUsing the above equations, please write down an interpretation of the regression parameters for the hypothetical example. (Note: this question is not asking for a numerical answer, it is just asking you to put the above equations into words.)\nSection 4.4.3\nThe Mstatus example with 4 categories:\n\n\nCode\nknitr::include_graphics(\"files/images/marital_status2.png\")\n\n\n\n\n\n\n\n\n\nHere we are interested in the model:\n\\[\\widehat Y = b_0 + b_1 X_1 + b_2 X_2 + b_3 X_3\\]\nPlease work through the following questions and be prepared to share your answers in class\n\nHow should \\(X_3\\) be coded so that “single” is the reference-group?\nWrite down the contrast matrix for the example.\nUsing the two-step approach illustrated above, write out the interpretation of the regression coefficients.\nLet’s imagine that we included an additional dummy variable for “Single” in the model. In this case we would have \\(C = 4\\) dummies in the model rather than \\(C-1 = 3\\) dummies. Please take a moment to write down what you think will happen (Hint: something goes wrong).\n\nSection 4.5.1\nPlease take a moment to write down the contrast matrix for the IDELA example:\n\\[\n\\begin{matrix} &  X_1 & X_2 \\\\\n                  \\text{Ethiopia} & ? & ? \\\\\n                  \\text{Vietnam} &?  & ? \\\\\n                  \\text{Bolivia}  & ? & ? \\\\\n\\end{matrix}\n\\]\nSection 4.5.2\nRegressing IDELA on the dummies we have:\n\\[\\widehat Y = b_0 + b_1 X_1 + b_2 X_2\\]\nIn order to interpret the regression coefficients, we proceed using the same two steps as in Section 4.3 and Section 4.4.\n\nStep 1. Plug the values for \\(X\\) into the regression equation to get the predicted values for each group.\n\n\\[\\begin{align}\n\\widehat Y (Ethiopia) & = b_0 + b_1 (1) + b_2 (0) = b_0 + b_1\\\\\n\\widehat Y (Vietnam) & = b_0 + b_1 (0) + b_2 (1) = b_0 + b_2 \\\\\n\\widehat Y (Bolivia) & = b_0 + b_1 (-1) + b_2 (-1) = b_0 - b_1 - b_2\n\\end{align}\\]\n\nStep 2. Solve for the model parameters in terms of the predicted values.\n\n\\[\\begin{align}\nb_1 &= \\widehat Y (Ethiopia) - b_0 \\\\\nb_2 &= \\widehat Y (Vietnam) - b_0 \\\\\nb_0 & = \\widehat Y (Bolivia) + b_1 + b_2 \\\\  \n\\end{align}\\]\nAt this point, we want to use the first two lines to substitute in for \\(b_1\\) and \\(b_2\\) in the last line, which leads to\n\\[\\begin{align}\nb_0 & = \\frac{\\widehat Y (Bolivia) + \\widehat Y (Ethiopia) + \\widehat Y (Vietnam)}{3}\n\\end{align}\\]\nIn the last line we see that \\(b_0\\) is equal to the (unweighted) mean of the predicted values.\nUsing the above equations, please write down an interpretation of the regression parameters for the hypothetical example. (Note: this question is not asking for a numerical answer, it is just asking you to put the above equations into words.)"
  },
  {
    "objectID": "ch4_categorical_predictors.html#sec-exercises-4",
    "href": "ch4_categorical_predictors.html#sec-exercises-4",
    "title": "4  Categorical predictors",
    "section": "4.9 Exercises",
    "text": "4.9 Exercises\nThese exercises provide an overview of contrast coding with categorical predictors in R. Two preliminary topics are also discussed: linear regression with a binary predictor, and the factor data class in R. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)\nBefore staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\n\n4.9.1 A single binary predictor\nRegression with a single binary predictor is equivalent to the independent samples t-test of group means. Let’s illustrate this using the NELS dataset with Reading achievement in grade 8 (achrdg08) as the outcome and gender as the predictor. Although gender need not be conceptualized as dichotomous or even categorical, the variable gender reported in NELS data is dichotomous, with values “Female” and “Male”.\nTo start with, let’s recode gender to binary_gender by setting Female = 0 and Male = 1. Below we use a little trick for doing this. First we create a logical vector (gender == \"Male\") and then coerce the logical vector to binary by multiplying it by 1. There are other ways to do this but this approach is pretty easy.\n\n\nCode\n# load(\"NELS.RData\")\n# attach(NELS)\n\n# Note that gender is non-numeric -- it is a factor with levels \"Female\" and \"Male\"\nhead(gender)\n\n\n[1] Male   Female Male   Female Male   Female\nLevels: Female Male\n\n\nCode\nclass(gender)\n\n\n[1] \"factor\"\n\n\nCode\n# A trick to create a binary indicator for males   \nbinary_gender &lt;- (gender == \"Male\") * 1 \n\n# Check that the two variables are telling us the same thing\ntable(gender, binary_gender)\n\n\n        binary_gender\ngender     0   1\n  Female 273   0\n  Male     0 227\n\n\nNext we regress Reading Achievement (achrdg08) on binary_gender and compare this to an independent samples t-test using the same two variables.\n\n\nCode\n# Regression with a binary variable\nmod1 &lt;- lm(achrdg08 ~ binary_gender)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = achrdg08 ~ binary_gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.7278  -6.1472   0.3784   6.9765  15.0045 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.4678     0.5342 105.703   &lt;2e-16 ***\nbinary_gender  -0.9223     0.7928  -1.163    0.245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.827 on 498 degrees of freedom\nMultiple R-squared:  0.00271,   Adjusted R-squared:  0.0007076 \nF-statistic: 1.353 on 1 and 498 DF,  p-value: 0.2452\n\n\nCode\n# Compare to the output of t-test\nt.test(achrdg08 ~ binary_gender, var.equal = T)\n\n\n\n    Two Sample t-test\n\ndata:  achrdg08 by binary_gender\nt = 1.1633, df = 498, p-value = 0.2452\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.6353793  2.4800586\nsample estimates:\nmean in group 0 mean in group 1 \n       56.46780        55.54546 \n\n\nNote that:\n\nThe intercept in mod1 is equal the mean of the group coded 0 (females) in the t-test:\n\n\\[b_0 = \\bar Y_0 = 56.4678\\]\n\nThe b-weight in mod1 is equal to difference between the means:\n\n\\[b_1 = \\bar Y_1 - \\bar Y_0 = 55.54546 - 56.4678 = -0.9223\\]\n\nThe t-test of the b-weight, and its p-value, are equivalent to the t-test of the mean difference (except for the sign):\n\n\\[ t(498) = 1.1633, p = .245\\]\nIn summary, a t-test of a b-weight of a binary predictor is equal equivalent to an independent samples t-test of means.\n\n\n4.9.2 Reference-group coding\nAs discussed in Section 4.4, the basic idea of contrast coding is to replace a categorical variable with \\(C\\) categories with \\(C-1\\) “dummy” variables. In reference-group coding, the dummy variables are binary, and the resulting interpretation is:\n\nThe intercept is interpreted as the mean of the reference group. The reference is group is chosen by the analyst – it is the group that is coded zero on all dummies, or the group whose dummy variable is “left out” of the \\(C-1\\) dummy variables.\nThe regression coefficients of the dummy variables are interpreted as the difference between the mean of the indicated group and the mean of the reference group\n\nReference-group coding is the default contrast coding in R. However, in order for contrast coding to be implemented, our categorical predictor needs to be represented in R as a special data type called a factor.\n\n\n4.9.3 Factors in R\nFactors are the way that R deals with categorical data. If you want to know if your variable is a factor or not, you can use the functions class or is.factor. Let’s illustrate this with the urban variable from NELS.\n\n\nCode\n# Two ways of checking what type a variable is \nclass(urban)\n\n\n[1] \"factor\"\n\n\nCode\nis.factor(urban)\n\n\n[1] TRUE\n\n\nCode\n# Find out the levels of a factor using \"levels\"\nlevels(urban)\n\n\n[1] \"Rural\"    \"Suburban\" \"Urban\"   \n\n\nCode\n# Find out what contrast coding R is using for a factor \"contrasts\"\ncontrasts(urban)\n\n\n         Suburban Urban\nRural           0     0\nSuburban        1     0\nUrban           0     1\n\n\nIn the above code, we see that urban is a factor with 3 levels and the default reference-group contrasts are set up so that Rural is the reference group. Below we will show how to change the contrasts for a variable.\nIf we are working with a variable that is not a factor, but we want R to treat it as a factor, we can use the factor command. Let’s illustrate this by turning binary_gender into a factor.\n\n\nCode\n# Change a numeric variable into a factor\nclass(binary_gender)\n\n\n[1] \"numeric\"\n\n\nCode\nfactor_gender &lt;- factor(binary_gender)\nclass(factor_gender)\n\n\n[1] \"factor\"\n\n\nCode\nlevels(factor_gender)\n\n\n[1] \"0\" \"1\"\n\n\nWe can also use the levels function to tell R what labels we want it to use for our factor.\n\n\nCode\n# Change the levels of factor_gender to \"F\" and \"M\"\nlevels(factor_gender) &lt;- c(\"Female\", \"Male\")\nlevels(factor_gender)\n\n\n[1] \"Female\" \"Male\"  \n\n\n\n\n4.9.4 Back to reference-group coding\nOK, back to reference coding. Let’s see what lm does when we regress achrdg08 on urban.\n\n\nCode\nmod2 &lt;- lm(achrdg08 ~ urban)\nsummary(mod2)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    54.9927     0.6885  79.876  &lt; 2e-16 ***\nurbanSuburban   0.6675     0.9117   0.732  0.46439    \nurbanUrban      3.1275     1.0480   2.984  0.00298 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nIn the output, we see that two regression coefficients are reported, one for Suburban and one for Urban. As discussed in Section 4.4, these coefficients are the mean difference between the indicated group and the reference group (Rural).\nWe can see that Urban students scores significantly higher than Rural students (3.125 percentage points), but there was no significant difference between Rural and Suburban students.\nThe intercept is the mean of the reference group (Rural) – about 55% on the reading test.\nNote the R-squared – Urbanicity accounts for about 2% of the variation reading achievement. As usual, the F-test of R-squared has degrees of freedom \\(K\\) and \\(N - K -1\\), but now \\(K\\) (the number of predictors) is equal to \\(C - 1\\) – the number of categories minus one.\n\n\n4.9.5 Changing the reference group\nWhat if we wanted to use a group other than Rural as the reference group? We can chose a different reference group using the cont.treatment function. This function takes two arguments\n\nn tells R how many levels there\nbase tells R which level should be the reference group\n\n\n\nCode\n# The current reference group is Rural\ncontrasts(urban)\n\n\n         Suburban Urban\nRural           0     0\nSuburban        1     0\nUrban           0     1\n\n\nCode\n# Chance the reference group to the Urban (i.e., the last level)\ncontrasts(urban) &lt;- contr.treatment(n = 3, base = 3)\ncontrasts(urban)\n\n\n         1 2\nRural    1 0\nSuburban 0 1\nUrban    0 0\n\n\nNote that when we first ran contrasts(urban), the column names were names of the levels. But after changing the reference group, the column names are just the numbers 1 and 2. To help interpret the lm output, it is helpful to name the contrast levels appropriately\n\n\nCode\n# Naming our new contrasts\ncolnames(contrasts(urban)) &lt;- c(\"Rural\", \"Suburban\")\ncontrasts(urban)\n\n\n         Rural Suburban\nRural        1        0\nSuburban     0        1\nUrban        0        0\n\n\nNow we are ready to run our regression again, this time using a different reference group.\n\n\nCode\nmod3 &lt;- lm(achrdg08 ~ urban)\nsummary(mod3)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    58.1202     0.7901  73.559  &lt; 2e-16 ***\nurbanRural     -3.1275     1.0480  -2.984  0.00298 ** \nurbanSuburban  -2.4600     0.9907  -2.483  0.01335 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nCompared to the output from mod2, note that\n\nThe intercept now represents the mean reading scores of the Urban group, because this is the new reference group.\nThe regression coefficients now represent the mean difference between the indicated group with the new reference group.\nThe R-square and F-test stay the same – in other words, the total amount of variation explained by the variable Urban does not change, just because we changed the reference group.\n\n\n\n4.9.6 Deviation coding\nIt is possible to change R’s default contrast coding to one of the other built-in contrasts (see help(contrasts) for more information on the built-in contrasts).\nFor instance, to change to deviation coding, we use R’s contr.sum function and tell it how many levels there are for the factor (n). In deviation coding, the intercept is equal to the unweighted mean of the predicted values, and the regression coefficients are difference between the indicated group and the unweighted mean.\n\n\nCode\ncontrasts(urban) &lt;- contr.sum(n = 3)\ncontrasts(urban)\n\n\n         [,1] [,2]\nRural       1    0\nSuburban    0    1\nUrban      -1   -1\n\n\nCode\n# As above, it is helpful to name the contrasts using \"colnames\"\ncolnames(contrasts(urban)) &lt;- c(\"Rural\", \"Suburban\")\n\n\nNow we are all set to use deviation coding with lm.\n\n\nCode\nmod4 &lt;- lm(achrdg08 ~  urban)\nsummary(mod4)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    56.2577     0.4021 139.897   &lt;2e-16 ***\nurbanRural     -1.2650     0.5654  -2.237   0.0257 *  \nurbanSuburban  -0.5975     0.5299  -1.128   0.2600    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nNote the following things about the output:\n\nThe regression coefficients compare each group’s mean to the unweighted mean of the groups. Rural and Suburban students are below the unweighted mean, but the difference is only significant for Rural.\nAlthough the output looks similar to that of mod3, the coefficients are all different and they all have different interpretations. The lm output doesn’t tell us this, we have to know what is going on under the hood.\nThe R-square does not change from mod2 – again, the type of contrast coding used doesn’t affect how much variation is explained by the predictor.\n\n\n\n4.9.7 Extra: Relation to ANOVA\nAs our next exercise, let’s compare the output of lm and the output of aov – R’s module for Analysis of Variance. If regression and ANOVA are really doing the same thing, we should be able to illustrate it with these two modules. Note that if you check out help(aov), it explicitly states that aov uses the lm function, so, finding that the two approaches give similar output shouldn’t be a big surprise!\n\n\nCode\n# Run our model as an ANOVA\naov1 &lt;- aov(achrdg08 ~ urban)\n\n# Compare the output with lm\nsummary(aov1)\n\n\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nurban         2    741   370.5   4.824 0.00841 **\nResiduals   497  38163    76.8                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nsummary(mod2)\n\n\n\nCall:\nlm(formula = achrdg08 ~ urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    54.9927     0.6885  79.876  &lt; 2e-16 ***\nurbanSuburban   0.6675     0.9117   0.732  0.46439    \nurbanUrban      3.1275     1.0480   2.984  0.00298 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.0151 \nF-statistic: 4.824 on 2 and 497 DF,  p-value: 0.008411\n\n\nIf we compare the output from aov to the F-test reported by lm we see that the F-test, degrees of freedom, and p-value are all identical. If we compute eta-squared from aov, we also find that it is equal to the R-squared value from lm.\n\n\nCode\n# Compute eta-squared from aov output\n741 / (741 + 38163)\n\n\n[1] 0.01904688\n\n\nIn short, ANOVA and regression are doing the same thing: R-squared is the same as eta-squared and the ANOVA omnibus F-test is the same as the F-test of R-squared.\n\n\n4.9.8 Extra: Weighted deviation coding with all groups\nThis section presents a “hack” for addressing the two main issues with deviation coding noted in Section 4.5. This is a hack in the sense that we are working around R’s usual procedures. The result of this hack is to provide deviation coding in which comparisons are made to the grand mean on the outcome for all \\(C\\) categories, not just \\(C-1\\) categories. As a side effect, the F-test for the R-squared statistic reported in summary(lm) no longer correct, so we show how to fix that too.\nFirst, note that if if we omit the intercept term, all of the reference-group coded dummies can be included in the model and the regression coefficients now correspond to means of each group. We omit the intercept using -1 in the model formula.\n\n\nCode\n# Omit the intercept using -1\nmod5 &lt;- lm(achrdg08 ~ -1 + urban)\nsummary(mod5)\n\n\n\nCall:\nlm(formula = achrdg08 ~ -1 + urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \nurbanRural     54.9927     0.6885   79.88   &lt;2e-16 ***\nurbanSuburban  55.6602     0.5976   93.14   &lt;2e-16 ***\nurbanUrban     58.1202     0.7901   73.56   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.9763,    Adjusted R-squared:  0.9761 \nF-statistic:  6822 on 3 and 497 DF,  p-value: &lt; 2.2e-16\n\n\nNote the following things about the output:\n\nThe coefficients are no longer interpreted as mean differences, just the raw means.\nThe R-square and F-test do change from mod2. When the intercept is omitted, R-squared can no longer interpretated as a proportion of variance unless the variable \\(Y\\) variable is centered. When the intercept is omitted, it also changes the degrees of freedom in the F-test, because there are now 3 predictors instead of 2. In general, when the intercept is omitted, R-square and its F-test do not have the usual interpretation, so we need to be careful.\n\nBy itself, the “group-means” appoach to contrast coding is not very interesting – we don’t usually want to test whether the group means are different from zero. However, this approach can be used to provide an alternative to deviation coding if we center the \\(Y\\) variable before using the above approach. After centering, the grand mean of \\(Y\\) is zero, and so the group means represent deviations from the grand mean (i.e., deviations from zero) and the tests of the regression coefficients are tests of whether the group means are different from the grand mean.\n\n\nCode\n# Center the outcome variable\ndev_achrdg08 &lt;- achrdg08 - mean(achrdg08)\n\n# Run the regression with group mean coding\nmod6 &lt;- lm(dev_achrdg08 ~ -1 + urban)\nsummary(mod6)\n\n\n\nCall:\nlm(formula = dev_achrdg08 ~ -1 + urban)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3002  -6.1620   0.2098   6.7948  15.5573 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \nurbanRural     -1.0564     0.6885  -1.534  0.12556   \nurbanSuburban  -0.3889     0.5976  -0.651  0.51554   \nurbanUrban      2.0711     0.7901   2.621  0.00903 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.763 on 497 degrees of freedom\nMultiple R-squared:  0.01904,   Adjusted R-squared:  0.01312 \nF-statistic: 3.216 on 3 and 497 DF,  p-value: 0.02264\n\n\nNote the following things about the output:\n\nThe regression coefficients compare each group’s mean to the overall mean on the outcome. Urban students are significantly above average, Rural and Suburban students are below average but the difference is not significant.\nThe R-square is the same as mod2 because the \\(Y\\) variable is centered, but the F test is not the same as from mod2. The next two paragraphs explain why, but the explanation requires some material we won’t discuss until Chapter 6.\n\nThe F-test of R-squared can be computed by comparing two different models, a reference model with no predictors and a focal model with the predictors. The reference model used by lm depends on whether the intercept is included in the focal model. If the focal model has an intercept, the reference model also has an intercept. In the focal model omits the intercept, the reference also omits the intercept.\nIn the present context, the focal model omits the intercept and use \\(C\\) dummies This model has the same R-squared as if we had used the intercept and \\(C-1\\) dummies. However, the F-statistic for the test of R-squared (as reported by summary(lm)) is different for the two approaches, because of the choice of reference model. You can change the reference model, but it takes a bit of fiddling around. If you are interested, just ask in class.\nWhat all of this mean is that we can compute the “correct” F-test of R-squared by comparing mod6 to a model with just the intercept. The following code shows how to do this.\n\n\nCode\n# A model with only the intercept\nmod7 &lt;- lm(dev_achrdg08 ~ 1)\n\n# Compute the F-test of comparing mod6 to mod7\nanova(mod6, mod7)\n\n\nAnalysis of Variance Table\n\nModel 1: dev_achrdg08 ~ -1 + urban\nModel 2: dev_achrdg08 ~ 1\n  Res.Df   RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    497 38163                                \n2    499 38904 -2   -740.91 4.8244 0.008411 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is the same F-test reported for mod2, mod3 and mod4 above."
  },
  {
    "objectID": "ch5_interactions.html#sec-example-5",
    "href": "ch5_interactions.html#sec-example-5",
    "title": "5  Interactions",
    "section": "5.1 An example from NELS",
    "text": "5.1 An example from NELS\nFor the first few sections of this chapter, we will focus on the gender gap in Math Achievement as our example (e.g., https://www.nctm.org/Publications/TCM-blog/Blog/Current-Research-on-Gender-Differences-in-Math/). The t-test reported below uses the NELS data to illustrate the gender gap in Math Achievement in 12th grade. The output shows that, on average, males scored about 3.18 percentage points higher than females on a Grade 12 Math test. This gap isn’t very big. However, it tends to grow rather than get smaller as students progress to higher grades, and it has implications for gender equality in STEM education and STEM professions.\n\n\nCode\nload(\"NELS.RData\")\nattach(NELS)\nt.test(achmat12 ~ gender, var.equal = T)\n\n\n\n    Two Sample t-test\n\ndata:  achmat12 by gender\nt = -4.5529, df = 498, p-value = 6.658e-06\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -4.527002 -1.797687\nsample estimates:\nmean in group Female   mean in group Male \n            55.47092             58.63326 \n\n\nIn this chapter, our goal is to use linear regression to better understand the gender gap in Math Achievement. To help us do this, we will also consider a third variable, Reading Achievement. The plot below shows the relationship between Math Achievement and Reading Achievement estimated just for males (Blue), and the same relationship estimated just for females (Black).\n\n\nCode\n# Create indicators for females and males\nfemales &lt;- gender == \"Female\"\nmales &lt;- gender == \"Male\"\n\n# Regress math on reading, for each group separately\nmod1 &lt;- lm(achmat12[females] ~ achrdg12[females])\nmod2 &lt;- lm(achmat12[males] ~ achrdg12[males])\n\n# Plot reading and math for females\nplot(achrdg12[females], \n     achmat12[females], \n     xlab = \"Reading\", \n     ylab = \"Math\")\n\nabline(mod1, lwd = 2)\n\n# Add again for males\npoints(achrdg12[males], \n       achmat12[males], \n       col = \"#4B9CD3\", \n       pch = 2)\n\nabline(mod2, col = \"#4B9CD3\", lwd = 2)\n\n# Add a legend\nlegend(x = \"topleft\", \n       legend = levels(gender), \n       pch = c(1, 2), \n       col = c(1, \"#4B9CD3\"))\n\n\n\n\n\nFigure 5.1: Math Achievement, Reading Achievement, and Gender.\n\n\n\n\nPlease take a minute to think about what this plot is telling us about the relationships among the three variables. In particular: \n\nDoes the gender gap in Math Achievement change as a function of Reading Achievement?\n\nIs the relationship between Math Achievement and Reading Achievement the the same for males and females?\nWhat do the results mean for gender equality in Math and STEM education?\n\nNote that in Figure 5.1, we estimated two separate simple regression models, one just for males and one just for females. In the next few sections, we will work our way towards a single multiple regression model that can be used to represent the relationships among these three variables."
  },
  {
    "objectID": "ch5_interactions.html#sec-binary-continuous-5",
    "href": "ch5_interactions.html#sec-binary-continuous-5",
    "title": "5  Interactions",
    "section": "5.2 Binary + continuous",
    "text": "5.2 Binary + continuous\nLet’s start by considering what happens when we include both Gender and Reading Achievement as predictors of Math Achievement in our usual multiple regression equation:\n\\[\\widehat Y = b_0 + b_1X_1 + b_2 X_2 \\tag{5.1}\\]\nwhere\n\n\\(Y\\) is Math Achievement in grade 12\n\\(X_1\\) is Reading Achievement in grade 12\n\\(X_2\\) is Gender (binary, with female = 0 and male = 1)\n\nNote that this model does not include an interaction between the two predictors – we are first going to consider what is “missing” from the usual regression model, and then use this to motivate inclusion of another predictor that represents the interaction. To get an initial sense of what is missing, the model in Equation 5.1 is plotted in Figure 5.2 using the NELS data – can you spot the difference with Figure 5.1?\n\n\nCode\n# Run the regression\nmod3 &lt;- lm(achmat12 ~ achrdg12 + gender, data = NELS)\n\na_females &lt;- coef(mod3)[1]\nb_females &lt;- coef(mod3)[2]\n\n# Get the slope and intercept for males\na_males &lt;- a_females + coef(mod3)[3]\nb_males &lt;- b_females \n\n# Plot reading and math for females\nplot(achrdg12[females], \n     achmat12[females], \n     xlab = \"Reading\", \n     ylab = \"Math\")\n\nabline(a_females, b_females, lwd = 2)\n\n# Add points and line for males\npoints(achrdg12[males], \n       achmat12[males], \n       col = \"#4B9CD3\", \n       pch = 2)\n\nabline(a_males, b_males, col = \"#4B9CD3\", lwd = 2)\n\n# Add a legend\nlegend(x = \"topleft\", \n       legend = levels(gender), \n       pch = c(1, 2), \n       col = c(1, \"#4B9CD3\"))\n\n\n\n\n\nFigure 5.2: Math Achievement, Reading Achievement, and Gender (No Interaction).\n\n\n\n\nIn order to interpret our multiple regression model, we can use the same overall approach as we used to interpret categorical predictors in Chapter 4. If we plug-in values for the categorical predictor, we get:\n\\[\n\\begin{align}\n\\text{Simple trend for females:   } \\widehat Y (Female) & = b_0 + b_1X_1 + b_2 (0) \\\\ & = b_0 + b_1X_1\n\\end{align}\n\\tag{5.2}\\]\n\\[\n\\begin{align}\n\\text{Simple trend for males:   } \\widehat Y (Male) & = b_0 + b_1X_1 + b_2 (1) \\\\ & = (b_0 + b_2) + b_1 X_1\n\\end{align}\n\\tag{5.3}\\]\n\\[\n\\begin{align}\n\\text{Predicted gender gap:   } \\widehat Y (Male) - \\widehat Y (Female) & = b_2\n\\end{align}\n\\tag{5.4}\\]\nThe equations for \\(\\widehat Y (Female)\\) and \\(\\widehat Y (Male)\\) are referred to as simple trends or simple slopes. These describe the regression of Math on Reading, simply for females, or simply for males. The difference between the two simple regression equations is the predicted gender gap in Math Achievement.\nBased on these equations we can interpret regression coefficients as follows\n\nThe regression intercept, \\(b_0\\), is the intercept of the simple trend for the group coded “0” (i.e., the intercept of the regression of Math on Reading, simply for females; see Equation 5.2).\nThe regression slope for the continuous predictor, \\(b_1\\), is the slope of both of the simple trends (see Equation 5.2 and Equation 5.3)\nThe regression slope for the binary predictor, \\(b_2\\), is the difference between the intercepts of the simple trends (i.e., we add \\(b_2\\) to \\(b_0\\) to get the intercept of the regression of Math on Reading, simply for males; see Equation 5.3).\nIn this model, \\(b_2\\) is the also the predicted gender gap in Math Achievement (see Equation 5.4).\n\nThe regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the simple trends and the gender gap in Math Achievement for the NELS example. (Don’t worry about statistical significance, just focus on the meaning of the coefficients reported in the “Estimate” column.)\n\n\nCode\nsummary(mod3)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 + gender, data = NELS)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.2448  -3.6075   0.3968   3.9836  15.5606 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 19.98122    1.86278  10.727  &lt; 2e-16 ***\nachrdg12     0.63551    0.03275  19.404  &lt; 2e-16 ***\ngenderMale   3.50166    0.52473   6.673 6.69e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.839 on 497 degrees of freedom\nMultiple R-squared:  0.4538,    Adjusted R-squared:  0.4516 \nF-statistic: 206.4 on 2 and 497 DF,  p-value: &lt; 2.2e-16\n\n\nHere is an example to help you get started:\n\nSimply for females, the regression of Math Achievement on Reading Achievement has an intercept equal to 19.98 and a slope equal to 0.64. The intercept tells us that a female student with Reading Achievement score of 0% is expected to have a Math Achievement score of 19.98% (the units of the two tests are percent correct). Since the lowest value of Reading Achievement in our example is about 35%, the intercept is not very meaningful for these data. The regression slope tells us that, for females, a 1 unit increase in Reading Achievement is associated with a .64 unit increase in Math Achievement.\nSimply for males, ….\nThe gender gap in Math Achievement was equal to …\n\n\n5.2.1 Marginal means\nBefore moving on to consider interactions, let’s revisit a topic from the previous chapter. In Chapter 4, we noted that the regression coefficients for categorical predictors (dummy variables) can be interpreted in terms of the group means of the outcome variable. However, when additional predictors are included in the regression model, this interpretation no longer holds. This section explains why.\nTo see what the issue is, let’s compare the output of the t-test in Section 5.1 with the regression output shown above. In the t-test, the mean Math Achievement for females was 55.47, and for males it was 58.63. The mean difference was\n\\[58.63 - 55.47 = 3.16\\]\nHowever, the regression coefficient on Gender in the multiple regression model above is equal to \\(3.50\\). Thus, unlike Chapter 4, the regression coefficient on Gender is no longer equal to the group-mean difference in Math Achievement. But why?\nRemember that in the multiple regression model, the regression coefficient on Gender is interpreted as the relationship between Math Achievement and Gender, holding Reading Achievement constant. So, the regression coefficient on Gender is still interpreted as a mean difference, but now it is a predicted mean difference that represents the gender gap in Math Achievement after controlling for Reading Achievement. The t-test doesn’t control for Reading.\nIn order to emphasize the distinction between “raw” group means computed from the data and the predicted group means obtained from a multiple regression model, the latter are referred to as marginal means, or sometimes as adjusted means or least squares means. I think they should be called predicted means, but, alas.\n\n\n5.2.2 Summary\nIn a regression model with one continuous predictor and one binary predictor (and no interaction):\n\nThe model results in two regression lines, one for each value of the binary predictor. These are called the simple trends.\nThe simple trends are parallel but can have different intercepts; the difference between the intercepts is equal to regression coefficient of the binary variable.\nThe difference between the simple trends is often called a “gap”, and the gap is also equal to the regression coefficient of the binary variable.\nIt is important to note that the predicted group means for the binary variable are no longer equal to the “raw” group means computed directly from the data, because the predicted group means control for the correlation between the predictors. The predicted group means are called marginal means to emphasize this distinction.\n\nKeep in mind that this summary applies to the multiple regression model without an interaction. In the next section we improve our model by adding an interaction."
  },
  {
    "objectID": "ch5_interactions.html#sec-binary-continuous-interaction-5",
    "href": "ch5_interactions.html#sec-binary-continuous-interaction-5",
    "title": "5  Interactions",
    "section": "5.3 Binary + continuous + interaction",
    "text": "5.3 Binary + continuous + interaction\nIn this section, we discuss what was missing from the multiple regression model in the previous section: The interaction between Gender and Reading.\nMathematically, an interaction is just the product between two variables. Equation 5.5 shows how to include this product in our multiple regression model – we just take the product of the two predictors and add it into the model as a third predictor:\n\\[\\hat Y = b_0 + b_1X_1 + b_2 X_2 + b_3 (X_1 \\times X_2).  \\tag{5.5}\\]\nIn terms of computation, we would literally take the product of our two predictors and save it as a new variable in our data set, then add the new variable in to the regression model. In practice, R will do all of this for us behind the scenes, so we don’t actually need to “hard code” new variables.\nFor the NELS example, the regression model with the interaction is depicted in Figure 5.3. Note the the simple trends are no longer parallel and the regression lines agree exactly with what we had in Section 5.1. So, as promised, we have now arrived at a single multiple regression model that captures the relationships among Math Achievement, Reading Achievement, and Gender.\n\n\nCode\n# Interaction via hard coding\ngenderXachrdg12 &lt;- (as.numeric(gender) - 1) * achrdg12\nmod4 &lt;- lm(achmat12 ~ achrdg12 + gender + genderXachrdg12)\n\n# Get the coefficients for females\na_females &lt;- coef(mod4)[1]\nb_females &lt;- coef(mod4)[2]\n\n# Get the coefficients for males\na_males &lt;- a_females + coef(mod4)[3]\nb_males &lt;- b_females + coef(mod4)[4]\n\n# Plot reading and math for females\nplot(achrdg12[females], \n     achmat12[females], \n     xlab = \"Reading\", \n     ylab = \"Math\")\n\nabline(a_females, b_females, lwd = 2)\n\n# Add points and line for males\npoints(achrdg12[males], \n       achmat12[males],\n       col = \"#4B9CD3\", \n       pch = 2)\n\nabline(a_males, b_males, col = \"#4B9CD3\", lwd = 2)\n\n# Add a legend\nlegend(x = \"topleft\",\n       legend = levels(gender), \n       pch = c(1, 2), \n       col = c(1, \"#4B9CD3\"))\n\n\n\n\n\nFigure 5.3: Math Achievement, Reading Achievement, and Gender (No Interaction).\n\n\n\n\nTo see how to interpret the coefficients in this model, let’s work through the model equations using our two-step procedure. As before, we first plug-in values for the categorical predictor, then we use the resulting equations solve for the simple trends and the gender gap in Math.\n\\[\\begin{align}\n\\widehat Y (Female) & = b_0 + b_1X_1 + b_2 (0) + b_3(X_1 \\times 0) \\\\ & = b_0 + b_1X_1\n\\end{align} \\tag{5.6}\\]\n\\[\\begin{align}\n\\widehat Y (Male) & = b_0 + b_1X_1 + b_2 (1) +  b_3(X_1 \\times 1)\\\\ & = (b_0 + b_2) + (b_1 + b_3) X_1 \\end{align} \\tag{5.7}\\]\n\\[\\begin{align}\n\\widehat Y (Male) - \\widehat Y (Female) & = b_2 + b_3 X_1\n\\end{align} \\tag{5.8}\\]\nThese equations are summarized graphically below in Figure 5.4.\n\n\n\nFigure 5.4: Interaction between a categorical and a continuous predictor. Image credit: Daniela Rodriguez-Mincey, Spring 2023\n\n\nBased on the equations and figure, we can interpret regression coefficients as follows. Some of these interpretations are the same as in the previous section, but some are different.\n\nThe regression intercept, \\(b_0\\), is the intercept of the simple trend for the group coded “0” (i.e., the intercept of the regression of Math on Reading, simply for females; see Equation 5.6). This is the same interpretation as for the model without the interaction discussed in Section 5.2.\nThe regression slope for the continuous predictor, \\(b_1\\), is the slope of the simple trend for the group coded “0” (females; see Equation 5.6). This is different from the interpretation of the model in Section 5.2 – in that model, \\(b_1\\) was the slope of both simple trends, not just the trend for females.\nThe regression slope for the binary predictor, \\(b_2\\), is the difference between the intercepts of the simple trends (i.e., we add \\(b_2\\) to \\(b_0\\) to get the intercept of the regression of Math on Reading, simply for males; see Equation 5.7). This is the same interpretation as for the model without the interaction discussed in Section 5.2.\nThe regression slope for the interaction term (or simply, the interaction), \\(b_3\\), is the difference between the slopes of the simple trends (i.e., we add \\(b_3\\) to \\(b_1\\) to get the slope of the regression of Math on Reading, simply for males; see Equation 5.7). This is different from the interpretation of the model in Section 5.2 – in that model, \\(b_1\\) was the slope of both simple trends.\nThe difference between the predicted values (i.e., the predicted gender gap in Math Achievement) is no longer constant, but is instead a function of \\(X_1\\) (see Equation 5.8). In particular, the predicted gender gap in Math changes by \\(b_3\\) units for each unit of increase in Reading. This is different from the interpretation of the model in Section 5.2 – in that model, the predicted gender gap was constant over Reading and equal to \\(b_2\\).\n\nThis last point is especially important in the context of our example. The gender gap in Math Achievement is a function of Reading Achievement. This is the mathematical meaning behind the concept of an interaction – the relationship between two variables (Math and Gender) is changing as a function of a third variable (Reading).\n\n5.3.1 Choosing the moderator\nInterpreting interactions can feel a bit unwieldy at first. This section introduces some additional terminology that helps better align the mathematical results with the kinds of research scenarios we considered in the introduction to this chapter.\nFirst, note that it is equally valid to say\n\nthe relationship between Math and Gender depends on Reading, or\nthe relationship between Math and Reading depends on Gender.\n\nIn other words, it is equally valid to interpret our interaction in terms of the gender gap (i.e., the relationship between Math and Gender) or in terms of the simple trends (the relationship between Math and Reading).\nAlthough both interpretations are equally valid, in most research settings we will be more interested in one of them rather than the other. For example, our research interest in Section 5.1 was about the gender gap in Math Achievement. So, we can simplify our lives by focusing the gender gap (i.e., the relationship between Math and Gender). For the purpose of our example, the simple trends are an equivalent but less interesting way of interpreting the interaction. The point is: we don’t have to report both the simple trends and the gender gap – we usually just choose one.\nIn the two bullet points above, whichever variable appears in the “depends on” clause is called the moderator, and the other two variables are called the focal variables. The researcher chooses which variable to treat as the moderator when interpreting an interaction. The overall idea here is to “break down” the interaction in the way that is most compatible with your research question(s).\nSince our focus is the gender gap in Math (i.e., the relationship is between Math and Gender), Reading is our moderator. In particular, we might interpret the interaction as follows\n\nThe predicted gender gap in Math changes by \\(b_3\\) units for each unit of increase in Reading.\n\nBy contrast, if we were mainly interested in the relationship between Math and Reading (i.e., the simple trends), then we could treat Gender as the moderator. For example, we might say:\n\nFor females, predicted Math Achievement changed by \\(b_1\\) units for each unit of increase in Reading, whereas for males, the predicted change was (\\(b_1 + b_3\\)) units for each unit of increase in Reading.\n\nThe simple trends might feel less intuitive than the gender gap, but the two interpretations are mathematically equivalent. It is just a matter of whether you want to interpret the interaction with reference to the gender gap, or with reference to the simple trends. When writing up your research, you don’t need to do both. But I am going to make you do both in the next section for practice.\n\n\n5.3.2 Back to the example\nThe regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the interaction between Gender and Reading. For practice, please attempt the interpret the interaction in terms of (a) the gender gap in Math, with Reading as the moderator; and (b) the relationship between Math and Reading, with Gender as the moderator. Don’t worry about statistical significance, just focus on the interpreting the coefficients reported in the “Estimate” column.\n\n\nCode\nsummary(mod4)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 + gender + genderXachrdg12)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     14.80310    2.64922   5.588  3.8e-08 ***\nachrdg12         0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale      13.39328    3.65828   3.661 0.000278 ***\ngenderXachrdg12 -0.17794    0.06514  -2.732 0.006524 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nSome potential answers are hidden in the “Code” tab below, but don’t peak until you have tried it for yourself!\n\n\nCode\n# Gender gap\n# General: The gender gap in Math is smaller for students who are also strong in Reading\n# Specific: The gender gap in Math Achievement decreases by .18 percentage points for each percentage point increase Reading Achievement\n\n# Simple slopes\n# General: The relationship between Math and Reading is stronger (i.e. has a larger slope) for females than for males\n# Specific:\n#  For females, Math scores are predicted to increase by .72 percentage points for each percentage point increase in Reading Achievement\n#  For males, Math scores are predicted to increase by .55 percentage points for each percentage point increase in Reading Achievement\n\n\n\n\n5.3.3 Centering the continuous predictor\nYou may have noticed that the regression coefficient on Gender was wildly different in regression models with and without the interaction. In the model without the interaction (Section 5.2) the coefficient on Gender was 3.50, and in the model with the interaction (above), it was 13.40. So in one model, the “effect” of being male was a 3.5 percentage point gain on a Math test, but in the other model, it was a 13.40 percentage point gain. Why this huge difference in the “effect” of Gender?\nThe answer can be seen in the equation for the gender gap. In the model without the interaction, the gender gap was constant and equal to the regression coefficient on Gender (denoted as \\(b_2\\) in the model):\n\\[\\widehat Y (Male) - \\widehat Y (Female) = b_2. \\]\nBut in the regression model with the interaction, the gender gap was a linear function of Reading and the regression coefficient on Gender is the intercept of that linear relationship.\n\\[ \\widehat Y (Male) - \\widehat Y (Female) = b_2 + b_3 X_1. \\]\nThis equation tell us that, in the model with the interaction, \\(b_2\\) is the gender gap for students who score 0% on the Reading test. Since the lowest score on Reading was around 35%, the intercept in this equation (i.e., \\(b_2\\), the regression coefficient on Gender) is not very meaningful.\nIn previous sections, we have ignored the regression intercept when it was not meaningful. But, ignoring the regression slopes for predictor variables can get confusing, and, in general, it is nice for the regression coefficients to be interpretable (otherwise, why are we doing this!).\nOne way to address this situation is to center Reading Achievement so that it has a mean of zero. To do this, we compute the deviation score\n\\[D_1 = X_1 - \\bar X_1.\\]\n\\(D_1\\) is the mean-centered version of \\(X_1\\) (Reading Achievement). If we regress Math Achievement on \\(D_1\\) rather than \\(X_1\\) we end up with the following equation for the gender gap in Math:\n\\[\\widehat Y (Male) - \\widehat Y (Female) = b_2 + b_3 D_1\\]\nSince \\(D_1 = 0\\) when \\(X_1 = \\bar X_1\\), the regression coefficient on Gender (\\(b_2\\)) is now interpretable as the gender gap in Math Achievement, for students with average Reading Achievement. This is a much more interpretable number than the coefficient in the original interaction model!\nUsing the example data, this approach yields the following regression coefficients (the _dev notation means the variable was centered):\n\n\nCode\n# compute the deviation scores for reading\nreading_dev &lt;- achrdg12 - mean(achrdg12, na.rm = T) \n\n# Run the interaction model as above\ngenderXreading_dev &lt;- (as.numeric(gender) - 1) * reading_dev\n\nmod5 &lt;- lm(achmat12 ~ reading_dev + gender + genderXreading_dev)\nsummary(mod5)\n\n\n\nCall:\nlm(formula = achmat12 ~ reading_dev + gender + genderXreading_dev)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        55.29439    0.35127 157.411  &lt; 2e-16 ***\nreading_dev         0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale          3.49930    0.52135   6.712 5.26e-11 ***\ngenderXreading_dev -0.17794    0.06514  -2.732  0.00652 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nThe regression coefficient for Gender is now pretty close to what it was in the multiple regression model without the interaction, but the interpretation is different (i.e., it is now the predicted gender gap in Math for students with an average level of Reading, rather than the predicted gender gap in Math for all students).\nNotice that the intercept in the model above has also changed compared to the previous model in which Reading was not centered. This is should make sense based on what you already know about the regression intercept.\nAlso note that centering Reading did not affect the regression coefficient for Reading or the interaction. So, centering makes the regression slope on Gender more interpretable, but it doesn’t affect our overall interpretation of the simple trends or the gender gap. To learn more about how centering works, see Section 5.5.5 (which is optional).\nPlease write down your interpretation of the intercept and the regression coefficient on Gender in the above regression output, and be prepared to share your answer in class.\n\n\n5.3.4 Summary\nThe interaction between two variables is just their product. When this product is added as a predictor in a multiple regression model with one continuous and one binary predictor:\n\nThe model again results in two regression lines (simple trends), one for each value of the binary predictor.\nHowever, the simple trends can now have different intercepts and different slopes. The difference in slopes is equal to the regression coefficient on the interaction term. In other words, the simple trends have different slopes because of the interaction.\nThe difference (“gap”) between the regression lines changes as a linear function of the continuous predictor, and the slope of this linear function is again equal to the regression coefficient on the interaction term.\nThe last two points are equivalent ways of stating the central idea behind a (two-way) interaction: the relationship between two variables changes as a function of a third variable.\nWhen interpreting an interaction, the researcher chooses which pair of variables will be the “focal relationship” and which variable will be the moderator.\n\nIn our example, we focused on the gender gap in Math (i.e., the relationship between Math and Gender) and Reading was the moderator.\nBut, if we were more interested in the relationship between Math and Reading, would could have focused on the simple trends and treated Gender as the moderator.\nIt is usual to only report one way of these two ways of “breaking down” the interaction – the one that is most relevant to your research question.\n\nCentering the continuous predictor can be helpful for ensuring that the regression coefficient on the binary predictor remains interpretable in the presence of an interaction."
  },
  {
    "objectID": "ch5_interactions.html#sec-inference-for-interactions-5",
    "href": "ch5_interactions.html#sec-inference-for-interactions-5",
    "title": "5  Interactions",
    "section": "5.4 Following-up an interaction",
    "text": "5.4 Following-up an interaction\nThe procedures discussed in this section are used typically used after we have concluded that there is a statistically significant between two predictors (i.e., after we have examined the summary(lm) output in the previous section). When we follow-up an interaction, the goal is to gain a better understanding of how the focal relationship depends on the moderator. Basically, the summary(lm) output from the previous section tells us whether or not the interaction is significant, and, if it is significant, the procedures discussed in this section let us describe the interaction in more detail.\nThe overall idea is illustrated Figure 5.5. Compared to the plots we have seen previously in this chapter, Figure 5.5 now includes confidence bands for the simple trends. The confidence bands show the values of Reading for which the gender gap in Math is statistically significant. In particular, it appears that the gap is not significant for students at the highest levels of Reading Achievement.\nNote that this information was not available from the summary(lm) output in the previous section. The summary(lm) output told us that the interaction term was statistically significant, which means that the gender gap in Math changes as a function of reading. The plot provides more information about how the gender gap depends on Reading – it is statistical significant for students at lower levels of Reading Achievement, but appears to shrink and eventually disappear as Reading Achievement increases.\n\n\nCode\n# Install the package if you haven't already done so\n# install.packages(\"visreg\")\n# Load the package into memory\nlibrary(visreg)\n\n# Run the regression model using R's syntax for interactions \"*\"\nmod6 &lt;- lm(achmat12 ~ gender*achrdg12, data= NELS)\n\n# One line of code to plot trends with confidence bands :) \nvisreg(mod6, xvar = \"achrdg12\", by = \"gender\", overlay = TRUE)\n\n\n\n\n\nFigure 5.5: Example of a plot using the visreg package.\n\n\n\n\nIn this section, we discuss how to translate the confidence bands in the plot into statistical tests that can provide more specific information about the how the gender gap in Math depends on Reading. Making these kinds of inferences about interactions is one of the main advantages of using multiple regression rather than just fitting the simple trends separately as we did in Section 5.1. Another advantage is that we can now easily produce nice plots like Figure 5.5 :)\nBefore moving on, it is important to emphasize that, if the interaction term in the summary(lm) output is not significant, we don’t use the procedures discussed in this section. This is because a non-significant interaction means that we have inferred the focal relationship does not depend on the moderator. Consequently, there is no need to describe this dependence in more detail, which is what the procedures in this section are for.\n\n5.4.1 Marginal effects\nSince we are interested in the gender gap in Math Achievement, we will start by considering the values of Reading for which the gap is statistically significant. Note that this information was not available from the standard summary(lm) output shown in Section 5.3 – the output showed that the regression coefficient representing the interaction was statistically significant, but it didn’t tell us for the values of Reading for which the gender gap was statistically significant. We can answer this type of question using the marginal “effects” discussed in this section.\nThere are three main types of marginal effects. For linear models, they are all basically the same, and we will only use one of them in this section. However, it is important that you can distinguish among them, especially when we get into non-linear models (e.g., logistic regression; see Chapter 10). In general, when you report marginal effects in your research, you should be able to tell your reader which approach you used so they understand what you did and how to interpret the results.\nTo explain the three approaches, first let’s write the gender gap in Math Achievement using a slightly more compact notation (“\\(\\Delta\\)” for difference):\n\\[ \\widehat Y (Male) - \\widehat Y (Female) = b_2 + b_3 X_1 = \\Delta(X_1)\\]\nThe three types of marginal effects are:\n\nMarginal effects at the mean (MEM): Report the gap at the mean value of \\(X_1\\)\n\n\\[MEM =  \\Delta(\\bar X_1) \\]\n\nAverage marginal effect (AVE): Average the effect over values of \\(X_1\\):\n\n\\[AVE =  \\frac{\\sum_i \\Delta(X_{i1})}{N} \\]\n\nMarginal effects at representative values (MERV): Report the marginal effect for a range of “interesting values” chosen by the researcher (denoted by *, \\(\\dagger\\), etc.)\n\n\\[ MERV =  \\{\\Delta(X^*_1), \\Delta(X^\\dagger_1), \\dots \\} \\]\nMEM and AVE are equivalent in linear models, but are different for nonlinear models (see Chapter 10). The MERV approach also overlaps with MEM and AVE, because usually we choose the mean (or median) as one of the representative values of the predictor.\nIn this section we focus on MERV, because it is widely used. One common choice for the “interesting values” is the quartiles of \\(X_1\\), which are reported below for the example data. The output shows the gender gap in Math Achievement for 5 values of Reading Achievement. The values of Reading Achievement are its 5 quartiles. You can think of the output as a tabular summary of Figure 5.5.\n\n\nCode\n# Install the emmeans package if you haven't already done so\n# install.packages(\"emmeans\")\n\n# Load the package into memory\nlibrary(emmeans)\n\n# Fit the model using R's formula syntax for interaction '*'\nmod6 &lt;- lm(achmat12 ~ gender*achrdg12, data = NELS)\n\n# Use the emmeans function to get the gender means on math, broken down by reading\ngap &lt;- emmeans(mod6, \n               specs = \"gender\",\n               by = \"achrdg12\", \n               cov.reduce = quantile)\n\n# Test whether the differences are significant\ncontrast(gap, method = \"pairwise\")\n\n\nachrdg12 = 31.8:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -7.74 1.637 496  -4.728  &lt;.0001\n\nachrdg12 = 51.3:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -4.27 0.593 496  -7.207  &lt;.0001\n\nachrdg12 = 57.0:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -3.25 0.529 496  -6.138  &lt;.0001\n\nachrdg12 = 61.7:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -2.41 0.658 496  -3.659  0.0003\n\nachrdg12 = 68.1:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -1.28 0.967 496  -1.321  0.1872\n\n\nPlease use the output to make a conclusion about the levels of Reading Achievement for which the gender gap was significant. Please be prepared to share your answer in class!\nNote that the computations going on “under the hood” when testing marginal effects can get pretty complicated. You can read more details here: https://cran.r-project.org/web/packages/emmeans/vignettes/basics.html\n\n\n5.4.2 Simple trends\nIn this section we test whether the slope of each of the simple trends is significantly different from zero. Like the marginal effects in the previous section, this is information was not entirely available in the summary(lm) output discussed in Section 5.3. From the summary(lm) output we learned the following two points about the simple trends:\n\nThe regression coefficient on Reading told us about the relationship between Math and Reading, simply for the group designated as zero on the binary predictor (simply for females).\nThe regression coefficient on the interaction term told us whether the simple trends differ for the two groups (e.g., whether the simple trend for males differs from the simple trend for females).\n\nNote that what is missing, or implicit, is a test of whether the simple trend for males is different from zero. This test is of less relevance to our example (since we are treating Reading as the moderator), but we consider it for illustrative purposes.\nThe tests of the simple trends for the example data are reported below. As stated, these aren’t super interesting in the context of our example, but you should check your understanding of simple trends by writing down an interpretation of the output below.\n\n\nCode\n# The regression coefficients on reading, broken down by gender\nsimple_slopes &lt;- emtrends(mod6, var = \"achrdg12\", specs = \"gender\")\ntest(simple_slopes)\n\n\n gender achrdg12.trend     SE  df t.ratio p.value\n Female          0.728 0.0470 496  15.487  &lt;.0001\n Male            0.550 0.0451 496  12.208  &lt;.0001\n\n\n\n\n5.4.3 Summary\nWhen making inferences about an interaction:\n\nIf the interaction isn’t significant in the summary(lm) output, we stop there. But if the interaction is significant, we may want to report more information about how the focal relationship depends on the moderator.\nWhen the focal predictor is categorical, we can follow-up a significant interaction by taking a closer look at the statistical significance of the marginal effects (e.g, how the gender gap in Math changes as a function of Reading)\nWhen the focal predictor is continuous, we can follow-up a significant interaction by taking a closer look at the statistical significance of the simple trends / simple slopes."
  },
  {
    "objectID": "ch5_interactions.html#sec-two-continuous-predictors-5",
    "href": "ch5_interactions.html#sec-two-continuous-predictors-5",
    "title": "5  Interactions",
    "section": "5.5 Two continuous predictors",
    "text": "5.5 Two continuous predictors\nAt this point, we have covered the main ideas behind two-way interactions. In this section and the next, we apply these ideas to different combinations of predictors variables. In this section we address interactions between two continuous predictors. In the next section we address two categorical predictors. In both cases, the regression equation and overall interpretation is the same as the previous sections – e.g., the relationship between \\(Y\\) and \\(X_1\\) changes as a function of \\(X_2\\). However, there are also some special details that crop up in these different settings.\nIn this section we will address:\n\nThe importance of centering the two continuous predictors. Centering helps us interpret the “main effects” (i.e., the regression coefficients on the individual predictors).\nHow to follow-up a significant interaction using simple trends. As was the case for a categorical and a continuous predictor, this helps us interpret the interaction in more detail.\n\nFirst, we introduce an new example.\n\n5.5.1 Another NELS example\nTo illustrate an interaction between two continuous predictors, let’s replace Gender with SES in our previous analysis. Apologies that this new example is mainly for convenience and doesn’t represent a great research question about, e.g., why the relationships between Math and Reading might change as a function of SES.\nThe overall approach with SES as the moderator is illustrated in Figure 5.6 below. It presents the simple trends for Math and Reading at three values of SES. The overall situation should hopefully feel pretty familiar from the previous sections. The displayed values of SES (9, 19, and 28) are its 10th, 50th, and 90th percentiles, which is the default choice for the software we are using for plotting. For visual clarity, the confidence bands are not shown.\n\n\nCode\n#Interaction without centering \nmod7 &lt;- lm(achmat12 ~ achrdg12*ses, data = NELS)\n\n# Note that band = F removes the confidence intervals\nvisreg(mod7, \n       xvar = \"achrdg12\", \n       by = \"ses\", \n       overlay = TRUE, \n       band = F)\n\n\n\n\n\nFigure 5.6: Math (achmat), Reading (achrdg), and SES\n\n\n\n\n\n\n5.5.2 Centering the predictors\nThe take home message of this section is that you should center continuous predictors when their interaction is included in the model. There are two reasons:\n\nJust like Section 5.3, centering makes it easier to interpret the “main effects” (i.e., the regression coefficients on the individual predictors).\nCentering can make the estimates of the main effects more precise. This is because centering can reduce the correlation between the individual predictors and the interaction term. Remember from Section 3.7.1 that highly correlated predictors lead to less precise estimates of the regression coefficients. We discuss this problem in more detail in ?sec-multicollinearity-6, but for now we just discuss how centering helps us avoid the problem.\n\nFirst lets consider how centering facilitates interpretation. Begin by noting that the coefficients \\(b_1\\) and \\(b_2\\) in the regression model\n\\[ \\widehat Y = b_0 + b_1X_1 + b_2X_2 + b_3 (X_1 \\times X_2) \\]\ncan be interpreted in terms of the following simple trends:\n\\[\\begin{align}\n\\widehat Y(X_2 =0) & = b_0 + b_1X_1 \\\\\n\\widehat Y(X_1 =0)&  = b_0 + b_2X_2\n\\end{align} \\tag{5.9}\\]\nThe first equation shows us that \\(b_1\\) is the slope of relationship between \\(Y\\) and \\(X_1\\), when \\(X_2\\) is equal to zero – in our example, the relationship between Math and Reading when SES is equal to zero. Similarly, the second equation shows us that \\(b_2\\) is the slope of relationship between \\(Y\\) and \\(X_2\\) when \\(X_1\\) is equal to zero – in our example, the relationship between Math and SES when Reading is equal to zero.\nIn general, the value of zero may not be meaningful for continuous predictors. But, when the predictor is centered (i.e., a deviation score), the value of zero is always the mean of the original variable. For example, if we centered SES, then \\(b_1\\) would represent the relationship between Math and Reading for students with average SES. Similar considerations apply if we treat Reading as the moderator instead of SES. Note that this is just the same trick as Section 5.3, but this time both predictors are continuous and so both can be centered.\nThe second main reason for centering is a bit more technical. It has to do with reducing the correlation between the predictors and their interaction. In general, the interaction term will be correlated with both predictors if (a) the predictors themselves are correlated and (b) both predictors take on strictly positive (or strictly negative) values. Highly correlated predictors lead to redundant information the model, so we want to avoid this situation (this is technically called multicollinearity and we discuss it in more detail in a ?sec-multicollinearity-6).\nTo see how centering can reduce the correlation between the predictors and their interaction, let’s take a look at Figure 5.7. The left hand panel shows the relationship between SES and its interaction with Reading. We can see that they are highly correlated. This is because (a) SES and Reading are themselves correlated, and (b) both SES and Reading take on strictly positive values. As mentioned above, the interaction term will be correlated with both predictors whenever these two conditions hold. (The figure shows the correlation just for SES and the interaction, but the same situation holds for Reading.)\n\n\nCode\n# Correlation without centering\nr &lt;- cor(ses, achrdg12*ses)\n\n# Plot\npar(mfrow = c(1, 2))\ntitle &lt;- paste0(\"correlation = \", round(r, 3))\nplot(ses, achrdg12*ses, \n     col = \"#4B9CD3\", \n     main = title, \n     xlab = \"SES\", \n     ylab = \"SES X Reading\")\n\n# Correlation with centering\nachrdg12_dev &lt;- achrdg12 - mean(achrdg12)\nses_dev &lt;- ses - mean(ses)\nr &lt;- cor(ses_dev, achrdg12_dev*ses_dev)\n\n# Plot\ntitle &lt;- paste0(\"correlation = \", round(r, 3))\nplot(ses_dev, \n     achrdg12_dev*ses_dev, \n     col = \"#4B9CD3\", \n     main = title, \n     xlab = \"SES Centered\", \n     ylab = \"SES Centered X Reading Centered\")\n\n\n\n\n\nFigure 5.7: Correlation Between SES and SES X Reading, With and Without Centering\n\n\n\n\nWe can see in the right hand panel of Figure 5.7 how centering the two predictors “breaks” the linear relationship between SES and its interaction with Reading. After centering, the relationship between SES and its interaction is now highly non-linear, and the correlation is approximately zero. Again, the same is true for the relationship between Reading and the interaction, but the figure only shows the situation for SES. The upshot of all this is that centering reduces multicollinearity between the “main effects” of the predictors and their interaction.\nBelow we show the output for two regression models. Both models regress Math on Reading, SES, and their interaction. The first model does not center the predictors, but the second model does (the _dev notation denotes the centered predictors).\nThe main difference between the models is that SES is a significant predictor in the centered model but not in the “un-centered” model. This is because the main effect of SES has a different meaning in the centered model, and it is also less correlated with the interaction. This is also true for Reading, but the differences between the two sets of results are less pronounced for Reading.\n\n\nCode\n# Without centering\nmod7 &lt;- lm(achmat12 ~ achrdg12*ses)\nsummary(mod7)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 * ses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1336  -3.8944   0.7278   4.1301  15.0153 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.849200   5.389797   4.796 2.14e-06 ***\nachrdg12      0.511601   0.099427   5.145 3.85e-07 ***\nses          -0.100114   0.291214  -0.344    0.731    \nachrdg12:ses  0.004270   0.005196   0.822    0.412    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.031 on 496 degrees of freedom\nMultiple R-squared:  0.4184,    Adjusted R-squared:  0.4149 \nF-statistic: 118.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n# With centering\nmod8 &lt;- lm(achmat12 ~ achrdg12_dev*ses_dev)\nsummary(mod8)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12_dev * ses_dev)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1336  -3.8944   0.7278   4.1301  15.0153 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          56.826225   0.286906 198.065   &lt;2e-16 ***\nachrdg12_dev          0.590313   0.036103  16.351   &lt;2e-16 ***\nses_dev               0.137305   0.041485   3.310    0.001 ** \nachrdg12_dev:ses_dev  0.004270   0.005196   0.822    0.412    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.031 on 496 degrees of freedom\nMultiple R-squared:  0.4184,    Adjusted R-squared:  0.4149 \nF-statistic: 118.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nPlease provide an interpretation of all four regression coefficients in the centered model. Your interpretations should make reference to the situation where one or both predictors are equal to zero (see Equation 5.9 above) and should also mentioned the interpretation of the value of zero for the centered variables.\nNote that the interaction and the R-squared are the same in the centered and uncentered models. This is discussed in more detail in the extra material at the end of this section (Section 5.6), but it is sufficient to note that centering only affects the interpretation of the main effects (and the intercept, of course).\n\n\n5.5.3 Simple trends\nCentering helps us interpret the main effects of the individual predictors, but we haven’t yet discussed how to interpret the interaction term. As shown in Figure 5.6, the overall situation is not that different than with a binary predictor.\nThe usual way to follow up a significant interaction between two continuous is using the MERV approach discussed in Section 5.4. Using this approach, we consider the focal relationship for some “interesting values” of the moderator. As with MERV, the choice of values of the moderator is up to the researcher, but some usual choices are\n\nThe quartiles of the moderator\nM \\(\\pm\\) 1 SD of the moderator\nA selection of percentiles of the moderator (The visreg plot in Figure 5.6 uses the 10th, 50th, and 90th)\n\nThese are all doing very similar things, so choosing among them usually isn’t super important.\nAlthough the interaction between Reading and SES was not significant in our example model, let’s break down the interaction using SES as the moderator, just to see how this approach works. The output below presents the simple slopes for the three values of SES shown in Figure 5.6 (i.e., the 10th, 50th, and 90th percentiles). We can see in the output that the simple slopes are all different from zero. (And the non-significant interaction in the summary(lm) output tells us that the slopes are not statistically different from one another.)\n\n\nCode\n# Break down interaction with SES as moderator\nsimple_slopes &lt;-emtrends(mod7, \n                         var = \"achrdg12\", \n                         specs = \"ses\", \n                         at = list(ses = c(9, 19, 28)))\ntest(simple_slopes)\n\n\n ses achrdg12.trend     SE  df t.ratio p.value\n   9          0.550 0.0583 496   9.429  &lt;.0001\n  19          0.593 0.0365 496  16.251  &lt;.0001\n  28          0.631 0.0639 496   9.878  &lt;.0001\n\n\n\n\n5.5.4 Summary\nWhen regressing an outcome on two continuous predictors and their interaction, the overall interpretation of the model is same as discussed in Section 5.3, but:\n\nIt is useful to center both predictors, to facilitate the interpretation of the main effects (i.e., regression coefficients on the individual predictors), and to improve the precision of the main effects (i.e., reduce multicollinearity).\nWhen following up a significant interaction, the usual approach is to report the simple trends for the focal variables at a selection of values of the moderator (e.g., a selection of percentiles). The example illustrated how to do this even though the interaction was not significant, but you shouldn’t follow up a non-significant interaction.\n\n\n\n5.5.5 Extra: How centering works*\nIt might seem that centering both predictors is a bit dubious – how can we just change the predictors in the model? This sections shows that using the centered or the un-centered predictors doesn’t make a difference in term of what predictors are in the model, it just changes the interpretation of the main effects (and intercept).\nUsing \\(D = X - \\bar X\\) for the centered variables, simple algebra shows:\n\\[\\begin{align}\n\\widehat Y & = b_0 + b_1D_1 + b_1D_2 + b_3 (D_1 \\times D_2) \\\\\n& = b_0^* + (b_1 - b_3 \\bar X_1) X_1 + (b_2 - b_3 \\bar X_2) X_2 +  b_3 (X_1 \\times X_2) \\\\\n\\text{where} & \\\\ \\\\\nb_0^* & = a - b_1\\bar X_1 - b_2\\bar X_2 - b_3\\bar X_1\\bar X_2.\n\\end{align}\\]\nThe second line of the equation shows that we are not changing what we regress \\(Y\\) on – i.e., the predictors are still \\(X_1\\) and \\(X_2\\). We are changing the interpretation of the main effects (and intercept), but this is exactly the purpose of this approach. Also note centering does not change the regression coefficient for the interaction at all. So, we get main effects (and intercept) that can be more easily interpreted, and the same interaction"
  },
  {
    "objectID": "ch5_interactions.html#sec-two-categorical-predictors-5",
    "href": "ch5_interactions.html#sec-two-categorical-predictors-5",
    "title": "5  Interactions",
    "section": "5.6 Two categorical predictors",
    "text": "5.6 Two categorical predictors\nThis section addresses interactions between two categorical predictors. Up until now, we have looked at interactions only for categorical predictors that are dichotomous. In this section, we address an example in which one of the categorical predictors has more than two levels. This requires combining what we learned about contrast coding (Chapter 4) with what we have learned about interactions. One nice aspect of interactions among categorical predictors is that we usually don’t need to use procedures like marginal effects to follow up significant interactions, so long as we make good use of contrast coding.\nIn experimental (as opposed to observational) settings, interactions among categorical predictors fall under the much larger topic of ANOVA and experimental design. The analysis we look at in this section is a two-way between-subjects ANOVA, meaning that there are two categorical predictors considered, as well as their interaction, and both predictors are cross-sectional. ANOVA is a big topic and is not the focus of this course. However, we will discuss how to summarize the results of our analysis in an ANOVA table, and consider how this differs from the standard regression approach.\n\n5.6.1 An example from ECLS\nFor this topic we will switch over to the ECLS data and examine how SES and Pre-K attendance interact to predict Math Achievement at the beginning of Kindergarten. The variables we will examine are\n\nMath Achievement at the beginning of K (c1rmscal). This is the number of correct questions on a test with approximately 70 items.\nWhether the child attended Pre-K (p1center). This is a binary variable that indicates pre-K attendance.\nSES, coded as quintiles (wksesq5). We will denote this variable as SES, but keep in mind it is quintiles in this example (e.g., SES = 1 are the respondents with SES between the minimum and the first quintile).\n\nCoding SES as quintiles allows us to consider it as a categorical predictor with 5 levels. This is a widely-used practice, because SES often has non-linear relationships with outcome variables of interest, and these relationships can be more easily captured by treating SES as a categorical variable. This approach to SES is also convenient for our illustration of interactions between categorical predictors.\nIn this analysis, our focus will be whether the “effect” of Pre-K on Math Achievement depends on (i.e., is moderated by) the child’s SES. Please note that I will use the term “effect” in this section to simplify language, but we know that Pre-K attendance was not randomly assigned in ECLS, so please keep in mind that this terminology is not strictly correct.\nThe relationship among the three variables is summarized in the visreg plot below. We can see that the effect of Pre-K on Math Achievement appears to differ as a function of SES – i.e., it appears that there is an interaction between Pre-K and SES. Our goal in this section is to produce an analysis corresponding to the figure.\n\n\nCode\n# \nload(\"ECLS2577.Rdata\")\necls$prek &lt;- factor(2 - ecls$p1center)\necls$wksesq5 &lt;- factor(ecls$wksesq5)\nmod &lt;- lm(c1rmscal ~ prek*wksesq5, data = ecls)\nvisreg::visreg(mod, xvar = \"wksesq5\", by = \"prek\", \n               partial = F, rug = F, overlay = T, \n               strip.names = T, xlab = \"SES\", \n               ylab = \"Math Achievement in K\")\n\n\n\n\n\nFigure 5.8: Math Achievement, Pre-K Attendence, and SES\n\n\n\n\nBefore moving on, please take a moment to write down your interpretation of Figure 5.8, focussing on how it illustrates an interaction between SES and Pre-K. Additionally, please describe how the figure would be different if there was no interaction between Pre-K and SES.\n\n\n5.6.2 The “no-interaction” model\nAs in Section 5.2, we will start with a model that includes only the main effects of SES and Pre-K. Seeing where that model “goes wrong” is a good way of understanding the interaction between the two predictors.\nIn order to represent a model with multiple categorical predictors, it is helpful to change our notation from the usual \\(Y\\) and \\(X\\) to the more informative “variable names” notation:\n\\[\n\\begin{align}\n\\widehat Y = b_0 + b_1 PREK + b_2SES_2 + b_3SES_3 + b_4 SES_4 + b_5 SES_5.\n\\end{align} \\tag{5.10}\\]\nIn this notation, the predictor variables are indicators (binary dummies). The variable \\(PREK\\) is just the indicator for Pre-K attendance, as defined above. The variable \\(SES_j\\) is an indicator for the j-th quintile of SES.\nBoth predictors use reference-group coding, as discussed in Chapter 4. For \\(PREK\\), reference-group coding is implied because it is a binary indicator. For \\(SES\\), reference-group coding is accomplished by omitting the binary dummy for the first quintile (i.e., the first quintile is the reference group).\nWe can interpret the coefficients in this model using the same two-step procedure described in Chapter 4. Since there are many terms in the model, things are going to start getting messy quickly, so brace yourself for some long equations (but simple math!).\nThe main points about the interpretation of this model are as follows.\n\nThe intercept is the predicted value of Math Achievement for students in the first SES quintile who did not attend Pre-K. This corresponds to the blue line in the first column of Figure 5.8.\n\n\\[\\begin{align}\n\\widehat Y(PREK = 0, SES = 1) & = b_0 + b_1 (0) + b_2(0)+ b_3(0) + b_4 (0) + b_5 (0) \\\\  \n& = b_0\n\\end{align}\\]\n\nThe effect of Pre-K attendance for students in the first SES quintile is equal to \\(b_1\\). This corresponds to the difference between the red and blue lines in the first column of Figure 5.8.\n\n\\[\\begin{align}\n\\widehat Y(PREK = 1, SES = 1) & = b_0 + b_1 (1) + b_2(0)+ b_3(0) + b_4 (0) + b_5 (0) \\\\  \n& = b_0 + b_1 \\\\\n\\implies &\n\\end{align}\\]\n\\[\\begin{align}\n\\widehat Y(PREK = 1, SES = 1) - \\widehat Y(PREK = 0, SES = 1) & = b_1\n\\end{align}\\]\n\nBecause the model in Equation 5.10 does not include an interaction, we already know that it implies that the effect of Pre-K is constant over levels of SES. Below we show that effect of Pre-K for SES = 2 is the same as the effect for SES = 1. The same approach can be used to show the effect is constant over all levels of SES. Note that while the model assumes the effect of Pre-K is constant over levels of SES, this is actually inconsistent with what is shown in Figure 5.8. We will improve on this model by adding an interaction in the following section.\n\n\\[\\begin{align}\n\\widehat Y(PREK = 0, SES = 2) & = b_0 + b_1 (0) + b_2(1 )+ b_3(0) + b_4 (0) + b_5 (0)\\\\  \n& = b_0 + b_2 \\\\ \\\\\n\\widehat Y(PREK = 1, SES = 2)& = b_0 + b_1 (1) + b_2(1)+ b_3(0) + b_4 (0) + b_5 (0)\\\\  \n& = b_0 + b_1 + b_2 \\\\ \\\\\n\\implies &\n\\end{align}\\]\n\\[\\begin{align}\n\\widehat Y(PREK = 1, SES = 2) - \\widehat Y(PREK = 0, SES = 2) = b_1  \n\\end{align}\\]\nThis equation says that the difference between the red and blue lines in the second column of Figure 5.8 is the same as the difference in the first column – i.e., they both equal \\(b_1\\). This is what it means for there to be no interaction between two categorical predictors.\nIf you want more practice with this, you can show that Equation 5.10 implies the effect of Pre-K is constant over all levels of SES. Additionally, you can use the 2-step approach to show that the effect of SES is constant over levels of Pre-K attendance.\n\n\n5.6.3 Adding the interaction(s)\nWe have just seen that Equation 5.10 implies that the effect of Pre-K is constant over levels of SES, and vise versa. In order to address our research question about whether the relationship between Pre-K attendance and Math Achievement depends on children’s SES, we will need to add something to the model – an interaction (surprise!).\nWe know that interactions are just products (multiplication) of predictor variables. Since SES is represented as 4 dummies, this means we need 4 products to represent the interaction of Pre-K with SES. The resulting model can be written:\n\\[\\begin{align}\n\\widehat Y  = & b_0 + b_1 PREK + b_2SES_2 + b_3SES_3 + b_4 SES_4 + b_5 SES_5 + \\\\\n&  b_6 (PREK \\times SES_2) + b_7(PREK \\times SES_3) + \\\\\n& b_8 (PREK \\times SES_4) + b_9 (PREK \\times SES_5)\n\\end{align} \\tag{5.11}\\]\nAs you can see, we have a lot of predictors in this model! Although we are only considering two distinct “conceptual” predictors, we have 9 coefficients in our regression model (+ the intercept).\nAgain, there are a few main things to notice:\n\nThe interpretation of the intercept has not changed. It still corresponds to the blue line in the first column of Figure 5.8.\nThe regression coefficient on \\(PREK\\) is still the “effect” of Pre-K for students in the first SES quintile (i.e., the difference between the red and blue line in the first column of Figure 5.8). This is because all the \\(SES_j\\) variables are equal to zero for students in the first SES quintile, and so all of the interaction terms in Equation 5.11 are equal to zero.\nThe effect of Pre-K is no longer constant over levels of SES. Again we will focus on SES = 2, but the same approach works for the other levels of SES.\n\n\\[\\begin{align}\n\\widehat Y(PREK = 0, SES = 2) & = b_0 + b_1 (0) + b_2(1 )+ b_3(0) + b_4 (0) + b_5 (0) + \\\\\n&  b_6 (0 \\times 1) + b_7(0 \\times 0) + b_8 (0 \\times 0) + b_9 (0\\times 0) \\\\  \n& = b_0 + b_2 \\\\ \\\\\n\\widehat Y(PREK = 1, SES = 2) & = b_0 + b_1 (1) + b_2(1)+ b_3(0) + b_4 (0) + b_5 (0) + \\\\\n&  b_6 (1 \\times 1) + b_7(1 \\times 0) + b_8 (1 \\times 0) + b_9 (1\\times 0) \\\\  \n& = b_0 + b_1 + b_2 + b_6 \\\\ \\\\\n\\implies &\n\\end{align}\\]\n\\[\\begin{align}\n\\widehat Y(PREK = 1, SES = 2) - \\widehat Y(PREK = 0, SES = 2) = b_1 + b_6\n\\end{align}\\]\nThe last line shows that the “effect” of Pre-K for students in the second SES quintile is \\(b_1 + b_6\\). This is not the same as the effect for students in the first quintile, which was just \\(b_1\\). In other words, the difference between the red and blue lines in the first column of Figure 5.8 (i.e., \\(b_1\\)) is not equal to the difference in the second column (i.e., \\(b_1 + b_6\\)) unless the interaction is equal to zero (i.e., \\(b_6 = 0\\)).\nThe same approach shows that the effect of Pre-K at each level of SES results in a similar equation:\n\\[\\begin{align}\n\\widehat Y(PREK = 1, SES = 3) - \\widehat Y(PREK = 0, SES = 3) & = b_1 + b_7 \\\\\n\\widehat Y(PREK = 1, SES = 4) - \\widehat Y(PREK = 0, SES = 4) & = b_1 + b_8 \\\\\n\\widehat Y(PREK = 1, SES = 5) - \\widehat Y(PREK = 0, SES = 5) & = b_1 + b_9 \\\\\n\\end{align}\\]\nThis pattern makes it clear that, to isolate the interactions (i.e., \\(b_6\\) through \\(b_9\\)), we need to subtract off \\(b_1\\) – i.e., we need to subtract off the effect of Pre-K for students in the first SES quintile. In anology with reference group coding for single predictor (see Section 4.4), we can think of \\(b_1\\) the “reference effect” or baseline to which the interaction terms are compared.\nFor example\n\nThe interaction between Pre-K and the second SES quintile is the effect Pre-K has on Math Achievement for students in the second SES quintile, as compared to the effect in the first SES quintile.\nThe interaction between Pre-K and the third SES quintile is the effect Pre-K has on Math Achievement for students in the 3rd SES quintile, as compared to the effect in the first SES quintile.\netc. etc.\n\nMathematically, the interaction terms are represented as “differences-in-differences”. For example,\n\\[\\begin{align}\nb_6 & = [\\widehat Y(PREK = 1, SES = 2) - \\widehat Y(PREK = 0, SES = 2)] -  b_1 \\\\\n    & = [\\widehat Y(PREK = 1, SES = 2) - \\widehat Y(PREK = 0, SES = 2)] \\\\\n    & - [\\widehat Y(PREK = 1, SES = 1) - \\widehat Y(PREK = 0, SES = 1)]\n\\end{align}\\]\nThis looks quite complicated but it is just an extension of reference-group coding. This equation is saying that the “reference effect” or “baseline” for interpreting the interaction (\\(b_6\\)) is the effect of Pre-K in the first SES quintile (i.e., \\(b_1\\)). As noted above, all of the interaction terms have the same reference effect.\n\n\n5.6.4 Back to the example\nThat last section was a lot to take in, so let’s put some numbers on the page to check our understanding. The output below shows the summary for a model that regresses Math Achievement on Pre-K, SES, and their interaction. Please write down an interpretation of magnitude, direction, and statistical significance of each regression coefficient in this output (including the intercept), and be prepared to share your answers in class. Remember that wksesq5 is the variable code for the SES quintiles – the digit that follows the variable code indicates the level of variable. It may be helpful to refer to Figure 5.8 when interpreting the coefficients.\n\n\nCode\nmod &lt;- lm(c1rmscal ~ prek*wksesq5, data = ecls)\nsummary(mod)\n\n\n\nCall:\nlm(formula = c1rmscal ~ prek * wksesq5, data = ecls)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.7682  -4.7682  -0.9755   3.9545  31.2318 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     16.0455     0.7355  21.816  &lt; 2e-16 ***\nprek1           -0.3735     0.9601  -0.389  0.69732    \nwksesq52         2.2931     0.9570   2.396  0.01663 *  \nwksesq53         2.9300     0.9127   3.210  0.00134 ** \nwksesq54         4.6310     0.9439   4.906 9.87e-07 ***\nwksesq55         7.2990     1.0343   7.057 2.19e-12 ***\nprek1:wksesq52   1.0638     1.2118   0.878  0.38012    \nprek1:wksesq53   2.1087     1.1544   1.827  0.06785 .  \nprek1:wksesq54   1.6715     1.1684   1.431  0.15267    \nprek1:wksesq55   2.7972     1.2340   2.267  0.02349 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.9 on 2567 degrees of freedom\nMultiple R-squared:  0.1621,    Adjusted R-squared:  0.1592 \nF-statistic: 55.19 on 9 and 2567 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n5.6.5 The ANOVA approach\nThe output in the previous section is detailed enough that it is not usually required to follow-up a significant interaction among categorical predictors using marginal effects. However, the summary output still omits some information we might be interested in. For example, the Pre-K indicator in the above output tells us the effect of Pre-K, but only for children in the first SES quintile. We might also want to know about the overall effect of Pre-K across levels of SES – i.e., is there a significant difference in Math Achievement for students who attended Pre-K, after controlling for their level of SES? Similarly, what is the overall main effect of SES?\nOne way to summarize the main effects of Pre-K and SES, as well as their interaction, by asking how much variance they explain after controlling for the other predictors in the model. This is the ANOVA approach we discussed last semester, but now applied to two categorical predictors.\nThe ANOVA table for our example is below, and it is followed by the R-squared coefficients for each predictor, which are called “eta-squared” (\\(\\eta^2\\)) in the context of ANOVA. These R-squared (eta-squared) coefficients tell us what proportion of the variance in Math Achievement is attributable to the main effects and the interaction.\n\n\nCode\n# ANOVA Table\nanova(mod)\n\n\nAnalysis of Variance Table\n\nResponse: c1rmscal\n               Df Sum Sq Mean Sq  F value Pr(&gt;F)    \nprek            1   3434  3434.3  72.1437 &lt;2e-16 ***\nwksesq5         4  19914  4978.4 104.5805 &lt;2e-16 ***\nprek:wksesq5    4    299    74.7   1.5701 0.1795    \nResiduals    2567 122198    47.6                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# R-squared (eta-squared)\n#install.packages(\"effectsize\")\neffectsize::eta_squared(mod, partial = F)\n\n\n# Effect Size for ANOVA (Type I)\n\nParameter    |     Eta2 |       95% CI\n--------------------------------------\nprek         |     0.02 | [0.01, 1.00]\nwksesq5      |     0.14 | [0.12, 1.00]\nprek:wksesq5 | 2.05e-03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nPlease write down your interpretation of the ANOVA table and R-squared (eta-squared) coefficients and be prepared to share you thoughts in class. Note that the ANOVA output leads to different conclusions than the regression output above. We will discuss the discrepancies between the ANOVA and regression output in class.\nLet’s end this discussion of ANOVA with two qualifications.\nFirst, I should clarify that the term “main effect” has a somewhat different meaning in ANOVA as compared to regression. In the regression examples, we talked about the effect of a predictor, conditional on the other predictor being zero. In ANOVA stuff above, we instead talked about the average or overall effect of a predictor, while holding the other predictor constant. These two interpretations are related but not the same, and in the ANOVA literature, “main effect” usually means the average or overall effect.\nSecond, some people claim that it is bad practice to interpret main effects qua average effects in the presence of an interaction. The basic argument is that we shouldn’t report the average effect when the “real message” of the interaction is that the effect changes as a function of the other predictor. I think that main effects and interactions aren’t really incompatible concepts, especially if we are talking about conditional main effects rather than average main effects. But, you should be aware that this topic is debated and you are free to make up your own mind (as always!)."
  },
  {
    "objectID": "ch5_interactions.html#workbook",
    "href": "ch5_interactions.html#workbook",
    "title": "5  Interactions",
    "section": "5.7 Workbook",
    "text": "5.7 Workbook\nThis section collects the questions asked in this chapter. The lessons for this chapter will focus on discussing these questions and then working on the exercises in Section 5.8. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 5.1\n\nWhat does the following plot is telling us about the relationships among the three variables. In particular:\n\nDoes the gender gap in Math Achievement change as a function of Reading Achievement?\nIs the relationship between Math Achievement and Reading Achievement the the same for males and females?\nWhat do the results mean for gender equality in Math and STEM education?\n\n\n\n\nCode\nmod1 &lt;- lm(achmat12[females] ~ achrdg12[females])\nmod2 &lt;- lm(achmat12[males] ~ achrdg12[males])\n\n# Plot reading and math for females\nplot(achrdg12[females], \n     achmat12[females], \n     xlab = \"Reading\", \n     ylab = \"Math\")\n\nabline(mod1, lwd = 2)\n\n# Add points and line for males\npoints(achrdg12[males], \n       achmat12[males], \n       col = \"#4B9CD3\", pch = 2)\n\nabline(mod2, col = \"#4B9CD3\", lwd = 2)\n\n# Add a legend\nlegend(x = \"topleft\", \n       legend = levels(gender),\n       pch = c(1, 2), \n       col = c(1, \"#4B9CD3\"))\n\n\n\n\n\n\n\n\n\nSection 5.2\n\nNo interaction model: The regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the simple trends and the gender gap in Math Achievement for the NELS example. Don’t worry about statistical significance, just focus on the meaning of the coefficients.\n\n\n\nCode\nmod3 &lt;- lm(achmat12 ~ achrdg12 + gender, data = NELS)\nsummary(mod3)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 + gender, data = NELS)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.2448  -3.6075   0.3968   3.9836  15.5606 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 19.98122    1.86278  10.727  &lt; 2e-16 ***\nachrdg12     0.63551    0.03275  19.404  &lt; 2e-16 ***\ngenderMale   3.50166    0.52473   6.673 6.69e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.839 on 497 degrees of freedom\nMultiple R-squared:  0.4538,    Adjusted R-squared:  0.4516 \nF-statistic: 206.4 on 2 and 497 DF,  p-value: &lt; 2.2e-16\n\n\nHere is an example to help you get started:\n\nSimply for females, the regression of Math Achievement on Reading Achievement has an intercept equal to 19.98 and a slope equal to 0.64. The intercept tells us that a female student with Reading Achievement score of 0% is expected to have a Math Achievement score of 19.98% (the units of the two tests are percent correct). Since the lowest value of Reading Achievement in our example is about 35%, the intercept is not very meaningful for these data. The regression slope tells us that, for females, a 1 unit increase in Reading Achievement is associated with a .64 unit increase in Math Achievement.\nSimply for males, ….\nThe gender gap in Math Achievement was equal to …\n\nSection 5.3\n\nInteraction model: The regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the interaction between Gender and Reading. Don’t worry about statistical significance, just focus on the meaning of the coefficients.\n\n\n\nCode\n# hard code the interaction term\ngenderXachrdg12 &lt;- (as.numeric(gender) - 1) * achrdg12\n\n# Rund the model with the interaction included\nmod4 &lt;- lm(achmat12 ~ achrdg12 + gender + genderXachrdg12)\nsummary(mod4)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 + gender + genderXachrdg12)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     14.80310    2.64922   5.588  3.8e-08 ***\nachrdg12         0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale      13.39328    3.65828   3.661 0.000278 ***\ngenderXachrdg12 -0.17794    0.06514  -2.732 0.006524 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\n\nInteraction model using the centered continuous predictor: Please write down your interpretation of the intercept and the regression coefficient for Gender in the regression output below.\n\n\n\nCode\n# compute the deviation scores for reading\nreading_dev &lt;- achrdg12 - mean(achrdg12, na.rm = T) \n\n# Run the interaction model as above\ngenderXreading_dev &lt;- (as.numeric(gender) - 1) * reading_dev\n\nmod5 &lt;- lm(achmat12 ~ reading_dev + gender + genderXreading_dev)\nsummary(mod5)\n\n\n\nCall:\nlm(formula = achmat12 ~ reading_dev + gender + genderXreading_dev)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        55.29439    0.35127 157.411  &lt; 2e-16 ***\nreading_dev         0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale          3.49930    0.52135   6.712 5.26e-11 ***\ngenderXreading_dev -0.17794    0.06514  -2.732  0.00652 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nSection 5.4\n\nThe output shows the gender gap in Math Achievement for 5 values of Reading Achievement. The values of Reading Achievement are its 5 quartiles. You can think of the output as a tabular summary of Figure 5.5. Please use the output to make a conclusion about the levels of Reading Achievement for which the gender gap was significant. Please be prepared to share your answer in class\n\n\n\nCode\n# Install the emmeans package if you haven't already done so\n# install.packages(\"emmeans\")\n\n# Load the package into memory\nlibrary(emmeans)\n\n# Fit the model using R's formula syntax for interaction '*'\nmod6 &lt;- lm(achmat12 ~ gender*achrdg12, data = NELS)\n\n# Use the emmeans function to get the gender means on math, broken down by reading\ngap &lt;- emmeans(mod6, \n               specs = \"gender\",\n               by = \"achrdg12\", \n               cov.reduce = quantile)\n\n# Test whether the differences are significant\ncontrast(gap, method = \"pairwise\")\n\n\nachrdg12 = 31.8:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -7.74 1.637 496  -4.728  &lt;.0001\n\nachrdg12 = 51.3:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -4.27 0.593 496  -7.207  &lt;.0001\n\nachrdg12 = 57.0:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -3.25 0.529 496  -6.138  &lt;.0001\n\nachrdg12 = 61.7:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -2.41 0.658 496  -3.659  0.0003\n\nachrdg12 = 68.1:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -1.28 0.967 496  -1.321  0.1872\n\n\n\nThe test of the slopes of the simple trends for the example are reported below. As previously stated, these aren’t super interesting in the context of our example, but you should check your understanding of simple trends by writing down an interpretation of the output below.\n\n\n\nCode\n# The regression coefficients on reading, broken down by gender\nsimple_slopes &lt;- emtrends(mod6, var = \"achrdg12\", specs = \"gender\")\ntest(simple_slopes)\n\n\n gender achrdg12.trend     SE  df t.ratio p.value\n Female          0.728 0.0470 496  15.487  &lt;.0001\n Male            0.550 0.0451 496  12.208  &lt;.0001\n\n\nSection 5.5\n\nPlease provide an interpretation of all four regression coefficients in the centered model. Your interpretations should make reference to the situation where one or both predictors are equal to zero (see Equation 5.9 above) and should also mentioned the interpretation of the value of zero for the centered variables.\n\n\n\nCode\n# With centering\nmod8 &lt;- lm(achmat12 ~ achrdg12_dev*ses_dev)\nsummary(mod8)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12_dev * ses_dev)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1336  -3.8944   0.7278   4.1301  15.0153 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          56.826225   0.286906 198.065   &lt;2e-16 ***\nachrdg12_dev          0.590313   0.036103  16.351   &lt;2e-16 ***\nses_dev               0.137305   0.041485   3.310    0.001 ** \nachrdg12_dev:ses_dev  0.004270   0.005196   0.822    0.412    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.031 on 496 degrees of freedom\nMultiple R-squared:  0.4184,    Adjusted R-squared:  0.4149 \nF-statistic: 118.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nSection 5.6\n\nPlease take a moment to write down your interpretation of the figure below, focussing on how it illustrates an interaction between SES and Pre-K. Additionally, please describe how the figure would be different if there was no interaction between Pre-K and SES.\n\n\n\nCode\n# \nload(\"ECLS2577.Rdata\")\necls$prek &lt;- factor(2 - ecls$p1center)\necls$wksesq5 &lt;- factor(ecls$wksesq5)\nmod &lt;- lm(c1rmscal ~ prek*wksesq5, data = ecls)\nvisreg::visreg(mod, xvar = \"wksesq5\", by = \"prek\", \n               partial = F, rug = F, overlay = T, \n               strip.names = T, xlab = \"SES\", \n               ylab = \"Math Achievement in K\")\n\n\n\n\n\n\n\n\n\n\nThe output below shows the summary for a model that regresses Math Achievement on Pre-K, SES, and their interaction. Please write down an interpretation of magnitude, direction, and statistical significance of each regression coefficient in this output (including the intercept), and be prepared to share your answers in class. Remember that wksesq5 is the variable code for the SES quintiles – the digit that follows the variable code indicates the level of variable. It may be helpful to refer to the previous figure in your interpretations.\n\n\n\nCode\nmod &lt;- lm(c1rmscal ~ prek*wksesq5, data = ecls)\nsummary(mod)\n\n\n\nCall:\nlm(formula = c1rmscal ~ prek * wksesq5, data = ecls)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.7682  -4.7682  -0.9755   3.9545  31.2318 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     16.0455     0.7355  21.816  &lt; 2e-16 ***\nprek1           -0.3735     0.9601  -0.389  0.69732    \nwksesq52         2.2931     0.9570   2.396  0.01663 *  \nwksesq53         2.9300     0.9127   3.210  0.00134 ** \nwksesq54         4.6310     0.9439   4.906 9.87e-07 ***\nwksesq55         7.2990     1.0343   7.057 2.19e-12 ***\nprek1:wksesq52   1.0638     1.2118   0.878  0.38012    \nprek1:wksesq53   2.1087     1.1544   1.827  0.06785 .  \nprek1:wksesq54   1.6715     1.1684   1.431  0.15267    \nprek1:wksesq55   2.7972     1.2340   2.267  0.02349 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.9 on 2567 degrees of freedom\nMultiple R-squared:  0.1621,    Adjusted R-squared:  0.1592 \nF-statistic: 55.19 on 9 and 2567 DF,  p-value: &lt; 2.2e-16\n\n\n\nPlease write down your interpretation of the ANOVA table and R-squared (eta-squared) coefficients below and be prepared to share you thoughts in class. Note that the ANOVA output leads to different conclusions than the regression output above. We will discuss the discrepancies between the ANOVA and regression output in class.\n\n\n\nCode\n# ANOVA Table\nanova(mod)\n\n\nAnalysis of Variance Table\n\nResponse: c1rmscal\n               Df Sum Sq Mean Sq  F value Pr(&gt;F)    \nprek            1   3434  3434.3  72.1437 &lt;2e-16 ***\nwksesq5         4  19914  4978.4 104.5805 &lt;2e-16 ***\nprek:wksesq5    4    299    74.7   1.5701 0.1795    \nResiduals    2567 122198    47.6                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# R-squared (eta-squared)\n#install.packages(\"effectsize\")\neffectsize::eta_squared(mod, partial = F)\n\n\n# Effect Size for ANOVA (Type I)\n\nParameter    |     Eta2 |       95% CI\n--------------------------------------\nprek         |     0.02 | [0.01, 1.00]\nwksesq5      |     0.14 | [0.12, 1.00]\nprek:wksesq5 | 2.05e-03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00]."
  },
  {
    "objectID": "ch5_interactions.html#sec-exercises-5",
    "href": "ch5_interactions.html#sec-exercises-5",
    "title": "5  Interactions",
    "section": "5.8 Exercises",
    "text": "5.8 Exercises\nThese exercises provide an overview of how to compute interactions using the lm function, how to center continuous predictors, and how to follow-up significant interactions with the emmeans package. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)\nBefore staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\n\n5.8.1 Binary + continuous + interaction\nThere are multiple ways of implementing interactions in R.\n\nWe can “hard code” new variables into our data (e.g., the product of a binary gender variable and reading)\nWe can use R’s formula notation for single term interactions (:)\nWe can use R’s formula notation for factorial interactions (*)\n\nThe following code illustrates the three approaches and shows that they all producing the same output. In general, the * syntax is the easiest to use, so we will stick with that one going forward. The variables used in the example are from the NELS data:\n\nachmat12 is Mat Achievement (percent correct on a math test) in grade 12.\nachrdg12 is Reading Achievement (percent correct on a reading test) in grade 12.\ngender is dichotomous encoding of gender with values Male and Female (it is not a binary variable, but a factor, as discussed in Section 4.9.\n\n\n\nCode\n# Interaction via hard coding\ngenderXreading &lt;- (as.numeric(gender) - 1) * achrdg12\nmod1 &lt;- lm(achmat12 ~ achrdg12 + gender + genderXreading)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 + gender + genderXreading)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    14.80310    2.64922   5.588  3.8e-08 ***\nachrdg12        0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale     13.39328    3.65828   3.661 0.000278 ***\ngenderXreading -0.17794    0.06514  -2.732 0.006524 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Interaction via `:` operator\nmod2 &lt;- lm(achmat12 ~ achrdg12 + gender + achrdg12:gender)\nsummary(mod2)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 + gender + achrdg12:gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         14.80310    2.64922   5.588  3.8e-08 ***\nachrdg12             0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale          13.39328    3.65828   3.661 0.000278 ***\nachrdg12:genderMale -0.17794    0.06514  -2.732 0.006524 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Interaction via `*` operator\nmod3 &lt;- lm(achmat12 ~ achrdg12*gender)\nsummary(mod3)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 * gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         14.80310    2.64922   5.588  3.8e-08 ***\nachrdg12             0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale          13.39328    3.65828   3.661 0.000278 ***\nachrdg12:genderMale -0.17794    0.06514  -2.732 0.006524 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nBefore moving on, check your interpretation of the coefficients in the models. In particular, what does the regression coefficient on the interaction term mean?\n\n\n5.8.2 Centering continuous predictors\nAs noted in Section 5.3, the regression coefficient on Gender is not very interpretable when there is an interaction in the model. In the above output, the coefficient on gender tells us the gender gap in Math Achievement when achrdg12 = 0. We can fix this issue by re-scaling achrdg12 so that zero has a meaningful value. One widely used approach is to center achrdg12 at its mean. When a variable is centered at its mean it is called a deviation score.\nLet’s see what happens to our regression output when we use deviation scores for achrdg12 instead of the “raw” score\n\n\nCode\n# Re-run the model with reading centered at its mean\nachrdg12_dev &lt;- achrdg12 - mean(achrdg12)\nmod4 &lt;- lm(achmat12 ~ achrdg12_dev*gender)\nsummary(mod4)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12_dev * gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             55.29439    0.35127 157.411  &lt; 2e-16 ***\nachrdg12_dev             0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale               3.49930    0.52135   6.712 5.26e-11 ***\nachrdg12_dev:genderMale -0.17794    0.06514  -2.732  0.00652 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nNote that the intercept and the regression coefficient on Gender have changed values compared to mod3. What is the interpretation of these coefficients in the new model?\n\n\n5.8.3 Breaking down a significant interaction\nNext, let’s plot our model with the interaction term. One advantage of having everything in a single model is that we can level-up our plotting! The following code uses the visreg package. Note that the error bands in the plot are produced using the standard errors from emmeans, which is discussed below. If you want to know more about how visreg works, type help(visreg).\n\n\nCode\n# Install the package if you haven't already done so\n# install.packages(\"visreg\")\n\n# Load the package into memory\nlibrary(visreg)\n\nvisreg(mod3, xvar = \"achrdg12\", by = \"gender\", overlay = TRUE)\n\n\n\n\n\nIf the interaction is significant, then we usually want to report a bit more information about how the focal relationship changes as a function of the moderators. There are two main ways to do this:\n\nMarginal effects (aka marginal means, least squares means, adjusted means): This approach is used when the focal predictor is categorical and we want to compare means across the categories, conditional on levels of the moderator.\nSimple trends (aka simple slopes): This approach is used when the focal predictor is continuous and we want to examine the slopes of the simple trends, conditional on the moderator.\n\nUsually, the researcher will chose one or the other approach, whichever is best suited to address the research questions of interest. Our example was motivated by consideration of the gender gap in STEM (i.e., the relationship between a STEM and a categorical predictor), so the marginal effects approach is better suited. We will also illustrate simple trends, just to show how that approach works.\n\n\n5.8.4 Marginal effects\nLet’s breakdown the interaction by asking how the relationship between Math and Gender (i.e., the gender achievement gap in Math) changes as a function of Reading. This can be done using emmeans package, and the main function in that pacakge is also called emmeans.\nThe three main arguments for the emmeans function:\n\nobject – the output of lm. This is the first argument\nspecs – which factors in the model we want the means of (i.e., the focal predictor)\nby – which predictor(s) we want to use to breakdown the means (i.e., the moderator(s))\n\nWe can use emmeans to compute the marginal effect at the mean (MEM) as follows:\n\n\nCode\n# Install the package if you haven't already done so\n  # install.packages(\"emmeans\")\n\n# Load the package into memory\nlibrary(emmeans)\n\n# Use the emmeans function to get the gender means on math, broken down by reading\ngap &lt;- emmeans(mod3, specs = \"gender\", by = \"achrdg12\")\nsummary(gap)\n\n\nachrdg12 = 55.6:\n gender emmean    SE  df lower.CL upper.CL\n Female   55.3 0.351 496     54.6     56.0\n Male     58.8 0.385 496     58.0     59.6\n\nConfidence level used: 0.95 \n\n\nCode\n# Test whether the difference is significant\ncontrast(gap, method = \"pairwise\")\n\n\nachrdg12 = 55.6:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male     -3.5 0.521 496  -6.712  &lt;.0001\n\n\nIn the above output, we only get one Gender difference in Math, and that is computed for the value of achrdg12 = 55.6, which is the mean value of Reading. As noted, this is called the marginal effect at the mean (MEM).\nIt is often more helpful to report Gender difference for multiple different values of achrdg12, which is called MERV (marginal effects at representative values). While there are many ways to chose the representative values, one convenient approach approach is to use the quartiles of achrdg12. This is accomplished using the cov.reduce argument of emmeans as follows.\n\n\nCode\n# Use the the covarate reduce option of emmeans with the quantile function\ngap_quartiles &lt;- emmeans(mod3, specs = \"gender\", by = \"achrdg12\", cov.reduce = quantile)\nsummary(gap_quartiles)\n\n\nachrdg12 = 31.8:\n gender emmean    SE  df lower.CL upper.CL\n Female   37.9 1.186 496     35.6     40.3\n Male     45.7 1.129 496     43.5     47.9\n\nachrdg12 = 51.3:\n gender emmean    SE  df lower.CL upper.CL\n Female   52.1 0.412 496     51.3     52.9\n Male     56.4 0.426 496     55.6     57.2\n\nachrdg12 = 57.0:\n gender emmean    SE  df lower.CL upper.CL\n Female   56.3 0.355 496     55.6     57.0\n Male     59.6 0.393 496     58.8     60.3\n\nachrdg12 = 61.7:\n gender emmean    SE  df lower.CL upper.CL\n Female   59.8 0.447 496     58.9     60.6\n Male     62.2 0.482 496     61.2     63.1\n\nachrdg12 = 68.1:\n gender emmean    SE  df lower.CL upper.CL\n Female   64.4 0.674 496     63.1     65.7\n Male     65.7 0.693 496     64.3     67.0\n\nConfidence level used: 0.95 \n\n\nCode\n# Test whether the gender difference in math achievement is significant at each quartile of reading achievement\ncontrast(gap_quartiles, method = \"pairwise\")\n\n\nachrdg12 = 31.8:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -7.74 1.637 496  -4.728  &lt;.0001\n\nachrdg12 = 51.3:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -4.27 0.593 496  -7.207  &lt;.0001\n\nachrdg12 = 57.0:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -3.25 0.529 496  -6.138  &lt;.0001\n\nachrdg12 = 61.7:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -2.41 0.658 496  -3.659  0.0003\n\nachrdg12 = 68.1:\n contrast      estimate    SE  df t.ratio p.value\n Female - Male    -1.28 0.967 496  -1.321  0.1872\n\n\nAt this point, you should be able to summarize your conclusions about the gender gap in Math and how it depends on Reading.\n\n\n5.8.5 Simple trends\nNext we will show how to use emtrends to test the conditional or “simple” slopes of Math on Reading, given Gender. As mentioned, this approach is not very well suited to the example, but we are going through it here just to illustrate how to do this type of analysis.\nThe three main arguments for emtrends are\n\nobject – the output of lm. This is the first argument\nvar – which continuous predictor in the model we want the slopes of\nspecs – which factor predictor(s) in the model to break the trend down by\n\nLet’s see how it works.\n\n\nCode\n# Use the emtrends function to get the regression coefficients on reading, broken down by gender\nsimple_slopes &lt;- emtrends(mod3, var = \"achrdg12\", specs = \"gender\")\nsummary(simple_slopes)\n\n\n gender achrdg12.trend     SE  df lower.CL upper.CL\n Female          0.728 0.0470 496    0.636    0.821\n Male            0.550 0.0451 496    0.462    0.639\n\nConfidence level used: 0.95 \n\n\nCode\ntest(simple_slopes)\n\n\n gender achrdg12.trend     SE  df t.ratio p.value\n Female          0.728 0.0470 496  15.487  &lt;.0001\n Male            0.550 0.0451 496  12.208  &lt;.0001\n\n\nThe foregoing analysis tells us how the relationship between reading and math changes as a function of gender, and, in particular, whether the simple slopes are significant for males and females. Recall that the simple slope for females (the group coded zero) is just the regression coefficient on reading in the original lm output. So, the only new thing this output gives us is the simple slope for males.\n\n\n5.8.6 Two continuous predictors\nInteractions with continuous predictors are basically the same as for continuous and categorical. One main issue is that we should always center the predictors, not only to facilitate interpretation of the regression coefficients, but also to reduce the correlation between the main effects and the interaction.\nFor an example, let’s replace gender with SES from our previous analysis. Apologies that this new example is mainly for convenience and doesn’t represent a great research question about, e.g., about why the relationships between math and reading might change as a function of SES!\nHere we will focus on how centering affects the results of a regression with interactions among continuous predictors.\n\n\nCode\n# Without centering\nmod5 &lt;- lm(achmat12 ~ achrdg12*ses)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 + gender + genderXreading)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    14.80310    2.64922   5.588  3.8e-08 ***\nachrdg12        0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale     13.39328    3.65828   3.661 0.000278 ***\ngenderXreading -0.17794    0.06514  -2.732 0.006524 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# With centering\nachrdg12_dev &lt;- achrdg12 - mean(achrdg12)\nses_dev &lt;- ses - mean(ses)\nmod6 &lt;- lm(achmat12 ~ achrdg12_dev*ses_dev)\nsummary(mod2)\n\n\n\nCall:\nlm(formula = achmat12 ~ achrdg12 + gender + achrdg12:gender)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.0582  -3.7864   0.5014   4.0775  16.2889 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         14.80310    2.64922   5.588  3.8e-08 ***\nachrdg12             0.72824    0.04702  15.487  &lt; 2e-16 ***\ngenderMale          13.39328    3.65828   3.661 0.000278 ***\nachrdg12:genderMale -0.17794    0.06514  -2.732 0.006524 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.801 on 496 degrees of freedom\nMultiple R-squared:  0.4619,    Adjusted R-squared:  0.4586 \nF-statistic: 141.9 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see that, while both models account for the same overall variation in math, SES is significant in the centered model. This has to do both with changing the interpretation of the coefficient (it now represents the relationship between math and reading for students with average reading) and because it is no longer so highly redundant with the interaction term.\nAlthough the interaction with SES was not significant in either model, let’s break down the interaction with emtrends just to see how it works. This time we will use the at option rather than the ’cov.reduceoption to break down the interaction. The values 9, 19, and 28 are the 10th, 50th, and 90th percentile of SES, which is the same approachvisreguses (You can overwrite the defaults using thebreaksargument -- seehelp(visreg)`).\n\n\nCode\n# Break down interaction with SES as moderator\nsimple_slopes &lt;-emtrends(mod5, var = \"achrdg12\", specs = \"ses\", at = list(ses = c(9, 19, 28)))\nsummary(simple_slopes)\n\n\n ses achrdg12.trend     SE  df lower.CL upper.CL\n   9          0.550 0.0583 496    0.435    0.665\n  19          0.593 0.0365 496    0.521    0.664\n  28          0.631 0.0639 496    0.506    0.757\n\nConfidence level used: 0.95 \n\n\nFinally let’s summarize our (non significant) interaction with a nice plot.\n\n\nCode\n# Note that band = F removes the confidence intervals\nvisreg(mod5, xvar = \"achrdg12\", by = \"ses\", overlay = TRUE, band = F)\n\n\n\n\n\n\n\n5.8.7 Two categorical predictors\nFor this topic we will switch over to the ECLS data and examine how SES and Pre-K attendance interact to predict Math Achievement at the beginning of Kindergarten. The variables we will examine are\n\nMath Achievement at the beginning of K (c1rmscal). This is the number of correct questions on a test with approximately 70 items.\nWhether the child attended Pre-K (p1center). This is a binary variable that indicates pre-K attendance.\nSES, coded as quintiles (wksesq5). We will denote this variable as SES, but keep in mind it is quintiles in this example (e.g., SES = 1 are the respondents with SES between the minimum and the first quintile).\n\nThe regression model is as follows. Note that both variables need to be converted to factors in R, so that R will treat them as categorical variables. Also recall that in R the default contrast coding for categorical predictors is reference-group coding.\n\n\nCode\nload(\"ECLS2577.Rdata\")\necls$prek &lt;- factor(2 - ecls$p1center)\necls$wksesq5 &lt;- factor(ecls$wksesq5)\nmod &lt;- lm(c1rmscal ~ prek*wksesq5, data = ecls)\nsummary(mod)\n\n\n\nCall:\nlm(formula = c1rmscal ~ prek * wksesq5, data = ecls)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.7682  -4.7682  -0.9755   3.9545  31.2318 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     16.0455     0.7355  21.816  &lt; 2e-16 ***\nprek1           -0.3735     0.9601  -0.389  0.69732    \nwksesq52         2.2931     0.9570   2.396  0.01663 *  \nwksesq53         2.9300     0.9127   3.210  0.00134 ** \nwksesq54         4.6310     0.9439   4.906 9.87e-07 ***\nwksesq55         7.2990     1.0343   7.057 2.19e-12 ***\nprek1:wksesq52   1.0638     1.2118   0.878  0.38012    \nprek1:wksesq53   2.1087     1.1544   1.827  0.06785 .  \nprek1:wksesq54   1.6715     1.1684   1.431  0.15267    \nprek1:wksesq55   2.7972     1.2340   2.267  0.02349 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.9 on 2567 degrees of freedom\nMultiple R-squared:  0.1621,    Adjusted R-squared:  0.1592 \nF-statistic: 55.19 on 9 and 2567 DF,  p-value: &lt; 2.2e-16\n\n\nTo facilitate interpretation of the ouput, you can refer to the plot below. Each regression coefficient in the output corresponds to a feature of this plot.\n\n\nCode\nvisreg::visreg(mod, xvar = \"wksesq5\", by = \"prek\", \n               partial = F, rug = F, overlay = T, \n               strip.names = T, xlab = \"SES\", \n               ylab = \"Math Achievement in K\")\n\n\n\n\n\nIn order to summarize the model as an ANOVA table, we can use the following code. Note that the ANOVA output tests the variance explained (i.e., R-squared) of the original variables, and does not include dummy variables.\n\n\nCode\nanova(mod)\n\n\nAnalysis of Variance Table\n\nResponse: c1rmscal\n               Df Sum Sq Mean Sq  F value Pr(&gt;F)    \nprek            1   3434  3434.3  72.1437 &lt;2e-16 ***\nwksesq5         4  19914  4978.4 104.5805 &lt;2e-16 ***\nprek:wksesq5    4    299    74.7   1.5701 0.1795    \nResiduals    2567 122198    47.6                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn an ANOVA context, the R-squared statistics are called eta-squared. They are reported below:\n\n\nCode\neffectsize::eta_squared(mod, partial = F)\n\n\n# Effect Size for ANOVA (Type I)\n\nParameter    |     Eta2 |       95% CI\n--------------------------------------\nprek         |     0.02 | [0.01, 1.00]\nwksesq5      |     0.14 | [0.12, 1.00]\nprek:wksesq5 | 2.05e-03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00]."
  },
  {
    "objectID": "ch6_model_building.html#sec-hierarchical-models-6",
    "href": "ch6_model_building.html#sec-hierarchical-models-6",
    "title": "6  Model building",
    "section": "6.1 Hierarhical models",
    "text": "6.1 Hierarhical models\nThe key ideas of hierarchical regression are:\n\nPartition the predictors into conceptual “blocks”. Each block contains at least one predictor, but they usually contain more than one (e.g., “control variables”, “home factors”). No predictor can be included in more than one block.\nBuild a series of regression models in which the blocks are entered into the model sequentially. For this to be hierarchical model building, it is required that each new model in the series includes the blocks from all previous models. So, with three blocks we would have:\n\n\\[\n\\begin{align}\n& \\text{Model 1: Block 1} \\\\\n& \\text{Model 2: Block 1 + Block 2} \\\\\n& \\text{Model 3: Block 1 + Block 2 + Block 3} \\\\\n\\end{align}\n\\]\n\nCompare the R-squared value of each model to that of the previous model. This allows us to determine whether each new block “adds anything” to the model. To see how this works, denote the R-squared values of our three models as \\(R^2_1, R^2_2, R^2_3\\), respectively. Then we proceed as follows:\n\nCompute \\(R^2_1\\) to evaluate the contribution of Block 1.\nCompute \\(\\Delta R^2_{21} = R^2_2 - R^2_1\\) to evaluate contribution of Block 2, controlling for Block 1\nCompute \\(\\Delta R^2_{32} = R^2_3 - R^2_2\\) to evaluate contribution of Block 3, controlling for Block 1 & 2.\n\n\nThe symbol \\(\\Delta R^2\\) is pronounced “delta R-squared.” It always has two subscripts, which denote the two models being compared.\nThe main advantage of hierarchical modeling is that we partition the total variance explained by all of the predictors into parts contributed uniquely by each block. In particular, it follows from the definition of delta R-squared that:\n\\[ R^2_3 = R^2_1 + \\Delta R^2_{21} + \\Delta R^2_{32}. \\] This means that the variation explained by all of the predictors together (\\(R^2_3\\)) can be separated into the 3 distinct components that are isolated by using hierarchical approach.\nSometimes we let \\(R^2_0\\) denote a model that contains no predictors (only the intercept). Then \\(R^2_1\\) can be equilvanently written as \\(\\Delta R^2_{10}\\). This makes the notation of the previous equation a bit more consistent:\n\\[ R^2_3 = \\Delta R^2_{10} + \\Delta R^2_{21} + \\Delta R^2_{32}.\\]\nThe above equation shows us that hierarchical modeling building is an approach to partitioning the total variance explained by a set of predictors (here denoted \\(R^2_3\\)) into the variance uniquely attributable to different blocks of predictors (the \\(\\Delta R^2\\)s). While there are many other approaches to model building, this equation is what uniquely defines hierarchical model building.\n\n6.1.1 Example of blocks\nWe have already seen some examples of blocks of predictors in the previous chapters. This section highlights two examples.\n\nCategorical predictors with \\(C\\) categories are represented as blocks of \\(C-1\\) dummy variables. The dummies are conceptually equivalent to a single predictor.\nInteractions are blocks of predictors. Often interactions are entered into a regression model on separate step, after the “main effects” have been entered. Below is an excerpt from a study that illustrates this approac. The study is about equitable access to gifted-and-talented (GT) programs in New York City. In order to apply for GT programs, students need to take a test in Kindergarten. The test is optional and not everyone takes it. The study is mainly interested in whether attending Pre-K increases the probability that a child will take the test, especially for students who have been traditionally underrepresented in GT programs. (cite: Lu & Wienberg (2016). Public Pre-K and Test Taking for the NYC Gifted-and-Talented Programs: Forging a Path to Equity. Educational Researcher, 45, 36-47):\n\n\n\n\n\n\n\n\n\n\nMore generally, blocks are any subset of predictors about which you want to ask a research question. It can be just one variable, or it can be a collection of conceptually related variables.\nPlease take a moment to write down one or more examples of a block of variables from your area of research, and I will invite you to share your examples in class.\n\n\n6.1.2 Nested models\nWhatever your blocks are, hiearhical modeling requires they are used to make a sequence of nested models. The idea behind nested models is depicted in Figure 6.1. In the figure, Model 2 contains \\(K\\) total predictors, and Model 1 contains a subset of those predictors. This relationship is shown by letting the area representing Model 1 be contained by the area representing Model 2. We can think of this like Matryoshka nesting dolls (https://en.wikipedia.org/wiki/Matryoshka_doll). Model 1 is contained within Model 2, in the sense that all of the predictors in Model 1 are also in Model 2.\n\n\n\n\n\nFigure 6.1: Nested Models\n\n\n\n\nA bit more technically: One regression model is nested within another if the former can be obtained by removing some predictors from the latter. Other than the difference in the predictors, it is assumed that the models equivalent (same outcome variable, sample samples, etc.).\nThe difference between nested and non-nested models is illustrated in Figure 6.2. The partially overlapping ovals are intended to indicate that the models share some predictors, but each model also has some unique predictors.\n\n\n\n\n\nFigure 6.2: Nested vs Non-Nested Models\n\n\n\n\nAs noted in the figure, a hierarchical model is a series of nested regression models. You might be wondering why it is so important for the models to be nested. Part of this is conceptual – the hierarchical approach allows us to partition the total variance explained into parts attributable to different subsets (blocks) of predictors (see Section 6.1). Nesting is also required to compute statistical tests for the \\(\\Delta R^2\\) statistics (more on this in Section 6.1.4).\nIn short, we can statistically compare Model 1 and Model 2 in the left hand panel of Figure 6.2, but not in the right hand panel. (Side note: In the right hand panel, we could compare Model 1 vs Model 3 and Model 2 vs Model 3, but this would not be a hierarchical model because the \\(\\Delta R^2\\)s wouldn’t add up to the \\(R^2\\) of Model 3.)\n\n\n6.1.3 Some practical advice\nSo far we have discussed hierarchical modeling conceptually, but have not really addressed how to use it in practice. The following advice is paraphrased from the late Jacob Cohen (cite: Cohen, Cohen, West, & Aitken (2003). Applied Multiple Regression Analysis. Mahwah, NJ: Lawrence Earlbaum. Section 5.7.3).\n\nThe hierarchical approach provides an effective strategy of inference if variables are entered into the model according to their relevance to the study.\nThe variables that are most central to the study should be entered first, those that are secondarily of interest should be entered second, and those that fall into the category, “I wonder if” or “just in case” should be entered last.\nThis principle may be succinctly stated as “least is last.” That is, “when research factors can be ordered as to their centrality, those of least relevance are appraised last in the hierarchy, and their results taken as indicative rather than conclusive”\n\nFigure 6.3 maps Cohen’s strategy onto our diagram for nested models. One addendum to this approach is that “control variables” are usually entered first. These are variables that have been established as important in past research, but are not of central interest to the present study. The rationale for entering them first is that new studies should add something beyond what has been established by past research.\n\n\n\n\n\nFigure 6.3: Cohen’s ‘Least is Last’ Principle\n\n\n\n\n\n\n6.1.4 Delta R-squared\nWe have already seen how to compute \\(\\Delta R^2\\) – by subtracting the R-squared value of a nested model from that of a nesting model. This is a very widely used way of comparing nested regression models.\nWhen adding predictors into a model one at a time (i.e., blocks with only a single predictor), testing \\(\\Delta R^2\\) is equivalent to testing the b-weight of the added predictor. But when adding multiple predictors into a model, \\(\\Delta R^2\\) provides important information that is not captured by any of the statistics we have considered so far. It is technically called a multiple semi-partial correlation, but “delta R-squared” or “R-squared change” are widely used terminology.\nStatistical inference for \\(\\Delta R^2\\) uses the F-test shown below. Let Model 1 be a regression model with \\(K_1\\) predictors that is nested within Model 2, with \\(K_2 &gt; K_1\\) predictors. Define \\(\\Delta R^2 = R^2_2 - R^2_1\\) and \\(\\Delta K = K_2 - K_1\\). To test the hypothesis \\(H_0: \\Delta R^2 = 0\\) versus \\(H_A : \\Delta R^2 &gt; 0\\) we can use the test statistic\n\\[F = \\frac{\\Delta R^2 / \\Delta K}{(1 - R^2_2) /(N - K_2 - 1)}  \\]\nwhich has an F-distribution on \\(\\Delta K\\) and \\(N - K_2 - 1\\) degrees of freedom, when the null hypothesis true. This test assumes that Model 2 satisfies the linear regression population model (see @sec-population-model-2) and that Model 1 is nested within Model 2."
  },
  {
    "objectID": "ch6_model_building.html#sec-worked-example-6",
    "href": "ch6_model_building.html#sec-worked-example-6",
    "title": "6  Model building",
    "section": "6.2 A worked example",
    "text": "6.2 A worked example\nTo illustrate hierarchical modeling, let’s use the ECLS data to address whether reading achievement at the beginning of Kindergarten (c1rrscal) is related to SES (wksesl), parental (mother’s and father’s) education (wkmomed and wkdaded, respectively), and attendance in center-based care before K (p1center; pre-K status, for short).\nThis example is intended to be a realistic illustration of how to use the techniques we have covered up to this point. The sections below can be thought of as excerpts from a research paper, and also shows how to report results in a regression table. But there are some caveats that should be mentioned before starting:\n\nWe used some tricks from later chapters to deal with non-linearity, and\nkeep in the mind the subset of ECLS data we are using is not a representative sample and the results should not be assumed to generalize to the general population.\n\n\n6.2.1 Research questions\nWe will address the following research questions.\n\nRQ1: Does SES predict Reading Achievement upon entry to K?\nRQ2: Does Parental Education (mother’s and father’s) predict Reading Achievement, after controlling for SES?\nRQ3: After controlling for both SES and Parental Education, is Pre-K Status associated with better Reading Achievement in K?\nRQ4: Does the relationship of SES or Parental Education to Reading Achievement change as function of Pre-K Status – in more causal language: does Pre-K participation reduce pre-existing disparities in Reading Achievement upon entry to K?\n\n\n\n6.2.2 Data analysis plan\nThe research questions were addressed via hierarchical regression modeling with the following blocks of predictors.\n\nBlock 1: wksesl\n\nBlock 2: wkmomed and wkdaded\nBlock 3: plcetner\nBlock 4: Interactions between p1center and the other predictors\n\nTo aid the interpretation of regression coefficients in the presence of interactions, all continuous variables (predictors and outcome) were transformed to z-scores. plcenter was coded as binary indicator for Pre-K status.\nThe blocks were entered into the model in the indicated order. The RQs were addressed by examining the R-square change (\\(\\Delta R^2\\)) for each corresponding block. (In practice, we would also interpret the sign and direction of the significant regression coefficients in the final model, but we will leave that as an exercise.) A significance level of .05 was used for all tests reported in this study.\nPreliminary examination of the model residuals indicated violation of the assumption of linearity. To address this, the outcome variable was log-transformed and a quadratic term was added for wksesl (wksesl_sq). These procedures for checking and addressing non-linearity are discussed in the coming chapters.\n\n\n6.2.3 Results\nThe F-tests of R-squared change for each block of predictors are reported in Figure 6.4. The results show that block 1 (linear and quadratic trends in SES) explained about 18% of the variance Reading Achievement. While blocks 2 (parental education) and 3 (Pre-K) contributed statistically significant increases in variance explained, the change in R-squared was not large. Block 4 containing the interactions with Pre-K was not statistically significant after controlling for the main effects.\n\n\nCode\nload(\"ECLS2577.RData\")\nattach(ecls)\n\n# Recode variables\np1center &lt;- 2 - p1center\nz_c1rrscal &lt;- scale(c1rrscal)\nz_wksesl &lt;- scale(wksesl)\nz_wksesl_sq &lt;- z_wksesl^2\nz_wkmomed &lt;- scale(wkmomed)\nz_wkdaded &lt;- scale(wkdaded)\nlog_c1rrscal &lt;- log(z_c1rrscal - min(z_c1rrscal) + 1)\n\n# Run models\n# models\nmod1 &lt;- lm(log_c1rrscal ~  z_wksesl + z_wksesl_sq)\nmod2 &lt;- lm(log_c1rrscal ~  z_wksesl + z_wksesl_sq + z_wkmomed + z_wkdaded)\nmod3 &lt;- lm(log_c1rrscal ~  z_wksesl + z_wksesl_sq + z_wkmomed + z_wkdaded + p1center)\nmod4 &lt;- lm(log_c1rrscal ~  (z_wksesl + z_wksesl_sq + z_wkmomed + z_wkdaded)*p1center)\n\nR2_1 &lt;- summary(mod1)$r.squared\nR2_2 &lt;- summary(mod2)$r.squared\nR2_3 &lt;- summary(mod3)$r.squared\nR2_4 &lt;- summary(mod4)$r.squared\n\nrsquared &lt;- c(0, R2_1, R2_2, R2_3, R2_4)\ndelta_rsquared &lt;- c(0, R2_1, R2_2 - R2_1, R2_3 - R2_2, R2_4 - R2_3)\nModels &lt;- paste0(\"Model \", 0:4)\nnames(delta_rsquared) &lt;- Models\n\n# Tests of R-square change\nmodel_results &lt;- knitr::kable(cbind(Rsquared = rsquared, \n                                  Delta.Rsquared = delta_rsquared, \n                                  anova(lm(log_c1rrscal ~  1), mod1, mod2, mod3, mod4)),\n                           digits = 3)\nmodel_results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRsquared\nDelta.Rsquared\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\nModel 0\n0.000\n0.000\n2576\n343.636\nNA\nNA\nNA\nNA\n\n\nModel 1\n0.176\n0.176\n2574\n283.114\n2\n60.522\n277.886\n0.000\n\n\nModel 2\n0.179\n0.003\n2572\n282.032\n2\n1.083\n4.970\n0.007\n\n\nModel 3\n0.184\n0.005\n2571\n280.271\n1\n1.761\n16.168\n0.000\n\n\nModel 4\n0.187\n0.002\n2567\n279.538\n4\n0.733\n1.683\n0.151\n\n\n\nFigure 6.4: R-squared change and F-tests\n\n\n\nAlthough we didn’t have research hypotheses about the individual predictors, it is often useful to report a table such as the one below to summarize that kind of information. The table was produced using the stargazer package in R and is representative of the usual format for reporting regression output from hierarchical regression models. The values reported for each variable are the regression coefficients, with the standard errors in parenthesis. Please take a moment to look over the table and write down any questions you have about its interpretation\n\n\n\n\n\n\n\n\n\n\n\n6.2.4 Interpretation\nThe focus of this section is on the interpretation of the \\(\\Delta R^2\\) statistics and their significance. In practice, we would also interpret the sign and direction of the significant regression coefficients in the final model. As noted above, the outcome variable was log transformed, but we ignore that detail here. It would usually not be necessary to report results both in a table and inline, but the latter are shown for illustrative purposes (practice, practice, practice!).\n\nRQ1: SES had significant linear and quadratic relationships with Reading Achievement. Together, both predictors explained about 17.6% of the variance in Reading Achievement (\\(R^2 = .176, F(2, 2574) = 275.12, p &lt; .001)\\).\nRQ2: After controlling for SES, Parental Education explained a small but statistically significant proportion of the variance in Reading Achievement (\\(\\Delta R^2 = .003, F(2, 2572) = 4.94, p = .007\\)),\nRQ3: After controlling for SES and Parental Education, Pre-K status explained an additional .5% of the variation in reading achievement (\\(\\Delta R^2 = .005, F(1, 2571) = 16.15, p &lt; .001\\)).\nRQ4: There was mixed evidence about the interactions between Pre-K Status and the other predictors. The F-test of R-squared change indicated that the interaction terms did not explain a significant amount of variation in reading (\\(\\Delta R^2 = .002, F(4, 2567) = 1.68, p = .15\\)). However, the interaction with paternal education was significant at the .05 level. After re-running Model 4 with only the paternal education interaction, it was found that this effect was not significant at the .05 level (\\(t(2570) = -0.856, p = 0.39\\); see output below). It may be concluded that there were no significant interactions with Pre-K status.\n\n\n\nCode\nmod4A &lt;- lm(log_c1rrscal ~  z_wksesl + z_wksesl_sq + z_wkmomed + z_wkdaded*p1center)\nsummary(mod4A)\n\n\n\nCall:\nlm(formula = log_c1rrscal ~ z_wksesl + z_wksesl_sq + z_wkmomed + \n    z_wkdaded * p1center)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88751 -0.23151 -0.00025  0.20858  1.34884 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.795829   0.015160  52.495  &lt; 2e-16 ***\nz_wksesl            0.115158   0.016322   7.056  2.2e-12 ***\nz_wksesl_sq        -0.019645   0.005306  -3.702 0.000218 ***\nz_wkmomed           0.018424   0.011174   1.649 0.099327 .  \nz_wkdaded           0.042770   0.017016   2.513 0.012016 *  \np1center            0.060274   0.016163   3.729 0.000196 ***\nz_wkdaded:p1center -0.014023   0.016373  -0.856 0.391830    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3302 on 2570 degrees of freedom\nMultiple R-squared:  0.1846,    Adjusted R-squared:  0.1827 \nF-statistic: 96.99 on 6 and 2570 DF,  p-value: &lt; 2.2e-16\n\n\nIn summary, it was found that SES and Parental Education were significant predictors of children’s Reading Achievement upon entry to Kindergarten, indicating that pre-existing disparities in reading due to economic factors are apparent at even this very early age. Attendance in Pre-K explained an additional .5% of variance in Reading Achievement and was associated with increased Reading Achievement; however, there was no evidence that participation in Pre-K reduced pre-existing disparities in Reading Achievement associated with the economic factors (i.e., there were no interactions between Pre-K participation and the economic factors)."
  },
  {
    "objectID": "ch6_model_building.html#sec-too-many-predictors-6",
    "href": "ch6_model_building.html#sec-too-many-predictors-6",
    "title": "6  Model building",
    "section": "6.3 Too many predictors?",
    "text": "6.3 Too many predictors?\nWhat is better: more predictors or fewer? While there is no definitive answer to this question, there are two points that we should keep in mind when adding predictors into a model:\n\nThe ratio of sample size to number of predictors: \\(N / K\\).\nThe correlation among the predictors (AKA multicollinearity).\n\nIn order to understand the effect of these factors in multiple regression, its helpful to refer to the formula for the standard error of a regression coefficient (from Section 3.7.1), which is presented again below:\n\\[\nSE({b_k}) = \\frac{s_Y}{s_k} \\sqrt{\\frac{1 - R^2}{N - K - 1}} \\times \\sqrt{\\frac{1}{1 - R_k^2}}.\n\\]\nRecall that \\(s\\) denotes standard deviations and that \\(R_k^2\\) is the R-squared from the regression of the \\(k\\)-th predictor on the remaining \\(K-1\\) predictors.\n\n6.3.1 \\(N/K\\)\nWe can see from the equation for \\(SE({b_k})\\) that increasing the number of predictors leads the term \\(N-K-1\\) to get smaller. Consequently, the ratio\n\\[ {\\frac{1 - R^2}{N - K - 1}} \\]\nwill get larger (and hence the standard error will get larger) if the increase in the number of predictors is not compensated by an increase in \\(R^2.\\) In other words, predictors that do not add explanatory power to the model (i.e., do not increase R-squared) will lead to reduced precision for all of the regression coefficients.\nSo long as \\(N &gt;&gt; K\\), the effect of the number of predictors is negligible. But when the number of predictors gets close to the sample size, the estimates of the regression coefficients can become very imprecise. Many seek “rules of thumb” for the ratio of observations to predictors. This is just a complicated way of asking how large the sample size needs to be, and the correct answer to that question is provided by power analysis (Section 2.8). That said, if \\(N/K &lt; 30\\)-ish you should start probably worrying about the number of predictors in your model.\n\n\n6.3.2 Multicollinearity\nRegardless of sample size, when a predictor is highly correlated with the other predictors, its precision is negatively affected via the term \\(R^2_k\\). When \\(R^2_k = 1\\) the situation is called multicollinearity. Note that multicollinearity can occur even if none of the pairwise correlations among the predictors is large – it has to do with the relation between predictor \\(k\\) and all of the other predictors, which is why it is called multicollinearity.\nThe term \\(1 - R_k^2\\) is called the tolerance and its reciprocal is called the variance inflation factor (VIF).\n\\[ VIF = \\frac{1}{1 - R_k^2} \\]\nNote that \\(\\sqrt{VIF}\\) appears in formula for the standard error of the regression coefficient.\nSome examples of how to interpret the \\(VIF\\) are provided below:\n\nIf \\(VIF = 5\\), the estimated regression coefficient is 5 times less precise than it would be without multicollinearity.\nIf \\(VIF = 10\\), the estimated regression coefficient is 10 times less precise than it would be without multicollinearity.\nIf \\(VIF &gt; 10\\), it is a good idea to reconsider whether the predictor needs to be recoded or omitted from the model.\n\nIn general, it is a good practice to check VIF as part of model diagnostics, which is the topic of the next chapter. For our example, the VIF statistics for Model 3 and Model 4 are reported below.\nFor Model 3:\n\n\nCode\n# install.packages(\"car\")\ncar::vif(mod3)\n\n\n   z_wksesl z_wksesl_sq   z_wkmomed   z_wkdaded    p1center \n   6.290370    1.206007    2.950358    3.377283    1.053738 \n\n\nFor Model 4:\n\n\nCode\ncar::vif(mod4)\n\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n            z_wksesl          z_wksesl_sq            z_wkmomed \n           28.157965             6.447179            12.799652 \n           z_wkdaded             p1center    z_wksesl:p1center \n           13.816244             1.812096            28.320202 \nz_wksesl_sq:p1center   z_wkmomed:p1center   z_wkdaded:p1center \n            7.562264            12.516708            14.011576 \n\n\nWe can see that including the interactions led to a high degree of multicollinearity for some of the regression coefficients in the model. Based on this information, as well as the non-significant \\(\\Delta R^2\\) for the interaction block, it would be advisable to focus our interpretation on Model 3 rather than Model 4."
  },
  {
    "objectID": "ch6_model_building.html#workbook",
    "href": "ch6_model_building.html#workbook",
    "title": "6  Model building",
    "section": "6.4 Workbook",
    "text": "6.4 Workbook\nThis section collects the questions asked in this chapter. There aren’t many, we will mainly be focussed on going through the worked example during class time.\nSection 6.1\nPlease take a moment to write down one or more examples of a block of variables from your area of research, and I will invite you to share your examples in class.\nSection 6.2\n\nLook over the tables below and write down any questions you have about their interpretation or the write up provided in Section 6.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRsquared\nDelta.Rsquared\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\nModel 0\n0.000\n0.000\n2576\n343.636\nNA\nNA\nNA\nNA\n\n\nModel 1\n0.176\n0.176\n2574\n283.114\n2\n60.522\n277.886\n0.000\n\n\nModel 2\n0.179\n0.003\n2572\n282.032\n2\n1.083\n4.970\n0.007\n\n\nModel 3\n0.184\n0.005\n2571\n280.271\n1\n1.761\n16.168\n0.000\n\n\nModel 4\n0.187\n0.002\n2567\n279.538\n4\n0.733\n1.683\n0.151"
  },
  {
    "objectID": "ch6_model_building.html#exercises",
    "href": "ch6_model_building.html#exercises",
    "title": "6  Model building",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\nThe exercises reproduce the worked example in Section 6.2. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)\nBefore staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\nFirst we set up the variables\n\n\nCode\n# Clean up env. and load data\n#rm(list = ls())\n#load(\"ECLS2577.RData\")\n#attach(ecls)\n\n# Recode variables\np1center &lt;- 2 - p1center\nz_c1rrscal &lt;- scale(c1rrscal)\nz_wksesl &lt;- scale(wksesl)\nz_wksesl_sq &lt;- z_wksesl^2\nz_wkmomed &lt;- scale(wkmomed)\nz_wkdaded &lt;- scale(wkdaded)\nlog_c1rrscal &lt;- log(z_c1rrscal - min(z_c1rrscal) + 1)\n\n\nNext, run the models.\n\nCode\n# Run models\nmod1 &lt;- lm(log_c1rrscal ~  z_wksesl + z_wksesl_sq)\nmod2 &lt;- lm(log_c1rrscal ~  z_wksesl + z_wksesl_sq + z_wkmomed + z_wkdaded)\nmod3 &lt;- lm(log_c1rrscal ~  z_wksesl + z_wksesl_sq + z_wkmomed + z_wkdaded + p1center)\n\n# Note the syntax here for interacting one variable with many\nmod4 &lt;- lm(log_c1rrscal ~  (z_wksesl + z_wksesl_sq + z_wkmomed + z_wkdaded)*p1center)\n\n# Regression table -- will discuss this in class\n# stargazer::stargazer(mod1, mod2, mod3, mod4, type = 'html', omit = \"Constant\")\nknitr::include_graphics(\"files/images/stargazer.png\")\n\n\nThe stargazer table doesn’t format nicely in markdown, it is attached as a screen shot instead. We will go through some different types of output with stargazer in class. You must install the stargazer package for the code to work.\nThe above table reports the \\(R^2\\) for each model but does not provide the \\(\\Delta R^2\\) statistics or the corresponding F-tests. The \\(\\Delta R^2\\) statistics require some additional processing:\n\n\nCode\n# Get R-squared \nR2_1 &lt;- summary(mod1)$r.squared\nR2_2 &lt;- summary(mod2)$r.squared\nR2_3 &lt;- summary(mod3)$r.squared\nR2_4 &lt;- summary(mod4)$r.squared\nrsquared &lt;- c(0, R2_1, R2_2, R2_3, R2_4)\n\n# Compute R-squared change\ndelta_rsquared &lt;- c(0, R2_1, R2_2 - R2_1, R2_3 - R2_2, R2_4 - R2_3)\n\n# Set up for pretty printing\nresults &lt;- cbind(Rsquared = rsquared, \n                 Delta.Rsquared = delta_rsquared, \n                 anova(lm(log_c1rrscal ~  1), mod1, mod2, mod3, mod4))\n             \nknitr::kable(results, digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRsquared\nDelta.Rsquared\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n0.000\n0.000\n2576\n343.636\nNA\nNA\nNA\nNA\n\n\n0.176\n0.176\n2574\n283.114\n2\n60.522\n277.886\n0.000\n\n\n0.179\n0.003\n2572\n282.032\n2\n1.083\n4.970\n0.007\n\n\n0.184\n0.005\n2571\n280.271\n1\n1.761\n16.168\n0.000\n\n\n0.187\n0.002\n2567\n279.538\n4\n0.733\n1.683\n0.151\n\n\n\n\n\nThe variance inflation factors for Model 3 and Model 4 are obtained use the vif function of the car package (you must install the car package for this code to work).\n\n\nCode\n# install.pacakges(\"car\")\ncar::vif(mod3)\n\n\n   z_wksesl z_wksesl_sq   z_wkmomed   z_wkdaded    p1center \n   6.290370    1.206007    2.950358    3.377283    1.053738 \n\n\nCode\ncar::vif(mod4)\n\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n            z_wksesl          z_wksesl_sq            z_wkmomed \n           60.867975            12.682305            28.145377 \n           z_wkdaded             p1center    z_wksesl:p1center \n           31.720745             1.812096            59.296155 \nz_wksesl_sq:p1center   z_wkmomed:p1center   z_wkdaded:p1center \n           12.217023            28.440378            30.766199 \n\n\n\n\nCode\n# clean up\nrm(list = ls())\ndetach(ecls)"
  },
  {
    "objectID": "ch7_assumption_checking.html#sec-recap-7",
    "href": "ch7_assumption_checking.html#sec-recap-7",
    "title": "7  Assumption checking",
    "section": "7.1 Recap of population model",
    "text": "7.1 Recap of population model\nLet’s start with a recap of the population model for linear regression. This was introduced for simple linear regression in Section 2.5. To restate the assumptions for multiple linear regression, we will use the vector notation\n\\[\\mathbf X = [X_1, X_2, \\dots, X_K]\\]\nto represent the predictor variables. For our purposes, the vector \\(\\mathbf X\\) is just a list of all of the predictors in a model. The outcome variable is denoted as \\(Y\\), as usual.\n\nNormality: The distribution of \\(Y\\) conditional on \\(\\mathbf X\\) is normal for all values of \\(\\mathbf X\\).\n\n\\[ Y | \\mathbf X \\sim  N(\\mu_{Y | \\mathbf X} , \\sigma_{Y | \\mathbf X}) \\]\n\nHomoskedasticity: The conditional distributions have equal variances (also called homogeneity of variance, or just equal variances).\n\n\\[ \\sigma_{Y| \\mathbf X} = \\sigma \\]\n\nLinearity: The means of the conditional distributions are a linear function of \\(\\mathbf X\\).\n\n\\[ \\mu_{Y| \\mathbf X} = b_0 + \\sum_{k = 1}^K b_k X_k \\]\nThese three assumptions can also be summarized in term of the regression residuals. Recall that residuals are computed as \\(\\epsilon = Y - \\mu_{Y|\\mathbf X}\\). If the three assumptions of linear regression hold, then the regression residuals should be normally distributed with mean zero and constant variance, for every value of the predictors:\n\\[\\epsilon \\mid \\mathbf {X} \\sim N(0, \\sigma). \\]\nFigure 7.1 presents the population model in terms of the residuals. This plot is similar Figure 2.4, but is modified for the multiple regression setting by using predicted values and residuals as the axes, rather than \\(X\\) and \\(Y\\). The plots we look at in the following sections are sample analogues to this population model – they plot residuals against predicted values.\n\n\n\n\n\nFigure 7.1: The Multiple Regression Population Model."
  },
  {
    "objectID": "ch7_assumption_checking.html#sec-linearity-7",
    "href": "ch7_assumption_checking.html#sec-linearity-7",
    "title": "7  Assumption checking",
    "section": "7.2 Linearity",
    "text": "7.2 Linearity\nThis assumption is about whether the regression function is “really” a line or if it could be better represented as some other relationship. A classic example is shown below. We address this example, which is taken from “Anscombe’s quartet”, in more detail in Section 7.6 (which is optional).\n\n\nCode\n# Non-linearity in Anscombe's second example\nattach(anscombe)\nmod &lt;- lm(y2 ~ x2)\n\n# Take a look at the raw data\npar(mfrow = c(1, 2))\nplot(x2, y2, col = \"#4B9CD3\", xlab = \"X\", ylab = \"Y\")\nabline(mod)\n\n# Compare to the residual vs fitted plot\nplot(mod, which = 1)\ndetach(anscombe)\n\n\n\n\n\nFigure 7.2: Anscombe’s second dataset\n\n\n\n\nThe left hand panel of Figure 7.2 shows the scatter plot of the example data. It should hopefully be obvious that the relationship between \\(Y\\) and \\(X\\) is not linear.\nThe right hand panel shows the residuals versus the predicted (“fitted”) values from the regression of \\(Y\\) on \\(X\\). It plots the residuals on the vertical axis and the fitted values (\\(\\hat Y\\)) on the horizontal axis. This is the sample analogue of the population model in Figure 7.1.\nIt is important to note the following about residual vs fitted plot:\n\nThe key idea is that deviations from the regression line in the left hand panel correspond to deviations from the horizontal line at Residuals = 0 in the right hand panel. Recall that the residuals should all be centered around this horizontal line if the population model is true (see Figure 7.1). The non-linear trend is apparent in the in both panels, but in the residual vs fitted plot the nonlinearity is with reference to Residuals = 0.\nThe red line in the right hand panel is a locally weighted smoothed (“lowess”) regression line – it follows whatever trend is in the residuals without assuming the trend is linear.\nThe overall interpretation of the residual vs fitted plot is as follows:\n\nIf the red line is roughly horizontal at Residuals = 0, we conclude that the assumption of linearity is not problematic for the data.\nIf the red line deviates systematically from a horizontal line at Residuals = 0, this is evidence that the assumption of linearity is problematic.\n\n\nIn Figure 7.2, the assumption of linearity is clearly not met. In fact, this is so obvious that we could see it in the regular scatter plot! So, you might be asking, why do we need the residual vs fitted plot? Well, the regular scatter plot is only useful for diagnosing linearity with a single predictor, whereas the residual vs fitted plots works any number of predictors. So, in general, it is much easier to check the assumption using the residual versus fitted plot, even if the patterns are a bit harder to interpret.\nFigure 7.3 illustrates the residual vs fitted plot using a model with 2 predictors. Please write down whether you think the linearity assumption is problematic for the example below, and be sure to explain why with reference to the figure. Keep in mind that interpreting plots takes a bit of practice and in general there is no “right” answer. Rather, what I am looking for is an explanation of why you think the assumption is problematic or not. Your explanation should refer to the interpretation of residual vs fitted plots, as outlined above.\n\n\nCode\nload(\"ECLS250.RData\")\nattach(ecls)\n\n# Run model for example\nmod2 &lt;- lm(c1rmscal ~ ses_orig + t1learn, data = ecls)\n\n# Plot resid vs fitted\nplot(mod2, which = 1)\n\n\n\n\n\nFigure 7.3: An example from ECLS.\n\n\n\n\nBefore moving on, let’s take a look at a few more examples. For each of the examples in Figure 7.4, please write down whether you think the linearity assumption is problematic and explain why with reference to the plots.. Hint: be careful not to over-interpret the lowess line in the tails of the plots, where only a few data points can have a big impact on the local trend. Focus your interpretation on the bulk of the data, and whether it shows a systemic trend away from a horizontal line at 0.\n\n\nCode\n# Example: regression c1rmscal on ses_orig and t1learn\nset.seed(101)\npar(mfrow = c(2, 2))\nfor(i in 1:4) {\n  x &lt;- rnorm(200)\n  e &lt;- rnorm(200)\n  y &lt;- x + e \n  plot(lm(y ~ x), which = 1)\n}  \n\n\n\n\n\nFigure 7.4: More examples.\n\n\n\n\n\n7.2.1 Summary\nTo check the assumption of linearity, we can use a residual vs predicted (fitted) plot.\n\nIf the plot does not show a systematic trend other than a horizontal line at Residuals = 0, then there is no evidence against the assumption.\nIf the residuals do show a trend away from Residuals = 0, then we should worry about the assumption.\nDon’t over interpret the tails of the lowess (red) lines in the R plots.\nIf the assumption is violated: consider a non-linear transformations of the \\(Y\\) variable (Chapter 8) or adding quadratic or other non-linear terms to the model (Chapter 9)."
  },
  {
    "objectID": "ch7_assumption_checking.html#sec-homoskedasticity-7",
    "href": "ch7_assumption_checking.html#sec-homoskedasticity-7",
    "title": "7  Assumption checking",
    "section": "7.3 Homoskedasticity",
    "text": "7.3 Homoskedasticity\nThis assumption means that the variance of the residuals should not change as a function of the predicted values. Because we are again concerned with residuals and predicted values, we can re-use the same plot we used to check linearity. However, we are no longer interested in whether the lowess trend (red line) systematically deviates from zero – now we are interested in whether the range of the residuals (on the vertical axis) changes over the predicted values (on the horizontal axis).\nFigure 7.5 illustrates two data sets in which the assumption of linearity is met, but the right hand panel shows evidence of heteroskedasticity. This is apparent by observing the range of the residuals over values of \\(\\widehat Y\\).\n\n\nCode\n# homoskedastic example\nset.seed(1)\nx &lt;- sort(rnorm(250))\ne &lt;- rnorm(250)\ny &lt;- x + e\nmod3 &lt;- lm(y~x)\npar(mfrow = c(1, 2))\nplot(mod3, which = 1)\n\n# Heteroskedastic example\ny2 &lt;- y\ny2[x &gt; 0] &lt;- x[x &gt; 0] + 3* e[x &gt; 0]\ny2[x &lt; -1] &lt;- x[x &lt; -1] + .3* e[x &lt; -1]\nmod4 &lt;- lm(y2~x)\nplot(mod4, which = 1)\n\n\n\n\n\nFigure 7.5: Illustration of homo- and heteroskedasicity.\n\n\n\n\nTo make it clearer what aspect of these plots is relevant for evaluating the assumption of homoskedasticity, the same figures are replicated below, but this time with blue lines represented my own “eye-balling” of the range of the residuals. In the left plot, the two lines are parallel, meaning the range is constant. In the right plot, the two lines form a cone, meaning the the range of the residuals increases for larger values of \\(\\widehat Y\\).\n\n\nCode\n# homoskedastic example with ref lines\npar(mfrow = c(1, 2))\nplot(mod3, which = 1)\nsegments(x0 = -1.5, y0 = 2, x1 = 1.5, y1 = 2, col = \"#4B9CD3\", lty = 2, lwd = 3)\nsegments(x0 = -1.5, y0 = -2, x1 = 1.5, y1 = -2, col = \"#4B9CD3\", lty = 2, lwd = 3)\n\n# Heteroskedastic example\nplot(mod4, which = 1)\nsegments(x0 = -1.5, y0 = 1, x1 = 1.5, y1 = 8, col = \"#4B9CD3\", lty = 2, lwd = 3)\nsegments(x0 = -1.5, y0 = -1, x1 = 1.5, y1 = -8, col = \"#4B9CD3\", lty = 2, lwd = 3)\n\n# remove y2 from memory to avoid naming conflicts later on\nrm(y2)\n\n\n\n\n\nFigure 7.6: Illustration of Homo- and Heteroskedasicity, with Reference Lines\n\n\n\n\nNote that I didn’t draw any lines in the tail ends of the plots – this is because there are fewer observations in the tails, so it is harder to make a judgment about the range of values. To avoid “reading the tea leaves” I focus on the values of \\(\\widehat Y\\) for which there are sufficient observations to judge the range of the residuals.\nTo repeat, the blue lines are just there for your reference, to highlight the relevant information in the plot. You wouldn’t generally include these lines in the plot.\nLet’s take another look at the plots in Figure 7.4. Please write down whether you think the homoskedasticity assumption is problematic and explain why with reference to the plots.\n\n7.3.1 Dealing with Heteroskedasticity\nHeteroskedasticity in linear regression, and corrections thereof, is a pretty big topic in the methodological literature (see (cite-fox?), section 12.2). In this section we are just going to discuss one widely used solution, and how to implement it in R.\nAs mentioned previously, heteroskedasticity affects the standard errors of the regression coefficients, and consequently their t-tests, p-values, and confidence intervals. In particular, the p-values for the regression coefficients will usually be too small if the data are heteroskedastic, but we mistakenly assume they are homoskedastic. Note that heteroskedasticity won’t affect the estimated values of the OLS regression coefficients (i.e., the \\(\\widehat{b}\\)’s), and it also doesn’t affect R-squared or its F-test.\nIf our data exhibit heteroskedasticity, one solution is to use heteroskedasticity-consistent (HC) standard errors. HC standard errors are also sometimes called heteroskedasticity-robust, or just robust. The are also informally referred to as “sandwich” estimates – see (cite-fox?) section 12.2.3 for an explanation of this terminology.\nAlthough there are many different version of HC standard errors, they are all equivalent with “large” samples. The simplest version is (see (cite-fox?), section 12.2.3)\n\\[ \\text{HC-SE}(\\hat{b}_k) = \\sqrt{\\frac{\\sum_{i=1}^N (X_{ik} - \\widehat{X}_{ik})^2 (Y_i-\\widehat{Y}_i)^2} {\\sum_{i=1}^N (X_{ik} - \\bar X_k)^2 (1 - R^2_k)}}\n\\tag{7.1}\\]\nIn this equation, \\(\\widehat{X}_{ij}\\) is the predicted value that results from regressing \\(X_k\\) on the remaining \\(K-1\\) predictors. The equation is not very intuitive to look at, but the general idea is that it can be derived without assuming homoskedasticity.\nIn terms of implementation, the procedure for using HC standard errors in R has three steps.\n\nFirst, we estimate the model as usual, (e.g., using the lm function)\nSecond, we compute the HC standard errors (e.g., using the hccm function of the car package.)\nThird, we use the HC standard errors to compute the correct t-tests / confidence intervals (e.g., using the coeftest function of the lmtest package. )\n\nYou can find a more complete discussion of robust standard errors in R in the vignettes linked here: http://jepusto.github.io/clubSandwich/\nThe following output shows the results for the heteroskedastic (cone-shaped) example data in Figure 7.6, using both the regular standard errors and HC standard errors. We can see that both sets of output are pretty similar: while the “Estimates” don’t change, the “Std. Errors” are a bit different in the two sets of output. In this case, the HC standard errors don’t affect conclusions about statistical significance, but in other cases they can lead to more dramatic differences in interpretation. Please examine these two sets of output and write down any questions you have about their interpretation.\n\nRegular OLS standard errors :\n\n\n\nCode\n## Make sure the required packages are installed\n# install.packages(\"car\")\n# install.packages(\"lmtest\")\n\n# Regular SE: \nsummary(mod4)\n\n\n\nCall:\nlm(formula = y2 ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9985 -1.0402 -0.0504  0.9252 11.5002 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.002712   0.154430  -0.018    0.986    \nx            0.968044   0.160719   6.023 6.13e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.441 on 248 degrees of freedom\nMultiple R-squared:  0.1276,    Adjusted R-squared:  0.1241 \nF-statistic: 36.28 on 1 and 248 DF,  p-value: 6.135e-09\n\n\n\nHC standard errors:\n\n\n\nCode\n# HC SE\n# Step 2. Use \"hccm\" to get the HC SEs for our model \nhcse &lt;- car::hccm(mod4)\n\n# Step 3. Use \"coeftest\" to compute t-tests with the HC SEs\nlmtest::coeftest(mod4, hcse)\n\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -0.0027123  0.1520067 -0.0178    0.9858    \nx            0.9680441  0.1908455  5.0724 7.701e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOne final note: HC standard errors do not assume the data are homoskedastic. So, they can be used regardless of whether the homoskedasticity assumption is met or not. But, when the data are homoskedastic, the regular OLS standard errors are usually more precise (i.e., smaller). So, we generally don’t want to use HC standard errors unless there is evidence of heteroskedasticity in the data. This is why we do graphical checks first!\n\n\n7.3.2 Summary\n\nThe assumption of homoskedasticity (or homogeneity variance, or just equal variances) means that the variance of the residuals should not change as a function of the predicted values.\nBecause we are again concerned with residuals and predicted values, we can re-use the same plot we used to check linearity. However, now we are interested in whether the range of the residuals (on the vertical axis) changes over the predicted values (on the horizontal axis).\nIf we suspect that heteroskedasticity is a problem, we can adjust how the standard errors of the regression coefficients are computed. These adjusted standard errors are variously referred to as heteroskedasticity-consistent, heteroskedasticity-robust, robust, or “sandwich” estimates.\nTo implement in HC standard errors in R, we can use a three step procedure:\nFirst, we estimate the model as usual, (e.g., using the lm function)\nSecond, we compute the HC standard errors (e.g., using the hccm function of the car package.)\nThird, we use the HC standard errors to compute the correct t-tests / confidence intervals (e.g., using the coeftest function of the lmtest package. )\n\nYou can find a more complete discussion of robust standard errors in R in the vignettes linked here: http://jepusto.github.io/clubSandwich/"
  },
  {
    "objectID": "ch7_assumption_checking.html#sec-normality-7",
    "href": "ch7_assumption_checking.html#sec-normality-7",
    "title": "7  Assumption checking",
    "section": "7.4 Normality",
    "text": "7.4 Normality\nThe last assumption we need to check is normality of the residuals. There are many ways to compare the empirical distribution of a variable (e.g., the residuals in a regression analysis) to a theoretical distribution (e.g., the normal). One general-purpose technique is a qq plot (short for quantile-quantile plot). A qq plot compares the quantiles (e.g., percentiles) of two different distributions.\nFor our assumption, we want to compare the quantiles of our standardized residuals to the quantiles of a standard normal distribution. Standardizing means the residuals should have variance equal to one, and, combined with the other population assumptions of linear regression, this implies that the residuals should have a standard normal distribution (see Section 7.1).\nSince qq plots might not be something you have seen before, we’ll take a look at a few examples. Each figure below pairs a histogram and qq plot. In the qq plot, data points should fall on the diagonal line if the data are normally distributed. It should be emphasized that the line in the qq plot is not a regression line! It is just the diagonal line \\(Y = X\\), and the bulk of the data points should fall on that line if the data were drawn from a normal distribution.\nIn the following examples, focus on how the pattern in the histogram shows up as deviations from the diagonal line in the qq plot. We will discuss the interpretation of these patterns together in class, but for now, please write down any questions you have about the interpretation of the qq plots.\n\n\nCode\n# Comparing histograms and q-q plots\ndistributions &lt;- read.csv(\"distributions.csv\")\nnames(distributions)[2] &lt;- \"normal\"\nattach(distributions)\ndist_names &lt;- names(distributions)\n\n# Normal \npar(mfrow = c(1, 2))\nhist(normal, col = \"#4B9CD3\", main = \"Normal\")\nqqnorm(normal, col = \"#4B9CD3\")\nqqline(normal)\n\n\n\n\n\nCode\n# Negative skew\npar(mfrow = c(1, 2))\nhist(neg_skew, col = \"#4B9CD3\", main = \"Negative Skew\")\nqqnorm(neg_skew, col = \"#4B9CD3\")\nqqline(neg_skew)\n\n\n\n\n\nCode\n# Positive skew\npar(mfrow = c(1, 2))\nhist(pos_skew, col = \"#4B9CD3\", main = \"Positive Skew\")\nqqnorm(pos_skew, col = \"#4B9CD3\")\nqqline(pos_skew)\n\n\n\n\n\nCode\n# Leptokurtic\npar(mfrow = c(1, 2))\nhist(lepto, col = \"#4B9CD3\", main = \"Lepotkurtosis\")\nqqnorm(lepto, col = \"#4B9CD3\")\nqqline(lepto)\n\n\n\n\n\nCode\n# Platykurtic\npar(mfrow = c(1, 2))\nhist(platy, col = \"#4B9CD3\",  main = \"Platykurtosis\")\nqqnorm(platy, col = \"#4B9CD3\")\nqqline(platy)\n\n\n\n\n\nCode\ndetach(distributions)\n\n\nNext, let’s consider a more realistic example using the default plotting from the lm function. Please write down whether you think the normality assumption is problematic for the data in Figure 7.7, and be sure to explain why with reference to the plot. Hint: if you think the data are non-normal, you should be able to interpret the pattern of deviations with reference to the examples given above (e.g. skew, kurtosis).\n\n\nCode\n# Example: regression c1rmscal on ses_orig and t1learn\nplot(mod2, which = 2)\n\n\n\n\n\nFigure 7.7: An example from ECLS.\n\n\n\n\nPractice makes perfect, so lets work through a few more examples. In each of the examples in Figure 7.8, please write down whether you think the normality assumption is problematic and explain why with reference to the plots.\n\n\nCode\n# Example: regression c1rmscal on ses_orig and t1learn\nset.seed(101)\npar(mfrow = c(2, 2))\nfor(i in 1:4) {\n  x &lt;- rnorm(200)\n  e &lt;- rnorm(200)\n  y &lt;- x + e \n  plot(lm(y ~ x), which = 2)\n}  \n\n\n\n\n\nFigure 7.8: More examples.\n\n\n\n\n\n7.4.1 Summary\nTo check the assumption of normality, we can use a qq plot of the standardized residuals against the standard normal distribution.\n\nIf the points from the qq plot follows the line Y = X, then there is no evidence against the assumption.\nIf the residuals do show a trend off of the diagonal line, then we should worry about the assumption.\nIf the assumption is violated, the central limit theorem implies that significance tests used in OLS regression will be robust to violations of normality when sample sizes are large.\n\nPractically this means we can often ignore mild violations of normality when \\(N / K &gt; 30\\) (but other guidelines are used too).\nFor some specific types of violations, notably positive skew, it is also common to transform the Y variable (see Chapter 9).\nWhen we are worried that non-normality arises from a relatively small subset of the data that may be unduly influencing the results, this can be addressed through regression diagnostics (see Section 7.6) and robust statistics (which is an advanced topic).\n\nOh, and one last thing: The line in a qq plot is not a regression line!"
  },
  {
    "objectID": "ch7_assumption_checking.html#sec-worked-example-7",
    "href": "ch7_assumption_checking.html#sec-worked-example-7",
    "title": "7  Assumption checking",
    "section": "7.5 A worked example",
    "text": "7.5 A worked example\nTo illustrate the assumption checking procedures outlined above, let’s revisit the example from Section 6.2. In that example, it was noted that there was evidence that one (or more) of the assumptions were problematic. Let’s take a look at why this was the case.\nFor this example, we will use the ECLS data to regress reading achievement at the beginning of Kindergarten (c1rrscal) on SES (wksesl), parental (mother’s and father’s) education (wkmomed and wkdaded, respectively), and attendance in center-based care before K (p1center). This is model 3 from Section 6.2 (i.e., we don’t consider the model with the interactions).\nThe workflow for assumption checking requires first running the model and then producing the residual vs fitted plot and a qq plot of the residuals. If the plots look OK, we go ahead and interpret the model results. If the plots don’t look OK, we give up and wonder why we ever bothered with regression in the first place. Just kidding :) – the next two chapters of these notes address how to deal with violations of the linearity and normality assumptions, and we already know how to deal with heteroskedasticity from Section 7.3.1.\nAfter running the model, we obtain the following two plots from the regression output. For each of the three population assumptions of linear regression, please write down whether you think the assumption is problematic and explain why with reference to the plots. Again, the purpose of this exercise is for you to think about how to interpret the plots with respect to the assumptions. I am looking for you to be explicit about how you reason from the plots to your conclusions. I am less interested in the conclusions per se, as this is something that requires practice to get right.\n\n\nCode\n# Clean up and load data\n# rm(list = ls())\n# load(\"ECLS2577.RData\")\n# attach(ecls)\n\n# Run model\nmod5 &lt;- lm(c1rrscal ~ factor(p1center) + wksesl + wkmomed + wkdaded, data = ecls)\n\n# Check assumptions\npar(mfrow = c(1,2))\nplot(mod5, 1)\nplot(mod5, 2)\n\n\n\n\n\nTo foreshadow the next couple of chapters, here is what the plots looked like after dealing with positive skew of the residuals and the non-normality of the regression line:\n\n\nCode\n# Run model\nlog_c1rrscal &lt;- log(c1rrscal - min(c1rrscal) + 1)\nwksesl_sq &lt;- wksesl^2\nmod6 &lt;- lm(log_c1rrscal ~ factor(p1center) + wksesl + wksesl_sq + wkmomed + wkdaded)\n\n# Check assumptions\npar(mfrow = c(1,2))\nplot(mod6, 1)\nplot(mod6, 2)\n\n\n\n\n\nNeither assumption has been perfectly addressed, and, in particular, it looks like we may have over-corrected the positive skew and ended up with some negative skew. Also note that the data continue to exhibit heteroskedasticity, which is apparent from looking at the residual vs fitted plot in the range -1 to -3 of the residuals. Because heteroskedasticity is still an issue for these data, we can improve on the analysis reported in Section 6.2 by using HC standard errors rather than “regular” OLS standard errors. The difference between the two approaches is illustrated below. Remember, the estimates stay the same, but the SE’s change (and consequently the t-tests and p-values).\n\nRegular SE (same as Model 3 in Section 6.2):\n\n\n\nCode\nsummary(mod6)\n\n\n\nCall:\nlm(formula = log_c1rrscal ~ factor(p1center) + wksesl + wksesl_sq + \n    wkmomed + wkdaded)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.59699 -0.29882  0.04381  0.37969  1.86787 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)       -2.7941636  1.6307966  -1.713  0.08791 . \nfactor(p1center)2 -0.1232061  0.1030926  -1.195  0.23321   \nwksesl             0.1758849  0.0665763   2.642  0.00878 **\nwksesl_sq         -0.0016162  0.0006514  -2.481  0.01378 * \nwkmomed            0.0698645  0.0388073   1.800  0.07305 . \nwkdaded            0.0506601  0.0364953   1.388  0.16636   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6327 on 244 degrees of freedom\nMultiple R-squared:  0.2252,    Adjusted R-squared:  0.2093 \nF-statistic: 14.18 on 5 and 244 DF,  p-value: 3.474e-12\n\n\n\nHC SE\n\n\n\nCode\nlmtest::coeftest(mod6, car::hccm(mod6))\n\n\n\nt test of coefficients:\n\n                     Estimate  Std. Error t value Pr(&gt;|t|)  \n(Intercept)       -2.79416359  1.69074543 -1.6526  0.09969 .\nfactor(p1center)2 -0.12320606  0.10855871 -1.1349  0.25752  \nwksesl             0.17588489  0.06776405  2.5955  0.01002 *\nwksesl_sq         -0.00161620  0.00065155 -2.4806  0.01379 *\nwkmomed            0.06986450  0.03806165  1.8356  0.06764 .\nwkdaded            0.05066012  0.03077238  1.6463  0.10099  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this example, the HC SE’s didn’t lead to substantively different inferences, but we should report the results with HC SE’s based on the assumption checking."
  },
  {
    "objectID": "ch7_assumption_checking.html#sec-diagnostics-7",
    "href": "ch7_assumption_checking.html#sec-diagnostics-7",
    "title": "7  Assumption checking",
    "section": "7.6 Diagnostics*",
    "text": "7.6 Diagnostics*\nThis section is optional and is currently under construction (i.e., many typos). It focuses on walking through the code so it is recommended to treat this like an Exercises section and scroll to the top of the page, click on the “&lt;/&gt; Code” menu, then select “Show All Code.”\nLike assumption checking, regression diagnostics also make extensive use of regression residuals, but this time the objective is to identify individual data points that are “outliers” with respect to the model. To work through the basic concepts of regression diagnostics, let’s again use the Anscombe’s quartet.\n\n\nCode\n# Plotting Anscombe's quartet\nattach(anscombe)\npar(mfrow = c(2,2))\nymax &lt;- max(anscombe[,5:8])\nymin &lt;- min(anscombe[,5:8])\nxmax &lt;- max(anscombe[,1:4])\nxmin &lt;- min(anscombe[,1:4])\n\n\nplot(x1, y1, col = \"#4B9CD3\", xlim = c(xmin, xmax), ylim = c(ymin, ymax))\nabline(lm(y1 ~ x1))\n\nplot(x2, y2, col = \"#4B9CD3\", xlim = c(xmin, xmax), ylim = c(ymin, ymax))\nabline(lm(y2 ~ x2))\n\nplot(x3, y3, col = \"#4B9CD3\", xlim = c(xmin, xmax), ylim = c(ymin, ymax))\nabline(lm(y3 ~ x3))\n\nplot(x4, y4, col = \"#4B9CD3\", xlim = c(xmin, xmax), ylim = c(ymin, ymax))\nabline(lm(y4 ~ x4))\n\n\n\n\n\nThe interesting thing about these four examples is that they all have the same univariate and bivariate summary statistics – e.g., the same mean, variance, covariance, correlation, and regression coefficients. But first example is the only one that would be suitable for analysis by using these statistics. The second example shows a non-linear relationship, which we addressed in Section @ref(linearity-8). In this section we will focus on the last two examples, since they have clear outliers.\n\n7.6.1 Leverage\nLeverage describes how “unusual” a data point is on the X variable(s) – i.e., how far from the mean it is on each predictor. The function hatvalues computes the leverage for each data point. Let’s check out the leverage for the 3rd and 4th examples from Anscombe’s quartet.\n\n\nCode\n# leverage for Anscombe 3\nanscombe3 &lt;- lm(y3 ~ x3)\nleverage3 &lt;- hatvalues(anscombe3)\n\n# Take a look at the leverage values for each data point\nleverage3\n\n\n         1          2          3          4          5          6          7 \n0.10000000 0.10000000 0.23636364 0.09090909 0.12727273 0.31818182 0.17272727 \n         8          9         10         11 \n0.31818182 0.17272727 0.12727273 0.23636364 \n\n\nCode\n# Show the leverage values in the scatter plot using the function \"text\"\npar(mfrow = c(1,2))\n\nplot(x3, y3, col = \"white\", xlim = c(xmin, xmax), ylim = c(ymin, ymax), main = \"Leverage for Anscombe 3\")\nabline(lm(y3 ~ x3))\n\ntext(y3 ~ x3, labels = round(leverage3, 2), col = \"#4B9CD3\")\n\n# leverage for Anscombe 4\nanscombe4 &lt;- lm(y4 ~ x4)\nleverage4 &lt;- hatvalues(anscombe4)\n\n# Take a look at the leverage values for each data point\nleverage4\n\n\n  1   2   3   4   5   6   7   8   9  10  11 \n0.1 0.1 0.1 0.1 0.1 0.1 0.1 1.0 0.1 0.1 0.1 \n\n\nCode\n# Show the leverage values in the scatter plot using the function \"text\"\nplot(x4, y4, col = \"white\", xlim = c(xmin, xmax), ylim = c(ymin, ymax),  main = \"Leverage for Anscombe 4\")\nabline(lm(y4 ~ x4))\n\ntext(y4 ~ x4, labels = round(leverage4, 2), col = \"#4B9CD3\")\n\n\n\n\n\nRecall from the lesson that\n\nh should be smaller (closer to 0) for values closer to the mean of X\nThe maximum value of h is 1\n\nBased on the plots, we can see that the largest leverage is for the outlier in Anscombe 4.\n\n\n7.6.2 Distance (residuals)\nDistance is about the size of the residuals. In order to judge the size of a residual, it helps to use the (externally) studentized residuals rather than the “raw” residuals. Because the studentized residual have a t-distribution on \\(N - K - 2\\) degrees of freedom, a rough ballpark for interpreting studentized residuals is that\n\nValues around +/- 2 are considered large.\nValues beyond +/- 3 are considered very large.\n\nLet’s see what we have for our examples:\n\n\nCode\n# Distance for Anscombe 3\ndistance3 &lt;- rstudent(anscombe3)\n\npar(mfrow = c(1,2))\nplot(x3, distance3, main = \"Leverage for Anscombe 3\",  col = \"#4B9CD3\")\n\n# Distance for Anscombe 4\ndistance4 &lt;- rstudent(anscombe4)\nplot(x4, distance4, main = \"Leverage for Anscombe 4\",  col = \"#4B9CD3\")\n\n\n\n\n\nClearly, the notion of distance is useful for describing what the problem is with Anscombe’s 3rd example. For the 4th example, the outlying data point is omitted because it has leverage of exactly 1, which means that the studentized residuals are undefined (divide by zero; R notes this in the console).\n\n\n7.6.3 Influence\nInfluence describes how much the model results would change if a data point were omitted. Roughly, the conceptual relationships among influence, distance, and leverage are given by the following equation:\n[ = ]\nThis equation tells us that, for a data point to have high influence, it must be a large distance from the regression line (have a large residual) and have high leverage (be far away from the mean on \\(X\\)).\nThere are a number of ways of computing influence. Like externally studentized residuals, they are all deletion statistics, or statistics computed using a “leave-one-out” approach.\nInfluence statistics can also be classified into global versus local. Global approaches consider how a data point affects the predicted values. Local approaches consider how a data point affects the value of a specific regression coefficient.\nLet’s start with DFFITS and Cook’s distance, two measures of global influence.\n\n\nCode\n# DFFITS for Anscombe 3\nDFFITS3 &lt;- dffits(anscombe3)\n\npar(mfrow = c(1,2))\nplot(x3, DFFITS3, main = \"DFFITS for Anscombe 3\",  col = \"#4B9CD3\")\n\n# Distance for Anscombe 4\nDFFITS4 &lt;- dffits(anscombe4)\nplot(x4, DFFITS4, main = \"DFFITS for Anscombe 4\",  col = \"#4B9CD3\")\n\n\n\n\n\n\n\nCode\n# Cook's distance for Anscombe 3\nCooks3 &lt;- cooks.distance(anscombe3)\n\npar(mfrow = c(1,2))\nplot(x3, Cooks3, main = \"Cook's D for Anscombe 3\",  col = \"#4B9CD3\")\n\n# Distance for Anscombe 4\nCooks4 &lt;- cooks.distance(anscombe4)\nplot(x4, Cooks4, main = \"Cook's D for Anscombe 4\",  col = \"#4B9CD3\")\n\n\n\n\n\nThese statistics are similar but Cook’s distance is more interpetable. Values greater than 1 are considered indicative of high influence. We can see that the outlier in the 3rd example is highly influence. The outlier in the 4th example is “NA” because h = 1 hence there is a divide by zero problem.\nFor local measures of influence, the interpretation is roughly the same as global measures with simple regression (i.e., a single predictor). For multiple regression, local measures can provided additional insight to consider which regression coefficients are most influenced by an outlier.\n\n\nCode\n# DFBETAs distance for Anscombe 3\nDFBETA3 &lt;- dfbetas(anscombe3)\n\n#Take a look at the output: We get values for each coefficient, including the intercept\nDFBETA3 \n\n\n     (Intercept)            x3\n1  -4.625738e-03 -4.412673e-02\n2  -3.713338e-02  1.864368e-02\n3  -3.579096e+02  5.252677e+02\n4  -3.289981e-02 -1.737209e-18\n5   4.915510e-02 -1.172274e-01\n6   4.897424e-01 -6.674064e-01\n7   2.700082e-02 -2.088417e-02\n8   2.409027e-01 -2.089150e-01\n9   1.374342e-01 -2.313597e-01\n10 -1.970229e-02  1.342485e-02\n11  1.053656e-01 -8.740210e-02\n\n\nCode\n# Plots for the regression coefficients\npar(mfrow = c(1,2))\nplot(x3, DFBETA3[,\"x3\"], main = \"DFBETA for Anscombe 3\",  col = \"#4B9CD3\")\n\n# Distance for Anscombe 4\nDFBETA4 &lt;- dfbetas(anscombe4)\nplot(x4, DFBETA4[,\"x4\"], main = \"DFBETA for Anscombe 4\",  col = \"#4B9CD3\")\n\n\n\n\n\nThe output for DFBETA3 looks a lot like DFFITS3 (because we only have one predictor).\nFor DFBETA4, it is strange that our unusual data point (x4 = 18) was not identified as problematic – keep in mind that when this data point is removed, the variance of X is zero and so the regression coefficient for the leave-one-out model is not defined. Still, it is not clear why R reports the value as zero, rather than omitted.\n\n\n7.6.4 A more realistic example\nAs a more realistic example, let’s consider Question 3 from Assignment 1 using the graphical output from lm. Note that the graphical output uses the internally studentized residuals, and refers to these as “standardized residuals”.\n\n\nCode\nmodel &lt;- lm(c4rmscal ~ wksesl + t1learn, data = ecls)\n\n# Influence via Cook's distance\nplot(model, which = 4)\n\n\n\n\n\nWe can see that, although R automatically labels the 3 data points with the highest values of Cook’s D, none of the data points are actually close to the cut off value of 1. In other words, none of the data points in this example have an undue influence on the results of the regression model."
  },
  {
    "objectID": "ch7_assumption_checking.html#sec-workbook-7",
    "href": "ch7_assumption_checking.html#sec-workbook-7",
    "title": "7  Assumption checking",
    "section": "7.7 Workbook",
    "text": "7.7 Workbook\nThis section collects the questions asked in this chapter. The lesson for this chapter will focus on discussing these questions and then working on the exercises in Section 7.8. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 7.2\n\nPlease write down whether you think the linearity assumption is problematic for the example below, and be sure to explain why with reference to the figure. Keep in mind that interpreting plots takes a bit of practice and in general there is no “right” answer. Rather, what I am looking for is an explanation of why you think the assumption is problematic or not. Your explanation should refer to the interpretation of residual vs fitted plots, as outlined above.\n\n\n\nCode\nplot(mod2, which = 1)\n\n\n\n\n\nAn example from ECLS.\n\n\n\n\n\nFor each of the four examples below, please write down whether you think the linearity assumption is problematic and explain why with reference to the plots. Hint: be careful not to over-interpret the lowess line in the tails of the plots, where only a few data points can have a big impact on the local trend. Focus your interpretation on the bulk of the data, and whether it shows a systemic trend away from the horizontal line at 0.\n\n\n\nCode\n# Example: regression c1rmscal on ses_orig and t1learn\nset.seed(101)\npar(mfrow = c(2, 2))\nfor(i in 1:4) {\n  x &lt;- rnorm(200)\n  e &lt;- rnorm(200)\n  y &lt;- x + e \n  plot(lm(y ~ x), which = 1)\n}  \n\n\n\n\n\nMore examples.\n\n\n\n\nSection 7.3\n\nLet’s take another look at the four plots in the above figure. For each plot, please write down whether you think the homoskedasticity assumption is problematic and explain why with reference to the plot.\nThe following output shows the results for the heteroskedastic (cone-shaped) example data, using both the regular standard error and HC standard errors. Please note the differences between these two sets of output and write down any questions you have about their interpretation.\n\nExample:\n\n\nCode\nplot(mod4, which = 1)\n\n\n\n\n\nRegular SE:\n\n\nCode\n## Make sure the required packages are installed\n# install.packages(\"car\")\n# install.packages(\"lmtest\")\n\n# Regular SE: \nsummary(mod4)\n\n\n\nCall:\nlm(formula = y2 ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.9985 -1.0402 -0.0504  0.9252 11.5002 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.002712   0.154430  -0.018    0.986    \nx            0.968044   0.160719   6.023 6.13e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.441 on 248 degrees of freedom\nMultiple R-squared:  0.1276,    Adjusted R-squared:  0.1241 \nF-statistic: 36.28 on 1 and 248 DF,  p-value: 6.135e-09\n\n\nHC SE:\n\n\nCode\n# HC SE\n# Step 2. Use \"hccm\" to get the HC SEs for our piecewise model \nhcse &lt;- car::hccm(mod4)\n\n# Step 3. Use \"coeftest\" to compute t-tests with the HC SEs\nlmtest::coeftest(mod4, hcse)\n\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -0.0027123  0.1520067 -0.0178    0.9858    \nx            0.9680441  0.1908455  5.0724 7.701e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSection 7.4\n\nPlease write down whether you think the normality assumption is problematic for the data in the figure below, and be sure to explain why with reference to the plot. Hint: if you think the data are non-normal, you should be able to interpret the pattern of deviations (e.g. skew, kurtosis).\n\n\n\nCode\n# Example: regression c1rmscal on ses_orig and t1learn\nplot(mod2, which = 2)\n\n\n\n\n\nAn example from ECLS.\n\n\n\n\n\nIn each of the examples below, please write down whether you think the normality assumption is problematic and explain why with reference to the plots.\n\n\n\nCode\n# Example: regression c1rmscal on ses_orig and t1learn\nset.seed(101)\npar(mfrow = c(2, 2))\nfor(i in 1:4) {\n  x &lt;- rnorm(200)\n  e &lt;- rnorm(200)\n  y &lt;- x + e \n  plot(lm(y ~ x), which = 2)\n}  \n\n\n\n\n\nMore examples.\n\n\n\n\nSection 7.5\n\nFor each of the three population assumptions of linear regression, please write down whether you think the assumption is problematic and explain why with reference to the plots. Again, the purpose of this exercise is for you to think about how to interpret the plots with respect to the assumptions. I am looking for you to be explicit about how you reason from the plots to your conclusions. I am less interested in the conclusions per se, as this is something that requires practice to get right.\n\n\n\nCode\n# Clean up and load data\n# rm(list = ls())\n# load(\"ECLS2577.RData\")\n# attach(ecls)\n\n# Run model\nmod5 &lt;- lm(c1rrscal ~ factor(p1center) + wksesl + wkmomed + wkdaded, data = ecls)\n\n# Check assumptions\npar(mfrow = c(1,2))\nplot(mod5, 1)\nplot(mod5, 2)"
  },
  {
    "objectID": "ch7_assumption_checking.html#sec-exercises-7",
    "href": "ch7_assumption_checking.html#sec-exercises-7",
    "title": "7  Assumption checking",
    "section": "7.8 Exercises",
    "text": "7.8 Exercises\nThese exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)\nBefore staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\nThere isn’t much new in terms of R code in this chapter. Once we run a model with lm, we just call the plot function on the lm output to produce the graphics requires for assumption checking. This section shows these steps for the worked example in Section 6.2 and Section 7.5.\n\n\nCode\n# Clearn up env and load data \n#rm(list = ls())\n#load(\"ECLS2577.RData\")\n\n# Run model 3 from the example (all predictors, but no interactions)\nmod &lt;- lm(c1rrscal ~ factor(p1center) + wksesl + wkmomed + wkdaded, data = ecls)\n\n# Puts both plots in one figure\npar(mfrow = c(1,2)) \n\n# Check assumptions\nplot(mod, 1)\nplot(mod, 2)\n\n\n\n\n\nThe next bit of code shows how to adjust the statistical tests for heteroskedasticity using HC standard errors. In the next two chapters, we show how to address the linearity and normality assumption violations.\n\nRegular SE:\n\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = c1rrscal ~ factor(p1center) + wksesl + wkmomed + \n    wkdaded, data = ecls)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.943  -5.080  -1.531   3.049  54.466 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)         7.0118     5.3078   1.321    0.188\nfactor(p1center)2  -1.7608     1.3901  -1.267    0.206\nwksesl              0.2828     0.1724   1.640    0.102\nwkmomed             0.6685     0.5203   1.285    0.200\nwkdaded             0.2352     0.4935   0.477    0.634\n\nResidual standard error: 8.56 on 245 degrees of freedom\nMultiple R-squared:  0.1553,    Adjusted R-squared:  0.1415 \nF-statistic: 11.26 on 4 and 245 DF,  p-value: 2.109e-08\n\n\n\nHC SE\n\n\n\nCode\n## Make sure the required packages are installed\n# install.packages(\"car\")\n# install.packages(\"lmtest\")\n\n# Step 1: fit the model (see above)\n\n# Step 2: Use \"hccm\" to get the HC SEs \nhcse &lt;- car::hccm(mod)\n\n# Step 3. Use \"coeftest\" to compute t-tests with the HC SEs\nlmtest::coeftest(mod, hcse)\n\n\n\nt test of coefficients:\n\n                  Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)        7.01177    5.82938  1.2028   0.2302\nfactor(p1center)2 -1.76083    1.11866 -1.5740   0.1168\nwksesl             0.28282    0.20506  1.3792   0.1691\nwkmomed            0.66853    0.66797  1.0008   0.3179\nwkdaded            0.23520    0.46189  0.5092   0.6111\n\n\nAs discussed above, the interpretation for both sets of output is essentially the same, but by using the HC SEs we can be sure that our inferences are not unduly affected by the assumption of homoskedasticity."
  },
  {
    "objectID": "ch8_loglinear.html#sec-math-review-8",
    "href": "ch8_loglinear.html#sec-math-review-8",
    "title": "8  Log-linear regression",
    "section": "8.1 Math review",
    "text": "8.1 Math review\nLet’s start with a review the requisite math – logs and exponents. We are going to use these concepts later in the course as well, so it is worth sorting them out. If you haven’t seen logs or exponents in while, brace yourself. This fist section of the math review covers the basic interpretation of logs and their utility for reducing skewness. The following sections add some details.\nIn this math review, we use the lowercase symbols \\(x\\) and \\(y\\) for generic mathematical variables. These symbols do not correspond to the uppercase symbols \\(X\\) and \\(Y\\) in regression notation. So, e.g., \\(x\\) is not a predictor, its just a generic variable.\nThe function \\(\\log_{10}(x)\\) returns the power to which we need to raise 10 to get the value \\(x\\). It answers the question: \\(10^? = x\\).\nSome examples:\n\n\\(\\log_{10}(10) = 1\\), because \\(10^1 = 10\\)\n\\(\\log_{10}(100) = 2\\), because \\(10^2 = 100\\)\n\\(\\log_{10}(1000) = 3\\), because \\(10^3 = 1000\\)\n…\n\nIn Figure 8.1, the three values computed above are shown as vertical dashed lines. In general, the plots shows that for every order of magnitude that \\(x\\) increases (e.g., from 10 to 100, or from 100 to 1000), its log only increases by one unit. Intuitively, this means that “big differences” on \\(x\\) translate into “small differences” on \\(\\log(x)\\).\n\n\n\n\n\nFigure 8.1: Log base 10\n\n\n\n\nThis “compression” of large values down to a smaller scale turns out to be useful for dealing with positive skew, as illustrated in Figure 8.2. This is a main reason that the log transform is widely used – to reduce positive skew.\n\n\n\n\n\nFigure 8.2: Log transform and positive skew\n\n\n\n\n\n8.1.1 Natural logarithm\nThe symbol \\(\\log_{10}\\) is read “log base 10”. In statistics, we usually use a different base – log base \\(e\\). Here \\(e = 2.7182...\\) is an irrational number called Euler’s number. We will see later on that using \\(\\log_e\\) makes the interpretation of log-linear regression simpler. Note that in this context, \\(e\\) is not the residual from a regression model. Whether \\(e\\) denotes Euler’s number or regression residuals should be clear from context.\nRegardless of which base we use, logs have the same overall compression effect, as shown in Figure 8.3. For example, \\(\\log_{10}(1000) = 3\\) and \\(\\log_{e}(1000) \\approx 7\\) – both numbers are much smaller than \\(1000\\), which means that both functions achieve the same overall result of making big values into smaller values. The same is true when applied to real data – in fact, the right hand panel in Figure 8.2 was computed using \\(\\log_e\\).\n\n\n\n\n\nFigure 8.3: Log base 10 vs log base e\n\n\n\n\nLog base \\(e\\), or \\(\\log_e\\),is called the natural logarithm and often denoted “\\(\\ln\\)”. But \\(\\ln\\) can be hard to read so we will just stick with \\(\\log\\) and omit the base symbol when we mean \\(\\log_e\\). This is consistent with what R’s log function does – its default base is \\(e\\).\n\n\n8.1.2 log and exp\nRecall that our overall modeling strategy in log-linear regression is “transform\\(\\rightarrow\\)analyze\\(\\rightarrow\\)reverse- transform”. In mathematics, the reverse transform is called the inverse of the original function. The inverse of the logarithm is exponentiation. So, if we start by log-transforming our \\(Y\\)-variable, we are going to end up doing exponents later on.\nTo see how \\(\\log\\) and \\(\\exp\\) work together, let’s start with the number \\(x= 100\\), and take the log of that number:\n\\[ \\log(100) =  4.60517 \\] This equation tells us that if we raise \\(e\\) to the power of \\(4.60517\\) the result will be \\(100\\). We can think of \\(x = 100\\) as our original data and \\(y = 4.60517\\) as our transformed value.\nBy definition of the logarithm, we can equivalently write \\(\\log(100) = 4.60517\\) as\n\\[e^{4.60517} = 100. \\] This equation uses exponents (raise \\(e\\) to a power) rather than logs, but it communicates the same information as the previous equation. In our second equation, we take the output of the log (\\(y = 4.60517\\)) and use it as input to the exponent. The exponent then returns the original value of \\(x = 100\\).\nThe mathematical symbol \\(\\exp(y)\\) is often used instead of \\(e^y\\) to avoid having to write complicated expressions in the superscript. So we can more clearly write the above equation as\n\\[\\exp(4.60517) = 100. \\] This is just a change of notation so we don’t have to typeset superscripts.\nTo summarize: the two equations presented below are equivalent by definition:\n\\[ \\log(100) =  4.60517 \\quad \\text{and} \\quad \\exp (4.60517 ) = 100.\\]\nIn the first one, we take the input value \\(x = 100\\) and transform it using the logarithm, which gives us the transformed value \\(y = 4.60517\\). In the second equation, we take the transformed value \\(y = 4.60517\\) as input into the exponent, which gives us back the original value of \\(x = 100\\).\nIn general, the relationship between the two functions is:\n\\[\\log(x) = y \\quad  \\text{and} \\quad \\exp (y) = x. \\]\n\n\n8.1.3 Pop quiz\nI’ll ask some questions like the following to start class off. Please write down your answers and be prepared to share them in class. You check your answers by pasting the questions into \\(R\\) or Google’s search bar.\n\n\\(\\log(2.7182) = ?\\)\n\\(\\log(1) = ?\\)\n\\(\\log(0) = ?\\)\n\\(\\log(-1) = ?\\)\n\\(\\exp(1) = ?\\)\n\\(\\exp(0) = ?\\)\n\\(\\exp(-1) = ?\\)\nWhat is larger, \\(\\log(10)\\) or \\(\\exp(10)\\)?\n\\(\\log(\\exp(x)) = ?\\)\n\\(\\exp(\\log(x)) = ?\\)\n\n\n\n8.1.4 Rules for log and exp*\nThere are some rules for working with logs and exponents that we will use later on to derive some results about the interpretation of log-linear regression. You don’t need to know these but you might find them useful.\n\nAddition with logs: \\[\\log(m) + \\log(n) = \\log(mn) \\]\nMultiplication with exponents:\n\\[ \\exp(m) \\times \\exp(n) = \\exp(m + n) \\]"
  },
  {
    "objectID": "ch8_loglinear.html#the-log-linear-model",
    "href": "ch8_loglinear.html#the-log-linear-model",
    "title": "8  Log-linear regression",
    "section": "8.2 The log-linear model",
    "text": "8.2 The log-linear model\nThere are a various ways to apply log-transformations in regression analysis, which are outlined in the following table and discussed in more detail in (cite-fox?). The focus of this chapter is log-linear regression, but the overall approach to linear-log and log-log is the similar. A worked example is coming up in Section 9.2. (Note that the term “log-linear” is also the name of a family of models for contingency table data – that is not what we are talking about here.)\n\n\n\n\n\nFigure 8.4: Logs in regression analysis\n\n\n\n\nBefore getting into the worked example, recall that a main theme of this chapter is that transforming the \\(Y\\) variable in a regression has three interrelated consequences:\n\nChanging the distribution Y variable / residuals\nChanging the (non-)linearity of its relationship with other variables\nChanging interpretation of the regression coefficients\n\nThe next few sections address each of these points in more detail."
  },
  {
    "objectID": "ch8_loglinear.html#sec-distribution-8",
    "href": "ch8_loglinear.html#sec-distribution-8",
    "title": "8  Log-linear regression",
    "section": "8.3 Distribution of \\(\\log(Y)\\)",
    "text": "8.3 Distribution of \\(\\log(Y)\\)\nBelow are some examples of the log transform applied to positively skewed data. We can see a range of results, from “wow, that definitely worked” in the first example to, “better, but still not great” in the second two examples. These are realistic reflections of how a log transform works to address skew.\n\n\n\n\n\nFigure 8.5: Example 1: Log transforming ECLS math\n\n\n\n\n\n\n\n\n\nFigure 8.6: Example 2: Log transforming ECLS reading\n\n\n\n\n\n\n\n\n\nFigure 8.7: Example 3: Log transforming hourly wages\n\n\n\n\n\n8.3.1 Why \\(\\log(x+1)\\)?\nThere are a few things to note about this approach to correcting positive skew. First, you might have nocited that we are computing \\(\\log(x + 1)\\) instead of \\(\\log(x)\\). This is because the log function has “weird” behavior for values of \\(x\\) between 0 and 1. These values of \\(x\\) were not shown in Figure 8.1 and Figure 8.3, but they are shown below in Figure 8.8.\n\n\n\n\n\nFigure 8.8: Logs for small values of X\n\n\n\n\nWe can see that \\(\\log(1) = 0\\), which is true regardless of the base (i.e., \\(b^0 = 1\\) for all choices of the \\(b\\)). But for values of \\(x &lt; 0\\), \\(\\log(x)\\) goes to negative infinity. This is because of how negative exponents are defined:\n\\[ b^{-x} = \\frac{1}{b^x}. \\]\nThe upshot for log-linear regression is as follows: if your \\(Y\\) variable takes on values in the range \\((0, 1)\\), the log transform is going to change those values into large negative numbers. This can result in negative skew, rather fixing your positive skew.\n?fig-skew4 provides an example. The top row shows is the example as Figure 8.5 above, (math achievement in ECLS), and the bottom shows the same variable but transformed to proportion correct. In the left-hand panels we can see that the distribution of the proportion is qualitatively the same the as the distribution of the original variable. However, the log transform behaves differently for these two cases. Why? Because logs are “weird” on the range \\((0, 1)\\).\n\n\nCode\npar(mfrow = c(2,2))\nprop_c1rmscal &lt;- (c1rmscal - min(c1rmscal))/100\nhist(c1rmscal, col = \"#4b9cd3\")\nhist(log(c1rmscal + 1), col = \"#4b9cd3\")\nhist(prop_c1rmscal, col = \"#4b9cd3\")\nhist(log(prop_c1rmscal), col = \"#4b9cd3\")\n\n\n{#fig-skew4, fig-align=‘center’ width=672}\n\n\nTo avoid this situation we can add 1 to a variable before taking its log. Technically, there is no reason to do this if the variable does not take on values in the range 0 to 1, but I always apply the “add 1” rule just so I don’t have to worry about it.\n\n\n8.3.2 What about negative values of \\(x\\)?\nYou may have noted that the “add 1” rule doesn’t cover cases where the variable can take on negative values. The problem here is that there is no way to turn a positive base \\(b\\) into a negative number through exponentiation - i.e., \\(\\log(x)\\) is undefined whenever \\(x &lt; 0\\).\nTo address situations where you want to log transform a variable that can take on negative values, we can use the transformation\n\\[ \\log(x - \\text{min}(x) + 1). \\]\nThe following table shows how this transformation works. The first column shows the original data, the second column shows how subtracting the minimum makes all of the negative numbers positive, and the third column shows how adding 1 makes all of the numbers less than 1 equal to or greater than 1.\n\n\n\n\n\nDealing with negative values of x in log transforms\n\n\n\n\nThe take home message: If we have negative values of \\(x\\), before we apply the log transform we need to:\n\nStep 1. “Add” the minimum value of \\(x\\) to prevent undefined values of log\nStep 2. Add 1 to make sure no values are less than 1\n\nCan you think of a log transformation that would work to address data that were negatively skewed, rather than positively skewed? Hint: the trick is similar to the one in this section\n\n\n8.3.3 Skew versus censoring\nIt is important to distinguish between skew and floor (or ceiling) effects. Floor and ceiling effects are collectively known as “censoring”. While transforming you data can help with skew, it isn’t going to do anything about censoring, so it is important to be able to tell the difference.\nCensoring can look like extreme skew, but is importantly different from skew because a large proportion of the observations have exactly the same value. An example is shown in Figure 8.9 below, which shows a floor effect in which a large proportion of the cases have a values of 0 (“are censored at zero”). Note that the log transformation does not do anything about the censored data. In general, if you transform a “heap” of values, then the same heap shows up in the transformed data.\nTo deal with censored data, we need to consider alternatives to linear regression (e.g., tobit regression). We won’t cover regression for censored data in this course. For our purposes, the moral is: don’t mistake censoring for skew, because they don’t have the same solution.\n\n\n\n\n\nFigure 8.9: A floor effect is not the same as positive skew\n\n\n\n\n\n\n8.3.4 Final comments on dealing with skew\nRecall that the assumptions of linear regression are not about the \\(Y\\) variable itself, but about the residuals (Section 7.1). So, a skewed outcome variable is not necessarily a problem. In particular, it is entirely possible that \\(Y\\) is positively skewed but the residuals are not, especially if one of the \\(X\\) variables has a similar distribution to \\(Y\\). So, in practice, you should not overly worry about non-normality of \\(Y\\) – its the residuals of your regression model that matter.\nSecond, even if the residuals are non-normal, this still doesn’t matter very much with very large samples. The central limit theorem assures us that, as the sample size gets larger, the sampling distribution of the regression coefficients converges to a normal distribution, regardless of the distribution of the residuals.\n\n\n\n\n\nThe moral of this section is that, if you are considering log-transforming an outcome variable due to skew, you should keep in mind:\n\nThe problem may not really need to be fixed:\n\nCheck the residuals!\nIf you have sufficient sample size, mild to moderate violations of normality are not really an issue.\n\nThe log-transformation has other important implications for you model, which we discuss next."
  },
  {
    "objectID": "ch8_loglinear.html#sec-relationship-8",
    "href": "ch8_loglinear.html#sec-relationship-8",
    "title": "8  Log-linear regression",
    "section": "8.4 Relationship with \\(X\\)",
    "text": "8.4 Relationship with \\(X\\)\n\nIn addition to affecting normality,(non-linear) transformations of the \\(Y\\) variable necessarily affect the linearity of regression. In particular, if \\(Y\\) is linearly related to \\(X\\), then \\(\\log(Y)\\) cannot also be linearly related to \\(X\\).\nStated more formally, if\n\\[ Y = a + bX + \\epsilon \\]\nthen\n\\[\n\\begin{align}\n\\log(Y) & = \\log(a + bX + \\epsilon) \\\\\n& \\neq a + bX + \\epsilon. \\\\\n\\end{align}\n\\]\nIn practice, this means that addressing positive skew using a log transform can sometimes create more problems than it solves. Figure 8.10 illustrates this issue. In the top row, we see the residual versus fitted plot and qqplot of residuals for the regression of Math Achievement (c1rmscal) on SES (wksesl). The linear regression resulted in a clear violation of normality (positive skew). In the bottom row, we see same output after log transforming Math Achievement – it addressed the issue with normality, but also made the nonlinearity more apparent.\n\n\n\n\n\nFigure 8.10: Nonlinear vs non-normality when log transforming Y\n\n\n\n\nThe take home message is that when we transform \\(Y\\) (or \\(X\\)) we necessarily affect the relationship between the variables, as well as their individual distributions. It is often the case that when we try to “fix” the normality of the \\(Y\\) variable, we can inadvertently make its relationship to \\(X\\) more non-linear.\nAs we discuss in the next chapter, we can address non-linearity of regression by transforming the \\(X\\) variable(s). But we should be sure that the problem we are trying to solve (normality) isn’t worse than the the problem we end up with (non-linearity)."
  },
  {
    "objectID": "ch8_loglinear.html#sec-interpretation-8",
    "href": "ch8_loglinear.html#sec-interpretation-8",
    "title": "8  Log-linear regression",
    "section": "8.5 Interpretation",
    "text": "8.5 Interpretation\nThe final point about the log-linear model is how it affects the interpretation of the model parameters. For example, in linear regression we know that a one-unit increase in \\(X\\) leads to a \\(b\\)-unit increase in \\(\\hat Y\\). But how do we interpret a \\(b\\)-unit increase in \\(\\log(Y)\\)?\nIn general, one of the main shortcomings of “transforming your data” is that it can lead to uninformative or unclear interpretations of model parameters. If the results don’t make sense, its not much of a consolation to know that the model assumptions looked OK!\nHowever, it turns out that there is very nice interpretation of the log-linear model. It is because of this interpretation that we often use the log transform to address skew in \\(Y\\), rather than some other transformation.\nThere are two ways to interpret the regression coefficients in the log-linear model. First, we consider the easy but approximate way (you’re welcome!). Second, we consider the more complicated but accurate way, which involves reverse transforming the regression coefficients using exponentiation (see Section 8.1).\n\n8.5.1 Approximate interpretation\nIn a log-linear model, the regression slope can be interpreted as the approximate proportionate change in \\(Y\\), in the original units. So, in this equation,\n\\[ \\log(Y) = a + bX + \\epsilon \\tag{8.1}\\]\nwe can interpret \\(b\\) as the proportionate change in \\(Y\\) associated with a one unit increase in \\(X\\). This should seem like surprising: \\(Y\\) is not the outcome variable – \\(\\log(Y)\\) is. But, we can still interpret the model with respect to \\(Y\\)!\nBelow are some numerical examples illustrating how to interpret the regression slope in Equation 8.1:\n\nif \\(b = .25\\), then \\(Y\\) is expected to increase by 25% when \\(X\\) increases by 1 unit.\nif \\(b = 0\\), then \\(Y\\) is expected to be the same when \\(X\\) increases by 1 unit (i.e., no relationship)\nif \\(b = -.45\\), then \\(Y\\) is expected to decrease by 45% when \\(X\\) increases by 1 unit.\n\nIt is important to note that this interpretation is approximate and it only applies when \\(|b|\\) is small (e.g., \\(|b|&lt; .5\\)). We will see how the approximation works and how to deal with other values of \\(b\\) when we get to the exact interpretation in Section 8.5.3.\nBefore moving on, let’s get clear on what “proportionate change” means. Consider two values of a variable. We let \\(Y\\) denote the original value and \\(Y^*\\) denote a new value. The “change” is the difference \\(Y^* - Y\\). Treating the change as a proportion of the original value \\(Y\\) gives us the proportionate change:\n\\[ \\text{proportionate change} = \\frac{Y^* - Y}{Y} = Y^*/Y - 1.\\]\nMultiplying the proportionate change by 100 gives us the percentage change, which is usually easier to talk about in a sentence (the numerical examples listed above used percentage change). If we want to talk about the change without specifying the units (e.g, proportion or percentage), we say “relative change.”\nNote that relative change is not the same as relative magnitude. \\(Y'/Y\\) is the magnitude of \\(Y^*\\) relative to \\(Y\\). As shown in the above equation for proportionate change, we need to subtract 1 from relative magnitude to get relative change.\nFor example, say I loan \\(Y = 100\\) dollars to a friend and they pay me back \\(Y^* = 150\\) dollars. The amount my friend paid me back is 1.5 times, or 150%, of what I initially lent them:\n\\[ Y^*/Y = 150/100 = 1.5 \\]\nThis is the relative magnitude of the two amounts.\nAlternatively, we could say that I have gained 50%, or 1/2, over the initial amount:\n\\[ Y^*/Y - 1 = 150/100 - 1 = .5 \\] This is relative change.\nAs an example of relative loss rather than gain, assume \\(Y^* = 40\\) and \\(Y = 100\\). Then \\(Y^*\\) is 40% of \\(Y\\) (relative magnitude) and the relative change (decrease) from 100 to 40 is 60%.\nHere are some more examples. Please write down your answers and be prepared to share them in class.\n\nHow many times larger is \\(Y^* = 6\\) than \\(Y = 2\\)?\nWhat is the relative increase from \\(Y = 2\\) to \\(Y^* = 6\\)?\nWhat is the percent increase from \\(Y = 200\\) to \\(Y^* = 600\\)?\nWhat is the percent decrease from \\(Y = 600\\) to \\(Y^* = 200\\)?\nWhat is the percent change from \\(Y = 1\\) to \\(Y^* = 1\\)?\n\nThe answers are hidden below and can be viewed by pressing the “code” button. But please keep in mind that you won’t learn anything by viewing the answers before trying the questions.\n\n\nCode\n# 1. 6 is 3 times larger than 2, or 300% percent larger than 2 (6/2 = 3)\n# 2. 6 is a 200% increase from 2 (6/2 - 1 = 2)\n# 3. 600 is a 200% increase from 200 \n# 4. 200 is a 66% decrease from 600\n# 5. 1 is a zero percent increase from 1 (but 100% as large as 1!)\n\n\n\n\n8.5.2 A hypothetical example\nThis section provides a hypothetical example comparing the interpretation of linear regression and log-linear regression. We will work though the example in class, so please write down your answers to the questions in this section and be prepared to share your responses.\nTo get a better idea of how the interpretation of log-linear regression differs from the interpretation of linear regression, let’s consider the following variables:\n\n\\(Y\\) is wages (dollars per week)\n\\(X\\) is hours worked per week\n\nA linear relationship between \\(Y\\) and \\(X\\) could be written\n\\[ \\widehat Y = a + bX. \\]\nLet’s further assume that \\(b = 20\\). With this information, you should be able to work out answers to the following two questions pretty easily:\n\nFor someone who is expected to earn $100/week, what would be their predicted earnings if they worked one hour more per week?\nWhat about for someone who was expected to earn $1000/week?\n\nNow let’s consider a log-linear model:\n\\[ \\log(\\widehat Y) = a + bX. \\] This time we will assume the regression coefficient is \\(b = .2\\). Using this log-linear model, please answer the following questions:\n\nFor someone who is expected to earn $100/week, what would be their predicted earnings if they worked one hour more per week? To answer this question, use the interpretation of \\(b\\) in terms of proportionate change, as outlined in the previous section.\nWhat about for someone who was expected to earn $1000/week?\n\nThe answers are hidden below. You can use the “code” button to see them, but please try to answer on your own first.\n\n\nCode\n# 1. 120 dollars per week\n# 2. 1020 dollars per week\n# 3. 120 dollars per week\n# 4. 1200 dollars per week\n\n\nThis example shows that, while the linear model represents change in fixed units (e.g., dollars), the log-linear model represents change that is relative to an initial value of \\(Y\\). This is particularly useful in applications involving monetary outcomes, where it has been argued that people’s “subjective evaluation” of money is more like the latter than the former.\nThe contrast between “fixed units” and “subject value” interpretions can be illustrated as follows.\n\nAt $100/week, $20 for an additional hour is equivalent to a 20% increase in wages for only a 2.5% (1/40) increase in hours worked.\nAt $1000/week, $20 is a 2% increase in wages, which is less than the 2.5% increase in hours worked.\n\nThus, the same fixed amount, $20, doesn’t have the same “subject value” for someone making $100/week as compared to someone making $1000/week. On the other hand, a 20% increase for both people might be considered as having equal subjective value. On this logic, the person making $1000/ week would need a $200 increase in order experience the same subject value as the $20 increase provides to the person making $100/ week.\nSome real life examples of this kind of interpretation are linked below.\n\nSliding scales for parking, car pool / day care late pick-up, bail and fines, https://www.theatlantic.com/business/archive/2015/03/finland-home-of-the-103000-speeding-ticket/387484/\nProportional tuition based on income and number of dependents. https://ronclarkacademy.com/tuition-and-financial-aid\n\n\n\n8.5.3 Exact interpretation\nIf we want an exact interpretation of the regression coefficients in a log-linear model, we need to do some “follow-up math” with exponents. The main result is about the relative magnitude of \\(\\hat Y\\) corresponding to a one-unit change in \\(X\\)\n\\[ \\frac{\\widehat Y(X + 1)}{\\widehat Y(X)} = \\exp(b)\n\\tag{8.2}\\]\nwhere \\(\\widehat Y(X)\\) is the predicted value of \\(Y\\) for a given value of \\(X\\). This is result derived in Section 8.5.5 below (optional).\nBased on Equation 8.2, we can interpret the exponentiated regression coefficient in a log-linear model in terms of relative magnitude. In practice, people often prefer to use relative change instead. From Equation 8.2 and the definition of proportionate change in Section 8.5.1, it follows that the proportionate change in \\(\\widehat Y\\) for a one-unit increase in \\(X\\) is exactly equal to \\(\\exp(b) - 1\\).\nIn the hypothetical example above (wages and hours worked), the exact proportion change in the log-linear model is\n\\[ \\exp(.2) - 1 = 1.2214 - 1= .2214 \\]\nor about 22%.\n\n\n8.5.4 Summary\nWhile there are many transformations that can be used to address skew, the log-linear model has the advantage that it is still interpretable in terms of the original \\(Y\\) variable. In fact, in many applications, notably those in which \\(Y\\) is monetary, the interpretation is arguably better than for linear regression!\nThere are two ways to interpret the regression coefficients in the log-linear model.\n\nThe approximate interpretation: a one-unit change in \\(X\\) is expected to yield a \\((b \\times 100)\\%\\) change in \\(Y\\). This interpretation only applies when \\(b\\) is small (e.g., less than .5 in absolute value).\nThe exact interpretation: a one-unit change in \\(X\\) is expected to yield a \\((\\exp(b) - 1) \\times 100\\%\\) change in \\(Y\\), and this applies for any value of \\(b\\).\n\n\n\n8.5.5 Derivation of Equation 8.2 *\nStart with the log-linear model, exponentiate both sides, and then use the multiplication rule for exponents:\n\\[\\begin{align}\n\\log(\\widehat Y) & = a + bX \\\\\n\\implies & \\\\\n\\exp(\\log(\\widehat Y)) & = \\exp(a + bX) \\\\\n\\implies & \\\\\n\\widehat Y & = \\exp(a)\\exp(bX) \\\\\n\\end{align}\\]\nThen do the same thing for \\(X+1\\) in place of \\(X\\):\n\\[\\begin{align}\n\\log(\\widehat Y^*) & = a + b(X + 1) \\\\\n\\implies & \\\\\n\\widehat Y^*& = \\exp(a)\\exp(bX)\\exp(b) \\\\\n\\end{align}\\]\nFinally, take the ratio of \\(\\widehat Y^*\\) to \\(\\widehat Y\\):\n\\[ \\frac{\\widehat Y^*}{\\widehat Y} = \\frac{\\exp(a)\\exp(bX)\\exp(b)}{\\exp(a)\\exp(bX)} = \\exp(b) \\]\nThis gives Equation 8.2.\nThe reason that the approximate interpretation in Section 8.5.1 works is because\n\\[ \\exp(b) - 1 \\approx b \\]\nfor \\(|b| &lt; .5\\). This is illustrated in the graph below, where the blue line is \\(\\exp(b)-1\\) and black line is \\(b\\). We can see that the two lines are pretty close over the range (-.5, .5).\n\n\n\n\n\nFigure 8.11: exp(b) - 1 (blue) versus b (black)"
  },
  {
    "objectID": "ch8_loglinear.html#sec-worked-example-8",
    "href": "ch8_loglinear.html#sec-worked-example-8",
    "title": "8  Log-linear regression",
    "section": "8.6 Worked example",
    "text": "8.6 Worked example\nThis section works through log-linear regression using a new example, the Wages.Rdata data (source: Weinberg & Abramowitz (2017) Statistics using Stata: An integrative approach. Cambridge University Press). The dataset was collected in 1985 from \\(N = 400\\) respondents and contains the following variables.\n\n\nCode\n# Load the data and take a look\nload(\"Wages.RData\")\nknitr::kable(head(wages))\n\n\n\n\n\neduc\nsouth\nsex\nexper\nwage\noccup\nmarr\ned\n\n\n\n\n12\n0\n0\n17\n7.50\n6\n1\n2\n\n\n13\n0\n0\n9\n13.07\n6\n0\n3\n\n\n10\n1\n0\n27\n4.45\n6\n0\n1\n\n\n9\n1\n0\n30\n6.25\n6\n0\n1\n\n\n9\n1\n0\n29\n19.98\n6\n1\n1\n\n\n12\n0\n0\n37\n7.30\n6\n1\n2\n\n\n\n\n\nWe will focus on regressing hourly wage (wage) on years of education (educ). The distribution of the outcome variable and its relationship with the predictor are depicted below.\n\n\nCode\nattach(wages)\npar(mfrow = c(1,2))\nhist(wage,  col = \"#4B9CD3\")\nplot(educ, wage, col = \"#4B9CD3\")\n\n\n\n\n\nThis example is a good candidate for log-linear regression because\n\nwages is positively skewed\nIts relationship with educ appears somewhat exponential\nThe log-transform provides a suitable interpretation when the outcome is monetary\n\nMoreover, a quick a look at the linear regression model (below) shows that all of the model assumptions are violated. So, time to try something other than regular regression!\n\n\nCode\nmod1 &lt;- lm(wage ~ educ)\n\n# Check out model fit\npar(mfrow = c(1,2))\nplot(mod1, 1, col = \"#4B9CD3\")\nplot(mod1, 2, col = \"#4B9CD3\")\n\n\n\n\n\n\n8.6.1 Log-linear regression of Wages on Education\nTo run a log-linear model, we can transform the \\(Y\\)-variable (wages) using the log function, and then run the model as usual. The diagnostic plots are shown below.\n\n\nCode\n# Create log transform of wage\nlog_wage &lt;- log(wage + 1)\n\n# Regress it on educ\nmod2 &lt;- lm(log_wage ~ educ)\n\n# Check out model fit\npar(mfrow = c(1,2))\nplot(mod2, 1, col = \"#4B9CD3\")\nplot(mod2, 2, col = \"#4B9CD3\")\n\n\n\n\n\nWe can see that the log transformation helped with positive skew, although didn’t do much for the negative tail. It also reduced the apparent heteroskedasticity. However, the relationship still appears non-linear, perhaps more so than in the original linear model. We will address non-linearity in the following chapter, and for now just focus on interpreting the log-linear model in this example.\nThe model output is shown below.\n\n\nCode\n#summary(mod2)\nknitr::include_graphics(\"files/images/log_linear_output.png\", dpi = 125)\n\n\n\n\n\nPlugging-in the values from the output using the equation for log-linear regression, we have\n\\[\n\\begin{align}\n\\widehat{\\log(WAGES)} & = a + b (EDUC) \\\\\n\\widehat{\\log(WAGES)} & = 1.188730 + 0.074802 (EDUC)\n\\end{align}\n\\]\nPlease take a moment to write down both the approximate and exact interpretation of the regression coefficient for educ. Hint: you need to do some follow-up math with exponents to get the exact interpretation."
  },
  {
    "objectID": "ch8_loglinear.html#sec-workbook-8",
    "href": "ch8_loglinear.html#sec-workbook-8",
    "title": "8  Log-linear regression",
    "section": "8.7 Workbook",
    "text": "8.7 Workbook\nThis section collects the questions asked in this chapter. The lessons for this chapter will focus on discussing these questions and then working on the exercises in Section 8.8. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 8.1\nI’ll ask some questions like the following to start class off. You check your answers by pasting the questions into \\(R\\) or Google’s search bar.\n\n\\(\\log(2.7182) = ?\\)\n\\(\\log(1) = ?\\)\n\\(\\log(0) = ?\\)\n\\(\\log(-1) = ?\\)\n\\(\\exp(1) = ?\\)\n\\(\\exp(0) = ?\\)\n\\(\\exp(-1) = ?\\)\nWhat is larger, \\(\\log(10)\\) or \\(\\exp(10)\\)?\n\\(\\log(\\exp(x)) = ?\\)\n\\(\\exp(\\log(x)) = ?\\)\n\nSection 8.5\n\nPlease write down your answers to the questions below and be prepared to share in clas: s\n\n\nHow many times larger is \\(Y^* = 6\\) than \\(Y = 2\\)?\nWhat is the relative increase from \\(Y = 2\\) to \\(Y^* = 6\\)?\nWhat is the percent increase from \\(Y = 200\\) to \\(Y^* = 600\\)?\nWhat is the percent decrease from \\(Y = 600\\) to \\(Y^* = 200\\)?\nWhat is the percent change from \\(Y = 1\\) to \\(Y^* = 1\\)?\n\n\nPlease write down your answers to the four questions about the hypothetical example in and be prepared to share your responses in class. *\n\nSection 8.6\n\nPlease take a moment to write down your interpretation of the regression coefficient for educ in the summary output below.\n\n\n\nCode\nattach(wages)\nlog_wage &lt;- log(wage + 1)\n\n# Regress it on educ\nmod2 &lt;- lm(log_wage ~ educ)\nsummary(mod2)\n\n\n\nCall:\nlm(formula = log_wage ~ educ)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.07475 -0.35961  0.02614  0.31244  1.18163 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.188730   0.102328   11.62   &lt;2e-16 ***\neduc        0.074802   0.007263   10.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4234 on 398 degrees of freedom\nMultiple R-squared:  0.2104,    Adjusted R-squared:  0.2085 \nF-statistic: 106.1 on 1 and 398 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ndetach(wages)"
  },
  {
    "objectID": "ch8_loglinear.html#sec-exercises-8",
    "href": "ch8_loglinear.html#sec-exercises-8",
    "title": "8  Log-linear regression",
    "section": "8.8 Exercises",
    "text": "8.8 Exercises\nThere isn’t much new in terms of R code in this chapter. The code for the worked example is presented below, but please refer to Section 8.6 for the interpretation.\nWe will go through this material in class together, so you don’t need to work on it before class (but you can if you want.)\nBefore staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\n\n\nCode\n# Load the data and take a look\nload(\"Wages.RData\")\nknitr::kable(head(wages))\n\n\n\n\n\neduc\nsouth\nsex\nexper\nwage\noccup\nmarr\ned\n\n\n\n\n12\n0\n0\n17\n7.50\n6\n1\n2\n\n\n13\n0\n0\n9\n13.07\n6\n0\n3\n\n\n10\n1\n0\n27\n4.45\n6\n0\n1\n\n\n9\n1\n0\n30\n6.25\n6\n0\n1\n\n\n9\n1\n0\n29\n19.98\n6\n1\n1\n\n\n12\n0\n0\n37\n7.30\n6\n1\n2\n\n\n\n\n\nWe will focus on regressing hourly wage (wage) on years of education (educ). The distribution of the outcome variable and its relationship with the predictor are depicted below.\n\n\nCode\nattach(wages)\npar(mfrow = c(1,2))\nhist(wage,  col = \"#4B9CD3\")\nplot(educ, wage, col = \"#4B9CD3\")\n\n\n\n\n\n\n8.8.1 Linear regression of Wages on Education\nWorking with linear model, we can see that all of the assumptions of the model are violated – we are going to need to try another approach!\n\n\nCode\nmod1 &lt;- lm(wage ~ educ)\n\n# Check out model fit\npar(mfrow = c(1,2))\nplot(mod1, 1, col = \"#4B9CD3\")\nplot(mod1, 2, col = \"#4B9CD3\")\n\n\n\n\n\n\n\n8.8.2 Log-linear regression of Wages on Education\nTo run a log-linear model, we can transform the Y-variable using the log function, and then run the model as usual.\n\n\nCode\n# Create log transform of wage\nlog_wage &lt;- log(wage + 1)\n\n# Regress it on educ\nmod2 &lt;- lm(log_wage ~ educ)\n\n# Check out model fit\npar(mfrow = c(1,2))\nplot(mod2, 1, col = \"#4B9CD3\")\nplot(mod2, 2, col = \"#4B9CD3\")\n\n\n\n\n\nThe log transformation helped with positive skew, although didn’t do much for the negative tail. It reduced the apparent heteroskedasticity. However, the relationship still appears non-linear, perhaps more so than in the original linear model. We will address non-linearity in the following chapter, and for now just focus on interpreting the log-linear model in this example.\nThe usual summary output provides the regression slope, which can be interpreted using the “approximate” interpretation of relative change:\n\n\nCode\nsummary(mod2)\n\n\n\nCall:\nlm(formula = log_wage ~ educ)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.07475 -0.35961  0.02614  0.31244  1.18163 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.188730   0.102328   11.62   &lt;2e-16 ***\neduc        0.074802   0.007263   10.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4234 on 398 degrees of freedom\nMultiple R-squared:  0.2104,    Adjusted R-squared:  0.2085 \nF-statistic: 106.1 on 1 and 398 DF,  p-value: &lt; 2.2e-16\n\n\nSome follow-up math to get the exact interpretation of the regression slope in terms of relative change. First \\(\\exp(b) - 1\\):\n\n\nCode\n# Get the regression slope from the output \nconf.b &lt;- coef(mod2)[2]\n\n# Exp and subtract one to get exact relative change\nexp(conf.b) - 1\n\n\n      educ \n0.07767034 \n\n\nLet’s also compute a confidence interval for \\(\\exp(b) - 1\\):\n\n\nCode\n# Compute a confidence interval for exact relative change\nexp(confint(mod2)[2, ]) - 1\n\n\n     2.5 %     97.5 % \n0.06239300 0.09316737 \n\n\n\n\n8.8.3 Write up\nApproximate:\n\nUsing a log-linear regression model, it was found that each additional year of education led to a predicted increase in wages of 7.4% (\\(b = .074, t(398) = 10.30, p &lt; .001\\)).\n\nExact:\n\nUsing a log-linear regression model, it was found that each additional year of education led to a predicted increase in wages of 7.8% (\\(\\exp(b) - 1 = .078, 95\\% \\text{ CI: }[0.062, 0.093]\\))."
  },
  {
    "objectID": "ch9_polynomial.html#sec-polynomial-9",
    "href": "ch9_polynomial.html#sec-polynomial-9",
    "title": "9  Polynomial regression, etc",
    "section": "9.1 Polynomial regression",
    "text": "9.1 Polynomial regression\nPolynomial regression means that we regress \\(Y\\) on a polynomial function of \\(X\\):\n\\[ \\widehat Y = b_0 + b_1 X + b_2 X^2 + b_3 X^3 + ....\\]\nYour first thought might be, “doesn’t this contradict the assumption that regression is linear?” The answer here is a bit subtle.\nAs with regular linear regression, the polynomial model is linear in the coefficients – we don’t raise the regression coefficients to a power (e.g., \\(b_1^2\\)), or multiply coefficients together (e.g, \\(b_1 \\times b_2\\)). This is the technical sense in which polynomial regression is still just linear regression, despite its name.\nPolynomial regression does use nonlinear functions of the predictor(s), but the model is agnostic to what you do with your data. The situation here is a lot like when we worked with interactions in Chapter 5. In order to model interactions, we computed the product of two predictors and entered the product into the model as a third predictor. Well, \\(X^2\\) is the product of a predictor with itself, so, in this sense, the quadratic term in a polynomial regression is just a special case of an interaction between two variables.\nAlthough we did not cover interactions among more than two variables in this course, they are computed in the same way – e.g., a “three-way” interaction is just the product of 3 predictors. Similarly, \\(X^3\\) is just the three-fold product of a variable with itself.\nWhile polynomial regression is formally similar to interactions, it is used for a different purpose. Interactions address how the relationship between two variables changes as a function of a third. Their inclusion in a model is usually motivated by a specific research question that is formulated before doing the data analysis (see Chapter 6).\nBy contrast, polynomial regression is used to address a non-linear relationship between \\(Y\\) and \\(X\\), and is usually motivated by a preliminary examination of data that indicates the presence of such a relationship (e.g., a scatter plot of \\(Y\\) versus \\(X\\); a residual versus fitted plot). While it is possible to formulate research questions about polynomial terms in a regression model, this is not necessarily or even usually the case when polynomial regression is used – often its just used to address violations of the linearity assumption.\n\n9.1.1 Recap of polynomials\nIn general, a polynomial of degree \\(n\\) (i.e., highest power of \\(n\\)) produces a curve that can have up to \\(n-1\\) bends (minima and maxima). Some examples are illustrated in Figure @ref(fig:poly) below.\n\nThe (orange) linear function of \\(X\\) is a polynomial of degree 1 and has zero bends.\nThe (green) quadratic function of \\(X\\) is a polynomial of degree 2 and has 1 bend (a minimum at \\(X = 0\\); this is also called a parabola).\netc.\n\n\n\n\n\n\nExamples of Polynomials\n\n\n\n\nAs we can see, this is a very flexible approach to capturing non-linear relationships between two variables. In fact, it can be too flexible! This is the topic of the next section.\n\n\n9.1.2 “Over-fitting” the data\nFigure 9.1 shows three different regression models fitted to the same (simulated) bivariate data.\n\nIn the left panel, a standard linear regression model is used, and we can see that the model does not capture the nonlinear (quadratic) trend in the data.\nThe middle panel uses a quadratic model (i.e., includes \\(X^2\\) as a predictor, as well as \\(X\\)), and fits the data quite well.\nThe right panel uses a 16-degree polynomial to fit the data. We can see that is has a higher R-squared than the quadratic model. But there is also something fishy about this model, don’t you agree?\n\n\n\n\n\n\nFigure 9.1: Polynomial Regression Examples\n\n\n\n\nTo help compare these three models, let’s simulate a second sample from the same population. In the plots below, the regression lines from Figure 9.1 were added to the plots from a second sample. Note that the regression parameters were not re-estimated using the second data set. The model parameters from the first data set were used to produce the regression lines for the second data set, so the regression lines are the same as in Figure 9.1.\n\n\n\n\n\nFigure 9.2: Polynomial Regression Examples (With New Data)\n\n\n\n\nThis procedure, which is called out-of-sample-prediction or cross-validation, is one widely used method for comparing the quality of predictions from different models. The R-squared values of the different models in the second sample are provided to help summarize the quality of predictions.\nWe will talk more about this example in class. Before moving on, please take a moment to write down your intuitions about what is going in Figure 9.1 and Figure 9.2. In particular, you might consider whether the model in the right-hand panel is better than the one in the middle panel. I will ask you to share your thoughts in class.\n\n\n9.1.3 Interpreting the model\nAs mentioned, polynomial terms are often added into a model as a way to address nonlinearity. When this is the case, the polynomial terms themselves are not necessarily of substantive interest – they can be added just to “patch up” the model after assumption checking.\nWe saw an example of this in Section 6.2. In that example, linear and quadratic terms for SES were entered into the model in the first block. The R-squared was interpreted for the entire block, but the interpretation of the regression coefficient for the quadratic term was not addressed. This is a pretty common way of using polynomial regression – the polynomial terms are included so that the model assumptions (linearity) are met, but they are not necessarily interpreted beyond this.\nHowever, we can interpret the regression slopes on the polynomial terms if we want to. This section addresses the interpretation of quadratic polynomials, which have the equation:\n\\[ \\widehat Y = b_0 +b_1X + b_2X^2.\\]\nA similar approach applies to models with higher-order terms as well (cubics, etc.).\nLet’s start with a classic example of a quadratic relationship: the Yerkes-Dodson law relating physiological arousal (“stress”) to task performance, which is represented in Figure 9.3. One way to interpret the law is in terms of the overall shape of the relationship. As stress goes up, so does performance – but only up to a point, after which more stress leads to a deterioration in performance.\n\n\n\n\n\nFigure 9.3: Yerkes-Dodson Law (Source: Wikipedia)\n\n\n\n\nThe overall shape of the trend depends on only the sign of the regression slope on the quadratic term:\n\nA U-shaped curve corresponds to a positive regression coefficient on \\(X^2\\) (think of a parabola from high school math)\nAn inverted-U-shaped curve corresponds to a negative regression coefficient on \\(X^2\\)\n\nBeyond the overall shape of the relationship, we might also want to know what level of stress corresponds to the “optimal” level of performance – i.e., where the maximum of the curve is. This exemplifies a more complicated interpretation of a quadratic relationship, and it requires some calculus (see Section 9.1.4, which is optional). The value of \\(X\\) that corresponds to the maximum (or minumum) of the quadratic curve is\n\\[ X = \\frac{-b_1}{2 b_2}. \\tag{9.1}\\]\nBased on this discussion, please answer the following questions bout the Yerkes-Dodson Law, using the following model parameters. Make sure to explain how you know the answer! \n\\[ \\widehat Y = 20 + 1 X - 2X^2.\\]\n\nWhat is the overall shape of the relationship: U or inverted-U? \nWhat is the value of stress at which predicted performance reaches a maximum?\nBonus: what is the maximum value of predicted performance? \n\n\n\n9.1.4 Analysing polynomials*\nThis sections shows how we get Equation 9.1 and some related details. Recall from intro calculus that the extrema (i.e., minima and maxima) of a function occur when the derivative of the function is equal to zero. The derivative of the quadratic regression equation is\n\\[\n\\frac{d}{dX} \\hat Y = \\frac{d}{dX} (b_0 + b_1X + b_2X^2) = b_1 + 2b_2X.\n\\]\nSetting the derivative to zero \\[\nb_1 + 2b_2X = 0\n\\]\nand solving for \\(X\\)\n\\[ X = -\\frac{b_1}{2b_2} \\]\ngives the value of \\(X\\) at which the \\(\\hat Y\\) reaches its minimum (or maximum) value. Let’s call this value of \\(X^*\\). Plugging \\(X^*\\) into the original equation tells us the minimum (or maximum) of \\(\\hat Y\\).\nWe can use the second derivative rule to determine whether \\(X^*\\) is a minimum or maximum of \\(\\hat Y\\).\n\\[\n\\frac{d^2}{dX^2} \\hat Y = \\frac{d}{dX} (b_1 + 2b_2X) = 2b_2\n\\]\nIf this value is positive (i.e., if \\(b_2 &gt;0\\)), then the second derivative rule tells that \\(X^*\\) is minimum, hence the curve is “U-shaped”. If the value is negative (i.e., if \\(b_2 &lt; 0\\)) then its a maximum, and hence the curve is “inverted-U-shaped”.\n\n\n9.1.5 Model building vs curve fitting\nUp to this point, we have discussed the use and interpretation of polynomials. In this section we consider how to build polynomial regression models in practice.\nA typical model-building process for polynomial regression might proceed as follows.\n\nEnter just the linear terms into the model and examine a residual versus fitted plot.\nIf there is evidence of non-linearity, look at the scatter plots between the outcome variable and each individual predictor to make a guess about which predictor(s) may be causing the non-linearity.\nAdd a quadratic term for a predictor of interest and examine whether there is a statistically significant increase in R-squared (see Section 6.1.4). If there is, you have found a source of non-linearity! If not, the quadratic term is not explaining variance in the outcome variable, so you can remove it from the model.\nKeep adding polynomial terms (quadratic terms for other predictors; higher-order terms for the same predictor) one at a time until the model assumptions looks reasonable. This might take a bit of trial and error.\n\nThis overall approach is illustrated in the next section. However, there are a couple of important points to mention first.\n\nMaking good use of polynomial regression requires walking a fine line between curve-fitting and theory-based modeling (see Figure 9.1). Sometimes, adding polynomial terms can provide an elegant and intuitive interpretation of the relationship between two variables. But, if you find yourself adding more than a couple of polynomial terms into a model and still have unresolved issues with nonlinearity, it is probably best to consider another approach (such as piecewise regression, coming up in Section 9.3)\nJust like with interactions, higher-order polynomial terms are often highly correlated with lower-order terms (e.g., if \\(X\\) takes on strictly positive values, \\(X\\) and \\(X^2\\) will be highly correlated). Recall that if two predictors are highly correlated, this can affect their regression coefficients (Section 3.4) as well as their standard errors (Section 6.3). In the context of polynomial regression, there are a couple of things that can be done about this.\n\nInterpret \\(\\Delta R^2\\) values rather than the individual regression coefficients and their \\(p\\)-values. This is the easiest thing to do, conceptually.\nUse “orthogonal polynomials”, which are designed to ensure the different polynomial terms for the same predictor are uncorrelated (orthogonal just means uncorrelated). The result of this approach is that numerical values of the regression coefficients are not directly interpretable beyond their sign, but the t-tests of the regression coefficients can be interpreted as testing the \\(\\Delta R^2\\) for each term in the polynomial. This is conceptually more complicated than first option, but leads to the same overall conclusions.\n\n\nBoth approaches are illustrated in the next section."
  },
  {
    "objectID": "ch9_polynomial.html#sec-worked-example-9",
    "href": "ch9_polynomial.html#sec-worked-example-9",
    "title": "9  Polynomial regression, etc",
    "section": "9.2 Worked Example",
    "text": "9.2 Worked Example\nIn Section 8.6 we saw that applying a log-transform to the Wages.Rdata example addressed non-normality of the residuals but did not do much to address nonlinearity. The summary output and diagnostic plots for the log-linear regression of wages on education are presented again below. We will go through this example in class together, so please note any questions you have about the procedures or their interpretation and be prepared to ask them in class. \n\n\nCode\n# Load the data and take a look\nload(\"Wages.RData\")\nattach(wages)\n\n# Create log transform of wage\nlog_wage &lt;- log(wage + 1)\n\n# Regress log_wages on educ\nmod1 &lt;- lm(log_wage ~ educ)\n\n# Check out model fit\npar(mfrow = c(1,2))\nplot(educ, log_wage, col = \"#4B9CD3\")\nabline(mod1)\nplot(mod1, 1, col = \"#4B9CD3\")\n\n\n\n\n\nCode\nsummary(mod1)\n\n\n\nCall:\nlm(formula = log_wage ~ educ)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.07475 -0.35961  0.02614  0.31244  1.18163 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.188730   0.102328   11.62   &lt;2e-16 ***\neduc        0.074802   0.007263   10.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4234 on 398 degrees of freedom\nMultiple R-squared:  0.2104,    Adjusted R-squared:  0.2085 \nF-statistic: 106.1 on 1 and 398 DF,  p-value: &lt; 2.2e-16\n\n\nBecause there is one prominent bend in our residual vs fitted plot (at \\(\\hat Y \\approx 2.1\\)), let’s see if adding a quadratic term to the model can improve the model fit.\nThe poly function in R makes it easy to do polynomial regression, without having to hard-code new variables like educ^2 into our dataset. In the summary output below, poly(...)n denote’s the \\(n\\)-th term in the polynomial. The diagnostic plots for the log-linear model with a quadratic term are also shown below.\n\n\nCode\nmod2 &lt;- lm(log_wage ~ poly(educ, 2, raw = T))\npar(mfrow = c(1,2))\nplot(educ, log_wage, col = \"#4B9CD3\")\n\n# To plot the trend we need to we first need to order the data and the predicted values ... \nsort_educ &lt;- educ[order(educ)]\nsort_fitted&lt;- fitted(mod2)[order(educ)]\npoints(sort_educ, sort_fitted, type = \"l\")\nplot(mod2, 1, col = \"#4B9CD3\")\n\n\n\n\n\nCode\nsummary(mod2)\n\n\n\nCall:\nlm(formula = log_wage ~ poly(educ, 2, raw = T))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.04723 -0.38939  0.01877  0.31820  1.14129 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              1.862958   0.406587   4.582 6.18e-06 ***\npoly(educ, 2, raw = T)1 -0.031492   0.062468  -0.504   0.6144    \npoly(educ, 2, raw = T)2  0.003985   0.002326   1.713   0.0875 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4224 on 397 degrees of freedom\nMultiple R-squared:  0.2162,    Adjusted R-squared:  0.2123 \nF-statistic: 54.77 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n\nWriting the R output in terms of the regression model, we have:\n\\[ \\widehat{\\log(WAGES)} = 1.862958 - 0.031492 (EDUC) + 0.003985 (EDUC)^2.\\]\nLet’s start by interpreting the plots. Based on the left-hand panel, it looks like a quadratic relationship provides a reasonable representation of the data. Based on the right-hand panel, I would conclude that the apparent non-linearity in the residual vs fitted plot has been sufficiently reduced. There is still a blip at \\(\\hat Y = 2.3\\), but there are 5 data points there so I am not to worried about it.\nTurning to the summary output, there are three main take-aways:\n\nAs discussed in the previous section, the sign of the quadratic term tells us something about the overall shape of the relationship (do you remember what that is?). However, interpreting the numerical values of the regression coefficients in a polynomial regression is not always useful. For example, using the approach to interpreting quadratic regression from Section 9.1.3, it turns out that the minimum predicted wages occur for someone with 3.95 years of education. The lowest level of education in the sample is 6 years, so this interpretation isn’t super relevant for our example. Consequently, rather than focusing on the interpretation of the regression coefficients, it is often sufficient to focus on whether the two predictors (i.e., \\(EDUC\\) and \\(EDUC^2\\)) together explained a significant proportion of variation in the outcome variable. This information is provided by the R-squared statistic and the F-test of R-squared in the summary output above.\nThere are two ways to test whether the addition of the quadratic term (poly(educ, 2)2 = 0.003985) improves the model. First, we can examine its test of significance. This test tells us that, controlling for the linear relationship between log-wages and education, the quadratic term is statistically significant at the .1 level (it is not statistically significant at the .05 level). Recall from Chapter 7 that this same information could be obtained by setting up a heirarhical model (Block 1 = linear term; Block 2 = quadratic term) and testing the change in R-squared. For the example, the F-test of R-squared change is\n\n\n\nCode\nanova(mod1, mod2)\n\n\nAnalysis of Variance Table\n\nModel 1: log_wage ~ educ\nModel 2: log_wage ~ poly(educ, 2, raw = T)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    398 71.363                              \n2    397 70.840  1   0.52368 2.9348 0.08747 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that the p-value is exactly the same as the t-test of regression coefficient reported above. In both cases, it seems that we don’t really need the quadratic term, based on the .05 level of significance (more on this Section 9.3)\n\nFinally, compared to the model without the quadratic term, we can see the linear term (poly(educ, 2)1 = - 0.031492 ) is now longer statistically significant. This can happen when we add higher order terms into a model. In this example, the linear and quadratic terms are highly correlated (the correlation is over .99 in the example!). Due to this correlation, the linear terms is not statistically significant, and the quadratic term is only “marginally” significant, even though the F-test of R-squared in the summary output is fatalistically significant with \\(p &lt; .001\\). In the next section, we will see how to avoid this issue of having highly correlated predictors in polynomial regression.\n\n** If you have any questions about the interpretation of the model results discussed in this section, please list them now and I will be happy to address the in class.**\n\n9.2.1 Orthogonal polynomails*\nBefore moving on, a quick (and optional) note on orthogonal vs. raw polynomials. Orthogonal polynomials are the default approach in R, and they make life easier, so they are worth knowing about.\nWhen using orthogonal polynomials, the different polynomial terms (e.g., \\(X, X^2, X^3\\)) are transformed so that they are uncorrelated. This means that the \\(t\\)-test of each regression coefficient can be interpreted as testing the proportion of variance associated uniquely with that term of the polynomial. Basically, using orthogonal polynomials means that we don’t need to do the model building stuff (e.g., sequential blocks, adding in each term one at a time)– it’s already built into the coefficients.\nThe downside of orthogonal polynomials is that, beyond their sign, the regression coefficients are complicated to interpret. But, these coefficients aren’t easy to interpret anyway, and we often don’t care much about their exact values. If you are in a situation where you don’t really care about the interpretation of the coefficients beyond the overall shape of the relationship, the orthogonal polynomials are definitely a good choice!\nThe use of orthogonal polynomials is illustrated below. You’ll see that most of the output is the same as in the previous section, except the numerical value and associated tests of the regression coefficients in the summary table. In particular, the linear trend is statistically significant in the output below, because it is no longer correlated with the quadratic trend. In fact the t-test and p-value are exactly the same as the first model we fit to our example data above (the model with just the linear trend, and no quadratic trend).\nIn summary, orthogonal polynomials provide a shortcut to hierarchical model buiding with polynomials. By transforming the data so that the different terms of the polynomial are uncorrelated, we get the similar information from a single model using orthogonal polynomials as we would if we fitted a series of hierarchical models, adding each additional term into the model one at a time.\n\n\nCode\nmod2a &lt;- lm(log_wage ~ poly(educ, 2, raw = F))\npar(mfrow = c(1,2))\nplot(educ, log_wage, col = \"#4B9CD3\")\n\n# To plot the trend we need to we first need to order the data and the predicted values ... \nsort_educ &lt;- educ[order(educ)]\nsort_fitted &lt;- fitted(mod2a)[order(educ)]\npoints(sort_educ, sort_fitted, type = \"l\")\nplot(mod2a, 1, col = \"#4B9CD3\")\n\n\n\n\n\nCode\nsummary(mod2a)\n\n\n\nCall:\nlm(formula = log_wage ~ poly(educ, 2, raw = F))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.04723 -0.38939  0.01877  0.31820  1.14129 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              2.21987    0.02112 105.103   &lt;2e-16 ***\npoly(educ, 2, raw = F)1  4.36133    0.42242  10.325   &lt;2e-16 ***\npoly(educ, 2, raw = F)2  0.72366    0.42242   1.713   0.0875 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4224 on 397 degrees of freedom\nMultiple R-squared:  0.2162,    Adjusted R-squared:  0.2123 \nF-statistic: 54.77 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n\nTo find out more, use help(poly). A good discussion of this point is also available on StatExchange: https://stats.stackexchange.com/questions/258307/raw-or-orthogonal-polynomial-regression?r=SearchResults&s=2%7C87.5473\n\n\n9.2.2 Summary\nThis section has addressed how to use, interpret, and implement polynomial regression. Some key points:\n\nWe don’t want to overfit the data by adding too many higher-order terms. If a quadratic or cubic polynomial doesn’t sort out any issues with linearity (as diagnosed by the residual vs fitted plots), then you probably want to try something else (see next section).\nOften the overall shape of a polynomial regression is of interest. This is communicated by the sign of the regression slopes on the higher-order terms. However we aren’t interested in a more specific interpretation of the regression slopes – it can be done (#sec-interpreting-the-model-9), but it is not very common.\nInstead, we usually approach polynomial regression from the perspective of hierarchical model building – if the higher order terms lead to a significant increase in the variance explained (i.e., R-squared change), we keep them in the model.\nOrthogonal polynomials provide a shortcut to doing heirarhical model building with polynomials. The make our life easier, but they aren’t doing anything different than the hierarchical.\nIn the worked example, it turned out that despite the apparent issue with linearity in the original model, and despite the apparently better fit of the quadratic model in terms of linearity, the statistical tests actually suggested we don’t need the quadratic term (using \\(\\alpha = .05\\)). The next example provides another perspective on this modelling issue."
  },
  {
    "objectID": "ch9_polynomial.html#sec-piecewise-9",
    "href": "ch9_polynomial.html#sec-piecewise-9",
    "title": "9  Polynomial regression, etc",
    "section": "9.3 Piecewise regression",
    "text": "9.3 Piecewise regression\nPiecewise or segmented regression is another approach to dealing with nonlinearity. Like polynomial regression, it is mathematically similar to interaction. Also like polynomial regression, it has a special interpretation and application that make it practically distinct from interaction.\nIn the simplest case, piecewise regression involves interacting a predictor variable with a binary re-coding of itself. To illustrate how the approach works, let’s again consider our wages and education example. The scatter plot of log-wages versus education is presented again below for reference.\n\n\n\n\n\nFigure 9.4: The Wages Example\n\n\n\n\nConsider the following reasoning about the example:\n\nFor people with 12 or less years of education (i.e., who did not obtain post-secondary education) the apparent relationship with wage is quite weak. This seems plausible, because if a job doesn’t require a college degree, education probably isn’t a big factor in determining wages.\nFor people with more than 12 years of education, the relationship with wage seems to be stronger. This also seems plausible: for jobs that require post secondary education, more education is usually associated with higher wages.\nTo restate this as an interaction: the relationship between wage and education appears different for people who have a post-secondary education versus those who do not.\n\nTo represent this reasoning visually we can modify Figure 9.4 as shown in Figure 9.5. This captures the basic idea behind piecewise regression – we have different regression lines over different ranges of the predictor, and the overall regression is piecewise or segmented. The next section shows how to build this model.\n\n\n\n\n\nFigure 9.5: The wages example\n\n\n\n\n\n9.3.1 The piecewise model\nWe have reasoned that the relationship between wages and education might depend on whether people have post-secondary education. We also noted that this sounds a lot like an interaction (because it is!), which is the basic approach we can use to create piecewise models.\nIn order to run our piecewise regression, first we need to create a dummy-coded version of education that indicates whether a person had more than 12 years education:\n\\[ EDUC_{12} = \\left\\{ \\begin{matrix}  \n                     1 & \\text{if } EDUC  &gt; 12\\\\\n                    0 & \\text{if } EDUC  \\leq 12\n                \\end{matrix} \\right.\n\\]\nThen, we enter the original variable, the dummy-coded indicator, and their interaction into the model:\n\\[ \\widehat{\\log(WAGES)} = b_0 + b_1 (EDUC) + b_2 (EDUC_{12}) + b_3 (EDUC \\times EDUC_{12}) \\]\nAs we can see, the resulting model is a special case of an interaction between a continuous predictor (\\(EDUC\\)) and binary predictor (\\(EDUC_{12}\\)).\nWhile the above model conveys the overall idea of piecewise regression, there are also more complex approaches that will search for breakpoints, smoothly connect the lines at the breakpoints, use nonlinear functions (e.g., polynomials) for the segments, etc. We won’t cover these more complex approaches here, but check out the following resource if you are interested and feel free to ask questions in class: https://rpubs.com/MarkusLoew/12164\n\n\n9.3.2 Back to the example\nThe output for the example is provided below. Following the output, some questions are posed about the interpretation of the model.\n\nDiagnostic plots for the piecewise model:\n\n\n\n\n\n\nFigure 9.6: The Wages Example\n\n\n\n\n\nSummary output and estimated model (the model doesn’t fit nicely on one line!):\n\n\n\nCode\nsummary(mod4)\n\n\n\nCall:\nlm(formula = log_wage ~ educ * educ12)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.98101 -0.36398  0.02055  0.30687  1.10305 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.78426    0.23531   7.583 2.43e-13 ***\neduc         0.01736    0.02159   0.804   0.4219    \neduc12      -0.64255    0.35573  -1.806   0.0716 .  \neduc:educ12  0.06144    0.02737   2.245   0.0253 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4203 on 396 degrees of freedom\nMultiple R-squared:  0.2261,    Adjusted R-squared:  0.2202 \nF-statistic: 38.56 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\n\\begin{align} \\widehat{\\log(WAGES)} = & 1.78426  + 0.01736 (EDUC) \\\\ & - 0.64255 (EDUC_{12})\n+ 0.06144 (EDUC \\times EDUC_{12})\n\\end{align}\n\\]\n\nSimple trends using of the emtrends function (see Section 5.4).\n\n\n\nCode\nemmeans::emtrends(mod4, specs = \"educ12\", var = \"educ\")\n\n\n educ12 educ.trend     SE  df lower.CL upper.CL\n      0     0.0174 0.0216 396  -0.0251   0.0598\n      1     0.0788 0.0168 396   0.0457   0.1119\n\nConfidence level used: 0.95 \n\n\nWe will discuss this model together in class. It should feel a lot like deja vu from Chapter 5, but even more complicated due to the interpretation of \\(EDUC_{12}\\) and the fact that the outcome is log-transformed. Fun!!! I’ll ask questions below in class:\nNote that the intercept and main effect of the binary variable educ12 are not of much interest in this application.\n\nUsing the 2-step approach from Section 5.3, please take a moment to work out the interpretation of main effect on \\(EDUC\\) \\(b_1 = 0.01736\\) and the interaction \\(b_3 = 0.06144\\) in the model above. It might help to draw a plot like Figure 9.5 and label it accordingly. (The interpretation of \\(b_0\\) and \\(b_2\\) is not very interesting but you can work them out too if you like.)\nPlease take a moment to write down your interpretation of the results of the piecewise regression, addressing the diagnostic plots, the parameter estimates, and the simple slopes.\n\n\n\n9.3.3 Summary\nPiecewise regression is another approach to dealing with nonlinearity. It can be especially powerful when we can conceptualize the nonlinearity in as an interaction between a variable and categorical encoding of itself (e.g., the relationship between years of education and wages depends on whether you went to college.) The overall interpretation and implementation of the model is also based on the material we already covered in Section 5.3, so take a look at the summary of that section for additional pointers."
  },
  {
    "objectID": "ch9_polynomial.html#sec-workbook-9",
    "href": "ch9_polynomial.html#sec-workbook-9",
    "title": "9  Polynomial regression, etc",
    "section": "9.4 Workbook",
    "text": "9.4 Workbook\nThis section collects the questions asked in this chapter. The lessons for this chapter will focus on discussing these questions and then working on the exercises in Section 9.5. The lesson will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\nSection 9.1.2\n\nPlease take a moment to write down your intuitions about what is going in Figure 9.1 and Figure 9.2. In particular, you might consider whether the model in the right-hand panel is better than the one in the middle panel. I will ask you to share your thoughts in class.\n\nSection 9.1.3\n\nPlease answer the following questions bout the Yerkes-Dodson Law, using the following model parameters. Make sure to explain how you know the answer.\n\n\\[ \\widehat Y = 20 + 1 X - 2X^2.\\]\n\nWhat is the overall shape of the relationship: U or inverted-U?\nWhat is the value of stress at which predicted performance reaches a maximum?\nBonus: what is the maximum value of predicted performance?\n\nSection 9.2\n\nWe will go through the example in class together, so please note any questions you have about the procedures or their interpretation and be prepared to ask them in class.\n\nSection 9.3\n\nThe output for the example is provided below. Following the output, some questions are posed about the interpretation of the model,\nDiagnostic plots for the piecewise model:\n\n\n\n\n\n\nThe Wages Example\n\n\n\n\n\nSummary output and estimated model:\n\n\n\nCode\nsummary(mod4)\n\n\n\nCall:\nlm(formula = log_wage ~ educ * educ12)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.98101 -0.36398  0.02055  0.30687  1.10305 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.78426    0.23531   7.583 2.43e-13 ***\neduc         0.01736    0.02159   0.804   0.4219    \neduc12      -0.64255    0.35573  -1.806   0.0716 .  \neduc:educ12  0.06144    0.02737   2.245   0.0253 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4203 on 396 degrees of freedom\nMultiple R-squared:  0.2261,    Adjusted R-squared:  0.2202 \nF-statistic: 38.56 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\n\\begin{align} \\widehat{\\log(WAGES)} = & 1.78426  + 0.01736 (EDUC) \\\\ & - 0.64255 (EDUC_{12})\n+ 0.06144 (EDUC \\times EDUC_{12})\n\\end{align}\n\\]\n\nSimple trends using of the emtrends function (see Section 5.4).\n\n\n\nCode\nemmeans::emtrends(mod4, specs = \"educ12\", var = \"educ\")\n\n\n educ12 educ.trend     SE  df lower.CL upper.CL\n      0     0.0174 0.0216 396  -0.0251   0.0598\n      1     0.0788 0.0168 396   0.0457   0.1119\n\nConfidence level used: 0.95 \n\n\n\nUsing the 2-step approach from Section 5.3, please take a moment to work out the interpretation of main effect on \\(EDUC\\) \\(b_1 = 0.01736\\) and the interaction \\(b_3 = 0.06144\\) in the model above. It might help to draw a plot like Figure 9.5 and label it accordingly. (The interpretation of \\(b_0\\) and \\(b_2\\) is not very interesting but you can work them out too if you like.)\nPlease take a moment to write down your interpretation of the results of the piecewise regression, addressing the diagnostic plots, the parameter estimates, and the simple slopes.\n\n\n\nCode\n# clean up!\ndetach(wages)"
  },
  {
    "objectID": "ch9_polynomial.html#sec-exercises-9",
    "href": "ch9_polynomial.html#sec-exercises-9",
    "title": "9  Polynomial regression, etc",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\nThere isn’t much new in terms of R code in this chapter, but the workflows for the two types of model are pretty complicated so we review them here. You’ll see that some of the plots require a lot of fiddling about, especially for the piecewise regression model. We will cover some tricks and shortcuts for producing these types plots during the open lab sessions for Assignment 4. So don’t worry too much about the complicated-looking coded for the plots at this point!\nWe will go through this material in class together, so you don’t need to work on it before class (but you can if you want.) Before staring this section, you may find it useful to scroll to the top of the page, click on the “&lt;/&gt; Code” menu, and select “Show All Code.”\n\n9.5.1 Polynomial regression\nLet’s start with the “Vanella” log-linear model for the wages examples\n\n\nCode\n# Load the data and take a look\nload(\"Wages.RData\")\nattach(wages)\n\n# Create log transform of wage\nlog_wage &lt;- log(wage + 1)\n\n# Regress it on educ\nmod1 &lt;- lm(log_wage ~ educ)\n\n# Check out model fit\npar(mfrow = c(1,2))\nplot(educ, log_wage, col = \"#4B9CD3\")\nabline(mod1)\nplot(mod1, 1, col = \"#4B9CD3\")\n\n\n\n\n\nBecause there is one prominent bend in our residual vs fitted plot of the log-linear model (at \\(\\hat Y \\approx 2.1\\)), let’s see if adding a quadratic term to the model can improve the model fit.\nThe poly function in R makes it easy to do polynomial regression, without having to hard-code new variables like EDUC^2 into our dataset. This function automatically uses orthogonal (uncorrelated) polynomials, so we don’t have to worry about centering, either. The basic interpretation of the model coefficients in an orthogonal polynomial regression is the same as discussed in #sec-polynomial-9, but the “more complicated” interpretation of the model parameters is not straightforward. To find out more, use help(poly).\nThe diagnostic plots for the log-linear model with a quadratic term included for education is shown below, along with the model summary. In the output, poly(educ, 2)n is the \\(n\\)-th degree term in the polynomial. Orthogonal polynomials were used.\n\n\nCode\n# Regress log_wage on a quadratic function of eduction \nmod2 &lt;- lm(log_wage ~ poly(educ, 2))\n\n# Model output\nsummary(mod2)\n\n\n\nCall:\nlm(formula = log_wage ~ poly(educ, 2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.04723 -0.38939  0.01877  0.31820  1.14129 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     2.21987    0.02112 105.103   &lt;2e-16 ***\npoly(educ, 2)1  4.36133    0.42242  10.325   &lt;2e-16 ***\npoly(educ, 2)2  0.72366    0.42242   1.713   0.0875 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4224 on 397 degrees of freedom\nMultiple R-squared:  0.2162,    Adjusted R-squared:  0.2123 \nF-statistic: 54.77 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Diagnostic plots\npar(mfrow = c(1,2))\n\n# scatter plot with trend\nplot(educ, log_wage, col = \"#4B9CD3\")\n# order the data and the predicted values ... \nsort_educ &lt;- educ[order(educ)]\nsort_fitted &lt;- fitted(mod2)[order(educ)]\npoints(sort_educ, sort_fitted, type = \"l\")\n\n# residual versus fitted\nplot(mod2, 1, col = \"#4B9CD3\")\n\n\n\n\n\nThe F-test of R-squared change between the first and second models (not really required since we used orhtogonal polynomials, but for illustrative purposes):\n\n\nCode\nanova(mod1, mod2)\n\n\nAnalysis of Variance Table\n\nModel 1: log_wage ~ educ\nModel 2: log_wage ~ poly(educ, 2)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    398 71.363                              \n2    397 70.840  1   0.52368 2.9348 0.08747 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIllustrating the same overall approach for the cubic model:\n\n\nCode\nmod3 &lt;- lm(log_wage ~ poly(educ, 3))\nsummary(mod3)\n\n\n\nCall:\nlm(formula = log_wage ~ poly(educ, 3))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0344 -0.3734  0.0203  0.3107  1.1612 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.2199     0.0211 105.213   &lt;2e-16 ***\npoly(educ, 3)1   4.3613     0.4220  10.336   &lt;2e-16 ***\npoly(educ, 3)2   0.7237     0.4220   1.715   0.0871 .  \npoly(educ, 3)3  -0.5717     0.4220  -1.355   0.1763    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.422 on 396 degrees of freedom\nMultiple R-squared:  0.2199,    Adjusted R-squared:  0.2139 \nF-statistic:  37.2 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Same plots as above, reusing variable names here\npar(mfrow = c(1,2))\nplot(educ, log_wage, col = \"#4B9CD3\")\nsort_fitted &lt;- fitted(mod3)[order(educ)]\npoints(sort_educ, sort_fitted, type = \"l\")\nplot(mod3, 1, col = \"#4B9CD3\")\n\n\n\n\n\n\n\nCode\nanova(mod1, mod2, mod3)\n\n\nAnalysis of Variance Table\n\nModel 1: log_wage ~ educ\nModel 2: log_wage ~ poly(educ, 2)\nModel 3: log_wage ~ poly(educ, 3)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    398 71.363                              \n2    397 70.840  1   0.52368 2.9410 0.08714 .\n3    396 70.513  1   0.32682 1.8354 0.17626  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n9.5.2 Write up\nThis section illustrates the write-up for the quadratic regression based on heirarhical modeling and based on orthogonal polynomials. In practice, just pick one of these.\n\nBased on heirachical modeling. Starting with a model containing only the linear term, years of education explained about 21.6% of the variance in log-wages (\\(R^2 = .216, F(2, 397) = 54.77, p &lt; .001\\)). Adding a quadratic term to address potential nonlinearity, it was found that only an additional .5% of variance was explained (\\(\\Delta R^2 = .005, F(1, 293) = 2.93, p = 0.088\\)).\nBased on orthogonal polynomials Regressing log wages on a second-degree orthogonal polynomial function of wages, it was found that the linear term was statistically significant at the .05 level (\\(b = 4.36, t(396) = 10.33, p &lt; .001\\)), but the quadratic term was not (\\(b = 0.72, t(396) = 1.71, p = 0.088\\)).\n\n\n\n9.5.3 Piecewise regression\nMoving on, let’s consider the piecewise model from Section 9.3\n\n\nCode\n# Create a dummy variable indicating if education is at least 12 years or more\neduc12 &lt;- (educ &gt; 12)*1\n\n# Interact the dummy with educ\nmod4 &lt;- lm(log(wage + 1) ~ educ*educ12) \n\n# The model output\nsummary(mod4)\n\n\n\nCall:\nlm(formula = log(wage + 1) ~ educ * educ12)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.98101 -0.36398  0.02055  0.30687  1.10305 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.78426    0.23531   7.583 2.43e-13 ***\neduc         0.01736    0.02159   0.804   0.4219    \neduc12      -0.64255    0.35573  -1.806   0.0716 .  \neduc:educ12  0.06144    0.02737   2.245   0.0253 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4203 on 396 degrees of freedom\nMultiple R-squared:  0.2261,    Adjusted R-squared:  0.2202 \nF-statistic: 38.56 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# The simple trends\ntrends &lt;- emmeans::emtrends(mod4, specs = \"educ12\", var = \"educ\")\nemmeans::test(trends)\n\n\n educ12 educ.trend     SE  df t.ratio p.value\n      0     0.0174 0.0216 396   0.804  0.4219\n      1     0.0788 0.0168 396   4.686  &lt;.0001\n\n\nCode\n# The diagnostic plots\npar(mfrow = c(1, 2))\nplot(mod4, 1, col = \"#4B9CD3\")\nplot(mod4, 2, col = \"#4B9CD3\")\n\n\n\n\n\nThe wages example\n\n\n\n\nWe can still see some evidence of heteroskedasticity in the residual versus fitted plot, so the last step is to use heteroskedasticity-corrected standard errors to ensure we are making the right conclusions about statistical significance\n\n\nCode\n## Make sure the required pacakges are installed\n# install.packages(\"car\")\n# install.packages(\"lmtest\")\n\n# Step 1. Use \"hccm\" to get the HC SEs for our piecewise model \nhcse &lt;- car::hccm(mod4)\n\n# Step 2. Use \"coeftest\" to compute t-tests with the HC SEs\nlmtest::coeftest(mod4, hcse)\n\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.784262   0.188861  9.4475  &lt; 2e-16 ***\neduc         0.017362   0.017859  0.9722  0.33156    \neduc12      -0.642555   0.323851 -1.9841  0.04793 *  \neduc:educ12  0.061436   0.024565  2.5010  0.01279 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe next bit is optional. It shows how to produce the piecewise regression plot, which takes quite a bit of messing about with R…Let me know if you find an easier way to do this (in base R).\n\n\nCode\n# Building the piecewise regression plot -- yeeesh\n\n# Add fitted values to dataset\nwages$fitted &lt;- fitted(mod4)\n\n# Sort data on educ\nwages &lt;- wages[order(educ), ]\n\n# Plot\npar(mfrow = c(1, 1))\nplot(educ, log(wage + 1), col = \"#4B9CD3\")\n\n# Change color for the points with educ ≤ 12\nwith(subset(wages, educ &lt;= 12), points(educ, log(wage + 1)))\n\n# Plot the predicted values for educ &gt; 12\nwith(subset(wages, educ &gt; 12), lines(educ, fitted, col = \"#4B9CD3\"))\n\n# Plot the predicted values for educ ≤ 12\nwith(subset(wages, educ &lt;= 12), lines(educ, fitted))\n\n\n\n\n\n\n\n9.5.4 Write up\nUsing a piecewise regression model with heteroskedasticity consistent standard errors, it was found that the relationship between log-wages and years of education depended on whether a person attended post-secondary education (\\(b = 0.061, t(396) = 2.50, p = .013\\)). Using simple trends, it was found for people with 12 or less years of education, the relationship between log-wages and years of education was not significant (\\(b = 0.017, t = (396) = .804, p = .42\\)), whereas the relationship was significant for people with at least some post-secondary education (\\(b = 0.078, t = (396) = 4.68, p &lt; .001\\)). The results indicate that, simply for those with some post-secondary education, each additional year of education was associated with a 8.1% increase in hourly wages (\\(\\exp(0.078) - 1 = .081\\)). Due to a software limitation, the simple trends did not utlize heteroskedastic consistent standard errors."
  },
  {
    "objectID": "ch10_logistic.html#sec-chd-example-10",
    "href": "ch10_logistic.html#sec-chd-example-10",
    "title": "10  Logistic regression",
    "section": "10.1 The CHD example",
    "text": "10.1 The CHD example\nAs a working example, we will use data contained in the file CHD.RData to explore the relationship between age in years (“age”) and evidence (absence or presence) of coronary heart disease (“chd”). The data set contains 100 cases. Respondents’ ages range from 20 to 69, while evidence of CHD is coded 0 when it is absent and 1 when it is present. A sample of 20 cases is shown below. (Source: Applied Logistic Regression by David W. Hosmer and Stanley Lemeshow, 1989, John Wiley and Sons.)\n\n\nCode\nload(\"CHD.RData\")\nattach(chd.data)\nknitr::kable(list(chd.data[sample(1:100, 10),2:3],   \n                  chd.data[sample(1:100, 10),2:3]), \n             row.names = F, \n             caption = \"The CHD example\")\n\n\n\n\n\nThe CHD example\n\n\n\n\n\n\n\nchd\nage\n\n\n\n\n0\n41\n\n\n0\n33\n\n\n1\n60\n\n\n1\n59\n\n\n1\n64\n\n\n0\n23\n\n\n0\n42\n\n\n0\n42\n\n\n0\n49\n\n\n1\n43\n\n\n\n\n\n\n\n\nchd\nage\n\n\n\n\n0\n32\n\n\n0\n43\n\n\n0\n37\n\n\n1\n53\n\n\n1\n59\n\n\n1\n42\n\n\n0\n49\n\n\n1\n34\n\n\n0\n44\n\n\n0\n30\n\n\n\n\n\n\n\n\n\n\nIf we regress CHD on age using linear regression, this is referred to as the “linear probability model.” The diagnostic plots and summary output are below:\n\n\n\nCall:\nlm(formula = chd ~ age, data = chd.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85793 -0.33992 -0.07274  0.31656  0.99269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.537960   0.168809  -3.187  0.00193 ** \nage          0.021811   0.003679   5.929 4.57e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.429 on 98 degrees of freedom\nMultiple R-squared:  0.264, Adjusted R-squared:  0.2565 \nF-statistic: 35.15 on 1 and 98 DF,  p-value: 4.575e-08\n\n\n\n\n\nFigure 10.1: Linear probability model with CHD example\n\n\n\n\nBefore moving on, please take a moment to write down you conclusions (and rationale) about whether the assumptions of linear regression are met for these data.\nI’ll note that researchers who have a strong preference for OLS methods (AKA economists) often approach binary outcomes using the linear probability model. As we can see, this approach violates all of the assumptions of linear regression, can lead to predicted probabilities outside of the range [0,1], produces incorrect standard errors for model parameters (need to use HC standard errors), and is, in a word, wrong. Yet, despite all this, it works pretty well in some situations and has the benefit of being easier to interpret than logistic regression. We will consider the situations in which the linear probability model is “close enough” at the end of the next section."
  },
  {
    "objectID": "ch10_logistic.html#sec-logit-10",
    "href": "ch10_logistic.html#sec-logit-10",
    "title": "10  Logistic regression",
    "section": "10.2 Logit & logistic functions",
    "text": "10.2 Logit & logistic functions\nThe general game plan for dealing with a binary outcome is to transform it into a different variable that is easier to work with, run the analysis, and then “reverse-transform” the model coefficients so that they are interpretable in terms of the original binary variable. This strategy should sound familiar from Chapter 8 – it’s the same overall approach we used for log-linear regression. Also in common with Chapter 8, we are going to use logs and exponents as the main workhorse for this approach (that is where the “log” in logistic comes from).\nHowever, the overall strategy for transforming the \\(Y\\) variable in logistic regression is a bit more complicated than the log-linear model. So, it is helpful to start wit an overall “roadmap”.\n\nStep 1 (from binary to probability). First, we are going to work with probabilities rather than the original binary variable. In terms of our example, we are going to shift focus from whether or not a person has CHD to the probability of a person having CHD.\nStep 2 (from probability to logistic). The logistic function is widely-used model for probabilities. In terms of our example, we are going to use the logistic function to relate the probability of a person having CHD to their age.\nStep 3 (from logistic to logit). The logistic function has a nice interpretation, but it is not a linear function of age. So, we are going to transform it into something that is linear in age, which will let us “port over” a lot of what we have learned about linear models. Actually, the reason we choose the logistic function as a model of probability is because this transform is relatively straightforward and can be “undone” afterwards when interpreting the model coefficients, just like with log-linear regression. The transformation two steps:\n\nStep3A (probability to odds). First we transform the probability of having CHD into the odds of having CHD. If \\(p\\) denotes probability then odds are just \\(p / (1-p)\\). We will spend a while talking about how to interpret odds.\nStep 3B (odds to logit). Then we take the log of the odds, which is called the logit. The logit turns out to be a linear function of age, so we can model the relationship between age and the logit of CHD in a way that is very similar to regular linear regression.\n\n\nSo, that’s the overall approach to dealing with a binary variable in logistic regression. Clear as mud, right? Don’t worry, we will walk through each step in the following subsections. If you find yourself getting lost in the details, it can be helpful to refer back to this overall strategy. In short, the overall game plan is:\n\\[ \\text{binary outcome} \\rightarrow \\text{probability} \\rightarrow \\text{logistic} \\rightarrow \\text{logit (log odds) }\\]\nOnce we have all these concepts in play, we can start doing logistic regression.\n\n10.2.1 From binary to probability\nThe following table presents the example data in terms of the proportion of cases with CHD, broken down by age groups. The first column shows the age groups, the second shows the number of cases without CHD, the third shows the number of cases with CHD, and the last column shows the proportion of cases with CHD.\n\n\nCode\nknitr::include_graphics(\"files/images/props.png\")\n\n\n\n\n\nFigure 10.2: From a binary variable to proportions\n\n\n\n\nRecall that a proportion is computed as the number of cases of interest over the total number of cases. In terms of the table above:\n\\[ p(CHD = 1) = \\frac{ N_1}{N_0 + N_1 }  \\tag{10.1}\\]\nwere \\(N_1\\) denotes the number of cases with CDH, and \\(N_0\\) is the number of cases without.\nThe number \\(p(CHD = 1)\\) can be interpreted in many ways, which leads to a lot of terminology here.\n\nThe proportion of cases in our sample with CHD.\nIf we multiply by 100, it is the percentage of cases with CHD (i.e., cases per 100) in our sample.\nIf we multiply by a number other than 100 (say 1000), it is the rate of CHD (e.g., cases per 1000) in our sample.\nSince a proportion is just the mean of binary variable, it is the mean or expected value of CHD in our sample.\nAnd finally, since proportions are one interpretation of probability, it is the probability of CHD in our sample.\n\nYou might hear all of these terms (i.e., proportion, percentage, rate, mean, probability) used in connection with logistic regression. But, they are all just different ways of interpreting the rightmost column of Figure 10.2. I will try to make a point of using all of these terms so you get used to interpreting them in this context :)\nAnother concept that will be useful for interpreting our data is odds. Odds are closely related to, but not the same as, probability. The figure below adds the odds of having CHD to Figure Figure 10.2.\n\n\nCode\nknitr::include_graphics(\"files/images/odds.png\")\n\n\n\n\n\nFigure 10.3: Proportions and odds\n\n\n\n\nAs shown in the table, the odds are also a function of the two sample sizes, \\(N_1\\) and \\(N_0\\):\n\\[\\text{odds}(CHD = 1) = \\frac{N_1}{N_0}. \\tag{10.2}\\]\nLet’s take a moment to compare the interpretation of probability versus odds.\n\nThe first row of the table tells us that the probability of having CHD in your 20’s is “1 in 10”. Loosely, this means that for every 10 people in their 20s, one of them will have CHD.\nBy contrast, the odds of having CHD in your twenties is “1 to 9”. Roughly, this means that for every person in their twenties with CHD, there are nine without CHD.\n\nClearly, probabilities and odds are just two different ways of packaging the same information. The following equations shows the relation between odds and probability (these are derived from Equations Equation 10.1 and Equation 10.2 using algebra)\n\\[\n\\begin{align}\np(CHD = 1) & = \\frac{\\text{odds}(CHD = 1)}{1 + \\text{odds}(CHD = 1)} \\\\ \\\\\n\\text{odds}(CHD = 1) & = \\frac{p(CHD = 1)}{1 - p(CHD = 1)}\n\\end{align}\n\\tag{10.3}\\]\nWe will see these relations again shortly. But, before moving on, let’s get some more practice interpreting odds and probabilities using the data in Figure 10.3. Please write down your answers to the following questions and be prepared to share them in class. For each question provide a verbal interpretation of the numerical answer (e.g, odds of 2 to 1 means that for every two people with a trait, there is one without.) \n\nWhat is the probability of a person in their 40s having CHD?\nWhat are the odds of a person in their 40s having CHD?\nWhat is the probability of someone in their 50s not having CHD?\nWhat are the odds of someone in their 50s not having CHD?\nWhat is probability of having CHD in your 40s, compared to your 30s? (e.g., is 3 times higher? 4 times higher?)\nWhat are the odds of having CHD in your 40s, compared to your 30s?\n\nThe answers hidden below (use the Code button to reveal), but you won’t learn anything if you don’t try the question yourself first!\n\n\nCode\n# 1. .39, so about 40% of people\n# 2. 11/17, so for 11 people with CHD there are 17 without\n# 3. 1 - .72 = .28, so about 28% of people \n# 4. (18/7)^-1 = 7/18, so 7 out ever 18 people\n# 5. .39 / .19 ~= 2, so the probability of having CHD in your 40s is about 2 times higher than the probability of having CHD in your 30s. This is called a relative risk, or a risk ratio. \n#6. (11/17)/(5/22) ~= 2.8, so the odds of having CHD in your 40s is about 2.8 times higher than the odds of having CHD in your 30s. This is called an odds ratio. \n\n\n\n\n10.2.2 From probability to logistic\nOn thing you may have noted about the CHD data is that the proportion of cases with CHD increases with age. This relationship is shown visually in Figure 10.4.\n\n\nCode\n# Sample proportions\nprop &lt;- tapply(chd, catage, mean)\n\n# Age categories\nyears &lt;- unique(catage)*10\n\n# Plot\nplot(years, \n     prop, \n     type = \"l\", \n     lwd = 2, \n     col = \"#4B9CD3\", \n     ylab = \"p(CHD =1)\", \n     xlab = \"Age categories\")\n\n\n\n\n\nFigure 10.4: Proportion of cases with CHD as a function of age\n\n\n\n\nLooking at the plot, we might suspect that the relationship between the probability of CHD and age is non-linear. In particular, we know that probabilities cannot take on values outside of the range \\((0, 1)\\), so the relationship is going to have to “flatten out” in the tails. For example, even if you are a baby, your probability of having CHD cannot be less than 0. And, even if you are centenarian, the probability can’t be great than 1.\nBased on this reasoning, we know that the relationship between age and the rate of CHD should take on a sort of “S-shaped” curve or “sigmoid”. This S-shape is hinted at in Figure 10.4 but is not very clear. Some clearer examples are shown in Figure 10.5.\n\n\nCode\n# Logistic function\nlogistic &lt;- function(x, a, b){exp(a*x + b) / (1 + exp(a*x + b))}\n\n# Generate data\nx &lt;- seq(-5, 5, by = .1)\n\n# Plots\nplot(x, logistic(x, 1, 0), \n     type = \"l\", \n     lwd = 2, \n     col = 2, \n     ylab = \"logistic\")\n\npoints(x, logistic(x, .75, -1.5), \n       type = \"l\", \n       lwd = 2, \n       col = 3, \n       ylab = \"logistic\")\n\npoints(x, logistic(x, 1.5,- 1), \n       type = \"l\", \n       lwd = 2,\n       col = 4, \n       ylab = \"logistic\")\n\npoints(x, logistic(x, 3, 2), \n       type = \"l\", \n       lwd = 2, \n       col = 5, \n       ylab = \"logistic\")\n\n\n\n\n\nFigure 10.5: Examples of sigmoids\n\n\n\n\nThe mathematical equation used to create these S-shaped curves is called the logistic function, the namesake of logistic regression. All you need to take-away from Figure 10.5 is that there is mathematical function that produces the kind of relations we are expecting between age (continuous) the the probability of having CHD (bounded to the interval \\((0, 1)\\)).\nReturning to our example, we can see in Figure 10.6 that the logistic function provides a reasonable approximation for the relationship between the rate of CHD and age.\n\n\nCode\npar(mfrow = c(1, 2))\nplot(years, prop, type = \"l\", lwd = 2, col = \"#4B9CD3\", ylab = \"p(CHD =1)\", xlab = \"Age categories\")\nplot(20:60, logistic(20:60, .12, -5.2), col = \"#4B9CD3\", ylab = \"logistic\", xlab = \"Age in years\")\n\n\n\n\n\nFigure 10.6: Proportion of cases with CHD, data versus logistic\n\n\n\n\nOne important thing to notice about Figure 10.6 is that the plot on the left required re-coding age into a categorical variable and computing the proportion of cases with CHD in each age category (see Figure 10.2). However, the logistic plot on the right did not require categorizing age. So, one advantage of using the logistic function is that we can model the probability of CHD as a function of age “directly”, without having to categorize our predictor variables.\nThe take home message of this section is that the logistic function is a nice way to model how a proportion depends on a continuous variable like age. Next, we’ll talk about the math of the logistic function in a bit more detail.\n\n\n10.2.3 Logistic to log odds (logit)\nThe formula for the logistic function (i.e., the function that produced the curves in Figure 10.5 is\n\\[p = \\frac{\\exp(x)}{1 + \\exp(x)}. \\tag{10.4}\\]\nThis function maps the variable \\(x\\) onto the interval \\((0, 1)\\). In Figure 10.6 we saw that the logistic function can provide a nice model for probabilities. We also saw that the logistic function is non-linear function of \\(x\\) (i.e., it is sigmoidal or S-shaped).\nHowever, a nice thing about the logistic function is that we can transform it into a linear function of \\(x\\). Since we already know how to deal with linear functions (that is what this whole course has been about!), transforming the logistic into a linear function of \\(x\\) will let us port over a lot of what we know about linear regression to situations in which the outcome variable is binary. (In fact, the real motivation for choosing the logistic function in the first place, rather than some other S-shaped curve.)\nSo, let’s see how to get from our S-shaped logistic function of \\(x\\) to a linear function of \\(x\\). Algebra with Equation 10.4 shows that we can re-express the logistic function in terms of the odds:\n\\[\\frac{p}{1- p} = \\exp(x). \\tag{10.5}\\]\nNote that Equations Equation 10.4 and Equation 10.5 directly parallel the two expressions in Equation Equation 10.3. The only difference is that, in the logistic model, the odds are represented as an exponential function of the variable \\(x\\), which is what Equation Equation 10.5 is telling us.\nIn order to turn Equation 10.5 into a linear function of \\(x\\), all we need to do is get rid of the exponent. Do you remember how?? That’s right, just take the log (see Section 8.1):\n\\[ \\log\\left(\\frac{p}{1- p}\\right) = x. \\tag{10.6}\\]\nThis equation is telling us that the log of the odds is linear in \\(x\\). The log-odds is also called the logit, which is short for “logistic unit.”\nThe relationship among the logistic, odds, and logit are summarized in Figure 10.7.\n\n\nCode\nknitr::include_graphics(\"files/images/logit.png\")\n\n\n\n\n\nFigure 10.7: Logistic, odds, and logit\n\n\n\n\n\nThe left-hand panel shows the logistic function. This is our “intuitive-but-nonlinear” model for probabilities. In terms of our example, this panel is saying that the probability of having CHD is a logistic or S-shaped function of age.\nThe middle panel shows that the odds are an exponential function of \\(x\\). In terms of our example, this means that the odds of having CHD are an exponential function of age. This is the main assumption of the logistic model, and we will revisit this assumption again when we get to ?sec-assumption-checking-10.\nFinally, the right-hand panel shows the “not-really-intuitive-but-definitely-linear” model for the logit. In terms of our example, the logit of having CHD is a linear function of age.\n\nThe next section discusses how to interpret the logit by reverse-transforming it back to the odds and probabilities. The situation is a lot like log-linear regression (Chapter 9).\n\n\n10.2.4 Pop quiz\nBefore moving, lets nail down the relation between probability, odds, and logits. Figure 10.8) presents the relationship in tabular form.\n\n\n\n\n\nFigure 10.8: Logistic, odds, and logit\n\n\n\n\nI will asks some questions along the following lines in class.\n\nIf probability of an event is equal to .1, what are the odds the event? What is the logit of the event?\nIf odds of an event are 4 to 1, what is the probability of the event? What is the logit of the event?\nIf logit &lt; 0 then probability &lt; ? and odds &lt; ?\nWhat is more likely: a event with probability of .9 or an event with odds of .9?\nIf a probability of \\(p\\) corresponds to a \\(\\text{logit}\\) of \\(x\\), what is the logit corresponding to \\(1-p\\)? (Hint, try some numerical examples from the table).\n\n\n\n10.2.5 Next steps\nThe logit is our workhorse for logistic regression. In Section 10.3, we will replace the variable \\(x\\) with a simple regression model \\(a + bX\\) to get simple logistic regression. In ?sec-multiple-10 we will extend simple logistic regression to multiple logistic regression, just like we did for multiple linear regression.\nAlthough the logit is the workhorse, we generally don’t want to work with the logit when it comes time to interpret the results. The situation here is a lot like log-linear regression (Chapter 9). In log-linear regression, we treated \\(\\log(Y)\\) as a linear function of our predictor variable(s). However, we didn’t want to interpret the model in terms of \\(\\log(Y)\\), because, well, who thinks in log units? Instead we wanted an interpretation in terms of the original outcome, \\(Y\\).\nThe same situation applies here. You may have already noted that the relationship between the logit (i.e., \\(\\log(\\text{odds})\\)) and \\(\\text{odds}\\) in logistic regression is the same as the relationship between \\(\\log(Y)\\) and \\(Y\\) in log-linear regression. The parallel between the two model is as follows:\n\nIn the log-linear model we interpreted a \\(b\\) unit increase in \\(\\log(Y)\\) in terms of an \\((\\exp(b) - 1) \\times 100\\%\\) change in \\(Y\\) (see Section (ref-interpretation-8?))).\nIn logistic regression we will interpret a \\(b\\) unit increase in \\(\\text{logit}(Y)\\) in terms of an \\((\\exp(b) - 1) \\times 100\\%\\) times change in \\(\\text{odds}(Y)\\).\n\nSo, while we use the logit function for modeling, we often use the odds for interpretation. One subtle difference to be aware of is that, in logistic regression, we usually report results in terms of relative magnitude (called the odds ratio) rather than relative change, although relative change is often used for verbal reporting. We will see examples in the next section.\nSome authors have argued that people don’t really know how to interpret odds properly. These authors suggest that we interpret the logistic model in terms of probabilities, rather than odds. We will discuss how to do this as well.\n\n\n10.2.6 Summary\nAt this point we have covered the overall logic of how we can model a binary outcome variable like CHD in terms of the logistic function. The overall situation is very similar to, but a bit more complicated than, log-linear regression. The main take aways are\n\nWe use the logit (log-odds) for statistical analysis, because it results in a linear function, and we already know how to deal with linear functions.\nWe use the odds for interpretation, because the logistic model leads to proportional change in the odds, in the same way that the log-linear model leads to proportional change in \\(Y\\).\nWe can also use probabilities for interpretation, but, since the logistic model implies that probabilities are non-linear (sigmoidal), things can get a bit complicated with this approach."
  },
  {
    "objectID": "ch10_logistic.html#sec-simple-10",
    "href": "ch10_logistic.html#sec-simple-10",
    "title": "10  Logistic regression",
    "section": "10.3 Simple logistic regression",
    "text": "10.3 Simple logistic regression\nIn this section we move onto logistic regression proper. For the CHD example, the model we are interested in is\n\\[\\text{logit}(CHD) = a + b (\\text{AGE}). \\]\nWe are going to skip a few steps and go right into the interpretation of the R output. Once we know how to interpret the output, we will loop back to discuss details of estimation and inference in the following sections.\nThe summary R output for the example is below. The focus for now is just the interpretation of the values under the “Estimate” heading.\n\n\nCode\nmod2 &lt;- glm(chd ~ age, family = binomial, data = chd.data)\nsummary(mod2)\n\n\n\nCall:\nglm(formula = chd ~ age, family = binomial, data = chd.data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.30945    1.13365  -4.683 2.82e-06 ***\nage          0.11092    0.02406   4.610 4.02e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 136.66  on 99  degrees of freedom\nResidual deviance: 107.35  on 98  degrees of freedom\nAIC: 111.35\n\nNumber of Fisher Scoring iterations: 4\n\n\nPlugging the estimates into our logit model, we have the following equation\n\\[ \\text{logit}(CHD) = -5.31  + .11 (\\text{age}). \\]\nThe “literal” interpretation of this equation is:\n\nWhen \\(\\text{age} = 0\\), \\(\\text{logit}(CHD) = -5.31\\).\nEach unit of increase in age (i.e., each additional year) is associated with a .11 unit increase in \\(\\text{logit}(CHD)\\).\n\nWhile this interpretation is perfectly correct, most applied audiences are not going to know how to interpret \\(\\text{logit}(CHD)\\). So, instead, we often work with the odds and probabilities, as outlined in the next few sections.\n\n10.3.1 Odds ratio\nThe logistic regression model implies\n\\[\\frac{\\text{odds} (X+1)}{\\text{odds}(X)} = \\exp(b) \\tag{10.7}\\]\nwhere \\(\\text{odds}(X)\\) are the odds of the outcome associated with a given value of the predictor \\(X\\). Equation Equation 10.7 is called the odds ratio (abbreviated OR) associated with a one-unit increase in \\(X\\).\nIf you refer back to section Section 8.5.5, you can see we are using the exact same approach from log-linear regression, but in Equation 10.7 we interpret the regression coefficient in term of the odds that \\(Y = 1\\), rather than the \\(Y\\) variable itself.\nFor the CHD example, the OR is:\n\\[\\exp(b) = \\exp(.11) = 1.1163 \\]\nThis means that each additional year of age is associated with an OR of 1.11. For example, the odds for someone aged 21 having CHD is 1.11 time larger (relative magnitude) that someone aged 20. The really useful thing about the OR is that it is constant over values of the predictor. So, regardless of whether we are comparing a 21-year-old to a 20-year-old, or 41-year-old to a 40-year-old, the OR is the same.\nJust like the log-linear model, we can also report the results of our analysis in terms of relative change rather than relative magnitude. In particular, the percent increase in the odds of CHD associated with each additional year of age is:\n\\[(\\exp(.11) - 1) \\times 100 = 11.63\\% \\]\nThis means that the predicted odds of CHD increase 11.63% for each additional year of age.\nWhether you use relative magnitude (i.e., the odds ratio) or relative change (i.e., percent change in odds) to report the results of logistic regression is up to you. In many fields, it is conventional to reports the odds ratios in tables, but to use percent change when writing about results in a sentence.\nBefore moving, please practice your interpretation of the OR in simple logistic regression using the following examples\n\nIf \\(b=0\\) what is the OR equal to? What is the percent change in the odds for a one unit increase in \\(X\\)?\nIf \\(b=.25\\) what is the OR equal to? What is the percent change in the odds for a one unit increase in \\(X\\)?\nIf \\(b=-.025\\) what is the OR equal to? What is the percent change in the odds for a 10 unit increase in \\(X\\)?\nIf the odds increase 100% for a one unit increase in \\(X\\), what \\(b\\) equal to?\n\nAnswers hidden below (use Code button), but please try out the questions yourself first!\n\n\nCode\n# 1. OR = exp(0) = 1 and percent change equals (exp(0) - 1) X 100 = 0%. So, \"no relationship\" means OR = 1. \n# 2. OR = exp(.25) = 1.2840 and percent change equals (exp(.25) - 1) X 100 = 28.40% increase\n# 3. OR = exp(-.025) = 0.9753 and percent change for one unit equals (exp(-.025) - 1) X 100 = (-.02469 X 100 -2.469%. For 10 units of change, multiply by 10, which gives 24.69% decrease (negative sign is decrease). \n# 4. (exp(b) - 1) X 100 = 100 --&gt; exp(b) = 2 --&gt; b = log(2) = .6931\n\n\n\n\n10.3.2 Other interpretations: Predicted probabilities\nAnother way to interpret the logistic model is in terms of the predicted probabilities, which are plotted below for the example data.\n\n\nCode\nvisreg::visreg(mod2, xvar = \"age\", scale = \"response\")\n\n\n\n\n\nFigure 10.9: Predicted probabilities\n\n\n\n\nUsing this plot, we can read off the probability of CHD for any given age. We might also want to report the probability of CHD for two or more chosen ages, which is an example of the MERV approach to marginal effects (see Section 5.4):\n\n\nCode\nlibrary(emmeans)\nemmeans(mod2, specs = \"age\", at = list(age = c(20, 40)), type = \"response\")\n\n\n age   prob     SE  df asymp.LCL asymp.UCL\n  20 0.0435 0.0279 Inf    0.0121     0.145\n  40 0.2947 0.0578 Inf    0.1951     0.419\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nThe ratio of two probabilities is often called the risk ratio or the relative risk. So, we could also say that the risk ratio of CHD for someone who in their 40s as compared to someone who is 20 is .2947 / .0435 = 6.77. Otherwise stated, the risk of having CHD in you are 40s is almost 7 times higher than in your 20s (relative magnitude).\n\n\n10.3.3 Other interpretations: Equal odds\nAnother interpretation of logistic regression is to report the value of \\(X\\) at which the \\(\\text{odds}\\) of the outcome are equal to 1 (equivalently, the probability of the outcome is equal to .5). This idea is illustrated in Figure 10.10.\n\n\nCode\nx &lt;- data.frame(age = 20:70)\nprob &lt;- predict(mod2, newdata = x, type = \"response\")\nplot(x$age, prob, xlab = \"age\", ylab = \"p(CHD)\", type = \"l\", col = \"#4B9CD3\")\nsegments(x0 = 15, y0 = .5, x1 = 48, y1 = .5, lty = 2, lwd = 2)\nsegments(x0 = 48, y0 = 0, x1 = 48, y1 = .5, lty = 3, lwd = 2)\n\n\n\n\n\nFigure 10.10: The Equal Odds Interpretation\n\n\n\n\nFirst we find the probability of .5 on the \\(Y\\) axis and then follow the horizontal dashed line to the logistic curve. Then we follow the vertical dashed line down to the value of \\(X\\). This gives use the age at which the probability of CHD is “50-50”. Based on the plot we can say that, after your 48th birthday, your chances of having CHD are above 50%.\nThe math behind this interpretation is below. Since\n\\[\\log(.5/.5) = \\log(1) = 0 \\]\nwe can solve\n\\[ a + b(\\text{age}) = 0 \\]\nto find the age at which someone has equal odds of CHD, leading to\n\\[\\text{age} = - a/b. \\]\nFor the example data\n\\[ \\text{age} = - a/b = - (-5.31) / .11 = 48.27, \\]\nwhich confirms the conclusion we made looking at the plot.\nIn summary, another way of interpreting regression coefficients in simple logistic regression is to compute \\(-a / b\\), which gives the value of \\(X\\) at which \\(p(Y = 1) = .5\\).\n\n\n10.3.4 Other interpretations: Rate of change\nYet another interpretation is in terms of the slope of the straight line (tangent) through the point \\(p(CHD = 1) = .5\\). The slope of this line describes the rate of change in the probability of CHD for people who are “close to” the age of equal odds (48 years in our example). The tangent line for our example is shown in Figure 10.11.\n\n\nCode\nx &lt;- data.frame(age = 20:70)\nprob &lt;- predict(mod2, newdata = x, type = \"response\")\nplot(x$age, prob, xlab = \"age\", ylab = \"p(CHD)\", type = \"l\", col = \"#4B9CD3\")\nabline(a = -.815, b = .11/4, lty = 2, lwd = 2)\n\n\n\n\n\nFigure 10.11: The Rate of Change Interpretation\n\n\n\n\nIt turns out that the slope of the tangent line is equal to exactly \\(b/4\\). The derivation requires calculus and is omitted (ask in class if you are interested!). For the example data \\(b / 4 = .11 / 4 = .0275\\). So, for every additional year, the predicted probability of CHD increases by .0275. Keep in mind, this interpretation only applies to people who “around” the age of equal odds (48 years old in this example). Looking at the plot, we can see that this approximation is pretty good for people between the ages of 40 and 60.\n\n\n10.3.5 Relation to linear probability model\nNotice that in our example, the logistic function is roughly linear for probabilities in the range \\([.2, .8]\\). As mentioned in the introduction of this chapter, this is the situation in which using linear regression with a binary outcome (i.e., the linear probability model) works “well enough”. Also note that the regression coefficient from the linear probability model in Section 10.1 (\\(b_{OLS} = .0218\\)) is in the ballpark of the coefficient computed above (\\(b_{logistic} / 4 = .0275\\)). These numbers are both describing how the probability of CHD is related to a person’s age.\nThe logistic model also provides us with a way “diagnosing” whether the linear probability model is a good approximation. As noted, the logistic function is roughly linear for probabilities in the range \\([.2, .8]\\). If we ran a linear regression on CHD and none of the fitted / predicted values are outside the range \\([.2, .8]\\), we would be in the situation where we might prefer to use linear regression (despite it being technically wrong). Referring back to residual vs. plotted in Section 10.1, we can see that the predicted values were outside of this range, so the logistic model is the better way to approach for this example.\n\n\n10.3.6 Summary\nThe simple logistic regression model\n\\[ \\text{logit}(CHD) = a + b (\\text{AGE}) \\]\nhas the following interpretations.\n\nIn terms of the logit:\n\nThe intercept (\\(a\\)) is the predicted value of the log-odds of CHD when age = 0.\nThe slope (\\(b\\)) is how much the predicted log-odds of CHD changes for one unit (year) of increase in age.\n\nIn terms of the odds:\n\nThe exponent of the regression parameter, \\(\\exp(b)\\), is odds ratio associated with a one unit of increase in age (relative magnitude).\n\\((\\exp(b) - 1) \\times 100\\) is the percentage change in the odds of CHD for a one unit increase in age (relative change).\nFor the intercept, \\(\\exp(a)\\) is the predicted odds of having CHD when \\(age = 0\\). This isn’t a very useful number in our example, but it can be useful when the predictor(s) are centered.\n\nIn terms of predicted probabilities\n\nThe logistic plot provides a visual summary of how the probability of CHD changes as a function of age – if you want to report in terms of probabilities, this plot is usually a good choice!\nPredicted probabilities can be reported for specific values of age (MERVs), and risk ratios / relative risk can be computed to compare the predicted probabilities at specific ages. This is the same approach we used for following-up interactions in Section 5.4 (i.e., you can use emmeans in R).\n\\(–a / b\\) corresponds the age at which the probability of having CHD is “50-50”\n\\(b / 4\\) is the approximate rate of increase (slope) of the probability of having CHD for people close to the “50-50” point.\n\n\nPlease write down the numerical values of each of the above summaries for the CHD example (except the predicted probability plot). You can select any values of age to report the predicted probabilities and risk ratios, and you can “eye ball” the probabilities using Figure 10.9.\nAnswers are hidden below (use the Code button), but please try them yourself first.\n\n\nCode\n# Logit: \n# predicted logit when age = 0: a = -5.31;\n# expected increase in logit when age increases by one year: b = .11\n\n# Odds:\n# OR = exp(b) = exp(.11) = 1.1163\n# % change in Odds = (exp(b) - 1) X 100 = 11.63%\n\n# Probability\n# In the example, used risk for Age = 40 relative to Age = 20: .2947/.0435 = 6.77\n# \"50-50\" age: -a/b = 5.31 / .11 = 48.27 \n# Rate of change at \"50-50\" age: b/4 = .11/4 = .0275"
  },
  {
    "objectID": "ch10_logistic.html#workbook",
    "href": "ch10_logistic.html#workbook",
    "title": "10  Logistic regression",
    "section": "10.4 Workbook",
    "text": "10.4 Workbook\nThis section collects the questions asked in this chapter. The lessons for this chapter will focus on discussing these questions and then working on the exercises in ?sec-exercises-10. The lessons will not be a lecture that reviews all of the material in the chapter! So, if you haven’t written down / thought about the answers to these questions before class, the lesson will not be very useful for you. Please engage with each question by writing down one or more answers, asking clarifying questions about related material, posing follow up questions, etc.\n?sec-CHD-example-10\n\nBefore moving on, please take a moment to write down you conclusions (and rationale) about whether the assumptions of linear regression are met for the CHD data.\n\n\n\n\n\n\nLinear probability model with CHD example\n\n\n\n\n\nCall:\nlm(formula = chd ~ age, data = chd.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85793 -0.33992 -0.07274  0.31656  0.99269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.537960   0.168809  -3.187  0.00193 ** \nage          0.021811   0.003679   5.929 4.57e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.429 on 98 degrees of freedom\nMultiple R-squared:  0.264, Adjusted R-squared:  0.2565 \nF-statistic: 35.15 on 1 and 98 DF,  p-value: 4.575e-08\n\n\nSection 10.2\n\n\nCode\nknitr::include_graphics(\"files/images/odds.png\")\n\n\n\n\n\nProportions and odds\n\n\n\n\n\nUsing the Table above, please write down your answers to the following questions and be prepared to share them in class. For each question provide a verbal interpretation of the numerical answer (e.g, odds of 2 to 1 means that for every two people with a trait, there is one without.)\n\nWhat is the probability of a person in their 40s having CHD?\nWhat are the odds of a person in their 40s having CHD?\nWhat is the probability of someone in their 50s not having CHD?\nWhat are the odds of someone in their 50s not having CHD?\nWhat is probability of having CHD in your 40s, compared to your 30s? (e.g., is 3 times higher? 4 times higher?)\nWhat are the odds of having CHD in your 40s, compared to your 30s?\n\n\n\n\n\n\n\nLogistic, odds, and logit\n\n\n\n\n\nUsing the Table above, please answer the following questions\n\nIf probability of an event is equal to .1, what are the odds the event? What is the logit of the event?\nIf odds of an event are 4 to 1, what is the probability of the event? What is the logit of the event?\nIf logit &lt; 0 then probability &lt; ? and odds &lt; ?\nWhat is more likely: a event with probability of .9 or an event with odds of .9?\nIf a probability of \\(p\\) corresponds to a \\(\\text{logit}\\) of \\(x\\), what is the logit corresponding to \\(1-p\\)? (Hint, try some numerical examples from the table).\n\n\nSection 10.3\n\nPlease practice your interpretation of the OR in simple logistic regression using the following examples:\n\nIf \\(b=0\\) what is the OR equal to? What is the percent change in the odds for a one unit increase in \\(X\\)?\nIf \\(b=.25\\) what is the OR equal to? What is the percent change in the odds for a one unit increase in \\(X\\)?\nIf \\(b=-.025\\) what is the OR equal to? What is the percent change in the odds for a 10 unit increase in \\(X\\)?\nIf the odds increase 100% for a one unit increase in \\(X\\), what \\(b\\) equal to?\n\nPlease write down the numerical values of each of the above summaries for the CHD example (except the predicted probability plot). You can select any values of age to report the predicted probabilities and risk ratios, and you can “eye ball” the probabilities using Figure 10.9.\n\n\\[ \\text{logit}(CHD) = -5.31  + .11 (\\text{age}). \\] * In terms of the logit. * In terms of the odds. * In terms of predicted probabilities * Risk ratios / relative risk for some chosen ages * The age at which the probability of having CHD is “50-50” * The approximate rate of increase (slope) of the probability of having CHD for people close to the “50-50” point."
  }
]