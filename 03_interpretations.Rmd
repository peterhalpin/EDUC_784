# Interpretations of Regression {#chapter-3}

```{r, echo = F}
# Reload example when knitting just this chapter
button <-  "position: relative;
            top: -25px;
            left: 85%;
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```


Before moving onto more complicated regression models, let's consider why we might be interested in them in the first place. As discussed in the following sections, regression has three main uses:

* Prediction (focus on $\hat Y$)
* Causation (focus on $b$)
* Explanation (focus on $R^2$)

By understanding these uses, you will have a better idea of how regression is applicable to your own research. Each of these interpretations also provides a different perspective on the importance of using multiple predictor variables, rather than only a single predictor.

## Prediction

```{r echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
#load("NELS.RData")
#attach(NELS)
```

<!-- I think ggplot is doing something weird because the prediction error is way too small for the full sample -- but then again N is in the denominator of the variance. Dobule check by hand and also use a second examples to show that prediction error decreases when adding another predictor -->

Prediction (etymology: “to make known beforehand”) means that we want to use $X$ to make a guess about $Y$. This use of regression makes the most sense when we know the value of $X$ before we know the value of $Y$.

When we are interested in using values of $X$  to make predictions about (yet unobserved) values of $Y$, we use $\hat Y$ as our guess. This is why $\hat Y$ is called the "predicted value" of $Y$.

When making predictions, we usually want some additional information about how good the predictions will be. In OLS regression, this information is provided by the prediction error variance (cite: Fox)

\[ s^2_{\hat Y_i} = \frac{SS_{\text{res}}}{N - 2} \left(1 +  \frac{1}{N} + \frac{(X_i - \bar X)^2}{(N-1) s^2_X} \right). \]

The prediction errors for the NELS example in Figure \@ref(fig:fig1) are represented in Figure \@ref(fig:fig3) as a gray band around the regression line.

<!-- This figure is plotting the in-sample prediction error, not the out-of-sample prediction error (i.e., the error band it too small. Need to do plot manually to get correct predictor bands) --> 

```{r, fig3, warning=F, message=F, fig.cap = 'Prediction Error for Example Data.', fig.align = 'center'}

load("NELS.RData")
library(ggplot2)
ggplot(NELS, aes(x = ses, y = achmat08)) +
               geom_point(color='#3B9CD3', size = 2) +
               stat_smooth(method = stats::lm, 
                           color = "grey35",
                           level = .9999999) +
               ylab("Math Achievement (Grade 8)") +
               xlab("SES") +
               theme_bw()

```

Notice that the prediction error variance increases with $SS_{\text{res}}$, which is the variance in the outcome that is *not* explained by the predictor (see Section \@ref(rsquared-2)). As we will see in the following chapters, one way to reduce $SS_{\text{res}}$ is by adding more predictors to the model. In other words, we can obtain better predictions if we use more predictor variables -- i.e., multiple regression. 

### More about prediction

Prediction was the original use of regression (see https://en.wikipedia.org/wiki/Regression_toward_the_mean#History). More recent methods developed in machine learning also focus mainly on prediction -- although the methods used in machine learning are often more complicated than OLS regression, and the research context is usually quite different, the basic problem is the same. Machine learning has led to the use of out of sample predictions, rather than prediction error, as the main criterion for judging the quality of predictions made from a model. Machine learning has also introduced some new techniques for choosing which potential predictors to include in a model (i.e., "variable selection"). We will touch on these topics later in the course, although our main focus is OLS regression.

### Regression toward the mean

Regression got its name from a statistical property of predicted scores called "regression toward the mean." To explain this property, let's assume $Y$ and $X$ are z-scores (i.e., both variables have $M = 0$ and $SD = 1$). Recall that this implies that $a = 0$ and $b = r_{XY}$, so the regression equation reduces to

\[ \hat Y = r_{XY} X \]

Since $|r_{XY} | ≤ 1$, the absolute value of the $\hat Y$ must be less than or equal to that of $X$. And, since both variables have $M = 0$, this implies that $\hat Y$ is closer to the mean of $Y$ than $X$ is to the mean of $X$. This is what is meant by regression toward the mean.


## Causation

A causal interpretation of regression means that that changing $X$ by one unit will change $\mu_{Y|X}$ by $b$ units. Note that this is a statement about the population conditional mean function, not the sample predicted values. 

The gold standard for ensuring a causal interpretation of a regression coefficient is to randomly assign people to the different levels of the predictor. While this type of experimental design is possible in some settings, there are many types of variables we cannot feasibly randomly assign (e.g., SES). 

In regression analysis, the concept of an omitted variable is used to  understand how lack of random assignment affects the interpretation of the model parameters. An omitted variable is any variable that is correlated with both $Y$ and $X$. In the experimental context, random assignment serves to ensure that $X$ is not correlated with any pre-treatment variables. In other words, random assignment can be interpreted as an experimental procedure for eliminating omitted variables. 

The problem that arises when we don't use random assignment is called *omitted variable bias*. This situation is nicely explained by Gelman and Hill (cite:Gelman), and a modified version of their discussion is provided below. The overall idea is the same as "correlation $\neq$ causation", but the discussion is a bit technical, so the take-home messages are summarized in the following points. 

* When a predictor variable that is correlated with $Y$ and with $X$ is left out of a regression model, it is called an omitted variable.

* The problem is not just that we have an incomplete picture of how the omitted variable is related to $Y$. 

* Omitted variable bias means that the predictor variable that *was included in the model* ends up having the wrong regression coefficient. Otherwise stated, the regression coefficient of the predictor in the model is biased. 

* In order to mitigate omitted variable bias, we want to include plausible omitted variables in our regression models -- i.e., multiple regression. 


### Omitted variable bias*

We start by assuming a "true" regression model with two predictors. In the context of our example, this means that there is one other variable, in addition to SES, that is important for predicting Math Achievement. Of course, there are many predictors of Math Achievement (see Section \@ref(example-2)), but we only need two to explain the problem of omitted variable bias.

Let's write the "true" model as:

\begin{equation}
Y = a + b_1 X_1 + b_2 X_2 + \epsilon
(\#eq:2parm)
\end{equation}

where $X_1$ is SES and $X_2$ is any other variable that is correlated with both $Y$ and $X_1$ (e.g., maternal education).

Next, imagine that instead of using the model in \@ref(eq:2parm), we analyze the data using the model with just SES. In our example, this would reflect a situation in which we don't have data on maternal education, so we have to make due with just SES, leading to the usual regression line (Section \@ref(regression-line-2)):

\[
\hat Y = a^* + b^*_1 X_1 + \epsilon^*
(\#eq:1parm)
\]

The basic problem of omitted variable bias is that $b_1 \neq b^*_1$ -- i.e., the regression coefficient in the true model is not the same as the regression coefficient in the model with only one predictor. This is perhaps surprising -- leaving out maternal education gives us the wrong regression coefficient for SES!

To see why, start by writing $X_2$ as a function of $X_1$.

\[
X_2 = \alpha + \beta X_1 + \nu
(\#eq:X2)
\]

where the regression coefficients are written with Greek letters to distinguish them from the previous equations, and the residual is denoted $\nu$ instead of $\epsilon$ for the same reason. 

Next we use Equation \@ref(eq:X2) to substitute for $X_2$ in Equation \@ref(eq:2parm),

\begin{align}
 Y & = a + b_1 X_1 + b_2 X_2 + \epsilon \\
 Y & = a + b_1 X_1 + b_2 (\alpha + \beta X_1 + \nu) \\
 Y & = \color{orange}{(a + \alpha)} + \color{green}{(b_1 + b_2\beta)} X_1 + (e + \nu)
(\#eq:3parm)
\end{align}

Notice that in the last line of Equation \@ref(eq:3parm), $Y$ is predicted using only $X_1$, so it is equivalent to Equation \@ref(eq:1parm). Based on this comparison, we can write

* $a^* = \color{orange}{a + \alpha}$
* $b^*_1 = \color{green}{b_1 + b_2\beta}$
* $\epsilon^* = \epsilon + \nu$

The equation for $b^*_1$ is what we are most interested in. It shows that the regression parameter in our one-parameter model ($b^*_1$) is not equal to the "true" regression parameter using both predictors ($b_1$).

This is what omitted variable bias means -- leaving out $X_2$ in Equation \@ref(eq:1parm) gives us the wrong regression parameter for $X_1$. This is one of the main motivations for including more than one predictor variable in a regression model -- i.e., to avoid omitted variable bias.


Notice that there two special situations in which omitted variable bias is not a problem:

* When the two predictors are not related -- i.e., $\beta = 0$.
* When the second predictor is not related to $Y$ -- i.e., $b_2 = 0$.

We will discuss the interpretation of these situations in class.  

## Explanation

Many uses of regression fall somewhere between prediction and causation. We want to do more than just predict outcomes of interest, but we often don't have a basis for making the strong assumptions required for a causal interpretation of regression coefficients. This grey area between prediction and causation can be referred to as explanation.

In terms of our example, we might want to explain why eighth graders differ in their Math Achievement in terms of a large number of potential predictors, such as

* Student factors
  * attendance
  * past academic performance in Math
  * past academic performance in other subjects (Question: why include this? Hint: see previous section)
  * ...
* School factors
  * their ELA teacher
  * the school they attend
  * their peers (e.g., the school's catchment area)
  * ...
* Home factors
  * SES
  * maternal education
  * paternal education
  * number of books in the household
  * ...
  
When the goal of an analysis is explanation, it usual to focus on the proportion of variation in the outcome variable that is explained by the predictors, i.e., R-squared (see Section \@ref(rsquared-2)). Later in the course we will see how to systematically study the variance explained by individual predictors, or blocks of multiple predictor (e.g., student factors, school factors). 

Note that even a long list of predictors such as that above leaves out potential omitted variables. While the addition of more predictors can help us explain more of the variation in Math Achievement, it is rarely the case that we can claim that all predictors have been included in the model. 


