---
output:
  word_document: default
  html_document: default
---
# Simple Regression {#chapter-2}

```{r, echo = F}
button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
options(digits = 4)
```

The focus of this course is linear regression with multiple predictors (AKA *multiple regression*), but we start by reviewing regression with one predictor (AKA *simple regression*). 

## An example from NELS {#example-2}
 
```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Figure \@ref(fig:fig1) shows the relationship between Grade 8 Math Achievement (percent correct on a math test) and Socioeconomic Status (SES; a composite measure on a scale from 0-35). The data are a subsample of the 1988 National Educational Longitudinal Survey (NELS; see https://nces.ed.gov/surveys/nels88/). 

```{r fig1, fig.cap = 'Math Achievement and SES (NELS88).', fig.align = 'center'}
# Load and attach the NELS88 data
load("NELS.RData")
attach(NELS)

# Scatter plot
plot(x = ses, y = achmat08, col = "#4B9CD3", ylab = "Math Achievement (Grade 8)", xlab = "SES")

# Run the regression model
mod <- lm(achmat08 ~ ses)

# Add the regression line to the plot
abline(mod) 
```

The strength and direction of the linear relationship between the two variables is summarized by their correlation (specifically, the Pearson product moment correlation). In this sample, the correlation is

```{r}
options(digits = 4)
cor(achmat08, ses)
```

This is a moderate, positive correlation between Math Achievement and SES. This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in math (higher Math Achievement). 

This relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., https://www.apa.org/pi/ses/resources/publications/education). **Please look over this web page and be prepared to share your thoughts about this relationship.** 


## The regression line {#regression-line-2}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

The line in the Figure \@ref(fig:fig1) can be represented mathematically as   

\[ 
\widehat Y = a + b X
(\#eq:yhat)
\]

where 

* $Y$ denotes Math Achievement
* $X$ denotes SES 
* $a$ represents the regression intercept (the value of $\widehat Y$ when $X = 0$)
* $b$ represents the regression slope (how much $\widehat Y$ increases for each unit of increase in $X$)

Note that $Y$ represents the values of Math Achievement in the data, whereas $\widehat Y$ represents the values computed from the regression equation (i.e., the values on the regression line). The difference $e = Y - \widehat Y$ is called a *residual*. The residuals for a subset of the data points in Figure \@ref(fig:fig1) are shown in pink in Figure \@ref(fig:fig2) 

```{r fig2, fig.cap = 'Residuals for a Subsample of the Example.', fig.align = 'center'}
# Get predicted values from regression model
yhat <- mod$fitted.values

# select a subset of the data
set.seed(10)
index <- sample.int(500, 30)

# plot again
plot(x = ses[index], y = achmat08[index], ylab = "Math Achievement (Grade 8)", xlab = "SES")
abline(mod)

# Add pink lines
segments(x0 = ses[index], y0 = yhat[index] , x1 = ses[index], y1 = achmat08[index], col = 6, lty = 3)

# Overwrite dots to make it look at bit better
points(x = ses[index], y = achmat08[index], col = "#4B9CD3", pch = 16)
```

Notice that $Y = \widehat Y + e$ by definition. So, we can use either Equation \@ref(eq:yhat) or the equation below to write out a regression model: 

\begin{align} 
Y = a + bX + e.  
(\#eq:y)
\end{align} 

Both equations say the same thing, but Equation \@ref(eq:y) lets us talk about the values of $Y$ in the data, not just the predicted values. 

Another way to write out the model is using the variable names (or abbreviations) in place of the more generic "X, Y" notation. For example, 

\begin{align} 
\widehat {MATH} = a + b(SES). 
(\#eq:read)
\end{align} 

This notation is more informative about the specific variables in the example. But it is also more clunky and doesn't lend itself to other mathematical expressions. For example, $r_{XY}$ is much clearer than $r_{SES, MATH}$ -- in general, we want most of the text on the "baseline", not the subscripts or superscripts.

You should be familiar with all 3 ways of presenting regression equations and you are free to use whatever approach you like best in your own writing.  

## OLS {#ols-2}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Intuitively, one approach to “fitting a line to the data” is to select the parameters of the line (its slope and intercept) to minimize the residuals. In ordinary least squares (OLS) regression, we minimize a related quantity, the sum of squared residuals: 

\[
\begin{align} 
SS_{\text{res}} & = \sum_{i=1}^{N} e_i^2 \\ 
& = \sum_{i=1}^{N} (Y_i - a - b X_i)^2 
\end{align} 
\]

where $i = 1 \dots N$ indexes the respondents in the sample. OLS regression is very widely used and is the main focus of this course, although we will visit some other approaches in the second half of the course. 

Solving the  minimization problem (setting derivatives to zero) gives the following equations for the regression parameters: 

\[ 
a = \bar Y - b \bar X \quad \quad \quad \quad b = \frac{\text{Cov}(X, Y)}{s^2_X} = r_{XY} \frac{s_X}{s_Y}
\]

(If you aren't familiar with the symbols in these equations, check out the review materials in Section \@ref(review-2) for a refresher.)  

For the NELS example, the regression intercept and slope are, respectively: 

```{r}
coef(mod)
```

**Please write down an interpretation of these numbers in terms of the line in Figure \@ref(fig:fig1), and be prepared to share your answers in class!** 

### Correlation and regression

Note that if $X$ and $Y$ are transformed to z-scores (i.e., to have mean of zero and variance of one), then 

* $a = 0$
* $b = \text{Cov}(X, Y) = r_{XY}$

(You can check these results by plugging the value 0 for the means and the value 1 for the variance in the equations above.) 

So, regression, correlation, and covariance are all very closely related when we consider only two variables at a time. This is why we didn't make a big deal about simple regression in EDUC 710 -- its basically just the same thing as correlation. But, when we get to multiple regression (i.e., more than one $X$ variable), we will see that relationship between regression and correlation (and covariance) gets more complicated.  

<!-- ## Residuals 
- show that Cor(\hatY e) = 0  and explain why that is important
- talk about standard error of estimate and how its used
-->

## R-squared {#rsquared-2}
```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```
In the previous section we saw that the predicted value of Math Achievement increased by .43 units (about half a percentage point) for each unit of increase in SES. Another way to interpret this relationship is in terms of the proportion of variance in Math Achievement that is associated with SES -- i.e., to what extent are individual differences in Math Achievement associated with, or explained by, individual differences in SES? 

This question is represented graphically in Figure \@ref(fig:rsquared). The horizontal line denotes the mean of Math Achievement. The difference between the indicated student's Math Achievement score and the mean can be divided into two parts. 

* The black dashed line shows how much closer we get to the student's Math Achievement score ($Y$) by considering the predicted values ($\widehat Y$) instead of the overall mean ($\bar Y$). This represents the extent to which this student's Math Achievement is explained by the linear relationship with SES. 

* The pink dashed line is the regression residual, which was introduced in Section \@ref(regression-line-2). This is the variation in Math Achievement that is "left over" after considering the linear relationship with SES. 


```{r rsquared, fig.cap = 'The Idea Behind R-squared.', fig.align = 'center', echo = F}
plot(x = ses[index], y = achmat08[index], ylab = "Math Achievement (Grade 8)", xlab = "SES")
abline(mod)

ybar <- mean(achmat08)
abline(h = ybar, col = "gray")

# Add pink lines for case 6
segments(x0 = ses[index[22]], y0 = yhat[index[22]] , x1 = ses[index[22]], y1 = achmat08[index[22]], col = 6, lty = 3, lwd = 3)

# Add black lines 
segments(x0 = ses[index[22]], y0 = yhat[index[22]] , x1 = ses[index[22]], y1 = ybar, col = 1, lty = 3, lwd = 3)

# Overwrite dots to make it look at bit better
points(x = ses[index], y = achmat08[index], col = "#4B9CD3", pch = 16)
```

The R-squared statistic averages the variation in Math Achievement associated with SES (i.e., the black dashed line) relative to the total variation in Math Achievement (i.e., black + pink) for all students in the sample. R-squared is a widely used statistic in regression analysis, so we will be seeing it a lot. Some authors call it the "coefficient of determination" instead of R-squared. 

Using all of the cases from the example (Figure \@ref(fig:fig1)), the R-squared statistic is: 

```{r}
options(digits = 5)
summary(mod)$r.squared
```

**Please write down an interpretation of this number and be prepared to share your answer in class.** 

### Derivation*

To derive the R-squared statistic we work the numerator of the variance, which is called the total sum of squares.  

\[SS_{\text{total}} = \sum_{i = 1}^N (Y_i - \bar Y)^2. \]

It can be re-written using the predicted values $\widehat Y$:

\[SS_{\text{total}} = \sum_{i = 1}^N [(Y_i - \widehat Y_i) + (\widehat Y_i - \bar Y)]^2. \]

The right hand side can be reduced to two other sums of squares using the rules of summation algebra (see the review in Section \@ref(review-2)):

\begin{align} 
SS_{\text{total}} & = \sum_{i = 1}^N (Y_i - \widehat Y_i)^2 + \sum_{i = 1}^N (\widehat Y_i - \bar Y)^2 \\
\end{align} 

The first part is just $SS_\text{res}$ from Section \@ref(ols-2). The second part is called the regression sum of squares and denoted $SS_\text{reg}$. Using this terminology we can re-write the above equation as

\[ SS_{\text{total}} = SS_\text{res} + SS_\text{reg} \] 

The R-squared statistic is 

\[R^2 = SS_{\text{reg}} / SS_{\text{total}}. \]

As discussed above, this is interpreted as the proportion of variance in $Y$ that is explained by its linear relationship with $X$. 

<!--
### Additional details* 

Another way to get to $R^2$ is through the definition of $R$ as the correlation between $Y$ and $\widehat Y$. In simple regression, this is just equal to the regular correlation coefficient (Again, you can use the rules of summation algebra to show this). 

\[ R = \text{cor}(Y, \widehat Y) = \text{cor}(Y, a + bX) = \text{cor}(Y, X)\]


Consequently, in simple regression, $R^2 = r^2_{XY}$ -- i.e., the proportion of variance explained by the predictor is just the squared Person product moment correlation. When we add multiple predictors, this relationship between $R^2$ and $r^2_{XY}$ no longer holds. 

Still need to show the two definitions of R-squared are equivalent
-->
## The population model {#population-model-2}  

In the NELS example, the population of interest is U.S. eighth graders in 1988. We want to be able to draw conclusions about that population based on the sample of eighth graders that participated in NELS. In order to do that, we make some statistical assumptions about the population, which are collectively referred to as the population model. We talk about how to check the plausibility of these assumptions in Chapter \@ref(chapter-8). 

The regression population model has the following three assumptions, which are also depicted in the diagram below. Recall that the notation $Y \sim N(\mu, \sigma)$ means that a variable $Y$ has a normal distribution with mean $\mu$ and standard deviation $\sigma$.

1. Normality: The values of $Y$ conditional on $X$, denoted $Y|X$, are normally distributed (the figure shows these distributions for three values of $X$):  

\[Y | X \sim  N(\mu_{Y | X} , \sigma_{Y | X}) \]

2. Homoskedasticity: The conditional distributions have equal variances (also called homegeneity of variance).

\[ \sigma_{Y| X} = \sigma \]

3. Linearity: The means of the conditional distributions are a linear function of $X$.

\[ \mu_{Y| Χ} = a + bX \]

```{r, pop-model, echo = F, fig.cap = "The Regression Population Model.", fig.align = 'center'}
knitr::include_graphics("images/population_model.png")
```

These three assumptions are summarized by writing 

\[ Y|X \sim N(a + bX, \sigma). \] 

Sometimes it will be easier to state the assumptions in terms of the population residuals, $\epsilon = Y - \mu_{Y|X}$, which subtracts off the regression line: $\epsilon \sim N(0, \sigma)$. 

An additional assumption is usually made about the data in the sample -- that they were obtained as a simple random sample from the population. We will see some ways of dealing with other types of samples later on the course, but for now we can consider this a background assumption that applies to all of the procedures discussed in this course.  

## Clarifying notation {#notation-2}

At this point we have used the mathematical symbols for regression (e.g., $a$, $b$) in two different ways:

* In Section \@ref(regression-line-2) they denoted sample statistics.
* In Section  \@ref(population-model-2) they denoted population parameters.

The population versus sample notation for regression is a bit of a hot mess, but the following conventions are widely used.

Concept                Sample statistic    Population parameter               
-------               ------------------  ----------------------
regression line         $\widehat Y$          $\mu_{Y|X}$ 
slope                   $\widehat b$          $b$                                 
intercept               $\widehat a$          $a$                                 
residual                $e$                   $\epsilon$                           
variance explained      $\widehat R^2$        $R^2$


The "hats" always denote sample quantities, and the Greek letters (in this table) always denote population quantities, but there is some lack of consistency. For example, why not use $\beta$ instead of $b$ for the population slope? Well, $\beta$ is conventionally used to denote standardized regression coefficients in the *sample*, so its already taken  (more on this in the Chapter \@ref(chapter-4) ). 

One thing to note is that the hats are usually omitted from the statistics $\widehat a$,  $\widehat b$, and $\widehat R^2$ if it is clear from context that we are talking about the sample rather than the population. This doesn't apply to $\widehat Y$, because the hat is required to distinguish the predicted values from the data points. 

Another thing to note is that while $\widehat Y$ are often called the predicted values, $\mu_{Y|X}$ is not usually referred to this way. It is called the conditional mean function or the conditional expectation function. We will introduce some other notations for $\widehat Y$ and $\mu_{Y|X}$ later in the course. 

**Please be prepared for a pop quiz on notation during class!**

<!-- Finally, recall that the symbol $E(\cdot)$ denotes the expected value of whatever statistic appears in the place of the dot. The expected value is a way of referring to the mean of the sampling distribution of the statistic (otherwise we would have to say things like "the mean of the mean", which would be horrible). So $E(Y\mid X)$ is the expected value of $Y$ for a given value of $X$. The $\mu_{Y|X}$ is the the population conditional mean, $\mu_{Y|X}$ is equal to $E(Y\mid X)$ when the population model is true (i.e., OLS regression is an unbiased estimator of the population conditional mean function). So, the two symbols are used interchangeably. (Omit this paragraph?) -->


## Inference for the slope {#inference-for-slope-2}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

When the population model is true, $\widehat b$ is an unbiased estimate of $b$. We also know the standard error of $\widehat b$, which is equal to (cite:fox)

\[ s_{\widehat b} = \frac{s_Y}{s_X} \sqrt{\frac{1-R^2}{N-2}} . \]

Using these two results, we can compute t-tests and confidence intervals for the regression slope in the usual way. These are summarized below. See the review in Section \@ref(review-2) for background information on bias, standard errors, t-tests, and confidence intervals.

### t-tests

The null hypothesis $H_0: \widehat b = b_0$ can be tested against the alternative $H_A: \widehat b \neq b_0$ using the test statistic: 

\[ t = \frac{\widehat b - b_0}{s_{\widehat b}} \]

which has a t-distribution on $N-2$ degrees of freedom when the null hypothesis is true. 

The test assumes that the population model is correct. The null hypothesis value of the parameter is usually chosen to be $b_0 = 0$, in which case the test is interpreted in terms of the "statistical significance" of the regression slope.  

### Confidence intervals

For a given Type I Error rate, $\alpha$, the corresponding $(1-\alpha) \times 100\%$ confidence interval is

\[ b_0 = \widehat b \pm t_{\alpha/2} \times s_{\widehat b} \]

where $t_{\alpha/2}$ denotes the $\alpha/2$ quantile of the $t$-distribution with $N-2$ degrees of freedom. 

For example, if $\alpha$ is chosen to be $.05$, the corresponding $95\%$ confidence interval uses $t_{.025}$, or the 2.5-th percentile of the t-distribution. 

### The NELS example 

For the NELS example, the t-test of the regression slope is shown in the second row of the table below (we cover the rest of the output in the next few sections):

```{r}
summary(mod)
```

The corresponding $95\%$ confidence interval is

```{r}
confint(mod)
```

**Please write down an interpretation of the t-test and confidence interval of the regression slope, and be prepared to share your answers in class!** 

## Inference for the intercept {#inference-for-intercept-2}

The situation for the regression intercept is similar to that for the slope: the OLS estimate is unbiased and its standard error is (cite:fox)

<!--- check this and get source -->
\[ 
s_{\widehat a} = \sqrt{\frac{SS_{\text{res}}}{N-2} \left(\frac{1}{N} + \frac{\bar X^2}{(N-1)s^2_X}\right)}.
\]

The t-tests and confidence intervals are constructed in the way same as for the slope, just by $a$ replacing $b$ in the notation of the previous slide. The t-distribution also has $N-2$ degrees of freedom for the intercept.  

It is not usually the case that the regression intercept is of interest in simple regression. Recall that the intercept is the value of $\widehat Y$ when $X = 0$. So, unless you have a hypothesis or research question about this particular value of $X$ (e.g., eighth graders with $SES = 0$), you won't be interested in this test (even though R always provides it). 

When we get to multiple regression, we will see some examples of regression models where the intercept is meaningful, especially when we talk about categorical predictors in Chapter \@ref(chapter-5) and interactions in Chapter \@ref(chapter-6). But, for now, we can put it on the back burner. 

## Inference for R-squared {#inference-for-rsquared-2}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Inference for R-squared is quite a bit different than for the regression parameters. As we saw in section \@ref(rsquared-2), R-squared is a ratio of two sums of squares. We know from our study of ANOVA last semester that ratios of sums of squares are tested using an F-test, rather than a t-test. The F-test for (the population) R-squared is summarized below. 

### F-tests

The null hypothesis $H_0: R^2 = 0$ can be tested against the alternative $H_A: R^2 \neq 0$ using the test statistic: 

\[ F = (N-2) \frac{\widehat R^2}{1-\widehat R^2} \]

which has a F-distribution on $1$ and $N – 2$ degrees of freedom when the null is true. The test assumes that the population model is true. Confidence intervals for R-squared are generally not reported. 

The R output from Section \@ref(inference-for-slope-2) is presented again below. **Please write down an interpretation of the F-test of R-squared and be prepared to share your answers in class!** Note that the output uses the terminology "multiple R-squared" to refer to R-squared. 


```{r}
summary(mod)
```

## Power analysis {#power-2}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Statistical power is the probability of rejecting the null hypothesis, when it is indeed false. Rejecting the null hypothesis when it is false is sometimes called a "true positive", meaning we have correctly inferred that a parameter of interest is not zero. Power analysis is useful for designing studies so that the statistical power / true positive rate is satisfactory. In practice, this comes down to having a large enough sample size. 

Power analysis in regression is very similar to power analysis for the tests we studied last semester. There are four ingredients that go into a power analysis: 

* The desired Type I Error rate, $\alpha$.
* The desired level of statistical power. 
* The sample size, $N$.
* The effect size, which for regression is Cohen's f-squared statistic (AKA the signal to noise ratio): 

\[ f^2 = {\frac{R^2}{1-R^2}}. \]

In principal, we can plug-in values for any three of these ingredients and then solve for the fourth. But, as mentioned, power analysis is most useful when we solve for $N$ while planning a study. When solving for $N$ "prospectively," the effect size $f^2$ should be based on reports of R-squared in past research. Power and $\alpha$ are usually chosen to be .8 and .05, respectively. 

When doing secondary data analysis (as in this class) there is not much point in solving for the sample size, since we already have the data. Instead, we can solve for the effect size. In the NELS example we have $N=500$ observations. The output below reports the smallest effect size we can detect with a power of .8 and $\alpha = .05$. This is sometimes called the "minimum detectable effect size" (MDES). Note that the output $u$ and $v$ denote the degrees of freedom in the numerator and denominator of the F-test of R-squared, respectively. 

```{r}
library(pwr)
pwr.f2.test(u = 1, v = 498, sig.level = .05, power = .8)
```

**What is the MDES for the NELS example? Please be prepared to share your answer in class.**

## Workbook {#workbook-2}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

This section collects the questions asked in this chapter. We will discuss these questions in class. If you haven't written down / thought about the answers to these questions before class, the lesson will not be very useful for you! So, please engage with each question by writing down one or more answers, asking clarifying questions, posing follow up questions, etc. 

**Section \@ref(example-2)**

```{r, fig.cap = 'Math Achievement and SES (NELS88).', fig.align = 'center'}
# Scatter plot
plot(x = ses, y = achmat08, col = "#4B9CD3", ylab = "Math Achievement (Grade 8)", xlab = "SES")

# Run the regression model
mod <- lm(achmat08 ~ ses)

# Add the regression line to the plot
abline(mod) 
```

The strength and direction of the linear relationship between the two variables is summarized by their correlation (specifically, the Pearson product moment correlation). In the plot above, the correlation is

```{r}
options(digits = 4)
cor(achmat08, ses)
```

This correlation means that eighth graders from more well-off families (higher SES) also tended to do better in Math (higher Math Achievement). 

This relationship between SES and academic achievement has been widely documented and discussed in education research (e.g., https://www.apa.org/pi/ses/resources/publications/education). Please look over this web page and be prepared to share your thoughts about this relationship.


**Section \@ref(ols-2)**

For the NELS example, the regression intercept and slope are, respectively: 

```{r}
coef(mod)
```

Please write down an interpretation of these numbers in terms of the line in Figure \@ref(fig:fig1), and be prepared to share your answers in class.

**Section \@ref(rsquared-2)**

Using all of the cases from the example (Figure \@ref(fig:fig1)), the R-squared statistic is: 

```{r}
options(digits = 5)
summary(mod)$r.squared
```

Please write down an interpretation of this number and be prepared to share your answer in class.

**Section \@ref(notation-2)**

Please be prepared for a pop quiz on notation during class!

Concept                Sample statistic    Population parameter               
-------               ------------------  ----------------------
regression line          
slope                                            
intercept                                        
residual                                          
variance explained      

**Section \@ref(inference-for-slope-2)**

```{r}
summary(mod)
```

The corresponding $95\%$ confidence interval is

```{r}
confint(mod)
```

Please write down an interpretation of the t-test and confidence interval of the regression slope, and be prepared to share your answers in class!

**Section \@ref(inference-for-rsquared-2)**

Using the same output as above, please write down an interpretation of the F-test of R-squared and be prepared to share your answers in class. Note that the output uses the terminology "multiple R-squared" to refer to R-squared. 

**Section \@ref(power-2)**

When doing secondary data analysis (as in this class) there is not much point in solving for the sample size, since we already have the data. Instead, we can solve for the effect size. In the NELS example we have $N=500$ observations. The output below reports the smallest effect size we can detect with a power of .8 and $\alpha = .05$. This is sometimes called the "minimum detectable effect size" (MDES). Note that the output $u $ and $v$ denote the degrees of freedom in the numerator and denominator of the F-test of R-squared, respectively. 

```{r}
library(pwr)
pwr.f2.test(u = 1, v = 498, sig.level = .05, power = .8)
```

What is the MDES for the NELS example? Please be prepared to share your answer in class.

## Exercises {#exercises-2}
  
These exercises collect all of the R input used in this chapter into a single step-by-step analysis. It explains how the R input works, and provides some additional exercises. We will go through this material in class together, so you don't need to work on it before class (but you can if you want.)

### The `lm` function

The function`lm`, short for "linear model", is used to estimate linear regressions using OLS. It also provides a lot of useful output. 

The main argument that the user provides to the `lm` function is a formula. For the simple regression of Y on X, a formula has the syntax:

`Y ~ X `

Here `Y` denotes the outcome variable and `X` is the predictor variable. The tilde `~` just means "equals", but the equals sign `=` is already used to assign values in R, so `~` is used in its place when writing a formula.  We will see more complicated formulas as we go through the course. For more information on R's formula syntax, see `help(formula)`. 

Let's take a closer look using the following two variables from the NELS data. 
  
  * `achmat08`: eighth grade math achievement (percent correct on a math test)
  
  * `ses`: a composite measure of socio-economic status, on a scale from 0-35 

```{r}
# Load the data. Note that you can click on the .RData file and RStudio will load it
# load("NELS.RData") #Un-comment this line to run

# Attach the data: will dicuss this in class
# attach(NELS) #Un-comment this line to run!

# Scatter plot of math achievment against SES
plot(x = ses, y = achmat08, col = "#4B9CD3")

# Regress math achievement on SES; save output as "mod"
mod <- lm(achmat08 ~ ses)

# Add the regression line to the plot
abline(mod)

# Print out the regression coefficients
coef(mod)
```

Let's do some quick calculations to check that the `lm` output corresponds the formulas for the slope and intercept in Section \@ref(ols-2): 

$$ a = \bar Y - b \bar X \quad \text{and} \quad b = \frac{\text{Cov}(X, Y)}{s_X^2} $$
We won't usually do these kind of "manual" calculations, but it is a good way consolidate knowledge presented in the readings with the output presented by R. It is also useful to refresh our memory about some useful R functions and how the R language works.  

```{r}
# Confirm that the slope from lm is equal to the covariance divided by the variance of X
cov_xy <- cov(achmat08, ses)
s_x <- var(ses)
b <- cov_xy / s_x
b

# Confirm that the y-intercept is obtained from the two means and the slope
xbar <- mean(ses)
ybar <- mean(achmat08)

a <- ybar - b * xbar
a
```

Let's also check our interpretation of the parameters. If the answers to these questions are not clear, please make sure to ask in class! 

* What is the predicted value of `achmat08` when `ses` is equal to zero? 

* How much does the predicted value of `achmat08` increase for each unit of increase in `ses`? 

### Variance explained

Above we found out that the regression coefficient was 0.4-ish. Another way to describe the relationships is by considering the amount of variation in $Y$ that is associated with (or explained by) its relationship with $X$. Recall that one way to do this is via the variance decomposition

$$ SS_{\text{total}} = SS_{\text{res}} + SS_{\text{reg}}$$

from which we can compute the proportion of variation in Y that is associated with the regression model 

$$R^2 = \frac{SS_{\text{reg}}}{SS_{\text{total}}}$$


The R-squared for the example is presented in the output below. You should be able to provide an interpretation of this number, so if it's not clear make sure to ask in class!  

```{r}
# R-squared from the example
summary(mod)$r.squared
```

As above, let's compute $R^2$ "by hand" for our example. 

```{r}
# Compute the sums of squares
ybar <- mean(achmat08)
ss_total <- sum((achmat08 - ybar)^2)
ss_reg <- sum((yhat - ybar)^2)
ss_res <-  sum((achmat08 - yhat)^2)

# Check that SS_total = SS_reg + SS_res
ss_total
ss_reg + ss_res

# Compute R-squared
ss_reg/ss_total

# Check that R-squared is really equal to the square of the PPMC
cor(achmat08, ses)^2
```

### Predicted values and residuals

The `lm` function also returns the residuals $e_i$ and the predicted values $\widehat{Y_i}$, which we can access using the `$` operator. These are useful for various reasons, especially model diagnostics which we discuss later in the course. For now, lets just take a look at the residual vs fitted plot to illustrate the code. 

```{r}
yhat <- mod$fitted.values
res <- mod$resid

plot(yhat, res, col = "#4B9CD3")
cor(yhat, res)
```

Note that the predicted values are uncorrelated with the residuals -- this is always the case in OLS. 

### Inference

Next let's talk about statistical inference, or how we can make conclusions about a population based on a sample from that population. 

We can use the `summary` function to test the coefficients in our model.

```{r}
summary(mod)
```

In the table, the t-test and p-values are for the null hypothesis that the corresponding coefficient is zero in the population. We can see that the intercept and slope are both significantly different from zero at the .05 level. However, the test of the intercept is not very meaningful (why?). 

The text below the table summarizes the output for R-squared, including its F-test, it's degrees of freedom, and the p-value. (We will talk about adjusted R-square in Chapter 4) 

We can also use the `confint` function to obtain confidence intervals for the regression coefficients. Use `help` to find out more about the `confint` function.

```{r}
confint(mod)
```

Be sure to remember the correct interpretation of confidence intervals: *there is a 95% chance that the interval includes the true parameter value* (not: there is a 95% chance that the parameter falls in the interval). For example, there is a 95% chance that the interval [.31, .54] includes the true regression coefficient for SES. 

### Power analysis 

Power analyses should ideally be done before the data are collected. Since this class will work with secondary data analyses, most of our analyses will be retrospective. But don't let this mislead you about the importance of statistical power -- you should always do a power analysis before collecting data!! 

To do a power analsyis in R, we can install and load the `pwr` package. If you haven't installed an R package before, it's pretty straight forward -- but just ask the instructor or a fellow student if you run into any issues. 

```{r, eval = F}
# Install the package 
install.packages("pwr")
```
```{r}
# Load the package by using the library command
library("pwr")
```
```{r}
# Use the help menu to see what the package does
help("pwr-package")
```

To do a power analysis for linear regression, it is common to use Cohen's $f^2$ as the effect size: 

$$f^2 = \frac{R^2}{1-R^2}.$$

Recall that $R^2$ is the proportion of variance in $Y$ explained by the model, and so $1 - R^2$ is the proportion of variance not explained by the model. Thus, $f^2$ can be interpreted as a signal to noise ratio. 

In addition to the effect size, we need to know the degrees of freedom for the F-test of R-square. The `pwr` functions use the following notation:

* `u` is the degrees of freedom in the numerator of an F-test. 
* `v` is the degrees of freedom in the denominator of an F-test. 

In simple regression, `u = 1` and `v = N - 2`. 


As an example of (prospective) power analysis, let's find out many observations would be required to detect an effet size of R-square = .1, using $\alpha = .05$ and power = .8. To find the answer, enter the provided information into the `pwr.f2.test` function, and the function will solve for the "missing piece" -- in this case $v = N - 2$.


```{r}
# Use the provided values of R2, alpha, power (and u = 1) to solve for v = N - 2
R2 <- .1
f2 <- R2/(1-R2)
pwr.f2.test(u = 1, f2 = f2, sig.level = .05, power = .8)
```

In this example we find that $v = 70.6$. Since $v = N - 2$, so we know that a sample size of $N = 72.6$ (rounded up to 73) is required to reject the null hypothesis that $R^2 = 0$, when the true population value is $R^2 = .1$, with a power of .8 and using a significance level of .05. 

### Additional exercises

If time permits, we will address these additional exercises in class.  

These exercises replace `achmat08` with

* `achrdg08`: eighth grade Reading Achievement (percent correct on a reading test)

Please answer the following questions using R. 

* Plot `achrdg08` against `ses`. Is there any evidence of nonlinearity in the relationship? 

* What is the correlation between `achrdg08` and `ses`? How does it compare to the correlation with Math and SES? 

* How much variation in Reading is explained by SES? Is this more or less than for Math? Is the proportion of variance explained significant at the .05 level? 

* How much do predicted Reading scores increase for a one unit of increase in SES? Is this a statistically significant at the .05 level?

* What are your overall conclusions about the relationship between Academic Achievement and SES in the NELS data? 

## Review {#review-2}

This is a short recap of stuff you need to know in this class. If this recap is too short, you'll need to go back to your notes from a previous Stats class. 

### Summation notation

Summation notation uses the symbol $\Sigma$ to stand in for summation. For example, instead of writing 

$$ X_1 + X_2 + X_3 + .... + X_N$$

to represent the sum of the values of the variable $X$ in a sample of size $N$, we can instead write: 

$$ \sum_{i=1}^{N} X_i. $$

The symbol $\Sigma$ stands in for the summation. The symbol is called "Sigma" -- it's the capital Greek letter corresponding to the Latin letter "S". The value $i$ is called the index, and $1$ is the starting value of the index and $N$ is the end value of the index. You can choose whatever start and end values you want to sum over. So, if you just want the second and third index, write

$$ \sum_{i=2}^{3} X_i = X_2 + X_3. $$


When it is clear from context, we can omit the start and end values:

$$ \sum_i X_i. $$
If you write it this way, the reader will assume that you mean to sum over all the values of $X$ in a sample. 

Summation notation is useful because it is more compact, and we use it a lot to write the formulas for statistics. 

There are also rules for manipulating summation notation that are useful for proving results in statistics. I'll often write things like "you can prove this result using the rules of summation algebra". You don't need to do mathematical proofs or derivations in this class, but you might like to (if not, you can skip to the next section). The rules are just things you learned in grade school, but they are presented using a new notation. Surprisingly, they can prove a lot about statistics. 

Here are the rules:  

* **Rule 1: Sum of a constant (multiplication)**. Summing the values of a constant $c$ is the same as multiplication. In this notation, we know the value $c$ is a constant because it doesn't have an index. Specifically, if you sum a constant $N$ times, this just $N$ times the constant: 

\begin{align}
\sum_{i = 1}^{N} c &= c + c +  .... \\
& = Nc 
\end{align}


* **Rule 2: Distributive property** The sum of a variable $X_i$ times a constant $c$ is equal to the constant times the sum. 

\begin{align}
\sum_{i = 1}^{N} c X_i &= cX_1 + cX_2 + .... \\
& = c(X_1 + X_2 + ....) \\ 
& = c \sum_{i = 1}^{N} X_i 
\end{align}

* **Rule 3: Associative property ** It doesn't matter what order we do addition in:

\begin{align}
\sum_{i = 1}^{N} (X_i + Y_i) &= (X_1 + Y_1) + (X_2 + Y_2) + .... \\
& = (X_1 + X_2 + ....) + (Y_1 + Y_2 + ....) \\
& = \sum_{i = 1}^{N} X_i  + \sum_{i = 1}^{N} Y_i 
\end{align}

### Sample statistics

The main sample statistics we use in the class are the mean, standard deviation, variance, covariance and correlation. These are the building blocks for regression. Their symbols and formulas are presented below (using the shorthand summation notation). If you don't remember their interpretation, you will need to go back to your Stat 1 notes. 

* The mean 

$$\bar X = \frac{\sum_i X_i}{N}$$

* The variance can be written as $\text{var}(X)$ or sometimes using the symbol $s^2$

$$ \text{var}(X) = \frac{\sum_i (X_i - \bar X)}{N - 1} $$

* The standard deviation can be written $\text{sd}(X)$ or using the letter $s$ 

$$ \text{sd}(X) = \sqrt{\text{var}(X)} $$

* The covariance is a generalization of the variance to two variables, it describes how they co-vary: 

$$\text{cov}(X, Y) = \frac{\sum_i (X_i - \bar X) (Y_i - \bar Y)}{N - 1} $$

* The correlation is the covariance divided by the product of the standard deviations of the variables. It takes on values between -1 and 1 and describes the strength and direction of the linear relationship between two variables.   

$$\text{cor}(X, Y) = \frac{\text{cov}(X, Y)}{\sqrt{\text{var}(X)} \sqrt{\text{var}(Y)}} $$

The code below shows how you compute these statistics in R, using reading and math in grade 8 from NELS as an example. The code should seem intuitive. The numbers (output) aren't important here, so they are suppressed.  

```{r, results = 'hide'}
mean(achmat08)
var(achmat08)
sd(achmat08)
cov(achmat08, achrdg08)
cor(achmat08, achrdg08)
```

### Bias and precision

A sampling distribution is the distribution of a statistic that would arise if you did the following: 

1. Take a sample of size $N$ from a population of interest.
2. Compute a statistic using the sample data. It can be any statistic, but let's say the mean, $\bar X$, for concreteness. 
3. Write down the value of the mean and then return the sample to the population.
 
After doing these 3 steps many times, you will have many values the sample mean, 

$$ \bar{X}_1, \bar{X}_2, \bar{X}_3, ... $$

These means are just like any other statistic -- i.e., they have a distribution. That distribution is called the sampling distribution (of the mean). 

The sampling distribution is just like any other distribution (except that it is purely imaginary) -- so it has its own mean, and its own variance, etc. etc. These statistics, when computed for a sampling distribution, have special names. We are mostly interested in the following two statistics. 

* **The expected value** of the mean, denoted $E(\bar X)$, is the mean of the sampling distribution of the mean. That is a mouthful alright! That is why we say the expected value of a statistic rather than the mean of a statistic. The latter would lead us to say ridiculous things like the mean of the variance, the mean of the standard deviation ... yuck! Anyway, it's called the expected value because it's the average value over many samples. 

* **The standard error** of the mean, denoted $SE(\bar X)$, is the standard deviation of the sampling distribution of the mean. It describes the sample-to-sample variation of the mean around its expected value. 

There are two additional important concepts related to expectation and standard error: 

* **Bias**: If the expected value of a statistic is equal to a population parameter, we say that the statistic is an unbiased estimated of that parameter. For example, the expected value of the sample mean is equal to the population mean (in symbols: $E(\bar{X}) = \mu)$, so we say the sample mean is an unbiased estimate of the population mean. 

* **Precision**: The inverse of the squared standard error (i.e., one over the variance of the sampling distribution) is called the precision of a statistic. So, the less a statistic varies from sample to sample, the more precise it is. That should hopefully make intuitive sense. The main thing to know about precision is that it is usually increasing in the sample size -- i.e., we get more precise estimates by using larger samples. Again, this should feel intuitive. 

Below is a figure that is often used to illustrate the ideas of bias and precision. The middle of the concentric circles represent the target parameter (like a bull's eye) and the dots represent the sampling distribution of a statistic. You should be able to describe each panel in terms of the bias and precision of the statistic. 

```{r, bp, echo = F, fig.cap = "Bias and precision", fig.align = 'center', fig.width = 2}
knitr::include_graphics("images/bias_and_precision.png", dpi = 200)
```


In general, we want to work with unbiased, precise statistics when making inferences from a sample back to the population from which it was drawn. 

### t-tests
The t-test is used to make an inference about the value of an unknown  population parameter using an unbiased sample statistic that is normally distributed (i.e., that has a normally distributed sampling distribution). This includes sample means and regression coefficients. 

The test compares the value of the statistic to the null-hypothesized value of the population parameter. The null hypothesis is usually that the population parameter is equal to zero. 

The general formula for a t-test is

$$ t = \frac{\text{sample value} - \text{null-hypothesized value}}{\text{standard error}} $$
For a specific example, see section \@ref(inference-for-slope-2). 

When we conduct a t-test, the basic rationale is, "if the sample statistic is close the null hypothesized value of the parameter, then  $t$ should be close to zero". The null hypothesis usually translates into "no effect", so if $t$ is close to zero, it means there was no effect. If $t$ is far away from zero, it means there was an effect. 

The problem in all of this is to give precise meaning to the terms "close" and "far". We can start to answer this question by looking at the t-distribution.  The t-distribution tells what are the typical values of the t-test, if the null hypothesis is true -- i.e., what values should $t$ have if the population parameter is equal to the null hypothesized value? Some examples of the t-distriubtion are shown below. 

```{r, t, echo = F, fig.cap = "t distribution (source: https://en.wikipedia.org/wiki/Student%27s_t-distribution)", fig.align = 'center', out.width = "60%"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Student_t_pdf.svg/1920px-Student_t_pdf.svg.png", dpi = 200)
```

You can see that the t-distribution looks like a normal distribution centered a zero. So, when the null hypothesis is true, the expected value of the t-test is zero. Values greater than $\pm 2$ are pretty unlikely if the null hypothesis is true, and values greater than $\pm 4$ are very unlikely. 

So now we can say which values of $t$ are large -- the ones that are unlikely under the null hypothesis. But the problem now is to determine how unlikely is "unlikely enough" to reject the null hypothesis. 

We answer this question by setting the significance level of the test, usually denoted $\alpha$. This number represents our tolerance for false negatives or Type I errors -- i.e., for incorrectly concluding that the null hypothesis is true (no effect). When we set $\alpha$ to a small number, we are saying that we want the probability of a false negative to be small. This means we are going to need strong evidence before we reject the null hypothesis -- i.e., the value of $t$ would need to be very unlikely under the null hypothesis. How unlikely?  Smaller than $\alpha$. 

In summary, we use the t-test to make an inference about the null hypothesis by comparing two probabilities: 

1. The probability of observing the value in our sample, under the assumption that the null hypothesis is true (i.e., under the t-distribution). This is called the $p$-value. (Actually the $p$-value is probability of observing a value $t$ at least as large as the one in our sample, but this is a just technicality.)

2. The significance level that represents our tolerance for a false negative, $\alpha$.

If $p < \alpha$ (i.e., if the likelihood of $t$ being a false negative is less than we have required), we reject the null hypothesis. Otherwise we retain the null hypothesis and conclude there was no effect.

There is a lot more to say here, but all of this is what Stat 1 was for. Usually we set $\alpha = .05$ and that is that. 

One last thing before moving on: unlike the normal distribution, the t-distribution has a single parameter called its "degrees of freedom", which is denoted as $\nu$ in the figure above. The degrees of freedom are always an increasing function of the sample size, so that larger samples lead to more degrees of freedom. When the degrees of freedom approach $\infty$, the t distribution approaches a normal distribution. When the degrees of freedom are small, the t distribution has wider tails than the normal distribution. This is important when doing statistical tests, because we are interested in values in the tails of the distribution -- i.e., the large / unlikely values of $t$. 


### Confidence intervals 

A confidence interval use the same equation as a t-test, except we solve for the population parameter rather than the value of $t$. Whereas a t-test lets us make a guess about specific value of the parameter of interest (i.e., the null-hypothesized value), a confidence interval gives us a range of values that we can be sure include the parameter of interest, with some degree of "confidence." 

Confidence intervals have the general formula:

$$\text{Interval} = \text{sample value} \pm t \times {\text{standard error}}. $$
We get the value of $t$ from the t distribution. In particular, if we want the interval to include the true population parameter $(1-\alpha) \times 100$ - percent of the time, the we choose $t$ to be the $\alpha/2 \times 100$ percentile of the $t$ distribution. For example, if we set $\alpha = .05$, we will have a $(1-\alpha) \times 100$ = 95% confidence interval by choosing $t$ to be the $\alpha/2 \times 100$ = 2.5th percentile of the t-distribution. 

As mentioned, t-tests and confidence intervals are closely related. In particular, if the confidence interval includes the value $0$ -- i.e., we are confident that the parameter could equal zero -- then this is the same as retaining the null hypothesis that the parameter is equal to zero. This relationship assumes we use the same level of $\alpha$ for both the test and confidence interval. 

In summary, if the confidence interval includes zero, we retain the null hypothesis. If the confidence interval does not include zero, we reject the null hypothesis. 

For an example of a confidence in R, see section \@ref(inference-for-slope-2).

### F-tests

As noted above, the t-test is used when the sample statistic we are testing is normally distributed, usually a mean or a regression coefficient. The F-test is used when the sample statistic we are testing is a variance -- actually, the ratio of two independent variances. 

A variance can be defined in general as a sum-of-squares divided by its degrees of freedom. For example, the sample variance shown above is just a sum-of-squared deviations from the sample mean (i.e., a sum of squares) divided by $N - 1$ (the degrees of freedom). The generic formula for an F-test is:

$$F = \frac{SS_A / df_A}{SS_B / df_B}, $$

where SS denotes sums-of-squares and df denotes degrees of freedom. 

In regression, we often see the F-test when we want to make an inference about whether R-squared is statistically different from zero. Recall that R-squared is computed as the ratio of two sums-of-squares (see section \@ref(rsquared-2)). The F-test shown in section \@ref(inference-for-rsquared-2) provides the degrees of freedom that will convert R-squared into an F-test. 

Just the like t-test, the F-test is called by the letter "F" because it has an F distribution when the null hypothesis is true. The plot below shows some example F distributions. These distributions tell us the values of F that are likely, if the null hypothesis is true (i.e., if R-squared is equal to zero in the population). 

```{r, F, echo = F, fig.cap = "F distribution (source: https://en.wikipedia.org/wiki/F-distribution)", fig.align = 'center', out.width = "60%"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/F-distribution_pdf.svg/1920px-F-distribution_pdf.svg.png", dpi = 200)
```

The F distribution has two parameters, which are referred to as the "degrees of freedom in the numerator" and the "degrees of freedom in the denominator" (in the figure, d1 and d2, respectively). We always give the numerator df first and then the denominator. So we might say the green line in the figure is "an F distribution on 10 and 1 degrees of freedom", which means the df in the numerator is 10 and the df in the denominator is 1. 

We use an F-test the same way we use a t-test -- we set a significance level and use this level to determine how large the F-test needs to be for us to reject the null hypothesis. The main difference is that F is non-negative, because it is the ratio of squared numbers. For this same reason, we don't usually compute confidence intervals for statistics with an F distribution. 

### APA reporting

It is important that you can write up the results of an analysis in a way that other people will understand. For this reason, there are conventions about how to report statistical results. In this class, we will mainly use Table and Figures (formatted in R) rather than inline text. But sometimes the latter is unavoidalbe, so you should know how to do it. 

The examples below illustrate APA conventions for reporting some of the statistics we have talked about in this review. Everything about the formatting is important (spacing, italics, number of decimal places, whether or not to use a leading zero before a decimal, etc). More details are available [here](https://psych.uw.edu/storage/writing_center/stats.pdf). 
You don't need to use APA (unless you are in psych or educ), but you should be familiar with some kind of conventions for reporting statistical results in your academic writing. 

  * Jointly, the two predictors explained about 22% of the variation in Academic Achievement, which was statistically significant at the .05 level ($R^2 = .22, F(2, 247) = 29.63, p < .001$).
  
 * After controlling for SES, a one unit of increase in Maternal Education was associated with $b = 1.33$ units of increase in Academic Achievement ($t(247) = 5.26, p < .001$).
  
 * After controlling for Maternal Education, a one unit of increase in SES was associated with $b = 0.32$ units of increase in Academic Achievement. This was a statistically significant relationship ($t(247) = 2.91, p < .01$).



