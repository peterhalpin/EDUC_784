# Assumption Checking {#chapter-8}

```{r, echo = F}
# Clean up check
rm(list = ls())
#detach(NELS)

button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```



In Section \@ref(population-model-2) we introduced the population model for linear regression. But how do we know whether this model applies to our data? That is the question we address in this chapter. In particular, we discuss data analyses that can be used to better understand whether the population assumptions of linear regression are consistent with our data. This is referred to *assumption checking*. 

Why do we care about assumption checking? Well, if the population assumptions of linear regression are not met, any of the following can happen: 

* Regression coefficients and R-squared could be biased
* Standard errors could be too small (or too large)
* t- and F-tests could be too small (or too large)
* p-values could be too small (or too large)
* Confidence intervals could be too small (or too large)

Basically, all of the numbers we get in the regression output from R could be wrong. 

The philosophy of assumption checking is not to verify the assumptions or prove beyond doubt that they are true -- i.e., we are not *testing* assumptions, we are just checking them. Checking means we want to know if there are any problems with a regression model that might affect our conclusions (e.g., which predictors are significant). So, for example, we don’t really care if the residuals are normally distributed – we just care if there are any violations of this assumption that might be affecting the results our analysis. If there is no evidence that an assumption is problematic, we proceed with the main analysis as intended. 

Assumption checking generally involves plotting the residuals from a regression model and trying to infer what the plots tell us about the population model. The focus of this chapter is to introduce you to these plots and how to interpret them. Sometimes assumption checking can feel a bit like reading tea leaves, because interpreting plots can be pretty subjective. Honing your interpretation of will happen gradually with experience. 

This chapter focusses on checking assumptions, and the following two chapters talk about strategies for dealing with assumption violations. There is one exception -- in this chapter we address how to deal with heteroskedasticity. The short version is that heteroskedasticity affects the standard errors of regression coefficients (and, consequently, the t-values, p-values, and confidence intervals). However, there are corrections to the standard errors that can be used to address this. Since the solution to this assumption is relatively "plug-and-play", we cover it in this chapter. In particular, we will focus on one widely used procedure called heteroskedasticity-corrected (HC) standard errors. 

*Regression diagnostics* are a related topic that we also introduced this chapter. These are procedures for detecting outliers. Unlike assumption checking, which focuses on the model *per se*, diagnostics focus on whether individual data points are having an undue influence on the model (e.g., the predicted values or the parameter estimates). We only cover the basics behind diagnostics in this chapter and the material is optional (i.e., it will not be assessed in this course, but you may find it useful for your research). 

Outlier detection can be useful for identifying influential data points, but it is almost never the the case that data points should be omitted because they are outliers. Unless you can find something specifically wrong with a data point (e.g., a data entry error) you should not omit data. A better way to deal with outliers is by using statistical procedures that are specifically designed to deal with them, called *robust statistics*. Robust regression is an advanced topic that we won't get to in this course, but check out this resource if you are interested and feel free to ask questions in class: https://cran.r-project.org/web/views/Robust.html. 

## Recap of population model {#recap-8} 

Before moving, let's recap the population model for linear regression. This was introduced for simple linear regression in Section \@ref(population-model-2). To restate the assumptions for multiple linear regression we will use the vector notation 

\[\mathbf X = [X_1, X_2, \dots, X_K]\]

to represent the predictor variables. For our purposes, the vector $\mathbf X$ is just a list of the predictors in a model. 

1. Normality: The distribution of $Y$ conditional on $\mathbf X$ is normal for all values of $\mathbf X$. 

\[Y | \mathbf X \sim  N(\mu_{Y | \mathbf X} , \sigma_{Y | \mathbf X}) \]

2. Homoskedasticity: The conditional distributions have equal variances (also called homegeneity of variance).

\[ \sigma_{Y| \mathbf X} = \sigma \]

3. Linearity: The means of the conditional distributions are a linear function of $\mathbf X$.

\[ \mu_{Y| \mathbf X} = b_0 + \sum_{k = 1}^K b_k X_k\]

These three assumptions can be summarized in term of the model residuals $\epsilon = Y - \mu_{Y|\mathbf X}$, which subtract off the regression line and therefore have mean of zero, but otherwise have the same distribution as $Y$: 

\[ \epsilon \sim N(0, \sigma). \] 

These assumptions about the residuals are presented graphically in Figure \@ref(fig:pop-model-e). It is instructive to compare this with Figure \@ref(fig:pop-model). In particular, note that the "X" axis is replaced by $\widehat Y$, which is an approach for plotting residuals used later in this chapter. 

```{r, pop-model-e, echo = F, fig.cap = "The Multiple Regression Population Model.", fig.align = 'center'}
knitr::include_graphics("images/pop_model_e.png")
```

## Linearity {#linearity-8}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

This assumption is about whether the regression function is “really” a line or if it could be better represented as some other relationship. A classic example is shown below. The example is from Anscombe's quartet, which we address in more detail in Section \@ref(diagnostics-8)

```{r linearity1, fig.width = 8, fig.cap = "Anscombe's second dataset", fig.align = 'center'}
# non linearity in Anscombe's second example
attach(anscombe)
mod <- lm(y2 ~ x2)

# Take a look at the raw data
par(mfrow = c(1, 2))
plot(x2, y2, col = "#4B9CD3", xlab = "X", ylab = "Y")
abline(mod)

# Compare to the residual vs fitted plot
plot(mod, which = 1)
detach(anscombe)
```

The left hand panel of Figure \@ref(fig:linearity1) shows the scatter plot of the example data. It is pretty obvious that the relationship between $Y$ and $X$ is not linear. 

The right hand panel shows the residuals versus the predicted ("fitted") values from the regression of $Y$ on $X$. This is the sample analogue of the population model in Figure \@ref(fig:pop-model-e). It is important to note the following about this plot:

* The key idea is that deviations from the regression line in the left hand panel correspond to deviations from the horizontal line (i.e., Residuals = 0) in the right hand panel. 

* The non-linear trend is still apparent in the right hand panel, but now the nonlinearity is with reference to Residuals = 0. Recall that the residuals should all be centered around this horizontal line if the population model is true (see Figure \@ref(fig:pop-model-e)). 

* The red line in the right hand panel is a locally weighted smoothed ("lowess") regression line -- it follows whatever trend is in the residuals without assuming the trend is linear. 

* The overall interpretation of the residual vs fitted plot is as follows: 
  * if the red line follows the horizontal line (Residuals = 0), we conclude that the the assumption of linearity is not problematic for the data. 
  * If the red line deviates systematically from the horizontal line, this is evidence that the assumption of lineary is not met. 
  
In this Figure \@ref(fig:linearity1), the assumption is clearly not met. In fact, this is so obvious that it doesn't require looking at the residual versus fitted plot at all. So, why do we use the residual versus plot? The answer is, when we have $K > 1$ predictors, the "raw" data are not so easy to interpret. In general, the assumption is much more easily checked using the residual versus fitted plot, even if the patterns are a bit harder to interpret. 

This situation is illustrated with reference to Figure \@ref(fig:linearity2). Note that in this example the regression model has 2 predictors. We could produce a 3-D plot in which $\widehat Y$ is represented by a plane, but the $Y$ versus $X$ approach will not extend beyond 2 predictors. On the other hand, the residual versus fitted plot can be used with any number of predictors. 

```{r linearity2, fig.width = 8, fig.cap = "An example from ECLS.", fig.align = 'center'}
# Example: regression c1rmscal on ses_orig and t1learn
load("ECLS250.RData")
mod2 <- lm(c1rmscal ~ ses_orig + t1learn, data = ecls)
plot(mod2, which = 1)
```

**Please write down whether you think the linearity assumption is problematic for this second example, and be sure to explain why with reference to Figure \@ref(fig:linearity2).**

Before moving on, let's take a look at a few more examples. In each of these examples, **please write down whether you think the linearity assumption is problematic and  explain why with reference to the plots.**. Hint: be careful not to over-interpret the lowess line in the tails of the plots, where only a few data points can have a big impact on the local trend. Focus your interpretation on the bulk of the data, and whether it shows a systemic trend away from the horizontal line at 0. 

```{r linearity3, fig.height = 10, fig.cap = "More examples.", fig.align = 'center'}
# Example: regression c1rmscal on ses_orig and t1learn
set.seed(101)
par(mfrow = c(2, 2))
for(i in 1:4) {
  x <- rnorm(200)
  e <- rnorm(200)
  y <- x + e 
  plot(lm(y ~ x), which = 1)
}  
```

### Summary 

To check the assumption of linearity, we can use a residual vs predicted (fitted)
plot. 

 * If the plot does not show a systematic trend other than a horizontal line at Residuals = 0, then there is no evidence against the assumption.

* If the residuals do show a trend away from 0, then we should worry about the assumption.

* Don't over interpret the tails of the lowess (red) lines in the R plots. 

* If the assumption is violated: consider adding quadratic or other non-linear terms to the model (Chapter 9), or non-linear transformations of the $Y$ variable (Chapter 10). 

## Homoskedasticity {#homoskedasticity-8}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

This assumption means that the variance of the residuals should not change as a function of the predicted values. Because we are again concerned with residuals and predicted values, we can re-use the same plot we used to check linearity. However, we are no longer interested in whether the lowess trend (red line) systematically deviates from zero – we are interested in whether the range of the residuals (on the vertical axis) changes over the predicted values (on the horizontal axis). 

Figure \@ref(fig:homo1) illustrates two data sets in which the assumption of linearity is met, but the right hand panel shows evidence of heteroskedasticity. This is apparent by observing the range of the residuals over values of $\widehat Y$. 

```{r homo1, fig.width = 8, fig.cap = "Illustration of homo- and heteroskedasicity.", fig.align = 'center'}
# homoskedastic example
set.seed(1)
x <- sort(rnorm(250))
e <- rnorm(250)
y <- x + e
mod3 <- lm(y~x)
par(mfrow = c(1, 2))
plot(mod3, which = 1)

# Heteroskedastic example
y2 <- y
y2[x > 0] <- x[x > 0] + 3* e[x > 0]
y2[x < -1] <- x[x < -1] + .3* e[x < -1]
mod4 <- lm(y2~x)
plot(mod4, which = 1)
```

To make it clearer what aspect of these plots is relevant for evaluating the assumption of homoskedasticity, the same figures are replicated below, but this time with blue lines represented my own "eye-balling" of the range of the residuals. In the left plot, the two lines are parallel, meaning the range is constant. In the right plot, the two lines form a cone, meaning the the range of the residuals increases for larger values of $\widehat Y$. 

Note that I didn't draw any lines in the tail ends of the plots -- this is because there are fewer observations in the tails, so it is harder to make a judgment about the range of values. To avoid "reading the tea leaves" I focus on the range of values of $\widehat Y$ for which there are sufficient observations to judge the range of the residuals. 

To repeat, *the blue lines are just there for your reference*, to highlight the relevant information in the plot. You wouldn't generally include these lines in the plot. 

```{r homo2, fig.width = 8, fig.cap = "Illustration of Homo- and Heteroskedasicity, with Reference Lines", fig.align = 'center'}
# homoskedastic example with ref lines
par(mfrow = c(1, 2))
plot(mod3, which = 1)
segments(x0 = -1.5, y0 = 2, x1 = 1.5, y1 = 2, col = "#4B9CD3", lty = 2, lwd = 3)
segments(x0 = -1.5, y0 = -2, x1 = 1.5, y1 = -2, col = "#4B9CD3", lty = 2, lwd = 3)

# Heteroskedastic example
plot(mod4, which = 1)
segments(x0 = -1.5, y0 = 1, x1 = 1.5, y1 = 8, col = "#4B9CD3", lty = 2, lwd = 3)
segments(x0 = -1.5, y0 = -1, x1 = 1.5, y1 = -8, col = "#4B9CD3", lty = 2, lwd = 3)

# remove y2 from memory to avoid naming conflicts later on
rm(y2)
```

Let's take another look at the plots in Figure \@ref(fig:linearity3). **Please write down whether you think the homoskedasticity assumption is problematic and  explain why with reference to the plots**.  


### Summary 
To check the assumption of homoskedasticity, we can use a residual versus fitted plot (again). 

* If the range of values on the vertical axis appears approximately constant over the horizontal axis, then there is no evidence against the assumption.

* If the range does show a pattern (e.g., cone, bow-tie, ellipse), then we should worry about this assumption. 

* If the assumption is violated, it will affect the standard errors of the regression coefficients as well as the resulting t-tests and confidence intervals. There are heteroskedasticity robust standard errors that can address this issue (Chapter 9). 


### Dealing with Heteroskedasticity {#heteroskedasticity-8}

**Under construction** 

As mentionend above, heteroskedasticity affects the standard errors of regression coefficients, and, consequently, the t-values, p-values, and confidence intervals. There are lots of ways to make standard errors robust to heteroskedasticity, and we will focus on one widely used procedure called heteroskedasticity-corrected (HC) standard errors. 
Heteroskedasticity in linear regression, and corrections thereof, is a pretty big topic in the methodological literature (see Cite:fox).  In this section we are just going to discuss one widely used solution, and how to implement it in `R`. 

In terms of our example, we can see in the residual versus fitted plot in Figure \@ref(fig:piecewise3) that the residuals are less spread out for lower ranges of the fitted values (i.e., for $<12$ years of educ). As mentioned previously, heteroskedasticity will affect the standard errors of the regression coefficients, and consequently the t-tests, p-values, and confidence intervals. What this means is that the p-values for the regression coefficients will be wrong (usually too small) if the data are heteroskedastic but we mistakenly assume they are homoskedastic. Note that heteroskedasticity won't affect the estimated values of the coefficients (i.e., the $\widehat{b}$'s, and also doesn't affect R-squared or its F-test. 


"Heteroskedasticity-corrected" (HC) standard errors do not assume the data homoskedastic. Technically, they can be used regardless of whether the homoskedasticity assumption is met. But, when the data are homoskedastic, the "regular" OLS standard errors are more efficient (i.e., precise). So, we usually don't want to make the correction unless there is evidence of heteroskedasticity in the data. HC standard errors are also sometimes called heteroskedasticity-consistent, heteroskedastcicity-robust, or just robust.  

There are many different version of HC standard errors, but they are all equivalent with "large" samples. The simplest version is (see cite:Wooldridge)

\[ \text{HCSE}_{\hat{b}_j} = \sqrt{\frac{\sum_{i=1}^N (X_{ij} - \widehat{X}_{ij})^2 (Y_i-\widehat{Y}_i)^2} {\sum_{i=1}^N (X_{ij} - \bar X_j)^2 (1 - R^2_j)}}
(\#eq:se-10)
\] 

In this equation, $\widehat{X}_{ij}$ is the predicted value that results from regressing $X_j$ on the remaining $J-1$ predictors. Comparing this equation to Equation  \@ref(eq:se-4), the main difference is that the $(1 - R^2)$ term for the $Y$ variable is now replaced with a crossproduct of residuals for the $X_j$ and $Y$ variables. The equation is not very intuitive to look at, but the general idea is that it can be derived without assuming homoskedasticity. 

The procedure for using HC standard errors in R has two steps. First, we estimate the HC standard errors. Then, we use the HC standard errors to compute the correct t-tests and p-values (or confidence intervals).  The following example shows how to implement HC standard errors in R, using the piecewise regression from Section \@ref(piecewise-10) wages example.

The code is shown by default, because the main thing about this example is to see how it works in R -- you already know how to interpret SEs, t-tests, p-values, etc. Your will need two packages installed -- `car` and `lmtest` -- for this example to work. 

```{r}
## Make sure the required pacakges are installed
# install.packages("car")
# install.packages("lmtest")

# Step 1. Use "hccm" to get the HC SEs for our piecewise model 
hcse <- car::hccm(mod4)

# Step 2. Use "coeftest" to compute t-tests with the HC SEs
lmtest::coeftest(mod4, hcse)
```

This example is a bit unusual because the HC standard errors were actually a bit smaller than the regular OLS standard errors (see the output in Section \@ref(piecewise-10)) -- more often the opposite is true. Nonetheless, you should have no problems interpreting the output above.  


## Normality {#normality-8}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```


There are many ways to check this normality of a set of data, but a widely applicable technique is a qq plot (short for quantile-quantile plot). A qq plot compares the quantiles (e.g., percentiles) of two different distributions.

For our assumption, we want to compare the quantiles of our standardized residuals to the quantiles of a standard normal distribution. Standardizing means the residuals should have variance equal to one, and, combined with the other population assumptions of linear regression, this implies that the residuals should have a standard normal distribution (see Section \@ref(recap-8)).

Since qq plots might not be something you have seen before, we'll take a look at a few examples. Each figure below pairs a histogram and qq plot. In the qq plot, data points should fall on the diagonal line if the data are normally distributed. Focus on how the pattern in the histogram shows up as in deviations from the diagonal line the qq plot. We will discuss the interpretation of these patterns together in class. *Please write down any questions you have about the interpretation of the qq plots and we can address them together in class.**

Also, it should be emphasized that the line in the normal qqplot is *not a regression line!* It is just the diagonal line $Y = X$, and the bulk of the data points should fall on that line if the data were drawn from a normal distribution. 


```{r}
# Comparing histograms and q-q plots
distributions <- read.csv("distributions.csv")
names(distributions)[2] <- "normal"
attach(distributions)
dist_names <- names(distributions)

# Normal 
par(mfrow = c(1, 2))
hist(normal, col = "#4B9CD3")
qqnorm(normal, col = "#4B9CD3")
qqline(normal)

# Negative skew
par(mfrow = c(1, 2))
hist(neg_skew, col = "#4B9CD3")
qqnorm(neg_skew, col = "#4B9CD3")
qqline(neg_skew)

# Positive skew
par(mfrow = c(1, 2))
hist(pos_skew, col = "#4B9CD3")
qqnorm(pos_skew, col = "#4B9CD3")
qqline(pos_skew)

# Leptokurtic
par(mfrow = c(1, 2))
hist(lepto, col = "#4B9CD3")
qqnorm(lepto, col = "#4B9CD3")
qqline(lepto)

# Platykurtic
par(mfrow = c(1, 2))
hist(platy, col = "#4B9CD3")
qqnorm(platy, col = "#4B9CD3")
qqline(platy)
```

Next, let's consider a more realistic example using the default plotting from the `lm` function. **Please write down whether you think the normality assumption is problematic for the data in Figure \@reff(fig:normality1), and be sure to explain why with reference to the plot.** Hint: if you think the data are non-normal, you should be able to interpret the pattern of deviations with reference to the examples given above (e.g. skew, kurtosis). 

```{r normality1, fig.width = 8, fig.cap = "An example from ECLS.", fig.align = 'center'}
# Example: regression c1rmscal on ses_orig and t1learn
plot(mod2, which = 2)
```

Practice makes perfect, so lets work through a few more examples. In each of the examples in Figure \@ref(fig:normality2), **please write down whether you think the normality assumption is problematic and explain why with reference to the plots.**

```{r normality2, fig.height = 10, fig.cap = "More examples.", fig.align = 'center'}
# Example: regression c1rmscal on ses_orig and t1learn
set.seed(101)
par(mfrow = c(2, 2))
for(i in 1:4) {
  x <- rnorm(200)
  e <- rnorm(200)
  y <- x + e 
  plot(lm(y ~ x), which = 2)
}  
```


### Summary 

To check the assumption of normality, we can use a qq plot of the standardized residuals against the standard normal distribution. 

* If the points from the qq plot follows the line Y = X, then there is no evidence against the assumption

* If the residuals do show a trend off of the diagonal line, then we should worry about the assumption

* If the assumption is violated, the central limit theorem implies that significance tests used in OLS regression will be robust to violations of normality when sample sizes are large.  
  * Practically this means N / K > 30 (but other guidelines are used too)
  * For some specific types of violations, notably positive skew, it is also common to transform the Y variable (Chapter 10)
  
* (The line in a normal qqplot is *not a regression line!*) 

## A worked example {#worked-example-8}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

To illustrate these assumption checking procedures, let's revisit the example from Section \@ref(worked-example-7). In that example, it was noted that there was evidence that one (or more) of the assumptions were problematic. Let's take a look at why this was the case. 

For this example, we will use the ECLS data to regress reading achievement at the beginning of Kindergarten (`c1rrscal`) on SES (`wksesl`), parental (mother's and father's) education (`wkmomed` and `wkdaded`, respectively), and attendance in center-based care before K (`p1center`). This is model 3 from Section \@ref(worked-example-7) (we don't consider the model with the interactions). 

The workflow for assumption checking requires first running the model and then producing the residual vs fitted plot and a qqplot of the residuals. If the plots look OK, we go ahead and interpret the model results. If the plots don't look OK, we give up and wonder why we ever bothered with regression in the first place. Just kidding :)  -- the next two chapters of these notes address how to deal with violations of the assumptions. 

After running the model, we obtain the following two plots from the regression output. **For each of the three population assumptions of linear regression, please write down whether you think the assumption is problematic and explain why with reference to the plots.**

```{r}
# Run model
load("ECLS2577.RData")
mod5 <- lm(c1rrscal ~ factor(p1center) + wksesl + wkmomed + wkdaded, data = ecls)

# Check assumptions
par(mfrow = c(1,2))
plot(mod5, 1)
plot(mod5, 2)
```

To foreshadow the next couple of chapters, here is what the plots looked like after dealing with the problematic assumptions. The data still exhibit heteroskedasticity, but we can address that using heteroskedasticity consistent standard errors (Chapter 9).  

```{r}
# Run model
attach(ecls)
log_c1rrscal <- log(c1rrscal - min(c1rrscal) + 1)
wksesl_sq <- wksesl^2
mod6 <- lm(log_c1rrscal ~ factor(p1center) + wksesl + wksesl_sq + wkmomed + wkdaded)

# Check assumptions
par(mfrow = c(1,2))
plot(mod6, 1)
plot(mod6, 2)
```


## Diagnostics* {#diagnostics-8}

This section is optional and is currently under construction. It focuses on walking through the code so the "hide code" button is currently omitted. 

Like assumption checking, regression diagnostics also make extensive use of regression residuals, but this time the objective is to identify individual data points that are "outliers" with respect to the model. To work through the basic concepts of regression diagnostics, let's again use the Anscombe's quartet.

```{r, fig.height= 8}
# Plotting Anscombe's quartet
attach(anscombe)
par(mfrow = c(2,2))
ymax <- max(anscombe[,5:8])
ymin <- min(anscombe[,5:8])
xmax <- max(anscombe[,1:4])
xmin <- min(anscombe[,1:4])


plot(x1, y1, col = "#4B9CD3", xlim = c(xmin, xmax), ylim = c(ymin, ymax))
abline(lm(y1 ~ x1))

plot(x2, y2, col = "#4B9CD3", xlim = c(xmin, xmax), ylim = c(ymin, ymax))
abline(lm(y2 ~ x2))

plot(x3, y3, col = "#4B9CD3", xlim = c(xmin, xmax), ylim = c(ymin, ymax))
abline(lm(y3 ~ x3))

plot(x4, y4, col = "#4B9CD3", xlim = c(xmin, xmax), ylim = c(ymin, ymax))
abline(lm(y4 ~ x4))
```

The interesting thing about these four examples is that they all have the same univariate and bivariate summary statistics -- e.g., the same mean, variance, covariance, correlation, and regression coefficients. 
But first example is the only one that would be suitable for analysis by using these statistics. The second example shows a non-linear relationship, which we addressed in Section \@ref(linearity-8). In this section we will focus on the last two examples, since they have clear outliers.

### Leverage

Leverage describes how "unusual" a data point is on the X variable(s) -- i.e., how far from the mean it is on each predictor. The function `hatvalues` computes the leverage for each data point. Let's check out the leverage for the 3rd and 4th examples from Anscombe's quartet.

```{r}
# leverage for Anscombe 3
anscombe3 <- lm(y3 ~ x3)
leverage3 <- hatvalues(anscombe3)

# Take a look at the leverage values for each data point
leverage3

# Show the leverage values in the scatter plot using the function "text"
par(mfrow = c(1,2))

plot(x3, y3, col = "white", xlim = c(xmin, xmax), ylim = c(ymin, ymax), main = "Leverage for Anscombe 3")
abline(lm(y3 ~ x3))

text(y3 ~ x3, labels = round(leverage3, 2), col = "#4B9CD3")

# leverage for Anscombe 4
anscombe4 <- lm(y4 ~ x4)
leverage4 <- hatvalues(anscombe4)

# Take a look at the leverage values for each data point
leverage4

# Show the leverage values in the scatter plot using the function "text"
plot(x4, y4, col = "white", xlim = c(xmin, xmax), ylim = c(ymin, ymax),  main = "Leverage for Anscombe 4")
abline(lm(y4 ~ x4))

text(y4 ~ x4, labels = round(leverage4, 2), col = "#4B9CD3")
```

Recall from the lesson that 
  
  * h should be smaller (closer to 0) for values closer to the mean of X
  
  * The maximum value of h is 1
  
Based on the plots, we can see that the largest leverage is for the outlier in Anscombe 4. 

### Distance (residuals)

Distance is about the size of the residuals. In order to judge the size of a residual, it helps to use the (externally) studentized residuals rather than the "raw" residuals. Because the studentized residual have a t-distribution on $N - K - 2$ degrees of freedom, a rough ballpark for interpreting studentized residuals is that 

  * Values around +/- 2 are considered large.
  
  * Values beyond +/- 3 are considered very large. 
  
Let's see what we have for our examples:

```{r}
# Distance for Anscombe 3
distance3 <- rstudent(anscombe3)

par(mfrow = c(1,2))
plot(x3, distance3, main = "Leverage for Anscombe 3",  col = "#4B9CD3")

# Distance for Anscombe 4
distance4 <- rstudent(anscombe4)
plot(x4, distance4, main = "Leverage for Anscombe 4",  col = "#4B9CD3")
```

Clearly, the notion of distance is useful for describing what the problem is with Anscombe's 3rd example. For the 4th example, the outlying data point is omitted because it has leverage of exactly 1, which means that the studentized residuals are undefined (divide by zero; R notes this in the console). 

### Influence 

Influence describes how much the model results would change if a data point were omitted. Roughly, the conceptual relationships among influence, distance, and leverage are given by the following equation:

\[ \text{Influence} = \text{Distance} \times \text{Leverage}  \]

This equation tells us that, for a data point to have high influence, it must be a large distance from the regression line (have a large residual) *and* have high leverage (be far away from the mean on $X$). 

There are a number of ways of computing influence. Like externally studentized residuals, they are all deletion statistics, or statistics computed using a “leave-one-out” approach. 

Influence statistics can also be classified into global versus local. Global approaches consider how a data point affects the predicted values. Local approaches consider how a data point affects the value of a specific regression coefficient.

Let's start with DFFITS and Cook's distance, two measures of global influence.

```{r}
# DFFITS for Anscombe 3
DFFITS3 <- dffits(anscombe3)

par(mfrow = c(1,2))
plot(x3, DFFITS3, main = "DFFITS for Anscombe 3",  col = "#4B9CD3")

# Distance for Anscombe 4
DFFITS4 <- dffits(anscombe4)
plot(x4, DFFITS4, main = "DFFITS for Anscombe 4",  col = "#4B9CD3")
```

```{r}
# Cook's distance for Anscombe 3
Cooks3 <- cooks.distance(anscombe3)

par(mfrow = c(1,2))
plot(x3, Cooks3, main = "Cook's D for Anscombe 3",  col = "#4B9CD3")

# Distance for Anscombe 4
Cooks4 <- cooks.distance(anscombe4)
plot(x4, Cooks4, main = "Cook's D for Anscombe 4",  col = "#4B9CD3")
```

These statistics are similar but Cook's distance is more interpetable. Values greater than 1 are considered indicative of high influence. We can see that the outlier in the 3rd example is highly influence. The outlier in the 4th example is "NA" because h = 1 hence there is a divide by zero problem. 

For local measures of influence, the interpretation is roughly the same as global measures with simple regression (i.e., a  single predictor). For multiple regression, local measures can provided additional insight to consider which regression coefficients are most influenced by an outlier. 

```{r}
# DFBETAs distance for Anscombe 3
DFBETA3 <- dfbetas(anscombe3)

#Take a look at the output: We get values for each coefficient, including the intercept
DFBETA3 

# Plots for the regression coefficients
par(mfrow = c(1,2))
plot(x3, DFBETA3[,"x3"], main = "DFBETA for Anscombe 3",  col = "#4B9CD3")

# Distance for Anscombe 4
DFBETA4 <- dfbetas(anscombe4)
plot(x4, DFBETA4[,"x4"], main = "DFBETA for Anscombe 4",  col = "#4B9CD3")
```

The output for DFBETA3 looks a lot like DFFITS3 (because we only have one predictor). 

For DFBETA4, it is strange that our unusual data point (x4 = 18) was not identified as problematic -- keep in mind that when this data point is removed, the variance of X is zero and so the regression coefficient for the leave-one-out model is not defined. Still, it is not clear why R reports the value as zero, rather than omitted. 

### A more realistic example

As a more realistic example, let's consider Question 3 from Assignment 1 using the graphical output from `lm`. Note that the graphical output uses the internally studentized residuals, and refers to these as "standardized residuals". 

```{r}
model <- lm(c4rmscal ~ wksesl + t1learn, data = ecls)

# Influence via Cook's distance
plot(model, which = 4)
```

We can see that, although R automatically labels the 3 data points with the highest values of Cook's D, none of the data points are actually close to the cut off value of 1. In other words, none of the data points in this example have an undue influence on the results of the regression model. 

## Workbook

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

This section collects the questions asked in this chapter. We will discuss these questions in class. If you haven't written down / thought about the answers to these questions  before class, the lesson will not be very useful for you! So, please engage with each question by writing down one or more answers, asking clarifying questions, posing follow up questions, etc. 

**Section \@ref(linearity-8)**

 * Please write down whether you think the linearity assumption is problematic for the example below, and be sure to explain why with reference to the figure.

```{r, fig.width = 8, fig.cap = "An example from ECLS.", fig.align = 'center'}
plot(mod2, which = 1)
```

 * For each of the four examples below, please write down whether you think the linearity assumption is problematic and explain why with reference to the plots. Hint: be careful not to over-interpret the lowess line in the tails of the plots, where only a few data points can have a big impact on the local trend. Focus your interpretation on the bulk of the data, and whether it shows a systemic trend away from the horizontal line at 0. 

```{r fig.height = 10, fig.cap = "More examples.", fig.align = 'center'}
# Example: regression c1rmscal on ses_orig and t1learn
set.seed(101)
par(mfrow = c(2, 2))
for(i in 1:4) {
  x <- rnorm(200)
  e <- rnorm(200)
  y <- x + e 
  plot(lm(y ~ x), which = 1)
}  
```

**Section \@ref(homoskedasticity-8)**

* Let's take another look at the four plots in the above figure. For each plot, please write down whether you think the homoskedasticity assumption is problematic and explain why with reference to the plot. 


**Section \@ref(normality-8)**

* Please write down whether you think the normality assumption is problematic for the data in the figure below, and be sure to explain why with reference to the plot. Hint: if you think the data are non-normal, you should be able to interpret the pattern of deviations (e.g. skew, kurtosis). 

```{r fig.width = 8, fig.cap = "An example from ECLS.", fig.align = 'center'}
# Example: regression c1rmscal on ses_orig and t1learn
plot(mod2, which = 2)
```

 *  In each of the examples below, please write down whether you think the normality assumption is problematic and explain why with reference to the plots.

```{r, fig.height = 10, fig.cap = "More examples.", fig.align = 'center'}
# Example: regression c1rmscal on ses_orig and t1learn
set.seed(101)
par(mfrow = c(2, 2))
for(i in 1:4) {
  x <- rnorm(200)
  e <- rnorm(200)
  y <- x + e 
  plot(lm(y ~ x), which = 2)
}  
```

**Section \@ref(worked-example-8)**

* For each of the three population assumptions of linear regression, please write down whether you think the assumption is problematic for the example shown below, and explain why with reference to the plots.

```{r}
par(mfrow = c(1,2))
plot(mod5, 1)
plot(mod5, 2)
```


## Exercises 

There isn't much new in terms of R code in this chapter. Once we run a model with `lm`, we just call the `plot` function on the `lm` output to produce the graphics requires for assumption checking. This section shows these steps for the worked example in Section \@ref(worked-example-8). Please refer to that section for interpretation of the output. 


```{r}
# Run model
load("ECLS2577.RData")
mod5 <- lm(c1rrscal ~ factor(p1center) + wksesl + wkmomed + wkdaded, data = ecls)

# Puts both plots in one figure
par(mfrow = c(1,2)) 

# Check assumptions
plot(mod5, 1)
plot(mod5, 2)
```