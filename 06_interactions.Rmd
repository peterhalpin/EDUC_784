---
output:
  word_document: default
  html_document: default
---
# Interactions {#chapter-6}


```{r, echo = F}
# Clean up check
# detach(NELS)
rm(list = ls())
options(digits = 4)
button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```


In statistics, the term *interaction* means that the relationship between two variables depends on a third variable. In the context of regression, we are usually interested in the situation where the relationship between the outcome $Y$ and a predictor $X_1$ depends on the value of another predictor $X_2$. This situation is also referred to as *moderation* or sometimes as *effect heterogeneity*.

Interactions are a "big picture" idea with a lot conceptual power, especially when describing topics related to social inequality or "gaps". Some examples of interactions are:  

* The relationship between wages and years of education depends on gender. (https://en.wikipedia.org/wiki/Gender_pay_gap)
* The relationship between reading achievement and age depends on race (https://cepa.stanford.edu/educational-opportunity-monitoring-project/achievement-gaps/race/)
* The effect of COVID-19 school shutdowns on academic achievement depends on SES. (https://www.mckinsey.com/industries/education/our-insights/covid-19-and-student-learning-in-the-united-states-the-hurt-could-last-a-lifetime) 


This chapter starts by considering what happens when both categorical and continuous predictors are used together in a model, and uses this combination of predictors as a way of digging into the math behind interactions. Later sections will consider what happens when we have interactions between two continuous predictors, or two categorical predictors. 


In this chapter we are exclusively interested in interactions between two predictors at a time -- *two-way interactions*. It is possible to consider "higher-order" interactions as well, e.g., interactions among three predictors or *three-way interactions*. But we will just focus on the simplest case. Sometimes we will also use the terminology *main effect* to describe the relationship between each individual predictor and the outcome variable, as distinct from the interactions among the predictors. Up to now, we have be working only with main effects. 

## An example from NELS{#example-6}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```
There is well-documented gender gap in STEM achievement by the end of high school. The t-test reported below illustrates the gap in Math Achievement using the NELS data. 

```{r}
load("NELS.RData")
attach(NELS)
t.test(achmat12 ~ gender, var.equal = T)
```

In this chapter, our first goal is to use linear regression to better understand this gender gap in Math Achievement. To do this, we consider a third variable, Reading Achievement. The plot below shows the relationship between Math Achievement and Reading Achievement estimated just for males (Blue), and the same relationship estimated just for females (Black). 

```{r math-reading-1, fig.cap = 'Math Achievement, Reading Achievement, and Gender.', fig.align = 'center'}
females <- gender == "Female"
males <- gender == "Male"

mod1 <- lm(achmat12[females] ~ achrdg12[females])
mod2 <- lm(achmat12[males] ~ achrdg12[males])

# Plot reading and math for females
plot(achrdg12[females], achmat12[females], xlab = "Reading", ylab = "Math")
abline(mod1, lwd = 2)

# Add points and line for males
points(achrdg12[males], achmat12[males], col = "#4B9CD3", pch = 2)
abline(mod2, col = "#4B9CD3", lwd = 2)

# Add a legend
legend(x = "topleft", legend = levels(gender), pch = c(1, 2), col = c(1, "#4B9CD3"))
```

**Take a minute to think about what this plot is telling us about the relationships among Math Achievement, Reading Achievement, and Gender.**

* **Is the gender gap in math constant?** 
* **Is the relationship between math and reading the same for males and females?**
* **Can you provide a summary of the plot in terms of what it says about the gender gap in Math Achievement?**

**Please write down your answers to these questions and be prepared to share them in class.**

Note that in the figure above, we estimated two separate simple regression models, one just for males and one just for females. In the next few sections, we will work our way towards a single multiple regression model that can be used to represent the relationships among these three variables.  
 
## Binary + continuous{#binary-continuous-6}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```
As a first step, let's consider what happens when we include both Gender and Reading Achievement as predictors of Math Achievement in our usual multiple regression equation: 

\[\widehat Y = b_0 + b_1X_1 + b_2 X_2 
(\#eq:yhat-6a)
\]

where

* $Y$ is Math Achievement in grade 12
* $X_1$ is Reading Achievement in grade 12
* $X_2$ is Gender (binary, with Female = 0 and Male = 1)

Note that this model does **not** include an interaction between the two predictors -- we are first going to consider what is "missing" from the usual regression model, and then use this to motivate inclusion of another predictor that represents the interaction. To get an initial sense of what is missing, the model in Equation \@ref(eq:yhat-6a) is plotted Figure \@ref(fig:math-reading-2) -- can you spot the difference with Figure \@ref(fig:math-reading-1)? 


```{r math-reading-2, fig.cap = 'Math Achievement, Reading Achievement, and Gender (No Interaction).', fig.align = 'center'}
mod3 <- lm(achmat12 ~ achrdg12 + gender, data = NELS)
a_females <- coef(mod3)[1]
b_females <- coef(mod3)[2]

a_males <- a_females + coef(mod3)[3]
b_males <- b_females 

# Plot reading and math for females
plot(achrdg12[females], achmat12[females], xlab = "reading", ylab = "math")
abline(a_females, b_females, lwd = 2)

# Add points and line for males
points(achrdg12[males], achmat12[males], col = "#4B9CD3", pch = 2)
abline(a_males, b_males, col = "#4B9CD3", lwd = 2)

# Add a legend
legend(x = "topleft", legend = levels(gender), pch = c(1, 2), col = c(1, "#4B9CD3"))
```

In order to interpret our multiple regression model, we can take the same two-step approach we used to interpret categorical predictors in Chapter \@ref(chapter-5). First, we plug in values for the categorical predictor, then we use the resulting equations solve for quantities of interest. In particular, 

\begin{align}
\widehat Y (Female) & = b_0 + b_1X_1 + b_2 (0) \\ & = b_0 + b_1X_1  \\\\
\widehat Y (Male) & = b_0 + b_1X_1 + b_2 (1) \\ & = (b_0 + b_2) + b_1 X_1 \\\\
\widehat Y (Male) - \widehat Y (Female) & = b_2
\end{align}

The equations for $\widehat Y (Female)$ and $\widehat Y (Male)$ are referred to *simple trends* or *simple slopes*. These describe the regression of Math on Reading, simply for Males, or simply for Females. This usage of the term "simple" is related to the simple regression model with only one predictor, but it is also used more widely. The difference between the two simple regression equations is, in this context, the predicted gender gap in Math Achievement. 

From these equations we can see that: 

* The simple trends for Females and Males have different intercepts, because the regression coefficient for Gender ($b_2$) is added to the intercept for Males. 
* The simple trends for Females and Males have the same slope, meaning that the regression lines are parallel (see Figure \@ref(fig:math-reading-1)).  
* The difference between the predicted values (i.e., the predicted gender gap in Math Achievement) is a constant, and is equal to the regression coefficient for Gender ($b_2$)

The regression coefficients for the example data are shown below. **Please use these numbers to provide an interpretation of the simple trends and the gender gap in Math Achievement for the NELS example** (Don't worry about statistical significance, just focus on the meaning of the coefficients.) 

```{r}
coef(mod3)
```

### Marginal means

Before moving on to discuss what is missing from the regression model in Equation \@ref(eq:yhat-6a), it is important to note that the interpretation of the regression coefficient on gender has changed from what we discussed in Chapter \@ref(chapter-5). In Chapter \@ref(chapter-5), we noted that the regression coefficient on a dummy variables can be interpreted as the mean of the group indicated by the dummy (e.g. the mean of Math Achievement for Males, or for Females). However, when additional predictors are included in the regression model, this relationship no longer holds in general. 

To see this we can compare the output of the t-test in Section \@ref(example-6) with the regression output shown above. In the t-test, the group means for Math Achievement were 55.47 for Females and 58.63 for males, so the mean difference was

\[58.63 - 55.47 58.63 - 55.47 = 3.16 \]

However, the regression coefficient on gender in the multiple regression model above is equal to $3.50$. Why the difference? 

Remember that in the multiple regression model, the regression coefficient on Gender controls for the relationship between Reading Achievement and Gender. So, the multiple regression coefficient represents the relationship between Gender and Math, after controlling for Reading. The t-test doesnt control for Reading.  

In order to emphasize the distinction between "raw" group means computed from the data and the predicted group means obtained from a multiple regression model, the latter are referred to as *marginal means*, or sometimes as *adjusted means* or *least squares means*. 

### Summary 

In a regression model with one continuous and one binary predictor (and no interaction):  

* The model results in two regression lines, one for each value of the binary predictor. These are called the simple trends.
* In a standard multiple regression model, the simple trends are parallel but can have a different intercept; the difference in the intercepts is equal to regression coefficient of the binary variable. 
* The difference between the simple trends is often called a "gap", and the gap is also equal to the regression coefficient of the binary variable. 
* It is important to note that the predicted group means for the binary variable are no longer equal to the "raw" group means computed directly from the data, because the predicted group means now control for the intercorrelation among the predictors. The predicted group means are called marginal means to emphasize this distinction. 

## Binary + continuous + interaction {#binary-continuous-interaction-6}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

In the previous section, the multiple regression model led us to conclude that the gender gap in Math Achievement is constant and equal to 3.50 (i.e., the regression coefficient on Gender). As we saw in Figure \@ref(fig:math-reading-2), this implies that the simple trends are parallel. However, these conclusions do not agree with what we saw in Section \@ref(example-6) when we fitted two simple regression model to the NELS data. In this section, we discuss what is missing from the multiple regression model in Equation \@ref(eq:yhat-6a): the interaction between Gender and Reading. 

Mathematically, an interaction is just the product between two variables. Equation \@ref(eq:yhat-6b) shows how to include this product in our multiple regression model -- we just take the product of the two predictors and add it into the model as a third predictor:

\[\hat Y = b_0 + b_1X_1 + b_2 X_2 + b_3 (X_1 \times X_2).
(\#eq:yhat-6b)
\]


For the NELS example, this regression model is depicted in Figure \@ref(fig:math-reading-3). Note the the simple trends are no longer parallel and the regression lines agree exactly with what we had in Section \@ref(example-6). So, as promised, we have now arrived at a single multiple regression model that captures the relationships among Math Achievement, Reading Achievement, and Gender.


```{r math-reading-3, fig.cap = 'Math Achievement, Reading Achievement, and Gender (No Interaction).', fig.align = 'center'}
# Interaction via hard coding
genderXachrdg12 <- (as.numeric(gender) - 1) * achrdg12
mod4 <- lm(achmat12 ~ achrdg12 + gender + genderXachrdg12)

a_females <- coef(mod4)[1]
b_females <- coef(mod4)[2]

a_males <- a_females + coef(mod4)[3]
b_males <- b_females + coef(mod4)[4]

# Plot reading and math for females
plot(achrdg12[females], achmat12[females], xlab = "reading", ylab = "math")
abline(a_females, b_females, lwd = 2)

# Add points and line for males
points(achrdg12[males], achmat12[males], col = "#4B9CD3", pch = 2)
abline(a_males, b_males, col = "#4B9CD3", lwd = 2)

# Add a legend
legend(x = "topleft", legend = levels(gender), pch = c(1, 2), col = c(1, "#4B9CD3"))
```

To see what the model says about the data, let's work through the model equations using our two-step procedure. As usual, we first plug in values for the categorical predictor, then we use the resulting equations solve for quantities of interest (the simple trends and the gender gap). 

\begin{align}
\widehat Y (Female) & = b_0 + b_1X_1 + b_2 (0) + b_3(X_1 \times 0) \\ & = b_0 + b_1X_1  \\\\
\widehat Y (Male) & = b_0 + b_1X_1 + b_2 (1) +  b_3(X_1 \times 1)\\ & = (b_0 + b_2) + (b_1 + b_3) X_1 \\\\
\widehat Y (Male) - \widehat Y (Female) & = b_2 + b_3 X_1
\end{align}

Similar to the results in Section \@ref(binary-continuous-6), we can that see that 

* The simple trends for Females and Males have different intercepts, because the regression coefficient for Gender ($b_2$) is added to the intercept for Males.

However, in contrast to Section \@ref(binary-continuous-6), 

* The simple trends for Females and Males no longer have same slope, because the regression coefficient for the interaction ($b_3$) is added to the slope for Males.

* The difference between the predicted values (i.e., the predicted gender gap in Math Achievement) is no longer constant, but is instead a function of $X_1$. In particular, the gender gap in Math changes by $b_3$ units for each unit of change in Reading. 

This last point is especially important in the context of our example. The gender gap in Math Achievement is a function of Reading Achievement. This is the mathematical meaning behind the concept of an interaction -- the relationship between two variables (Math and Gender) is changing as a function of a third variable (Reading). This is what is means when we say, e.g., that the relationship between Gender and Math depends on Reading. 

### Choosing the moderator 

The concept of an interaction is "symmetrical" in the sense that we can chose which variable goes in the "depends on" clause. For example, it is equally valid to say 

* the relationship between Math and Gender depends on Reading, or
* the relationship between Math and Reading depends on Gender. 

Whichever variable appears in the "depends on" clause is called the *moderator*, and the other two variables are called the focal variables. The researcher chooses which variable to treat as the moderator when interpreting an interaction. The overall idea here is to "break down" the interaction in the way that is most compatible with the research question(s). 

For example, our research question was about the gender gap in STEM Achievement. So, our focal relationship is between Math and Gender, and Reading would be our moderator. Taking this approach, it would make sense to focuses our interpretation on difference $\widehat Y (Male) - \widehat Y (Female)$ -- i.e., the predicted gender gap. So, we could say that the relationship between Math and Gender depends on Reading, or, more specifically, that the predicted gender gap in Math changes by $b_3$ units for each unit of increase in Reading.  

By contrast, if we were more interested in the relationship between Math and Reading, then we could treat Gender as the moderator. Here it would make sense to focus our interpretation on the simple trends, and in particular on the slope parameters in these equations. For example, we might say: 

* For females, predicted Math Achievement changed by $b_1$ units for each unit of increase in Reading, 
* whereas for males, the predicted change was ($b_1 + b_3$) units for each unit of increase in Reading. 

This might feel less intuitive than talking about the gender gap, but the two interpretations are mathematicaly equivalent. Its just a matter of whether you want to interpret the regression coefficient $b_3$ with reference to the gender gap, or with reference to the simple trends. 

### Back to the example 

The regression coefficients for the example data are shown below. **Please use these numbers to provide an interpretation of the interaction between Gender and Reading.** (Don't worry about statistical significance, just focus on the meaning of the coefficients.) 

```{r}
coef(mod4)
```

Some potential answers are hidden below, but don't peak until you have tried it for yourself!

```{r}
# Gender gap
# General: The gender gap in Math is smaller for students who are also strong in Reading
# Specific: The gender gap in Math Achievement decreases by .18 percentage points for each percentage point increase Reading Achievement

# Simple slopes
# General: The relationship between Math and Reading is stronger (i.e. has a larger slope) for Females than for Males
# Specific:
#  For Females, Math scores are predicted to increase by .72 percentage points for each percentage point increase in Reading Achievement
#  For Males, Math scores are predicted to increase by .55 percentage points for each percentage point increase in Reading Achievement
```

### Centering the continuous predictor

You may have noticed that the coefficient on gender was wildly different in regression models with and without the interaction. In the model without the interaction (Section \@ref(binary-continuous-6)) the coefficient on Gender was 3.50, and in the model with the interaction (above), it was 13.40. So in one model, the "effect" of being Male was a 3.5 percentage points gain on a Math exam, but in the other model, it was a 13.40 percentage point gain. Why this huge difference in the effect of Gender? 

The answer can be seen in the equation for the gender gap. In the model **without the interaction**, the gender gap was constant and equal to the regression coefficient on Gender (denoted as $b_2$ in the model): 

\[ \widehat Y (Male) - \widehat Y (Female) = b_2 \]

But in the regression model **with the interaction**, the gender gap was a linear function of Reading and the regression coefficient on Gender is actually the intercept for the linear relationship. 

\[ \widehat Y (Male) - \widehat Y (Female) = b_2 + b_3 X_1 \]

So, in the model with the interaction, $b_2$ is the gender gap for students who score zero on Reading Achievement. Since the lowest score on Reading Achievement was around 35, the intercept in this equation (i.e., the regression coefficient on gender, $b_2$) is not very meaningful. 

One way to address this situation is to center the Reading variable so that it has a mean of zero. To do this, let

\[ D_1 = X_1 - \bar X_1 \] 

denote the deviation scores for $X_1$ (i.e., the mean-centered version of $X_1$). Then we just regress Math Achievement on $D_1$ rather than $X_1$. Working through the equations shows that the gender gap is now 

\[ \widehat Y (Male) - \widehat Y (Female) = b_2 + b_3 D_1 \] 

Since $D_1 = 0$ when $X_1 = \bar X_1$, the regression coefficient on Gender ($b_2$) is now interpretable as the gender gap in Math Achievement, for students with average Reading Achievement. This is a much more interpretable number than the coefficient in the original interaction model! 

In the example, this approach yields the following model parameters: 

```{r}
# compute the deviation scores for reading
reading_dev <- achrdg12 - mean(achrdg12, na.rm = T) 

# Run the interaction model as above
genderXreading_dev <- (as.numeric(gender) - 1) * reading_dev

mod5 <- lm(achmat12 ~ reading_dev + gender + genderXreading_dev)
coef(mod5)
```

The regression coefficient for Gender is now pretty close to what it was in the original model without the interaction, but the interpretation is different. Notice that the intercept in the multiple regression model with Reading centered has changed compared to the previous model in which Reading was not centered. However, the centering did not affect the regression coefficient for Reading or the interaction. 

**Please write down your interpretation of the intercept and the regression coefficient for Gender in the above regression output, and be prepared to share your answer in class**. 

### Summary

The interaction between two variables is just their product. When this product is added as a predictor in a multiple regression model with one continuous and one binary predictor:

* The model again results in two regression lines, one for each value of the binary predictor, but we get regression lines with
different intercepts *and different slopes*. 
* The difference in slopes is equal to the regression coefficient on the interaction term.
* The difference (“gap”) between the regression lines changes as a linear function of the continuous predictor, and this change is again equal to the regression coefficient on the interaction term.
* These last two points are equivalent ways of stating the central idea behind a (two-way) interaction: the relationship between two variables changes as a function of a third variable.
* When interpreting an interaction, the researcher chooses which pair of variables will be the "focal relationship" and which variable will be the moderator.  
* Centering the continuous predictor can be helpful for ensuring that the regression coefficient on the binary variable remains interpretable in the presense of an interaction. 

## Inference for interactions {#inference-for-interactions-6}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

Compared to the two simple regression models discussed in Section \@ref(example-6), the real power of the multiple regression model is that it facilitates statistical inference about the simple trends and "gaps". 

For example, in the standard summary output for the regression model, we now have a test of whether the interaction terms is statistically significant. You should feel comfortable interpreting the regression coefficients in this output based on the work we have done so far in this chapter, but now is a good time double check. The interpretation of the standard errors, tests of statistical significance, R-squared, etc, are all the same as in Chapter \@ref(chapter-4). 

```{r}
summary(mod5)
```

Note that the output above doesn't answer all of the questions we might have about a  interaction. For example, since we are interested in the gender gap in Math Achievement, we might want to know for which students the gap is statistically significant. In particular, does the gap disappear for students with higher levels of Reading Achievement? We can answer this type of question by testing marginal effects, which are a generalization of the marginal means discussed in section \@ref(binary-continuous-6). **In general, marginal effects are useful when the goal is to "follow up" a significant interaction in which the focal predictor is categorical (the moderator cab be categorical or continuous).** 

Note that marginal effects are only of interest when the interaction is significant -- if the interaction is not significant, then we know that the relationship between the focal variables doesn't depend on the moderator. 

### Marginal effects

There are three main types of marginal effects. To explain these three approaches, first let's write the gender gap in Math Achievement using a slightly more compact notation. 

\[ \widehat Y (Male) - \widehat Y (Female) = b_2 + b_3 X_1 = \Delta(X_1) \] 

Then we having the following marginal effects that can be computed: 

* Marginal effects at the mean (MEM): Report the gap at the mean value of $X_1$

\[ MEM =  \Delta(\bar X_1) \] 
 
 * Average marginal effect (AVE): Report the average of the marginal effect: 
 
 \[ AVE =  \frac{\sum_i \Delta(X_{i1})}{N} \] 

* Marginal effects at represenative values (MERV): Report the marginal effect for a range of "interesting" values 

 \[ MERV =  \{\Delta(X^*_1), \Delta(X^\dagger_1), \dots \} \] 
 
 MEM and AVE are equivalent for linear regression models (but we will visit the distinction again when we get to logistic regression).  In this section we focus on MERV, which is the more widely used approach. One usual choice for the "interesting values" is the quartiles of $X_1$, which is reported below. Another popular choice is the mean of $X_1$ plus or minus 1 SD. 

```{r}
# Install the package if you haven't already done so
# install.packages("emmeans")

# Load the package into memory
library(emmeans)

# Fit the model using R's formula syntax for interaction '*'
mod6 <- lm(achmat12 ~ gender*achrdg12, data = NELS)

# Use the emmeans function to get the gender means on math, broken down by reading
gap <- emmeans(mod6, specs = "gender", by = "achrdg12", cov.reduce = quantile)

# Test whether the differences are significant
contrast(gap, method = "pairwise")
```
 
The output shows the gender gap in Math Achievement for 5 values of Reading Achievement. The values of Reading Achievement are its 5 quartiles. **Please examine the statistical significance of the gender gap in Math Achievement at the 5 quartiles of Reading Achievement and make a conclusion whether the gap "dissapeared" for students with higher levels of Reading Achievement. Please be prepared to share your answer in class!**

Note that the computations going on "under the hood" for testing marginal means are pretty complicated. You can read more details here: https://cran.r-project.org/web/packages/emmeans/vignettes/basics.html 


### Simple trends

Another way to follow-up a significant interaction is to examine the statistical significance of the simple trends. As discussed in Section \@ref(binary-continuous-interaction-6), the simple trends aren't very meaningful in the context of our example, so this section is just about illustrating the technique, not about adding anything to our discussion of the gender gap in STEM. **In general, testing simple trends can be useful when following-up a significant interaction in which the focal predictor is continuous (the moderator can be categorical or continuous).** 

We can understand simple trends in reference to what they add to the standard `summary` output for the `lm` function (reported at the beginning of Section \@ref(inference-for-interactions-6)): 

* The regression coefficient on the continuous predictors tells us about simple trend for the group designated as zero on the binary predictor (e.g, simply for  females). 

* The regression coefficient on the interaction term tells whether the simple trends differ for the two groups (e.g., whether the simple trend for males differs from the simple trend for females). 

Note that what is missing, or implicit, in this output is a test of the simple trend for males (the group designated as one on the binary predictor). The standard regression output does not provide a statistical test of whether this trend is different from zero. To get this, we need to test the simple trends. 

The test of the simple trends for the example are reported below. Again, these aren't super interesting in the context of our example, but you should **check your understanding of simple trends by writing down an interpretation of the output below**. 

```{r}
# Use the emtrends function to get the regression coefficients on reading, broken down by gender
simple_slopes <- emtrends(mod6, var = "achrdg12", specs = "gender")
test(simple_slopes)
```

### A note on plotting
 
 Another nice advantage of having everything in one model is that we can level-up our plotting. Check out this plot from the `visreg` package (and its only line of code!). You should be able to draw a similar conclusion from the plot as you did from looking at the MERVs. 
 
 
```{r visreg, fig.cap = 'Example of a plot using the `visreg` package.', fig.align = 'center'}
# Install the package if you haven't already done so
# install.packages("visreg")
# Load the package into memory
library(visreg)

mod6 <- lm(achmat12 ~ gender*achrdg12, data= NELS)
visreg(mod6, xvar = "achrdg12", by = "gender", overlay = TRUE)
```

### Summary

When making inferences about an interaction: 

* Often we can get all of the information we need from the regression coefficient on the interaction term, and its associated test of significance. If the interaction isn't significant, we stop there. But if the interaction is significant, we may want to report more information about how the focal relationship depends on the moderator. 

* When the focal predictor is categorical it can be interesting to "follow-up" a significant interaction by taking a closer look at the statistical significance of the marginal means (e.g, how the gender gap in Math changes as a function of Reading)

* When the focal predictor is continuous, it can be interesting to "follow-up" a significant interaction by taking a closer look at the statistical significance of the simple trends / simple slopes. 

## Two continuous predictors {#two-continuous-predictors-6}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

In this section we address the situation in there is an interaction between two continuous predictors. The regression equation and overall interpretation is the same as the previous sections – e.g., the relationship between Y and X1 changes as a function of X2. 

However, there are also some special details that crop up when considering an interaction between two continuous predictors. In this section we will address: 

* The importance of centering the two predictors. When there are two continuous predictors, centering helps interpret the coefficients on the predictors (just like in Section \@ref(binary-continuous-interaction-6)), and can additionally be helpful for reducing the correlation between the predictors and the interaction.  

* How to follow-up a significant interaction using simple trends. Because the predictors are continuous, the focus is on simple trends rather than marginal means.

First, we introduce an new example. 

### Another NELS example

To illustrate an interaction between two continuous predictors, let's replace Gender with SES in our previous analysis. Apologies that this new example is mainly for convenience and doesn't represent a great research question about, e.g., why the relationships between Math and Reading might change as a function of SES. 

The example data and overall approach with SES as the moderator are illustrated below (note that the values 9, 19, and 28 are 10th, 50th, and 90th percentiles SES, respectively). 

```{r readingXses, fig.cap = 'Math (achmat), Reading (achrdg), and SES', fig.align = 'center'}
#Interaction without centering 
mod7 <- lm(achmat12 ~ achrdg12*ses, data = NELS)

# Note that band = F removes the confidence intervals
visreg(mod7, xvar = "achrdg12", by = "ses", overlay = TRUE, band = F)
```

### Centering the predictors

Centering the predictors facilitates the interpretation of their regression coefficients in the presence of an interaction, just as it did Section \@ref(binary-continuous-interaction-6). In particular, the coefficients $b_1$ and $b_2$ in the regression model

\[ \widehat Y = b_0 + b_1X_1 + b_2X_2 + b_3 (X_1 \times X_2) \]

can be interpreted in terms of the following simple trends: 

\begin{align} 
\widehat Y(X_2 =0) & = b_0 + b_1X_1 \\
\widehat Y(X_1 =0)&  = b_0 + b_2X_2. 
(\#eq:simple)
\end{align}

For example, $b_1$ is the relationship between $Y$ and $X_1$, when $X_2$ is equal to zero. 

In general, setting a variable to the value of zero may not be meaningful. But, when setting a *centered* variable (i.e., a deviation score) to zero, this is equivalent to setting the original variable to its mean. So, if the variables in Equation \@ref(eq:simple) were centered, we could say that $b_1$ is the relationship between $Y$ and $X_1$, when $X_2$ is equal to its mean. Again, this is just the same trick as Section \@ref(binary-continuous-interaction-6), but this time both predictors are continuous and both are centered. 

There is another reason for centering continuous predictors when there is an interaction in the model, and this has to do with reducing the correlation among the predictors and their interaction. In general, the interaction term will be positively correlated with both predictors if (a) the predictors themselves are positively correlated and (b) the predictors take on strictly positive (or strictly negative) values. Highly correlated predictors lead to redundant information the model, so we generally want to avoid this situation (this is technically called *multicollinearity* and we discuss it in more detail in a  Chapter \@ref(chapter-7)). Centering can "break" the correlation between the preditors and the interaction, thereby making the predictors less redundant with their interaction. 

To see how this works, let's take a look at Figure \@ref(fig:centering). The left hand panel shows that SES and its interaction with reading are highly correlated. This is because (a) SES and Reading are themselves positively correlated, and (b)  both SES and Reading take on strictly positive values. As mentioned, the interaction term will be positively correlated with both predictors whenever these two conditions hold. (The figure just shows the correlation between SES and the interaction, but the same situation holds for Reading.) 

```{r centering, fig.cap = 'Correlation Between SES and SES X Reading, With and Without Centering', fig.align = 'center'}
# Correlation without centering
r <- cor(ses, achrdg12*ses)

# Plot
par(mfrow = c(1, 2))
title <- paste0("correlation = ", round(r, 3))
plot(ses, achrdg12*ses, col = "#4B9CD3", main = title, xlab = "SES", ylab = "SES X Reading")

achrdg12_dev <- achrdg12 - mean(achrdg12)
ses_dev <- ses - mean(ses)
r <- cor(ses_dev, achrdg12_dev*ses_dev)

# Plot
title <- paste0("correlation = ", round(r, 3))
plot(ses_dev, achrdg12_dev*ses_dev, col = "#4B9CD3", main = title, xlab = "SES Centered", ylab = "SES Centered X Reading Centered")
```

We can see in the right hand panel of Figure \@ref(fig:centering) how centering the two predictors "breaks" the linear relationship between SES and its interaction with Reading. After centering, the relationship between the SES and its interaction is now highly non-linear, and the correlation is approximately zero. Again, the same is true for the relationship between Reading and the interaction, but the figure only shows the situation for SES. The upshot of all this is that centering reduces multicollinearity between the "main effects" of the predictors and their interaction. 

The two sets of output below show the regression of Math on Reading, SES, and their interaction. The first ouptut does not center the predictors, but the second output does (the `_dev` notation denotes the centered predictors).  

We can see that  SES is a significant predictor in the centered model but not in the "un-centered" model. This has to do with changing the interpretation of the coefficient (it now represents the relationship between Math and SES for students with average Reading), and also reflects the fact that SES is no longer so highly correlated with the interaction term after centering. 

```{r}
# Without centering
mod7 <- lm(achmat12 ~ achrdg12*ses)
summary(mod7)
```

```{r}
# With centering
mod8 <- lm(achmat12 ~ achrdg12_dev*ses_dev)
summary(mod8)
```

**To check your understanding of the output above, please provide an interpretation of all four regression coefficients in the centered model. Your interpretations should make reference to the situation where one or both predictors are equal to zero (see Equation \@ref(eq:simple) above) and should also mentioned the interpretation of the value of zero for the centered variables**

Note that the interaction, the R-squared, and their associated tests do not change value. This is discussed in more detail in the extra material at the end of this section, but it is sufficient to note that centering only affects the interpretation of the main effects (and the intercept, of course). 

### Simple trends

Centering helps us interpret the "main effects" of the individual predictors, but we haven't yet discussed how to interpret the interaction term when both predictors are continuous. In the case with a binary predictor, we had two different regression lines. So what do we get when both predictors are continuous? 

The usual way to answer this question is an extension of the MERV approach to marginal effects, discussed in Section \@ref(inference-for-interactions-6). The basic idea is to present the relationship between the two focal variables, for a selection of values of the moderator. This idea is shown in Figure \@ref(fig:readingXses) above. As with MERV, the choice of values of the moderator is up to the researcher, but some usual choices are 

* The quartiles of the moderator (i.e., the five number summary) or subset thereof
* M $\pm$ 1 SD of the moderator
* A selection of percentiles (`visreg` uses the 10th, 50th, and 90th)

These are all doing very similar things, so choosing among them isn't very important. 

Although the interaction between Reading and SES was not significant in our example model, let's break down the interaction using SES as the moderator, just to see how this approach works. The first part output below shows the simple slopes for the three values of SES shown in Figure \@ref(fig:readingXses) (i.e., the 10th, 50th, and 90th deciles). We can see that the simple slopes are all different from zero (and the non-significant interaction tells us that they are not different from one-another). 

```{r}
# Break down interaction with SES as moderator
simple_slopes <-emtrends(mod7, var = "achrdg12", specs = "ses", at = list(ses = c(9, 19, 28)))
test(simple_slopes)
```

### Summary

When regressing an outcome on two continuous predictors and their interaction, the overall interpretation of the model is same as discussed in Section \@ref(binary-continuous-interaction-6), but: 

* It is useful to center both predictors, to facilitate the interpretation of the main effects (i.e., regression coefficients on the individual predictors), and to reduce the correlation between the predictors and their interaction (i.e., reduce multicollinearity). 

* When following up a significant interaction, the usual approach is to report the simple trends between the focal variables for a selection of values of the moderator (e.g., a selection of percentiles). The example illustrated how to do this even though the interaction was not significant, but you shouldn't follow up a non-significant interaction. 

### Extra: How centering works* {#how-centering-works-6}

It might seem that centering both predictors to improve the chances of getting significant main effects is a dubious practice. However, using the centered or the un-centered variables doesn’t really make a difference in term of what predictors are in the model are. The follow algebra show why, using $D = X - \bar X$ for the centered variables: 

\begin{align} 
\widehat Y & = b_0 + b_1D_1 + b_1D_2 + b_3 (D_1 \times D_2) \\ 
& = b_0^* + (b_1 - b_3 \bar X_1) X_1 + (b_2 - b_3 \bar X_2) X_2 +  b_3 (X_1 \times X_2) \\ 
\text{where} & \\ \\ 
b_0^* & = a - b_1\bar X_1 - b_2\bar X_2 - b_3\bar X_1\bar X_2. 
\end{align}

The second line of the equation shows that we are not changing what we regress $Y$ on -- i.e., the predictors are still $X_1$ and $X_2$. We are re-packaging the intercept and main effects, which is exactly the purpose of this approach. But, centering does not change the regression coefficient for the interaction -- it is still interpreted with respect to the un-centered variables.  

## Two categorical predictors {#two-categorical-predictors-6}

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

This section addresses interactions between two categorical predictors. Up until now, we have looked at interactions only for categorical predictors that are dichotomous. In this section we address an example in which one of the categorical predictors has more than two levels. This requires combining what we learned about contrast coding (Chapter \@ref(chapter-5)) with what we have learned about interactions. One nice aspect of interactions among categorical predictors is that we usually don't need to use procedures like marginal effects to follow up significant interactions, so long as we make good use of contrast coding. 

In experimental (as opposed to observational) settings, interactions among categorical predictors fall under the much larger topic of ANOVA and experimental design. The analysis we look at in this section is a two-way between-subjects ANOVA, meaning that there are two categorical predictors considered, as well as their interaction, and both predictors are cross-sectional. ANOVA is a big topic and is not the focus of this course. However, we will discuss how to summarize the results of our analysis in an ANOVA table, and consider how this differs from the standard regression approach.  

### An example from ECLS

For this topic we will switch over to the ECLS data and examine how SES and Pre-K attendance interact to predict Math Achievement at the beginning of Kindergarten. The variables we will examine are

* Math Achievement at the beginning of K (`c1rmscal`). This is the number of correct questions on a test with approximately 70 items. 

* Whether the child attended Pre-K (`p1center`). This is a binary variable that indicates pre-K attendance. 

* SES, coded as quintiles (`wksesq5`). We will denote this variable as SES, but keep in mind it is quintiles in this example (e.g., SES = 1 are the respondents with SES between the minimum and the first quintile). 

Coding SES as quintiles allows us to consider it as a categorical predictor with 5 levels. This is convenient for our illustration of interactions between categorical predictors. It is also a widely-used practice in policy research, because SES often has non-linear relationships with outcome variables of interest, and these relationships can be more easily captured by treating SES as a categorical variable. 

In this analysis, our focus will be whether the "effect" of Pre-K on Math Achievement depends on (i.e., is moderated by) the child's SES. Please note that I will use the term "effect" in this section to simplify language, but we know that Pre-K attendance was not randomly assigned in ECLS, so please keep in mind that this terminology is not strictly correct. 

The relationship among the three variables is summarized in the `visreg` plot below. We can see that the effect of Pre-K on Math Achievement appears to differ as a function of SES -- i.e., it appears that there is an interaction between Pre-K and SES. Our goal in this section is to produce an analysis corresponding to the figure. **Before moving on, please take a moment to write down your interpretation of  Figure \@ref(fig:prek-by-ses), focussing on how it illustrates an interaction between SES and Pre-K. Additionally, please describe how the figure would be different if there was no interaction between Pre-K and SES**. 

```{r, prek-by-ses, fig.cap = 'Math Achievement, Pre-K Attendence, and SES', fig.align = 'center'}
load("ECLS2577.Rdata")
ecls$prek <- factor(2 - ecls$p1center)
ecls$wksesq5 <- factor(ecls$wksesq5)
mod <- lm(c1rmscal ~ prek*wksesq5, data = ecls)
visreg::visreg(mod, xvar = "wksesq5", by = "prek", 
               partial = F, rug = F, overlay = T, 
               strip.names = T, xlab = "SES", 
               ylab = "Math Achievement in K")
```

### The "no-interaction" model 

As in Section \@ref(binary-continuous-6), we will start with a model that includes only the main effects of SES and Pre-K. Seeing where that model "goes wrong" is a good way of understanding the interaction between the two predictors. 

In order to represent a model with multiple categorical predictors, it is helpful to change our notation from the usual $Y$ and $X$ to the more informative "variable names" notation:

\begin{align}
\widehat Y = b_0 + b_1 PREK + b_2SES_2 + b_3SES_3 + b_4 SES_4 + b_5 SES_5. 
(\#eq:prek-ses1)
\end{align}

In this notation, the predictor variables are indicators (binary dummies) that use the variable names rather than $X_1$, $X_2$, etc. The variable $PREK$ is just the indicator for Pre-K attendance, as defined above. The variable $SES_j$ is an indicator for the j-th quintile of SES. 

Note that both predictors use reference-group coding, as discussed in Chapter \@ref(chapter-5). For $PREK$, reference-group coding is implied because it is a binary indicator. For $SES$, reference-group coding is accomplished by omitting the dummy for the first quintile (i.e., the first quintile is the reference group). 

We can interpret the coefficients in this model using the same two-step procedure described in Chapter \@ref(chapter-5). Since there are many terms in the model, things are going to start getting messy quickly, so brace yourself for some long equations (but simple math!). 

The main points about the interpretation of this model are as follows. 

* The intercept is the predicted value of Math Achievement for students in the first SES quintile who did not attend Pre-K. This corresponds to the blue line in the first column of Figure \@ref(fig:prek-by-ses). 

\begin{align}
\widehat Y(PREK = 0, SES = 1) & = b_0 + b_1 (0) + b_2(0)+ b_3(0) + b_4 (0) + b_5 (0) \\  
& = b_0
\end{align}

* The effect of Pre-K attendance for students in the first SES quintile is equal to $b_1$. This corresponds to the difference between the red and blue lines in the first column of Figure \@ref(fig:prek-by-ses). 

\begin{align}
\widehat Y(PREK = 1, SES = 1) & = b_0 + b_1 (1) + b_2(0)+ b_3(0) + b_4 (0) + b_5 (0) \\  
& = b_0 + b_1 \\
\implies & 
\end{align}

\begin{align}
\widehat Y(PREK = 1, SES = 1) - \widehat Y(PREK = 0, SES = 1) & = b_1
\end{align}

* Because the model in Equation \@ref(eq:prek-ses1) does not include an interaction, we know that it implies the effect of pre-K is constant over levels of SES. Below we consider SES = 2, but the same approach works for the other levels of SES.

\begin{align}
\widehat Y(PREK = 0, SES = 2) & = b_0 + b_1 (0) + b_2(1 )+ b_3(0) + b_4 (0) + b_5 (0)\\  
& = b_0 + b_2 \\ \\ 
\widehat Y(PREK = 1, SES = 2)& = b_0 + b_1 (1) + b_2(1)+ b_3(0) + b_4 (0) + b_5 (0)\\  
& = b_0 + b_1 + b_2 \\ \\ 
\implies & 
\end{align}

\begin{align}
\widehat Y(PREK = 1, SES = 2) - \widehat Y(PREK = 0, SES = 2) = b_1  
\end{align}

This equation says that the difference between the red and blue lines in the second column of Figure \@ref(fig:prek-by-ses) is the same as the difference in the first column -- i.e., they both equal $b_1$. This is what it means for there to be no interaction between two categorical predictors. 

If you want more practice with this, you can show that Equation \@ref(eq:prek-ses1) implies the effect of Pre-K is constant over all levels of SES. Additionally, you can use the 2-step approach to show that the effect of SES is constant over levels of Pre-K attendance. 

### Adding the interaction(s)

We have just seen that Equation \@ref(eq:prek-ses1) implies that the effect of Pre-K is constant over levels SES, and vise versa. In order to address our research question about whether the relationship between Pre-K attendance and Math Achievement depends on children's SES, we will need to add something to the model -- an interaction (surprise!). 

We know that interactions are just products (multiplication) of predictor variables. But, since SES is represented as 4 dummies, this means we need 4 products in order to represent the interaction of Pre-K with SES. The resulting model can be written: 

\begin{align}
\widehat Y  = & b_0 + b_1 PREK + b_2SES_2 + b_3SES_3 + b_4 SES_4 + b_5 SES_5 + \\ 
&  b_6 (PREK \times SES_2) + b_7(PREK \times SES_3) + \\
& b_8 (PREK \times SES_4) + b_9 (PREK \times SES_5) 
(\#eq:prek-ses2)
\end{align}

As you can see, we have a lot of predictors in this model! Although we are only considering two distinct "conceptual" predictors, we have 9 coefficients in our regression model (+ the intercept). 

Again, there are a few main things to notice: 

* The interpretation of the intercept has not changed. It still corresponds to the blue line in the first column of Figure \@ref(fig:prek-by-ses). 

* The regression coefficient on $PREK$ is still the "effect" of Pre-K for students in the first SES quintile. It corresponds to the difference between the red and blue line in the first column of Figure \@ref(fig:prek-by-ses). This is because all the $SES_j$ variables are equal to zero for students in the first SES quintile, and so all of the interaction terms in Equation \@ref(eq:prek-ses2) are equal to zero for this case. 

* The effect of Pre-K is no longer constant over levels of SES. Again we will focus on SES = 2, but the same approach works for the other levels of SES.

\begin{align}
\widehat Y(PREK = 0, SES = 2) & = b_0 + b_1 (0) + b_2(1 )+ b_3(0) + b_4 (0) + b_5 (0) + \\
&  b_6 (0 \times 1) + b_7(0 \times 0) + b_8 (0 \times 0) + b_9 (0\times 0) \\  
& = b_0 + b_2 \\ \\ 
\widehat Y(PREK = 1, SES = 2) & = b_0 + b_1 (1) + b_2(1)+ b_3(0) + b_4 (0) + b_5 (0) + \\
&  b_6 (1 \times 1) + b_7(1 \times 0) + b_8 (1 \times 0) + b_9 (1\times 0) \\  
& = b_0 + b_1 + b_2 + b_6 \\ \\ 
\implies & 
\end{align}

\begin{align}
\widehat Y(PREK = 1, SES = 2) - \widehat Y(PREK = 0, SES = 2) = b_1 + b_6
\end{align}

The last line shows that the "effect" of Pre-K for students in the second SES quintile is $b_1 + b_6$. This is not the same as for the effect for students in the first quintile, which was just $b_1$. In other words, the difference between the red and blue lines in the first column of Figure \@ref(fig:prek-by-ses) (i.e., $b_1$) is not equal to the difference in the second column (i.e., $b_1 + b_6$). Consequently, our new model with the interaction better reflects the example data. 

The same approach shows that the effect of Pre-K at each level of SES results in a similar equation:

\begin{align}
\widehat Y(PREK = 1, SES = 3) - \widehat Y(PREK = 0, SES = 3) & = b_1 + b_7 \\ 
\widehat Y(PREK = 1, SES = 4) - \widehat Y(PREK = 0, SES = 4) & = b_1 + b_8 \\ 
\widehat Y(PREK = 1, SES = 5) - \widehat Y(PREK = 0, SES = 5) & = b_1 + b_9 \\ 
\end{align}

This pattern makes it clear that, to isolate effect of each interaction (i.e., $b_6$ through $b_9$), we need to subtract off $b_1$ -- i.e., we need to subtract off the effect of Pre-K for students in the first SES quintile. In this sense, the interpretation of $b_1$ is quite similar the interpretation of the intercept in regular reference-group coding (see Section \@ref(reference-group-coding)). It is the "reference effect" or baseline to which the interaction terms are compared. 

For example

* The interaction between Pre-K and the second SES quintile is the additional effect pre-K has on Math Achievement for students in the second SES quintile, *as compared to the effect in the first SES quintile.* 

* The interaction between Pre-K and the third SES quintile is the additional effect pre-K has on Math Achievement for students in the 3rd SES quintile, *as compared to the effect in the first SES quintile.* 

* etc etc. 

Mathematically, the interaction terms are represented as "differences-in-differences". For example, 

\begin{align} 
b_6 & = [\widehat Y(PREK = 1, SES = 2) - \widehat Y(PREK = 0, SES = 2)] -  b_1 \\ 
    & = [\widehat Y(PREK = 1, SES = 2) - \widehat Y(PREK = 0, SES = 2)] \\
    & - [\widehat Y(PREK = 1, SES = 1) - \widehat Y(PREK = 0, SES = 1)]
\end{align}

This looks quite complicated but it is just an extension of reference-group coding. This equation is saying that the "reference effect" or "baseline" for interpreting the interaction ($b_6$) is the effect of Pre-K in the first SES quintile (i.e., $b_1$). As noted above, all of the interaction terms have the same reference effect. 

### Back to the example

That last section was a lot to take in, so let's put some numbers on the page to check our understanding. The output below shows the summary for a model that regresses Math Achievement on Pre-K, SES, and their interaction. **Please write down an interpretation of magnitude, direction, and statistical significance of each regression coefficient in this output (including the intercept), and be prepared to share your answers in class.** Remember that `wksesq5` is the variable code for the SES quintiles -- the digit that follows the variable code indicates the level of variable. It may be helpful to refer to Figure \@ref(fig:prek-by-ses) in your interpretations. 

```{r}
mod <- lm(c1rmscal ~ prek*wksesq5, data = ecls)
summary(mod)
```

### The ANOVA approach 
The output in the previous section is detailed enough that it is not usually required to follow-up a significant interaction among categorical predictors using marginal effects. However, the summary output omits some information we might be interested in. For example, the Pre-K indicator in the above output tells us the effect of Pre-K, but only for children in the first SES quintile. We might also want to know, what is the main effect of Pre-K across levels of SES -- i.e., is there a significant difference in Math Achievement for students who attended Pre-K or not, after controlling for their level of SES? Similarly, what is the main effect of SES? 

I'll note that some people think it is bad practice to interpret main effects in the presence of an interaction. The main effects tell us what the effect of a predictor is "on average" or overall, after controlling for the other predictor(s). But the interaction tells us that this effect depends on the other predictor(s). Some people think that you shouldn't report the overall effect when the "real message" of the interaction is that the effect changes as a function of the other predictors. I think that main effects and interactions aren't really incompatible concepts, but you should be careful with them. 

Anyway, we can summarize the main effects of Pre-K and SES, well as their interaction, by asking how much variance they explain, after controlling for the other predictor(s). This is the ANOVA approach we discussed last semester, but now applied to two categorical predictors. 

The ANOVA table for our example is below, and it is followed by the R-squared coefficients for each predictor, which are called "eta-squared" ($\eta^2$) in the context of ANOVA. These R-squared (eta-squared) coefficients tell us what proportion of the variance in Math Achievement is attributable to the main effects and the interaction. Note that the R-squared (eta-squared) values do not add up to the total R-squared reported in the `lm` output above. This is because any variance that is jointly explained by the predictors is not attributed to individual predictors (e.g., the area "B" in the Venn diagram in Figure \@ref(fig:venn-diagram) is not counted towards the effects for the individual predictors). In the next chapter, we will address how to build models so that we can systematically divide up the variance explained by the predictors. 


```{r}
# ANOVA Table
anova(mod)

# R-squared (eta-squared)
#install.packages("effectsize")
effectsize::eta_squared(mod, partial = F)
```

**Please write down your interpretation of the ANOVA table and R-squared (eta-squared) coefficients and be prepared to share you thoughts in class.** Note that the ANOVA output leads to different conclusions than the regression output above. We will discuss the discrepancies between the ANOVA and regression output in class. 

## Workbook 

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "hide", style = button)
```

This section collects the questions asked in this chapter. We will discuss these questions in class. If you haven't written down / thought about the answers to these questions  before class, the lesson will not be very useful for you! So, please engage with each question by writing down one or more answers, asking clarifying questions, posing follow up questions, etc. 


**Section \@ref(example-6)**

```{r example-1, fig.cap = 'Math Achievement, Reading Achievement, and Gender.', fig.align = 'center'}
mod1 <- lm(achmat12[females] ~ achrdg12[females])
mod2 <- lm(achmat12[males] ~ achrdg12[males])

# Plot reading and math for females
plot(achrdg12[females], achmat12[females], xlab = "Reading", ylab = "Math")
abline(mod1, lwd = 2)

# Add points and line for males
points(achrdg12[males], achmat12[males], col = "#4B9CD3", pch = 2)
abline(mod2, col = "#4B9CD3", lwd = 2)

# Add a legend
legend(x = "topleft", legend = levels(gender), pch = c(1, 2), col = c(1, "#4B9CD3"))
```

Take a minute to think about what this plot is telling us about the relationships among Math Achievement, Reading Achievement, and Gender.

* Is the gender gap in math constant?
* Is the relationship between math and reading the same for males and females?
* Can you provide a summary of the plot in terms of what it says about the gender gap in Math Achievement?


**Section \@ref(binary-continuous-6)**

* No interaction model: The regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the simple trends and the gender gap in Math Achievement for the NELS example. (Don't worry about statistical significance, just focus on the meaning of the coefficients.) 

```{r}
mod3 <- lm(achmat12 ~ achrdg12 + gender, data = NELS)
coef(mod3)
```

**Section \@ref(binary-continuous-interaction-6)**

* Interaction model: The regression coefficients for the example data are shown below. Please use these numbers to provide an interpretation of the interaction between Gender and Reading. (Don't worry about statistical significance, just focus on the meaning of the coefficients.) 

```{r}
genderXachrdg12 <- (as.numeric(gender) - 1) * achrdg12
mod4 <- lm(achmat12 ~ achrdg12 + gender + genderXachrdg12)
coef(mod4)
```

* Interaction model with centered continuous predictor: Please write down your interpretation of the intercept and the regression coefficient for Gender in the regression output below. 

```{r}
# compute the deviation scores for reading
reading_dev <- achrdg12 - mean(achrdg12, na.rm = T) 

# Run the interaction model as above
genderXreading_dev <- (as.numeric(gender) - 1) * reading_dev

mod5 <- lm(achmat12 ~ reading_dev + gender + genderXreading_dev)
coef(mod5)
```

**Section \@ref(inference-for-interactions-6)**

* The output shows the gender gap in Math Achievement for 5 values of Reading Achievement. The values of Reading Achievement are the 5 quartiles. Please examine the statistical significance of the gender gap in Math Achievement at the 5 quartiles of Reading Achievement and make a conclusion whether the gap "dissapeared" for students with higher levels of Reading Achievement.

```{r}
# Install the package if you haven't already done so
# install.packages("emmeans")

# Load the package into memory
library(emmeans)

# Fit the model using R's formula syntax for interaction '*'
mod6 <- lm(achmat12 ~ gender*achrdg12, data= NELS)

# Use the emmeans function to get the gender means on math, broken down by reading
gap <- emmeans(mod6, specs = "gender", by = "achrdg12", cov.reduce = quantile)

# Test whether the differences are significant
contrast(gap, method = "pairwise")
```
 

**Section \@ref(two-continuous-predictors-6)**

* To check your understanding of centering with two continuous predictors, please provide an interpretation of all four regression coefficients in the centered model (below). Your interpretations should make reference to the situation where one or both predictors are equal to zero (see Equation \@ref(eq:simple)) and should also mentioned the interpretation of the value of zero for the centered variables.
 
```{r}
mod8 <- lm(achmat12 ~ achrdg12_dev*ses_dev)
summary(mod8)
```

**Section \@ref(two-categorical-predictors-6)**

* Please take a moment to write down your interpretation of the  figure below, focussing on how it illustrates an interaction between SES and Pre-K. Additionally, please describe how the figure would be different if there was no interaction between Pre-K and SES.

```{r}
load("ECLS2577.Rdata")
ecls$prek <- factor(2 - ecls$p1center)
ecls$wksesq5 <- factor(ecls$wksesq5)
mod <- lm(c1rmscal ~ prek*wksesq5, data = ecls)
visreg::visreg(mod, xvar = "wksesq5", by = "prek", 
               partial = F, rug = F, overlay = T, 
               strip.names = T, xlab = "SES", 
               ylab = "Math Achievement in K")
```


 * Please write down an interpretation of magnitude, direction, and statistical significance of each regression coefficient in this output (including the intercept), and be prepared to share your answers in class. Remember that `wksesq5` is the variable code for the SES quintiles -- the digit that follows the variable code indicates the level of variable. 
  
```{r}
summary(mod)
```

* Please write down your interpretation of the ANOVA table and R-squared (eta-squared) coefficients reported below, and be prepared to share you thoughts in class.** .

**Please write down your interpretation of the ANOVA table and R-squared (eta-squared) coefficients and be prepared to share you thoughts in class.** 

```{r}
anova(mod)
effectsize::eta_squared(mod, partial = F)
```

## Exercises {#exercises-6}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

These exercises provide an overview of how to add interactions using the `lm` function, how to center continuous predictors, and how to follow-up significant interactions with the `emmeans` package. 

### Binary + continuous + interaction

There are multiple ways of implementing interactions in R. 

  * We can "hard code" new variables into our data (e.g., the product of a binary gender variable and reading)
  
  * We can use R's formula notation for single term interactions (`:`)
  
  * We can use R's formula notation for factorial interactions (`*`)
  
The following code illustrates the three approaches and shows that they all producing the same output. In general, the `*` syntax is the easiest to use, so we will stick with that one going forward. The variables used in the example are from the NELS data: 

* `achmat12` is Mat Achievement (percent correct on a mat test) in grade 12. 
* `achrdg12` is Reading Achievement (percent correct on a reading test) in grade 12. 
* `gender` is dichotomous encoding of gender with values `Male` and `Female` (it is not a binary variable, but a factor, as discussed in Section \@ref(exercises-5)). 

```{r}
# Interaction via hard coding
genderXreading <- (as.numeric(gender) - 1) * achrdg12
mod1 <- lm(achmat12 ~ achrdg12 + gender + genderXreading)
summary(mod1)

# Interaction via `:` operator
mod2 <- lm(achmat12 ~ achrdg12 + gender + achrdg12:gender)
summary(mod2)

# Interaction via `*` operator
mod3 <- lm(achmat12 ~ achrdg12*gender)
summary(mod3)
```

Before moving on, check your interpretation of the coefficients in the models. In particular, what does the regression coefficient on the interaction term mean?

### Centering continuous predictors

As noted in Section \@ref(binary-continuous-interaction-6), the regression coefficient on Gender is not very interpretable when there is an interaction in the model. In the above output, the coefficient on gender tells us the gender gap in Math Achievement when `achrdg12 = 0`. We can fix this issue by re-scaling `achrdg12` so that zero has a meaningful value. One easy and widely used approach is to center `achrdg12` at its mean. When a variable is centered at its mean it is called a deviation score.

Let's see what happens when we use deviation scores `achrdg12` instead of the "raw" score

```{r}
# Re-run the model with reading centered at its mean
achrdg12_dev <- achrdg12 - mean(achrdg12)
mod4 <- lm(achmat12 ~ achrdg12_dev*gender)
summary(mod4)
```
  
Note that the intercept and the regression coefficient on gender have changed values compared to `mod3`. What is the interpretation of these coefficients in the new model? Also note the coefficient on reading and the interaction don't change. We showed why in section \@ref(how-centering-works-6).

Next, let's plot our model with the interaction term.  One advantage of having everything in a single model is that we can level-up our plotting! The following code uses the `visreg` package.  Note that the error bands in the plot are produced using the standard errors from `emmeans`, which is discussed in the following section. If you want to know more about how visreg works, type `help(visreg)`.

```{r}
# Install the package if you haven't already done so
# install.packages("visreg")

# Load the package into memory
library(visreg)

visreg(mod3, xvar = "achrdg12", by = "gender", overlay = TRUE)
```


### Breaking down a significant interaction

If an interaction is significant, then we usually want to report a bit more information about how the focal relationship changes as a function of the moderators. There are two main ways to do this: 

* Marginal effects (aka marginal means, least squares means, adjusted means): This approach is used when the focal predictor is categorical and we want to compare means across the categories, as a function of the moderator. 

* Simple trends (aka simple slopes): This approach is used when the focal predictor is continuous and we want to examine the slopes of the simple trends as a function of the moderator. 

Usually, the researcher will chose one or the other approach, whichever is best suited to address the research questions of interest. Our example was motivated by consideration of the gender gap in STEM (i.e., the relationship between a STEM and a categorical predictor), so the marginal effects approach is better suited. We will also illustrate simple trends, just to show how that approach works. 

### Marginal effects

Let's breakdown the interaction by asking how the relationship between Math and Gender (i.e., the gender achievement gap in Math) changes as a function of Reading. This can be done using `emmeans` package, and the main function in that pacakge  is also called `emmeans`.

The three main arguments for the `emmeans` function: 

  * `object` -- the output of `lm`. This is the first argument
  * `specs` -- which factors in the model we want the means of (i.e., the focal predictor)
  * `by` -- which predictor(s) we want to use to breakdown the means (i.e., the moderator(s))
  
We can use `emmeans` to compute the marginal effect at the mean (MEM) as follows: 

```{r}
# Install the package if you haven't already done so
  # install.packages("emmeans")

# Load the package into memory
library(emmeans)

# Use the emmeans function to get the gender means on math, broken down by reading
gap <- emmeans(mod3, specs = "gender", by = "achrdg12")
summary(gap)

# Test whether the difference is significant
contrast(gap, method = "pairwise")
```

In the above output, we only get one Gender difference in Math, and that is computed for the value of `achrdg12 = 55.6`, which is the mean value of Reading. As noted, this is called the marginal effect at the mean (MEM).   

It is often more helpful to report Gender difference for multiple different values of `achrdg12`, which is called MERV (marginal effects at representative values). While there are many ways to  chose the representative values, one convenient approach approach is to use the quartiles of `achrdg12`. This is accomplished using the `cov.reduce` argument of `emmeans` as follows.   

```{r}
# Use the the covarate reduce option of emmeans with the quantile function
gap_quartiles <- emmeans(mod3, specs = "gender", by = "achrdg12", cov.reduce = quantile)
summary(gap_quartiles)

# Test whether the gender difference in math achievement is significant at each quartile of reading achievement
contrast(gap_quartiles, method = "pairwise")
```

At this point, you should be able to summarize your conclusions about the gender gap in Math and how it depends on Reading.  

### Simple trends

Next we will show how to use  `emtrends` to test the conditional or "simple" slopes of Math on Reading, given Gender. As mentioned, this approach is not very well suited to the example, but we are going through it here just to illustrate how to do this type of analysis. 

The three main arguments for `emtrends` are

  * `object` -- the output of `lm`. This is the first argument
  * `var` -- which continuous predictor in the model we want the slopes of
  * `specs` -- which factor predictor(s)  in the model to break the trend down by

Let's see how it works.

```{r}
# Use the emtrends function to get the regression coefficients on reading, broken down by gender
simple_slopes <- emtrends(mod3, var = "achrdg12", specs = "gender")
summary(simple_slopes)
test(simple_slopes)
```

The foregoing analysis tells us how the relationship between reading and math changes as a function of gender, and, in particular, whether the simple slopes are significant for males and females. Recall that the simple slope for females (the group coded zero) is just the regression coefficient on reading in the original `lm` output. So, the only new thing this output gives us is the simple slope for males. 

### Two continuous predictors

Interactions with continuous predictors are basically the same as for continuous and categorical. One main issue is that we should always center the predictors, not only to facilitate interpretation of the regression coefficients, but also to reduce the correlation between the main effects and the interaction. 

For an example, let's replace gender with SES from our previous analysis. Apologies that this new example is mainly for convenience and doesn't represent a great research question about, e.g., about why the relationships between math and reading might change as a function of SES!

Here we will focus on how centering affects the results of a regression with interactions among continuous predictors.

```{r}
# Without centering
mod5 <- lm(achmat12 ~ achrdg12*ses)
summary(mod1)

# With centering
achrdg12_dev <- achrdg12 - mean(achrdg12)
ses_dev <- ses - mean(ses)
mod6 <- lm(achmat12 ~ achrdg12_dev*ses_dev)
summary(mod2)
```

We can see that, while both models account for the same overall variation in math, SES is significant in the centered model. This has to do both with changing the interpretation of the coefficient (it now represents the relationship between math and reading for students with average reading) and because it is no longer so highly redundant with the interaction term. 

Although the interaction with SES was not significant in either model, let's break down the interaction with `emtrends` just to see how it works. This time we will use the `at` option rather than the 'cov.reduce` option to break down the interaction. The values 9, 19, and 28 are the 10th, 50th, and 90th percentile of SES, which is the same approach `visreg` uses (You can overwrite the defaults using the `breaks` argument -- see `help(visreg)`).

```{r}
# Break down interaction with SES as moderator
simple_slopes <-emtrends(mod5, var = "achrdg12", specs = "ses", at = list(ses = c(9, 19, 28)))
summary(simple_slopes)
```

Finally let's summarize our (non significant) interaction with a nice plot. 

```{r}
# Note that band = F removes the confidence intervals
visreg(mod5, xvar = "achrdg12", by = "ses", overlay = TRUE, band = F)
```

### Two categorical predictors

For this topic we will switch over to the ECLS data and examine how SES and Pre-K attendance interact to predict Math Achievement at the beginning of Kindergarten. The variables we will examine are

* Math Achievement at the beginning of K (`c1rmscal`). This is the number of correct questions on a test with approximately 70 items. 
* Whether the child attended Pre-K (`p1center`). This is a binary variable that indicates pre-K attendance. 
* SES, coded as quintiles (`wksesq5`). We will denote this variable as SES, but keep in mind it is quintiles in this example (e.g., SES = 1 are the respondents with SES between the minimum and the first quintile). 

The regression model is as follows. Note that both variables need to be converted to factors in R, so that R will treat them as categorical variables. Also recall that in R the default contrast coding for categorical predictors is reference-group coding. 

```{r}
load("ECLS2577.Rdata")
ecls$prek <- factor(2 - ecls$p1center)
ecls$wksesq5 <- factor(ecls$wksesq5)
mod <- lm(c1rmscal ~ prek*wksesq5, data = ecls)
summary(mod)
```

To facilitate interpretation of the ouput, you can refer to the plot below. Each regression coefficient in the output corresponds to a feature of this plot. 

```{r}
visreg::visreg(mod, xvar = "wksesq5", by = "prek", 
               partial = F, rug = F, overlay = T, 
               strip.names = T, xlab = "SES", 
               ylab = "Math Achievement in K")
```

In order to summarize the model as an ANOVA table, we can use the following code. Note that the ANOVA output tests the variance explained (i.e., R-squared) of the original variables, and does not include dummy variables. 

```{r}
anova(mod)
```

In an ANOVA context, the R-squared statistics are called  eta-squared. They are reported below: 

```{r}
effectsize::eta_squared(mod, partial = F)
```

```{r, echo = F}
detach(NELS)
```

